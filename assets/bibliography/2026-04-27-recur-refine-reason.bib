@inproceedings{saunshi2024inductive,
  title={On the Inductive Bias of Stacking Towards Improving Reasoning},
  author={Saunshi, Nikunj and Karp, Stefani and Krishnan, Shankar and Miryoosefi, Sobhan and Jakkam Reddi, Sashank and Kumar, Sanjiv},
  booktitle={Neural Information Processing Systems},
  year={2024}
}

@inproceedings{bae2025mixture,
  title={Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation},
  author={Bae, Sangmin and Kim, Yujin and Bayat, Reza and Kim, Sungnyun and Ha, Jiyoun and Schuster, Tal and Fisch, Adam and Harutyunyan, Hrayr and Ji, Ziwei and Courville, Aaron and others},
  booktitle={Neural Information Processing Systems},
  year={2025},
}

@inproceedings{schuster2022confident,
  title={Confident Adaptive Language Modeling},
  author={Schuster, Tal and Fisch, Adam and Gupta, Jai and Dehghani, Mostafa and Bahri, Dara and Tran, Vinh and Tay, Yi and Metzler, Donald},
  booktitle={Neural Information Processing Systems},
  year={2022}
}

@inproceedings{lan2020albert,
  title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{bae2023fast,
  title={Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding},
  author={Bae, Sangmin and Ko, Jongwoo and Song, Hwanjun and Yun, Se-Young},
  booktitle={Empirical Methods in Natural Language Processing},
  year={2023}
}

@article{glorioso2024zamba,
  title={Zamba: A Compact 7B SSM Hybrid Model},
  author={Glorioso, Paolo and Anthony, Quentin and Tokpanov, Yury and Whittington, James and Pilault, Jonathan and Ibrahim, Adam and Millidge, Beren},
  journal={arXiv preprint arXiv:2405.16712},
  year={2024}
}

@article{karp2024landscape,
  title={Landscape-Aware Growing: The Power of a Little LAG},
  author={Karp, Stefani and Saunshi, Nikunj and Miryoosefi, Sobhan and Reddi, Sashank J and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2406.02469},
  year={2024}
}

@inproceedings{shazeer2017outrageously,
  title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@article{fukushima1988neocognitron,
  title={Neocognitron: A hierarchical neural network capable of visual pattern recognition},
  author={Fukushima, Kunihiko},
  journal={Neural networks},
  volume={1},
  number={2},
  pages={119--130},
  year={1988},
  publisher={Elsevier}
}

@inproceedings{goyal2024think,
  title={Think Before You Speak: Training Language Models With Pause Tokens},
  author={Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv and Nagarajan, Vaishnavh},
  booktitle={International Conference on Learning Representations},
  year	= {2024}
}

@article{hwang2025dynamic,
  title={Dynamic Chunking for End-to-End Hierarchical Sequence Modeling},
  author={Hwang, Sukjun and Wang, Brandon and Gu, Albert},
  journal={arXiv preprint arXiv:2507.07955},
  year={2025}
}

@incollection{almeida1988backpropagation,
  title={Backpropagation in perceptrons with feedback},
  author={Almeida, Lu{\'\i}s B},
  booktitle={Neural computers},
  pages={199--208},
  year={1988},
  publisher={Springer}
}

@article{pineda1989recurrent,
  title={Recurrent backpropagation and the dynamical approach to adaptive neural computation},
  author={Pineda, Fernando J},
  journal={Neural Computation},
  volume={1},
  number={2},
  pages={161--172},
  year={1989},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}
@article{ho2024block,
  title={Block Transformer: Global-to-Local Language Modeling for Fast Inference},
  author={Ho, Namgyu and Bae, Sangmin and Kim, Taehyeon and Jo, Hyunjik and Kim, Yireun and Schuster, Tal and Fisch, Adam and Thorne, James and Yun, Se-Young},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={48740--48783},
  year={2024}
}

@inproceedings{dabre2019recurrent,
  title={Recurrent stacking of layers for compact neural machine translation models},
  author={Dabre, Raj and Fujita, Atsushi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={6292--6299},
  year={2019}
}

@article{bhirangi2024hierarchical,
  title={Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling},
  author={Bhirangi, Raunaq and Wang, Chenyu and Pattabiraman, Venkatesh and Majidi, Carmel and Gupta, Abhinav and Hellebrekers, Tess and Pinto, Lerrel},
  journal={arXiv preprint arXiv:2402.10211},
  year={2024}
}

@article{bronstein2021geometric,
  title={Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges},
  author={Bronstein, Michael M and Bruna, Joan and Cohen, Taco and Veli{\v{c}}kovi{\'c}, Petar},
  journal={arXiv preprint arXiv:2104.13478},
  year={2021}
}

@article{lecun2022path,
  title={A Path Towards Autonomous Machine Intelligence version 0.9. 2, 2022-06-27},
  author={LeCun, Yann},
  journal={Open Review},
  volume={62},
  number={1},
  pages={1--62},
  year={2022}
}

@article{lecun1989backpropagation,
  title={Backpropagation Applied to Handwritten Zip Code Recognition},
  author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  journal={Neural computation},
  volume={1},
  number={4},
  pages={541--551},
  year={1989},
  publisher={MIT Press}
}

@article{kaplan2020scaling,
  title={Scaling Laws for Neural Language Models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{bai2019deep,
  title={Deep Equilibrium Mdels},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  booktitle={Neural Information Processing Systems},
  year={2019}
}

@article{hao2024training,
  title={Training Large Language Models to Reason in a Continuous Latent Space},
  author={Hao, Shibo and Sukhbaatar, Sainbayar and Su, DiJia and Li, Xian and Hu, Zhiting and Weston, Jason and Tian, Yuandong},
  journal={arXiv preprint arXiv:2412.06769},
  year={2024}
}

@article{jolicoeur2025less,
  title={Less is More: Recursive Reasoning with Tiny Networks},
  author={Jolicoeur-Martineau, Alexia},
  journal={arXiv preprint arXiv:2510.04871},
  year={2025}
}

@inproceedings{wei2022chain,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  booktitle={Neural Information Processing Systems},
  year={2022}
}

@article{wang2025hierarchical,
  title={Hierarchical Reasoning Model}, 
  author={Guan Wang and Jin Li and Yuhao Sun and Xing Chen and Changling Liu and Yue Wu and Meng Lu and Sen Song and Yasin Abbasi Yadkori},
  year={2025},
  journal={arXiv preprint arXiv:2506.21734},
}

@article{alabdulmohsin2025recursive,
  title={Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems},
  author={Alabdulmohsin, Ibrahim and Zhai, Xiaohua},
  journal={arXiv preprint arXiv:2502.07503},
  year={2025}
}

@article{mcleish2025teaching,
  title={Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence},
  author={McLeish, Sean and Li, Ang and Kirchenbauer, John and Kalra, Dayal Singh and Bartoldson, Brian R and Kailkhura, Bhavya and Schwarzschild, Avi and Geiping, Jonas and Goldstein, Tom and Goldblum, Micah},
  journal={arXiv preprint arXiv:2511.07384},
  year={2025}
}

@article{geiping2025efficient,
  title={Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models},
  author={Geiping, Jonas and Yang, Xinyu and Su, Guinan},
  journal={arXiv preprint arXiv:2510.14961},
  year={2025}
}

@inproceedings{dehghani2019universal,
  title={Universal Transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, Lukasz},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{bae2025relaxed,
  title={Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA},
  author={Bae, Sangmin and Fisch, Adam and Harutyunyan, Hrayr and Ji, Ziwei and Kim, Seungyeon and Schuster, Tal},
  booktitle={International Conference on Learning Representations},
  year={2025}
}



@article{geiping2025scaling,
  title={Scaling Up Test-time Compute with Latent Reasoning: A Recurrent Depth Approach},
  author={Geiping, Jonas and McLeish, Sean and Jain, Neel and Kirchenbauer, John and Singh, Siddharth and Bartoldson, Brian R and Kailkhura, Bhavya and Bhatele, Abhinav and Goldstein, Tom},
  journal={arXiv preprint arXiv:2502.05171},
  year={2025}
}


@article{chen2025inner,
  title={Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking},
  author={Chen, Yilong and Shang, Junyuan and Zhang, Zhenyu and Xie, Yanxi and Sheng, Jiawei and Liu, Tingwen and Wang, Shuohuan and Sun, Yu and Wu, Hua and Wang, Haifeng},
  journal={arXiv preprint arXiv:2502.13842},
  year={2025}
}

@article{wang2023learning,
  title={Learning to grow pretrained models for efficient transformer training},
  author={Wang, Peihao and Panda, Rameswar and Hennigen, Lucas Torroba and Greengard, Philip and Karlinsky, Leonid and Feris, Rogerio and Cox, David Daniel and Wang, Zhangyang and Kim, Yoon},
  journal={arXiv preprint arXiv:2303.00980},
  year={2023}
}

@article{gesmundo2023composable,
  title={Composable Function-Preserving Expansions for Transformer Architectures},
  author={Gesmundo, Andrea and Maile, Kaitlin},
  journal={arXiv preprint arXiv:2308.06103},
  year={2023}
}


@article{mathur2025change,
  title={Change of Thought: Adaptive Test-Time Computation},
  author={Mathur, Mrinal and Doan, Mike and Pearlmutter, Barak and Plis, Sergey},
  journal={arXiv preprint arXiv:2507.13569},
  year={2025}
}


@article{xin2020deebert,
  title={{DeeBERT}: Dynamic early exiting for accelerating BERT inference},
  author={Xin, Ji and Tang, Raphael and Lee, Jaejun and Yu, Yaoliang and Lin, Jimmy},
  journal={arXiv preprint arXiv:2004.12993},
  year={2020}
}

@inproceedings{gong2019efficient,
  title={Efficient training of {BERT} by progressively stacking},
  author={Gong, Linyuan and He, Di and Li, Zhuohan and Qin, Tao and Wang, Liwei and Liu, Tieyan},
  booktitle={International conference on machine learning},
  year={2019},
}

@article{kim2023solar,
  title={Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling},
  author={Kim, Dahyun and Park, Chanjun and Kim, Sanghoon and Lee, Wonsung and Song, Wonho and Kim, Yunsu and Kim, Hyeonwoo and Kim, Yungi and Lee, Hyeonju and Kim, Jihoo and others},
  journal={arXiv preprint arXiv:2312.15166},
  year={2023}
}


@article{gromov2024unreasonable,
  title={The unreasonable ineffectiveness of the deeper layers},
  author={Gromov, Andrey and Tirumala, Kushal and Shapourian, Hassan and Glorioso, Paolo and Roberts, Daniel A},
  journal={arXiv preprint arXiv:2403.17887},
  year={2024}
}


@article{yang2023looped,
  title={Looped transformers are better at learning learning algorithms},
  author={Yang, Liu and Lee, Kangwook and Nowak, Robert and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2311.12424},
  year={2023}
}

@inproceedings{saunshi2025reasoning,
  title={Reasoning with Latent Thoughts: On the Power of Looped Transformers},
  author={Saunshi, Nikunj and Dikkala, Nishanth and Li, Zhiyuan and Kumar, Sanjiv and Reddi, Sashank J},
  booktitle={International Conference on Learning Representations},
  year={2025}
}

@inproceedings{gu2022efficiently,
  title={Efficiently Modeling Long Sequences with Structured State Spaces},
  author={Gu, Albert and Goel, Karan and R\'e, Christopher},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{du2024stacking,
  title={Stacking your transformers: A closer look at model growth for efficient {LLM} pre-training},
  author={Du, Wenyu and Luo, Tongxu and Qiu, Zihan and Huang, Zeyu and Shen, Yikang and Cheng, Reynold and Guo, Yike and Fu, Jie},
  booktitle={Neural Information Processing Systems},
  year={2024}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{csordas2025language,
  title={Do Language Models Use Their Depth Efficiently?},
  author={Csord{\'a}s, R{\'o}bert and Manning, Christopher D and Potts, Christopher},
  journal={arXiv preprint arXiv:2505.13898},
  year={2025}
}

@article{vaswani2017attention,
  title={Attention is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Neural Information Processing Systems},
  year={2017}
}

@article{wang20251000,
  title={1000 Layer Networks for Self-Supervised {RL}: Scaling Depth Can Enable New Goal-Reaching Capabilities},
  author={Wang, Kevin and Javali, Ishaan and Bortkiewicz, Micha{\'L} and Eysenbach, Benjamin and others},
  journal={arXiv preprint arXiv:2503.14858},
  year={2025}
}

@article{devvrit2024matformer,
  title={Matformer: Nested transformer for elastic inference},
  author={Devvrit, Fnu and Kudugunta, Sneha and Kusupati, Aditya and Dettmers, Tim and Chen, Kaifeng and Dhillon, Inderjit and Tsvetkov, Yulia and Hajishirzi, Hanna and Kakade, Sham and Farhadi, Ali and others},
  journal={Neural Information Processing Systems},
  year={2024}
}

@article{csordas2024moeut,
  title={{MOEUT}: Mixture-of-experts universal transformers},
  author={Csord{\'a}s, R{\'o}bert and Irie, Kazuki and Schmidhuber, J{\"u}rgen and Potts, Christopher and Manning, Christopher D},
  journal={Neural Information Processing Systems},
  year={2024}
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{lindsey2025biology,
  author={Lindsey, Jack and Gurnee, Wes and Ameisen, Emmanuel and Chen, Brian and Pearce, Adam and Turner, Nicholas L. and Citro, Craig and Abrahams, David and Carter, Shan and Hosmer, Basil and Marcus, Jonathan and Sklar, Michael and Templeton, Adly and Bricken, Trenton and McDougall, Callum and Cunningham, Hoagy and Henighan, Thomas and Jermyn, Adam and Jones, Andy and Persic, Andrew and Qi, Zhenyi and Thompson, T. Ben and Zimmerman, Sam and Rivoire, Kelley and Conerly, Thomas and Olah, Chris and Batson, Joshua},
  title={On the Biology of a Large Language Model},
  journal={Transformer Circuits Thread},
  year={2025},
  url={https://transformer-circuits.pub/2025/attribution-graphs/biology.html}
}


@inproceedings{giannou2023looped,
  title={Looped transformers as programmable computers},
  author={Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D and Papailiopoulos, Dimitris},
  booktitle={International Conference on Machine Learning},
  year={2023},
}

@article{sun2025curse,
  title={The curse of depth in large language models},
  author={Sun, Wenfang and Song, Xinyuan and Li, Pengxiang and Yin, Lu and Zheng, Yefeng and Liu, Shiwei},
  journal={arXiv preprint arXiv:2502.05795},
  year={2025}
}

@article{yin2023outlier,
  title={Outlier weighed layerwise sparsity ({OWL}): A missing secret sauce for pruning llms to high sparsity},
  author={Yin, Lu and Wu, You and Zhang, Zhenyu and Hsieh, Cheng-Yu and Wang, Yaqing and Jia, Yiling and Li, Gen and Jaiswal, Ajay and Pechenizkiy, Mykola and Liang, Yi and others},
  journal={arXiv preprint arXiv:2310.05175},
  year={2023}
}

@article{petty2023impact,
  title={The impact of depth and width on transformer language model generalization},
  author={Petty, Jackson and van Steenkiste, Sjoerd and Dasgupta, Ishita and Sha, Fei and Garrette, Dan and Linzen, Tal},
  journal={CoRR},
  year={2023}
}

@article{men2024shortgpt,
  title={Short{GPT}: Layers in large language models are more redundant than you expect},
  author={Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng},
  journal={arXiv preprint arXiv:2403.03853},
  year={2024}
}

@article{lad2024remarkable,
  title={The remarkable robustness of {LLM}s: Stages of inference?},
  author={Lad, Vedang and Lee, Jin Hwa and Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2406.19384},
  year={2024}
}

@article{hagele2024scaling,
  title={Scaling laws and compute-optimal training beyond fixed training durations},
  author={H{\"a}gele, Alex and Bakouch, Elie and Kosson, Atli and Von Werra, Leandro and Jaggi, Martin and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={76232--76264},
  year={2024}
}
