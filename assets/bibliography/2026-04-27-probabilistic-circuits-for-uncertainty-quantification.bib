@online{bytez.com_nonasymptotic_2024,
  title = {Non-Asymptotic Approximation Error Bounds of Parameterized Quantum Circuits},
  author = {Bytez.com and Yu, Zhan and Chen, Qiuhao and Jiao, Yuling and Li, Yinan and Lu, Xiliang and Wang, Xin and Yang, Jerry Zhijian},
  date = {2024-12-10T06:00:00},
  url = {https://bytez.com/docs/neurips/94787/paper},
  urldate = {2025-11-22},
  abstract = {This paper explores how quantum circuits, called parameterized quantum circuits (PQC), can be used to learn and understand complex functions, which is important for tasks like machine learning. Th...},
  langid = {english}
}

@article{choi_probabilistic_2020,
  title = {Probabilistic Circuits: Representation and Inference},
  author = {Choi, YooJung and Vergari, Antonio},
  date = {2020},
  abstract = {These lecture notes accompany the AAAI 2020 tutorial on probabilistic circuits, by Antonio Vergari, Robert Peharz, YooJung Choi, and Guy Van den Broeck. They cover the first half of the tutorial, that is, the motivation of tractable models, the probabilistic circuit representation, and its inference algorithms.},
  langid = {english}
}

@article{choi_probabilistic_2020a,
  title = {Probabilistic Circuits: A Unifying Framework for Tractable Probabilistic Models},
  author = {Choi, YooJung and Vergari, Antonio},
  date = {2020},
  url = {https://starai.cs.ucla.edu/papers/ProbCirc20.pdf},
  langid = {english}
}

@online{gal_dropout_2016,
  title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  shorttitle = {Dropout as a Bayesian Approximation},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  date = {2016-10-04},
  eprint = {1506.02142},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1506.02142},
  url = {http://arxiv.org/abs/1506.02142},
  urldate = {2025-12-05},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  pubstate = {prepublished}
}

@online{grivas_fast_2025,
  title = {Fast and Expressive Multi-Token Prediction with Probabilistic Circuits},
  author = {Grivas, Andreas and Loconte, Lorenzo and van Krieken, Emile and Nawrot, Piotr and Zhao, Yu and Wielewski, Euan and Minervini, Pasquale and Ponti, Edoardo and Vergari, Antonio},
  options = {useprefix=true},
  date = {2025-11-14},
  eprint = {2511.11346},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2511.11346},
  url = {http://arxiv.org/abs/2511.11346},
  urldate = {2025-11-22},
  abstract = {Multi-token prediction (MTP) is a prominent strategy to significantly speed up generation in large language models (LLMs), including byte-level LLMs, which are tokeniser-free but prohibitively slow. However, existing MTP methods often sacrifice expressiveness by assuming independence between future tokens. In this work, we investigate the trade-off between expressiveness and latency in MTP within the framework of probabilistic circuits (PCs). Our framework, named MTPC, allows one to explore different ways to encode the joint distributions over future tokens by selecting different circuit architectures, generalising classical models such as (hierarchical) mixture models, hidden Markov models and tensor networks. We show the efficacy of MTPC by retrofitting existing byte-level LLMs, such as EvaByte. Our experiments show that, when combined with speculative decoding, MTPC significantly speeds up generation compared to MTP with independence assumptions, while guaranteeing to retain the performance of the original verifier LLM. We also rigorously study the optimal trade-off between expressiveness and latency when exploring the possible parameterisations of MTPC, such as PC architectures and partial layer sharing between the verifier and draft LLMs.},
  pubstate = {prepublished}
}

@article{kimpton_challenges_2025,
  title = {Challenges and Opportunities in Uncertainty Quantification for Healthcare and Biological Systems},
  author = {Kimpton, Louise M. and Paun, L. Mihaela and Colebank, Mitchel J. and Volodina, Victoria},
  date = {2025-03-13},
  journaltitle = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {383},
  number = {2292},
  pages = {20240232},
  publisher = {Royal Society},
  doi = {10.1098/rsta.2024.0232},
  url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2024.0232},
  urldate = {2025-11-22},
  abstract = {Uncertainty quantification (UQ) is an essential aspect of computational modelling and statistical prediction. Multiple applications, including geophysics, climate science and aerospace engineering, incorporate UQ in the development and translation of new technologies. In contrast, the application of UQ to biological and healthcare models is understudied and suffers from several critical knowledge gaps. In an era of personalized medicine, patient-specific modelling, and digital twins, a lack of UQ understanding and appropriate implementation of UQ methodology limits the success of modelling and simulation in a clinical setting. The main contribution of our review article is to emphasize the importance and current deficiencies of UQ in the development of computational frameworks for healthcare and biological systems. As the introduction to the special issue on this topic, we provide an overview of UQ methodologies, their applications in non-biological and biological systems and the current gaps and opportunities for UQ development, as later highlighted by authors publishing in the special issue.This article is part of the theme issue ‘Uncertainty quantification for healthcare and biological systems (Part 1)’.}
}

@online{li_quantum_2021,
  title = {Quantum Generative Models for Small Molecule Drug Discovery},
  author = {Li, Junde and Topaloglu, Rasit and Ghosh, Swaroop},
  date = {2021-01-09},
  eprint = {2101.03438},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2101.03438},
  url = {http://arxiv.org/abs/2101.03438},
  urldate = {2025-11-22},
  abstract = {Existing drug discovery pipelines take 5-10 years and cost billions of dollars. Computational approaches aim to sample from regions of the whole molecular and solid-state compounds called chemical space which could be on the order of 1060 . Deep generative models can model the underlying probability distribution of both the physical structures and property of drugs and relate them nonlinearly. By exploiting patterns in massive datasets, these models can distill salient features that characterize the molecules. Generative Adversarial Networks (GANs) discover drug candidates by generating molecular structures that obey chemical and physical properties and show affinity towards binding with the receptor for a target disease. However, classical GANs cannot explore certain regions of the chemical space and suffer from curse-of-dimensionality. A full quantum GAN may require more than 90 qubits even to generate QM9-like small molecules. We propose a qubit-efficient quantum GAN with a hybrid generator (QGAN-HG) to learn richer representation of molecules via searching exponentially large chemical space with few qubits more efficiently than classical GAN. The QGANHG model is composed of a hybrid quantum generator that supports various number of qubits and quantum circuit layers, and, a classical discriminator. QGAN-HG with only 14.93\% retained parameters can learn molecular distribution as efficiently as classical counterpart. The QGAN-HG variation with patched circuits considerably accelerates our standard QGANHG training process and avoids potential gradient vanishing issue of deep neural networks. Code is available on GitHub https://github.com/jundeli/quantum-gan.},
  pubstate = {prepublished}
}

@article{liu_fast_2024,
  title = {Fast Inverse Design of Microwave and Infrared Bi-Stealth Metamaterials Based on Equivalent Circuit Model},
  author = {Liu, Shiju and Zhu, Fengjie and Huang, Jianguang and Zhao, Hua and Han, Mengqi and Fan, Kebin and Chen, Ping},
  date = {2024-09-20},
  journaltitle = {Journal of Applied Physics},
  shortjournal = {J. Appl. Phys.},
  volume = {136},
  number = {11},
  pages = {113106},
  issn = {0021-8979},
  doi = {10.1063/5.0222949},
  url = {https://doi.org/10.1063/5.0222949},
  urldate = {2025-11-22},
  abstract = {This work proposed a fast inverse design method for microwave and infrared (IR) bi-stealth metamaterials based on the equivalent circuit model (ECM). Using this method, we designed a microwave and IR bi-stealth metamaterial by deploying a multilayered structure of the indium tin oxide (ITO) film based metasurface. First, the IR emissivity of the ITO film was calculated in the framework of the ECM. Then, an ITO metasurface was proposed to implement low IR emission and high microwave transmission simultaneously. Based on the ECM of the square patch, the ECM of the whole metamaterial was established at the microwave band. An inverse design program was built by incorporating the ECM with genetic algorithm (GA). Structure parameters of the metamaterial were optimized by GA to achieve the broadest microwave stealth bandwidth for the given thickness. Finally, the sample of the optimized bi-stealth metamaterial was prepared and tested. The calculated, simulated, and measured results are in good agreement, showing that such a metamaterial has an IR emissivity of 0.18 in the band from 3 to 14\,μm and an efficient microwave stealth band from 4.8 to 17\,GHz with a thickness of 4.9\,mm. The proposed method will benefit the design and application of microwave and IR bi-stealth metamaterials.}
}

@online{liu_image_2024,
  title = {Image Inpainting via Tractable Steering of Diffusion Models},
  author = {Liu, Anji and Niepert, Mathias and Van den Broeck, Guy},
  date = {2024-12-11},
  eprint = {2401.03349},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.03349},
  url = {http://arxiv.org/abs/2401.03349},
  urldate = {2025-11-22},
  abstract = {Diffusion models are the current state of the art for generating photorealistic images. Controlling the sampling process for constrained image generation tasks such as inpainting, however, remains challenging since exact conditioning on such constraints is intractable. While existing methods use various techniques to approximate the constrained posterior, this paper proposes to exploit the ability of Tractable Probabilistic Models (TPMs) to exactly and efficiently compute the constrained posterior, and to leverage this signal to steer the denoising process of diffusion models. Specifically, this paper adopts a class of expressive TPMs termed Probabilistic Circuits (PCs). Building upon prior advances, we further scale up PCs and make them capable of guiding the image generation process of diffusion models. Empirical results suggest that our approach can consistently improve the overall quality and semantic coherence of inpainted images across three natural image datasets (i.e., CelebA-HQ, ImageNet, and LSUN) with only ∼ 10\% additional computational overhead brought by the TPM. Further, with the help of an image encoder and decoder, our method can readily accept semantic constraints on specific regions of the image, which opens up the potential for more controlled image generation tasks. In addition to proposing a new framework for constrained image generation, this paper highlights the benefit of more tractable models and motivates the development of expressive TPMs.},
  langid = {english},
  pubstate = {prepublished}
}

@article{liu_scaling_,
  title = {Scaling Tractable Probabilistic Circuits: A Systems Perspective},
  author = {Liu, Anji and Ahmed, Kareem},
  abstract = {Probabilistic Circuits (PCs) are a general framework for tractable deep generative models, which support exact and efficient probabilistic inference on their learned distributions. Recent modeling and training advancements have enabled their application to complex real-world tasks. However, the time and memory inefficiency of existing PC implementations hinders further scaling up. This paper proposes PyJuice, a general GPU implementation design for PCs that improves prior art in several regards. Specifically, PyJuice is 1-2 orders of magnitude faster than existing systems (including very recent ones) at training large-scale PCs. Moreover, PyJuice consumes 2-5x less GPU memory, which enables us to train larger models. At the core of our system is a compilation process that converts a PC into a compact representation amenable to efficient block-based parallelization, which significantly reduces IO and makes it possible to leverage Tensor Cores available in modern GPUs. Empirically, PyJuice can be used to improve state-of-the-art PCs trained on image (e.g., ImageNet32) and language (e.g., WikiText, CommonGen) datasets. We further establish a new set of baselines on natural image and language datasets by benchmarking existing PC structures but with much larger sizes and more training epochs, with the hope of incentivizing future research. Code is available at https: //github.com/Tractables/pyjuice.},
  langid = {english}
}

@online{liu_scaling_2025,
  title = {Scaling Tractable Probabilistic Circuits: A Systems Perspective},
  shorttitle = {Scaling Tractable Probabilistic Circuits},
  author = {Liu, Anji and Ahmed, Kareem and Van den Broeck, Guy},
  date = {2025-10-30},
  eprint = {2406.00766},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2406.00766},
  url = {http://arxiv.org/abs/2406.00766},
  urldate = {2025-12-06},
  abstract = {Probabilistic Circuits (PCs) are a general framework for tractable deep generative models, which support exact and efficient probabilistic inference on their learned distributions. Recent modeling and training advancements have enabled their application to complex real-world tasks. However, the time and memory inefficiency of existing PC implementations hinders further scaling up. This paper proposes PyJuice, a general GPU implementation design for PCs that improves prior art in several regards. Specifically, PyJuice is 1-2 orders of magnitude faster than existing systems (including very recent ones) at training large-scale PCs. Moreover, PyJuice consumes 2-5x less GPU memory, which enables us to train larger models. At the core of our system is a compilation process that converts a PC into a compact representation amenable to efficient block-based parallelization, which significantly reduces IO and makes it possible to leverage Tensor Cores available in modern GPUs. Empirically, PyJuice can be used to improve state-of-the-art PCs trained on image (e.g., ImageNet32) and language (e.g., WikiText, CommonGen) datasets. We further establish a new set of baselines on natural image and language datasets by benchmarking existing PC structures but with much larger sizes and more training epochs, with the hope of incentivizing future research. Code is available at https://github.com/Tractables/pyjuice.},
  pubstate = {prepublished}
}

@article{ma_probabilistic_2019,
  title = {Probabilistic Representation and Inverse Design of Metamaterials Based on a Deep Generative Model with Semi-Supervised Learning Strategy},
  author = {Ma, Wei and Cheng, Feng and Xu, Yihao and Wen, Qinlong and Liu, Yongmin},
  date = {2019-08},
  journaltitle = {Advanced Materials (Deerfield Beach, Fla.)},
  shortjournal = {Adv Mater},
  volume = {31},
  number = {35},
  eprint = {31259443},
  eprinttype = {pubmed},
  pages = {e1901111},
  issn = {1521-4095},
  doi = {10.1002/adma.201901111},
  abstract = {The research of metamaterials has achieved enormous success in the manipulation of light in a prescribed manner using delicately designed subwavelength structures, so-called meta-atoms. Even though modern numerical methods allow for the accurate calculation of the optical response of complex structures, the inverse design of metamaterials, which aims to retrieve the optimal structure according to given requirements, is still a challenging task owing to the nonintuitive and nonunique relationship between physical structures and optical responses. To better unveil this implicit relationship and thus facilitate metamaterial designs, it is proposed to represent metamaterials and model the inverse design problem in a probabilistically generative manner, enabling to elegantly investigate the complex structure-performance relationship in an interpretable way, and solve the one-to-many mapping issue that is intractable in a deterministic model. Moreover, to alleviate the burden of numerical calculations when collecting data, a semisupervised learning strategy is developed that allows the model to utilize unlabeled data in addition to labeled data in an end-to-end training. On a data-driven basis, the proposed deep generative model can serve as a comprehensive and efficient tool that accelerates the design, characterization, and even new discovery in the research domain of metamaterials, and photonics in general.},
  langid = {english}
}

@article{manfredi_probabilistic_2023,
  title = {Probabilistic Uncertainty Quantification of Microwave Circuits Using Gaussian Processes},
  author = {Manfredi, Paolo},
  date = {2023-06},
  journaltitle = {IEEE Transactions on Microwave Theory and Techniques},
  volume = {71},
  number = {6},
  pages = {2360--2372},
  issn = {1557-9670},
  doi = {10.1109/TMTT.2022.3228953},
  url = {https://ieeexplore.ieee.org/document/9996177/},
  urldate = {2025-11-22},
  abstract = {In this article, a probabilistic machine learning framework based on Gaussian process regression (GPR) and principal component analysis (PCA) is proposed for the uncertainty quantification (UQ) of microwave circuits. As opposed to most surrogate modeling techniques, GPR models inherently carry information on the model prediction uncertainty due to unseen data. This article shows how the inherent uncertainty of GPR pointwise predictions can be combined with the uncertainty of the design parameters to provide global statistical information on the device performance with the inclusion of confidence bounds. The model confidence is possibly improved by increasing the amount of training data. In addition, PCA is employed to effectively deal with problems with multiple and possibly complex-valued output components, such as those involving the UQ of time-domain responses or multiport scattering parameters. The proposed technique is successfully applied to two low-noise amplifier designs subject to the process variation of up to 25 parameters. Comparisons against the state-of-the-art polynomial chaos expansion method demonstrates that GPR achieves superior accuracy, while additionally providing information on the prediction confidence.}
}

@online{martires_probabilistic_2024,
  title = {Probabilistic Neural Circuits},
  author = {Martires, Pedro Zuidberg Dos},
  date = {2024-03-10},
  eprint = {2403.06235},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.06235},
  url = {http://arxiv.org/abs/2403.06235},
  urldate = {2025-11-22},
  abstract = {Probabilistic circuits (PCs) have gained prominence in recent years as a versatile framework for discussing probabilistic models that support tractable queries and are yet expressive enough to model complex probability distributions. Nevertheless, tractability comes at a cost: PCs are less expressive than neural networks. In this paper we introduce probabilistic neural circuits (PNCs), which strike a balance between PCs and neural nets in terms of tractability and expressive power. Theoretically, we show that PNCs can be interpreted as deep mixtures of Bayesian networks. Experimentally, we demonstrate that PNCs constitute powerful function approximators.},
  pubstate = {prepublished}
}

@article{papamakarios_normalizing_2021,
  title = {Normalizing Flows for Probabilistic Modeling and Inference},
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  date = {2021},
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  langid = {english}
}

@online{paris_sumproduct_2020,
  title = {Sum-Product Networks: A Survey},
  shorttitle = {Sum-Product Networks},
  author = {París, Iago and Sánchez-Cauce, Raquel and Díez, Francisco Javier},
  date = {2020-04-02},
  eprint = {2004.01167},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2004.01167},
  url = {http://arxiv.org/abs/2004.01167},
  urldate = {2025-11-22},
  abstract = {A sum-product network (SPN) is a probabilistic model, based on a rooted acyclic directed graph, in which terminal nodes represent univariate probability distributions and non-terminal nodes represent convex combinations (weighted sums) and products of probability functions. They are closely related to probabilistic graphical models, in particular to Bayesian networks with multiple context-specific independencies. Their main advantage is the possibility of building tractable models from data, i.e., models that can perform several inference tasks in time proportional to the number of links in the graph. They are somewhat similar to neural networks and can address the same kinds of problems, such as image processing and natural language understanding. This paper offers a survey of SPNs, including their definition, the main algorithms for inference and learning from data, the main applications, a brief review of software libraries, and a comparison with related models},
  pubstate = {prepublished}
}

@inproceedings{peharz_einsum_2020,
  title = {Einsum Networks: Fast and Scalable Learning of Tractable Probabilistic Circuits},
  shorttitle = {Einsum Networks},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Peharz, Robert and Lang, Steven and Vergari, Antonio and Stelzner, Karl and Molina, Alejandro and Trapp, Martin and Van den Broeck, Guy and Kersting, Kristian and Ghahramani, Zoubin},
  date = {2020-11-21},
  pages = {7563--7574},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/peharz20a.html},
  urldate = {2025-11-22},
  abstract = {Probabilistic circuits (PCs) are a promising avenue for probabilistic modeling, as they permit a wide range of exact and efficient inference routines. Recent “deep-learning-style” implementations of PCs strive for a better scalability, but are still difficult to train on real-world data, due to their sparsely connected computational graphs. In this paper, we propose Einsum Networks (EiNets), a novel implementation design for PCs, improving prior art in several regards. At their core, EiNets combine a large number of arithmetic operations in a single monolithic einsum-operation, leading to speedups and memory savings of up to two orders of magnitude, in comparison to previous implementations. As an algorithmic contribution, we show that the implementation of Expectation-Maximization (EM) can be simplified for PCs, by leveraging automatic differentiation. Furthermore, we demonstrate that EiNets scale well to datasets which were previously out of reach, such as SVHN and CelebA, and that they can be used as faithful generative image models.},
  eventtitle = {International Conference on Machine Learning},
  langid = {english}
}

@unpublished{peharz_probabilistic_2023,
  title = {Probabilistic Circuits},
  author = {Peharz, Robert},
  date = {2023-06-29},
  url = {https://robert-peharz.github.io/doc/GeMSS23ProbabilisticCircuits.pdf},
  eventtitle = {Generative Modeling Summer School},
  langid = {english},
  venue = {Copenhagen}
}

@online{pevny_sumproducttransform_2020,
  title = {Sum-Product-Transform Networks: Exploiting Symmetries Using Invertible Transformations},
  shorttitle = {Sum-Product-Transform Networks},
  author = {Pevny, Tomas and Smidl, Vasek and Trapp, Martin and Polacek, Ondrej and Oberhuber, Tomas},
  date = {2020-05-04},
  eprint = {2005.01297},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.2005.01297},
  url = {http://arxiv.org/abs/2005.01297},
  urldate = {2025-11-22},
  abstract = {In this work, we propose Sum-Product-Transform Networks (SPTN), an extension of sum-product networks that uses invertible transformations as additional internal nodes. The type and placement of transformations determine properties of the resulting SPTN with many interesting special cases. Importantly, SPTN with Gaussian leaves and affine transformations pose the same inference task tractable that can be computed efficiently in SPNs. We propose to store affine transformations in their SVD decompositions using an efficient parametrization of unitary matrices by a set of Givens rotations. Last but not least, we demonstrate that G-SPTNs achieve state-of-the-art results on the density estimation task and are competitive with state-of-the-art methods for anomaly detection.},
  pubstate = {prepublished}
}

@article{rockovich_improved_2025,
  title = {An Improved Update Rule for Probabilistic Computers},
  author = {Rockovich, Andrew and Lafyatis, Gregory and Gauthier, Daniel J.},
  date = {2025-11-04},
  journaltitle = {Physical Review Applied},
  shortjournal = {Phys. Rev. Applied},
  volume = {24},
  number = {5},
  eprint = {2504.00818},
  eprinttype = {arXiv},
  eprintclass = {physics},
  pages = {054009},
  issn = {2331-7019},
  doi = {10.1103/t1wc-j7yp},
  url = {http://arxiv.org/abs/2504.00818},
  urldate = {2025-11-22},
  abstract = {Many hard combinatorial problems can be mapped onto Ising models, which replicate the behavior of classical spins. Recent advances in probabilistic computers are characterized by parallelization and the introduction of novel hardware platforms. An interesting application of probabilistic computers is to operate them in `reverse' mode, where the network self-organizes its behavior to find the input bits that result in an output state. This can be used, for example, as a factorizer of semiprimes. One issue with simulating probabilistic computers on standard logic devices, such as field-programmable gate arrays, is that the update rules for each spin involve many multiplications, evaluation of a hyperbolic tangent, and a high-resolution numerical comparison. We simplify these rules, which improves the spatial and temporal circuit complexity when simulating a probabilistic computer on a field-programmable gate array. Applying our method to factorizing semiprimes, we achieve at least an order-of-magnitude reduction in the on-chip resources and the time-to-solution compared to recently reported methods. For a 32-bit semiprime, we achieve an average factorization in \$\textbackslash sim\$100 s. Our approach will inspire new physical realizations of probabilistic computers because we relax some of their update-rule requirements.}
}

@online{sidheekh_building_2024,
  title = {Building Expressive and Tractable Probabilistic Generative Models: A Review},
  shorttitle = {Building Expressive and Tractable Probabilistic Generative Models},
  author = {Sidheekh, Sahil and Natarajan, Sriraam},
  date = {2024-06-06},
  eprint = {2402.00759},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.00759},
  url = {http://arxiv.org/abs/2402.00759},
  urldate = {2025-11-22},
  abstract = {We present a comprehensive survey of the advancements and techniques in the field of tractable probabilistic generative modeling, primarily focusing on Probabilistic Circuits (PCs). We provide a unified perspective on the inherent trade-offs between expressivity and tractability, highlighting the design principles and algorithmic extensions that have enabled building expressive and efficient PCs, and provide a taxonomy of the field. We also discuss recent efforts to build deep and hybrid PCs by fusing notions from deep neural models, and outline the challenges and open questions that can guide future research in this evolving field.},
  pubstate = {prepublished},
  version = {3}
}

@inproceedings{sidheekh_probabilistic_2023,
  title = {Probabilistic Flow Circuits: Towards Unified Deep Models for Tractable Probabilistic Inference},
  shorttitle = {Probabilistic Flow Circuits},
  author = {Sidheekh, Sahil and Kersting, Kristian and Natarajan, Sriraam},
  date = {2023-06-26},
  url = {https://openreview.net/forum?id=1oE7YizXHf},
  urldate = {2025-11-22},
  abstract = {We consider the problem of increasing the expressivity of probabilistic circuits by augmenting them with the successful generative models of normalizing flows. To this effect, we theoretically establish the requirement of decomposability for such combinations to retain tractability of the learned models. Our model, called Probabilistic Flow Circuits, essentially extends circuits by allowing for normalizing flows at the leaves. Our empirical evaluation clearly establishes the expressivity and tractability of this new class of probabilistic circuits.},
  eventtitle = {The 39th Conference on Uncertainty in Artificial Intelligence},
  langid = {english}
}

@online{siekiera_counterfactual_2025,
  title = {Counterfactual Explanations in Medical Imaging: Exploring SPN-Guided Latent Space Manipulation},
  shorttitle = {Counterfactual Explanations in Medical Imaging},
  author = {Siekiera, Julia and Kramer, Stefan},
  date = {2025-07-25},
  eprint = {2507.19368},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2507.19368},
  url = {http://arxiv.org/abs/2507.19368},
  urldate = {2025-11-22},
  abstract = {Artificial intelligence is increasingly leveraged across various domains to automate decision-making processes that significantly impact human lives. In medical image analysis, deep learning models have demonstrated remarkable performance. However, their inherent complexity makes them black box systems, raising concerns about reliability and interpretability. Counterfactual explanations provide comprehensible insights into decision processes by presenting hypothetical "what-if" scenarios that alter model classifications. By examining input alterations, counterfactual explanations provide patterns that influence the decision-making process. Despite their potential, generating plausible counterfactuals that adhere to similarity constraints providing human-interpretable explanations remains a challenge. In this paper, we investigate this challenge by a model-specific optimization approach. While deep generative models such as variational autoencoders (VAEs) exhibit significant generative power, probabilistic models like sum-product networks (SPNs) efficiently represent complex joint probability distributions. By modeling the likelihood of a semi-supervised VAE's latent space with an SPN, we leverage its dual role as both a latent space descriptor and a classifier for a given discrimination task. This formulation enables the optimization of latent space counterfactuals that are both close to the original data distribution and aligned with the target class distribution. We conduct experimental evaluation on the cheXpert dataset. To evaluate the effectiveness of the integration of SPNs, our SPN-guided latent space manipulation is compared against a neural network baseline. Additionally, the trade-off between latent variable regularization and counterfactual quality is analyzed.},
  pubstate = {prepublished},
  version = {1}
}

@online{smith_bridging_2025,
  title = {Bridging Quantum and Classical Computing in Drug Design: Architecture Principles for Improved Molecule Generation},
  shorttitle = {Bridging Quantum and Classical Computing in Drug Design},
  author = {Smith, Andrew and Guven, Erhan},
  date = {2025-06-01},
  eprint = {2506.01177},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2506.01177},
  url = {http://arxiv.org/abs/2506.01177},
  urldate = {2025-11-22},
  abstract = {Hybrid quantum-classical machine learning offers a path to leverage noisy intermediate-scale quantum (NISQ) devices for drug discovery, but optimal model architectures remain unclear. We systematically optimize the quantum-classical bridge architecture for generative adversarial networks (GANs) in molecular discovery using multi-objective Bayesian optimization. Our optimized model (BO-QGAN) significantly improves performance, achieving a 2.27-fold higher Drug Candidate Score (DCS) than prior quantum-hybrid benchmarks and 2.21-fold higher than the classical baseline, using over 60\% fewer parameters. Key findings favor layering multiple (3-4) shallow (4-8 qubit) quantum circuits sequentially, while classical architecture shows less sensitivity above a minimum capacity. This work provides the first empirically grounded architectural guidelines for hybrid models, enabling more effective integration of current quantum computers into pharmaceutical research pipelines.},
  pubstate = {prepublished},
  version = {1}
}

@book{sullivan_introduction_2015,
  title = {Introduction to Uncertainty Quantification},
  author = {Sullivan, T.J.},
  date = {2015},
  series = {Texts in Applied Mathematics},
  volume = {63},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-23395-6},
  url = {https://link.springer.com/10.1007/978-3-319-23395-6},
  urldate = {2025-12-05},
  isbn = {978-3-319-23394-9 978-3-319-23395-6},
  langid = {english}
}

@online{thoma_recowns_2021,
  title = {RECOWNs: Probabilistic Circuits for Trustworthy Time Series Forecasting},
  shorttitle = {RECOWNs},
  author = {Thoma, Nils and Yu, Zhongjie and Ventola, Fabrizio and Kersting, Kristian},
  date = {2021-07-29},
  eprint = {2106.04148},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.04148},
  url = {http://arxiv.org/abs/2106.04148},
  urldate = {2025-11-22},
  abstract = {Time series forecasting is a relevant task that is performed in several real-world scenarios such as product sales analysis and prediction of energy demand. Given their accuracy performance, currently, Recurrent Neural Networks (RNNs) are the models of choice for this task. Despite their success in time series forecasting, less attention has been paid to make the RNNs trustworthy. For example, RNNs can not naturally provide an uncertainty measure to their predictions. This could be extremely useful in practice in several cases e.g. to detect when a prediction might be completely wrong due to an unusual pattern in the time series. Whittle Sum-Product Networks (WSPNs), prominent deep tractable probabilistic circuits (PCs) for time series, can assist an RNN with providing meaningful probabilities as uncertainty measure. With this aim, we propose RECOWN, a novel architecture that employs RNNs and a discriminant variant of WSPNs called Conditional WSPNs (CWSPNs). We also formulate a Log-Likelihood Ratio Score as better estimation of uncertainty that is tailored to time series and Whittle likelihoods. In our experiments, we show that RECOWNs are accurate and trustworthy time series predictors, able to "know when they do not know".},
  pubstate = {prepublished}
}

@article{vadeboncoeur_fully_2023,
  title = {Fully Probabilistic Deep Models for Forward and Inverse Problems in Parametric Pdes},
  author = {Vadeboncoeur, Arnaud and Akyildiz, Ömer Deniz and Kazlauskaite, Ieva and Girolami, Mark and Cirak, Fehmi},
  date = {2023-10},
  journaltitle = {Journal of Computational Physics},
  shortjournal = {Journal of Computational Physics},
  volume = {491},
  eprint = {2208.04856},
  eprinttype = {arXiv},
  eprintclass = {stat},
  pages = {112369},
  issn = {00219991},
  doi = {10.1016/j.jcp.2023.112369},
  url = {http://arxiv.org/abs/2208.04856},
  urldate = {2025-11-22},
  abstract = {We introduce a physics-driven deep latent variable model (PDDLVM) to learn simultaneously parameter-to-solution (forward) and solution-to-parameter (inverse) maps of parametric partial differential equations (PDEs). Our formulation leverages conventional PDE discretization techniques, deep neural networks, probabilistic modelling, and variational inference to assemble a fully probabilistic coherent framework. In the posited probabilistic model, both the forward and inverse maps are approximated as Gaussian distributions with a mean and covariance parameterized by deep neural networks. The PDE residual is assumed to be an observed random vector of value zero, hence we model it as a random vector with a zero mean and a user-prescribed covariance. The model is trained by maximizing the probability, that is the evidence or marginal likelihood, of observing a residual of zero by maximizing the evidence lower bound (ELBO). Consequently, the proposed methodology does not require any independent PDE solves and is physics-informed at training time, allowing the real-time solution of PDE forward and inverse problems after training. The proposed framework can be easily extended to seamlessly integrate observed data to solve inverse problems and to build generative models. We demonstrate the efficiency and robustness of our method on finite element discretized parametric PDE problems such as linear and nonlinear Poisson problems, elastic shells with complex 3D geometries, and time-dependent nonlinear and inhomogeneous PDEs using a physics-informed neural network (PINN) discretization. We achieve up to three orders of magnitude speed-up after training compared to traditional finite element method (FEM), while outputting coherent uncertainty estimates.}
}

@online{ventola_probabilistic_2023,
  title = {Probabilistic Circuits That Know What They Don't Know},
  author = {Ventola, Fabrizio and Braun, Steven and Yu, Zhongjie and Mundt, Martin and Kersting, Kristian},
  date = {2023-06-12},
  eprint = {2302.06544},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.06544},
  url = {http://arxiv.org/abs/2302.06544},
  urldate = {2025-11-22},
  abstract = {Probabilistic circuits (PCs) are models that allow exact and tractable probabilistic inference. In contrast to neural networks, they are often assumed to be well-calibrated and robust to out-of-distribution (OOD) data. In this paper, we show that PCs are in fact not robust to OOD data, i.e., they don't know what they don't know. We then show how this challenge can be overcome by model uncertainty quantification. To this end, we propose tractable dropout inference (TDI), an inference procedure to estimate uncertainty by deriving an analytical solution to Monte Carlo dropout (MCD) through variance propagation. Unlike MCD in neural networks, which comes at the cost of multiple network evaluations, TDI provides tractable sampling-free uncertainty estimates in a single forward pass. TDI improves the robustness of PCs to distribution shift and OOD data, demonstrated through a series of experiments evaluating the classification confidence and uncertainty estimates on real-world data.},
  pubstate = {prepublished}
}

@online{wang_relationship_2025,
  title = {On the Relationship between Monotone and Squared Probabilistic Circuits},
  author = {Wang, Benjie and Van den Broeck, Guy},
  date = {2025-02-24},
  eprint = {2408.00876},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.00876},
  url = {http://arxiv.org/abs/2408.00876},
  urldate = {2025-11-22},
  abstract = {Probabilistic circuits are a unifying representation of functions as computation graphs of weighted sums and products. Their primary application is in probabilistic modeling, where circuits with non-negative weights (monotone circuits) can be used to represent and learn density/mass functions, with tractable marginal inference. Recently, it was proposed to instead represent densities as the square of the circuit function (squared circuits); this allows the use of negative weights while retaining tractability, and can be exponentially more expressive efficient than monotone circuits. Unfortunately, we show the reverse also holds, meaning that monotone circuits and squared circuits are incomparable in general. This raises the question of whether we can reconcile, and indeed improve upon the two modeling approaches. We answer in the positive by proposing Inception PCs, a novel type of circuit that naturally encompasses both monotone circuits and squared circuits as special cases, and employs complex parameters. Empirically, we validate that Inception PCs can outperform both monotone and squared circuits on a range of tabular and image datasets.},
  langid = {english},
  pubstate = {prepublished}
}

@online{wehenkel_unconstrained_2021,
  title = {Unconstrained Monotonic Neural Networks},
  author = {Wehenkel, Antoine and Louppe, Gilles},
  date = {2021-03-31},
  eprint = {1908.05164},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1908.05164},
  url = {http://arxiv.org/abs/1908.05164},
  urldate = {2025-11-22},
  abstract = {Monotonic neural networks have recently been proposed as a way to define invertible transformations. These transformations can be combined into powerful autoregressive flows that have been shown to be universal approximators of continuous probability distributions. Architectures that ensure monotonicity typically enforce constraints on weights and activation functions, which enables invertibility but leads to a cap on the expressiveness of the resulting transformations. In this work, we propose the Unconstrained Monotonic Neural Network (UMNN) architecture based on the insight that a function is monotonic as long as its derivative is strictly positive. In particular, this latter condition can be enforced with a free-form neural network whose only constraint is the positiveness of its output. We evaluate our new invertible building block within a new autoregressive flow (UMNN-MAF) and demonstrate its effectiveness on density estimation experiments. We also illustrate the ability of UMNNs to improve variational inference.},
  pubstate = {prepublished}
}

@online{zhang_restructuring_2025,
  title = {Restructuring Tractable Probabilistic Circuits},
  author = {Zhang, Honghua and Wang, Benjie and Arenas, Marcelo and Van den Broeck, Guy},
  date = {2025-04-30},
  eprint = {2411.12256},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2411.12256},
  url = {http://arxiv.org/abs/2411.12256},
  urldate = {2025-11-22},
  abstract = {Probabilistic circuits (PCs) are a unifying representation for probabilistic models that support tractable inference. Numerous applications of PCs like controllable text generation depend on the ability to efficiently multiply two circuits. Existing multiplication algorithms require that the circuits respect the same structure, i.e. variable scopes decomposes according to the same vtree. In this work, we propose and study the task of restructuring structured(-decomposable) PCs, that is, transforming a structured PC such that it conforms to a target vtree. We propose a generic approach for this problem and show that it leads to novel polynomial-time algorithms for multiplying circuits respecting different vtrees, as well as a practical depth-reduction algorithm that preserves structured decomposibility. Our work opens up new avenues for tractable PC inference, suggesting the possibility of training with less restrictive PC structures while enabling efficient inference by changing their structures at inference time.},
  pubstate = {prepublished}
}

@inproceedings{zhang_scaling_2025,
  title = {Scaling Probabilistic Circuits via Monarch Matrices},
  author = {Zhang, Honghua and Dang, Meihua and Wang, Benjie and Ermon, Stefano and Peng, Nanyun and Van den Broeck, Guy},
  date = {2025-06-18},
  url = {https://openreview.net/forum?id=uzVShD7wHO&noteId=fRFELVgfJR},
  urldate = {2025-11-22},
  abstract = {Probabilistic Circuits (PCs) are tractable representations of probability distributions allowing for exact and efficient computation of likelihoods and marginals. Recent advancements have improved the scalability of PCs either by leveraging their sparse properties or through the use of tensorized operations for better hardware utilization. However, no existing method fully exploits both aspects simultaneously. In this paper, we propose a novel sparse and structured parameterization for the sum blocks in PCs. By replacing dense matrices with sparse Monarch matrices, we significantly reduce the memory and computation costs, enabling unprecedented scaling of PCs. From a theory perspective, our construction arises naturally from circuit multiplication; from a practical perspective, compared to previous efforts on scaling up tractable probabilistic models, our approach not only achieves state-of-the-art generative modeling performance on challenging benchmarks like Text8, LM1B and ImageNet, but also demonstrates superior scaling behavior, achieving the same performance with substantially less compute as measured by the number of floating-point operations (FLOPs) during training.},
  eventtitle = {Forty-Second International Conference on Machine Learning},
  langid = {english}
}

@article{talts_validating_2018,
	title = {Validating Bayesian inference algorithms with simulation-based calibration},
	url = {https://arxiv.org/pdf/1804.06788},
	journal = {arXiv preprint arXiv:1804.06788},
	author = {Talts, Sean and Betancourt, Michael and Simpson, Daniel and Vehtari, Aki and Gelman, Andrew},
	year = {2018},
}

@article{diebold_evaluating_1997,
	title = {Evaluating density forecasts},
	url = {https://www.nber.org/system/files/working_papers/t0215/t0215.pdf},
	publisher = {National Bureau of Economic Research Cambridge, Mass., USA},
	author = {Diebold, Francis X and Gunther, Todd A and Tay, Anthony},
	year = {1997},
}
