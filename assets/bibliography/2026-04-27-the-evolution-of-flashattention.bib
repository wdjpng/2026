@article{dao2022flashattention,
  title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2205.14135},
  year={2022}
}


@misc{bahdanau2016neuralmachinetranslationjointly,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1409.0473}, 
}

@article{sun2025efficient,
  title={Efficient attention mechanisms for large language models: A survey},
  author={Sun, Yutao and Li, Zhenyu and Zhang, Yike and Pan, Tengyu and Dong, Bowen and Guo, Yuyi and Wang, Jianyong},
  journal={arXiv preprint arXiv:2507.19595},
  year={2025}
}


@misc{keles2022computationalcomplexityselfattention,
      title={On The Computational Complexity of Self-Attention}, 
      author={Feyza Duman Keles and Pruthuvi Mahesakya Wijewardena and Chinmay Hegde},
      year={2022},
      eprint={2209.04881},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2209.04881}, 
}

@article{gholami2024ai,
  title={Ai and memory wall},
  author={Gholami, Amir and Yao, Zhewei and Kim, Sehoon and Hooper, Coleman and Mahoney, Michael W and Keutzer, Kurt},
  journal={IEEE Micro},
  volume={44},
  number={3},
  pages={33--39},
  year={2024},
  publisher={IEEE}
}

@article{vaswani2023attentionneed,
  title={Attention Is All You Need}, 
  author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017},
  url={https://arxiv.org/abs/1706.03762}
}



@techreport{nvidia2022h100,
  title={NVIDIA H100 Tensor Core GPU Architecture},
  author={{NVIDIA Corporation}},
  year={2022},
  institution={NVIDIA},
  url={https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-architecture-whitepaper},
  note={Whitepaper}
}

@techreport{nvidia2020a100,
  title={NVIDIA A100 Tensor Core GPU Architecture},
  author={{NVIDIA Corporation}},
  year={2020},
  institution={NVIDIA},
  url={https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf},
  note={Whitepaper}
}

@misc{hazyresearch2024brrr,
  title={GPUs Go Brrr},
  author={{Hazy Research}},
  year={2024},
  howpublished={Stanford Hazy Research Blog},
  url={https://hazyresearch.stanford.edu/blog/2024-05-12-tk},
  note={Accessed: 2025-11-29}
}


@misc{wang2020linformerselfattentionlinearcomplexity,
      title={Linformer: Self-Attention with Linear Complexity}, 
      author={Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
      year={2020},
      eprint={2006.04768},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.04768}, 
}


@misc{choromanski2022rethinkingattentionperformers,
      title={Rethinking Attention with Performers}, 
      author={Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and David Belanger and Lucy Colwell and Adrian Weller},
      year={2022},
      eprint={2009.14794},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2009.14794}, 
}

@article{flashattention3,
  title={FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2407.08608},
  year={2024}
}

@online{gpu-mode-fa4,
  title = {How FlashAttention 4 Works},
  author = {GPU Mode},
  year = {2025},
  url = {https://www.youtube.com/watch?v=YOUR_VIDEO_ID},
  note = {YouTube video, accessed 2025-12-07}
}

@online{tri-dao-hotchips,
  title = {Domain-Specific Languages for GPU Kernels and Automatic Kernel Authoring with LLMs},
  author = {Tri Dao},
  year = {2024},
  url = {https://www.youtube.com/watch?v=YOUR_VIDEO_ID},
  note = {Hot Chips talk, YouTube, accessed 2025-12-07}
}

@online{modal-fa4,
  title = {Reverse engineering FlashAttention 4},
  author = {{Modal Labs}},
  year = {2025},
  url = {https://modal.com/blog/reverse-engineer-flash-attention-4},
  note = {Blog post, accessed 2025-12-07}
}

@online{wu-fa4-medium,
  title = {FlashAttention-4: Breaking the Petaflop Barrier in GPU Attention Kernels},
  author = {Chang-Tim Wu},
  year = {2025},
  url = {http://medium.com/@changtimwu/flashattention-4-breaking-the-petaflop-barrier-in-gpu-attention-kernels-be9444311af0},
  note = {Medium blog post, accessed 2025-12-07}
}

@article{schraudolph1999fast,
  title={A fast, compact approximation of the exponential function},
  author={Schraudolph, Nicol N},
  journal={Neural Computation},
  volume={11},
  number={4},
  pages={853--862},
  year={1999},
  publisher={MIT Press}
}

@article{kwon2023efficientmemorymanagementlarge,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph E. and Zhang, Hao and Stoica, Ion},
  journal={arXiv preprint arXiv:2309.06180},
  year={2023}
}

@article{juravsky2024hydragenhighthroughputllminference,
  title={HydraGen: High-Throughput LLM Inference with Shared Prefixes},
  author={Juravsky, Jordan and others},
  journal={arXiv preprint arXiv:2402.05099},
  year={2024}
}
      eprint={2009.14794},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2009.14794}, 
}

@misc{kitaev2020reformerefficienttransformer,
      title={Reformer: The Efficient Transformer}, 
      author={Nikita Kitaev and Łukasz Kaiser and Anselm Levskaya},
      year={2020},
      eprint={2001.04451},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.04451}, 
}

@article{milakov2018online,
  title={Online normalizer calculation for softmax},
  author={Milakov, Maxim and Gimelshein, Natalia},
  journal={arXiv preprint arXiv:1805.02867},
  year={2018},
  url={https://arxiv.org/abs/1805.02867}
}


@misc{child2019generatinglongsequencessparse,
      title={Generating Long Sequences with Sparse Transformers}, 
      author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
      year={2019},
      eprint={1904.10509},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1904.10509}, 
}

@article{dao2023flashattention2,
  title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@article{shah2024flashattention3,
  title={FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision},
  author={Shah, Jay and Bikshandi, Ganesh and Zhang, Ying and Thakkar, Vijay and Ramani, Pradeep and Dao, Tri},
  journal={arXiv preprint arXiv:2407.08608},
  year={2024}
}


@misc{kwon2023efficientmemorymanagementlarge,
      title={Efficient Memory Management for Large Language Model Serving with PagedAttention}, 
      author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
      year={2023},
      eprint={2309.06180},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.06180}, 
}

@misc{juravsky2024hydragenhighthroughputllminference,
      title={Hydragen: High-Throughput LLM Inference with Shared Prefixes}, 
      author={Jordan Juravsky and Bradley Brown and Ryan Ehrlich and Daniel Y. Fu and Christopher R{\'e} and Azalia Mirhoseini},
      year={2024},
      eprint={2402.05099},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.05099}, 
}


@inproceedings{dukhan2020two,
  title={Two-pass softmax algorithm},
  author={Dukhan, Marat and Ablavatski, Artsiom},
  booktitle={2020 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
  pages={386--395},
  year={2020},
  organization={IEEE}
}

@inproceedings{demaine2018red,
  title={Red-blue pebble game: Complexity of computing the trade-off between cache size and memory transfers},
  author={Demaine, Erik D and Liu, Quanquan C},
  booktitle={Proceedings of the 30th on Symposium on Parallelism in Algorithms and Architectures},
  pages={195--204},
  year={2018}
}


@online{gpu-mode-fa4,
  title = {How FlashAttention 4 Works},
  author = {GPU Mode},
  year = {2025},
  url = {https://youtu.be/ZIEq-WTquy4},
  note = {YouTube video, accessed 2025-12-07}
}


@online{tri-dao-hotchips,
  title = {Domain-Specific Languages for GPU Kernels and Automatic Kernel Authoring with LLMs},
  author = {Tri Dao},
  year = {2024},
  url = {https://youtu.be/_sRkawqEMCs},
  note = {Hot Chips talk, YouTube, accessed 2025-12-07}
}

@online{modal-fa4,
  title = {Reverse engineering FlashAttention 4},
  author = {{Modal Labs}},
  year = {2025},
  url = {https://modal.com/blog/reverse-engineer-flash-attention-4},
  note = {Blog post, accessed 2025-12-07}
}

@online{wu-fa4-medium,
  title = {FlashAttention-4: Breaking the Petaflop Barrier in GPU Attention Kernels},
  author = {Chang-Tim Wu},
  year = {2025},
  url = {http://medium.com/@changtimwu/flashattention-4-breaking-the-petaflop-barrier-in-gpu-attention-kernels-be9444311af0},
  note = {Medium blog post, accessed 2025-12-07}
}



@online{cutlass-tma,
  title = {CUTLASS Tutorial: Mastering the NVIDIA® Tensor Memory Accelerator (TMA)},
  author = {Colfax Research},
  year = {2024},
  url = {https://research.colfax-intl.com/tutorial-hopper-tma/},
  note = {Accessed 2025-12-07}
}

@online{modal-tma,
  title = {Tensor Memory Accelerator (TMA) - GPU Glossary},
  author = {{Modal Labs}},
  year = {2024},
  url = {https://modal.com/gpu-glossary/device-hardware/tensor-memory-accelerator},
  note = {Accessed 2025-12-07}
}

@online{nvidia-hopper-depth,
  title = {NVIDIA Hopper Architecture In-Depth},
  author = {NVIDIA},
  year = {2022},
  url = {https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/},
  note = {Accessed 2025-12-07}
}

@online{nvidia-blackwell-brief,
  title = {NVIDIA Blackwell Architecture Technical Brief},
  author = {NVIDIA},
  year = {2024},
  url = {https://resources.nvidia.com/en-us-blackwell-architecture?ncid=no-ncid},
  note = {Accessed 2025-12-07}
}

@article{schraudolph1999fast,
  title={A fast, compact approximation of the exponential function},
  author={Schraudolph, Nicol N},
  journal={Neural Computation},
  volume={11},
  number={4},
  pages={853--862},
  year={1999},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}