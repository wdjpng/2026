@misc{xiao2024efficientstreaminglanguagemodels,
      title={Efficient Streaming Language Models with Attention Sinks}, 
      author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
      year={2024},
      eprint={2309.17453},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.17453}, 
}

@misc{barbero2025llmsattendtoken,
      title={Why do LLMs attend to the first token?}, 
      author={Federico Barbero and Álvaro Arroyo and Xiangming Gu and Christos Perivolaropoulos and Michael Bronstein and Petar Veličković and Razvan Pascanu},
      year={2025},
      eprint={2504.02732},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.02732}, 
}

@misc{zuhri2025softpickattentionsinkmassive,
      title={Softpick: No Attention Sink, No Massive Activations with Rectified Softmax}, 
      author={Zayd M. K. Zuhri and Erland Hilman Fuadi and Alham Fikri Aji},
      year={2025},
      eprint={2504.20966},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2504.20966}, 
}

@misc{jiang2024minference10acceleratingprefilling,
      title={MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention}, 
      author={Huiqiang Jiang and Yucheng Li and Chengruidong Zhang and Qianhui Wu and Xufang Luo and Surin Ahn and Zhenhua Han and Amir H. Abdi and Dongsheng Li and Chin-Yew Lin and Yuqing Yang and Lili Qiu},
      year={2024},
      eprint={2407.02490},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.02490}, 
}

@misc{xu2025xattentionblocksparseattention,
      title={XAttention: Block Sparse Attention with Antidiagonal Scoring}, 
      author={Ruyi Xu and Guangxuan Xiao and Haofeng Huang and Junxian Guo and Song Han},
      year={2025},
      eprint={2503.16428},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.16428}, 
}

@misc{yang2024tidaldecodefastaccuratellm,
      title={TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention}, 
      author={Lijie Yang and Zhihao Zhang and Zhuofu Chen and Zikun Li and Zhihao Jia},
      year={2024},
      eprint={2410.05076},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.05076}, 
}

@misc{yuan2025nativesparseattentionhardwarealigned,
      title={Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention}, 
      author={Jingyang Yuan and Huazuo Gao and Damai Dai and Junyu Luo and Liang Zhao and Zhengyan Zhang and Zhenda Xie and Y. X. Wei and Lean Wang and Zhiping Xiao and Yuqing Wang and Chong Ruan and Ming Zhang and Wenfeng Liang and Wangding Zeng},
      year={2025},
      eprint={2502.11089},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.11089}, 
}

@misc{gao2025seerattentionlearningintrinsicsparse,
      title={SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs}, 
      author={Yizhao Gao and Zhichen Zeng and Dayou Du and Shijie Cao and Peiyuan Zhou and Jiaxing Qi and Junjie Lai and Hayden Kwok-Hay So and Ting Cao and Fan Yang and Mao Yang},
      year={2025},
      eprint={2410.13276},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.13276}, 
}

@misc{hooper2025squeezedattentionacceleratinglong,
      title={Squeezed Attention: Accelerating Long Context Length LLM Inference}, 
      author={Coleman Hooper and Sehoon Kim and Hiva Mohammadzadeh and Monishwaran Maheswaran and Sebastian Zhao and June Paik and Michael W. Mahoney and Kurt Keutzer and Amir Gholami},
      year={2025},
      eprint={2411.09688},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.09688}, 
}

@misc{kim2025epicacheepisodickvcache,
      title={EpiCache: Episodic KV Cache Management for Long Conversational Question Answering}, 
      author={Minsoo Kim and Arnav Kundu and Han-Byul Kim and Richa Dixit and Minsik Cho},
      year={2025},
      eprint={2509.17396},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2509.17396}, 
}

@misc{zhang2023h2oheavyhitteroracleefficient,
      title={H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models}, 
      author={Zhenyu Zhang and Ying Sheng and Tianyi Zhou and Tianlong Chen and Lianmin Zheng and Ruisi Cai and Zhao Song and Yuandong Tian and Christopher Ré and Clark Barrett and Zhangyang Wang and Beidi Chen},
      year={2023},
      eprint={2306.14048},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.14048}, 
}

@misc{acharya2025starattentionefficientllm,
      title={Star Attention: Efficient LLM Inference over Long Sequences}, 
      author={Shantanu Acharya and Fei Jia and Boris Ginsburg},
      year={2025},
      eprint={2411.17116},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.17116}, 
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{pope2022efficientlyscalingtransformerinference,
      title={Efficiently Scaling Transformer Inference}, 
      author={Reiner Pope and Sholto Douglas and Aakanksha Chowdhery and Jacob Devlin and James Bradbury and Anselm Levskaya and Jonathan Heek and Kefan Xiao and Shivani Agrawal and Jeff Dean},
      year={2022},
      eprint={2211.05102},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.05102}, 
}

@misc{tay2022efficienttransformerssurvey,
      title={Efficient Transformers: A Survey}, 
      author={Yi Tay and Mostafa Dehghani and Dara Bahri and Donald Metzler},
      year={2022},
      eprint={2009.06732},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2009.06732}, 
}

@article{radford2018improving,
      title={Improving Language Understanding by Generative Pre-Training},
      author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
      journal={OpenAI Blog},
      year={2018},
      url={https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}
}

@article{radford2019language,
      title={Language Models are Unsupervised Multitask Learners},
      author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
      journal={OpenAI Blog},
      volume={1},
      number={8},
      pages={9},
      year={2019},
      url={https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
}

@misc{oono2021graphneuralnetworksexponentially,
      title={Graph Neural Networks Exponentially Lose Expressive Power for Node Classification}, 
      author={Kenta Oono and Taiji Suzuki},
      year={2021},
      eprint={1905.10947},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1905.10947}, 
}

@misc{child2019generatinglongsequencessparse,
      title={Generating Long Sequences with Sparse Transformers}, 
      author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
      year={2019},
      eprint={1904.10509},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1904.10509}, 
}

@misc{beltagy2020longformerlongdocumenttransformer,
      title={Longformer: The Long-Document Transformer}, 
      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2020},
      eprint={2004.05150},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.05150}, 
}

@misc{zaheer2021bigbirdtransformerslonger,
      title={Big Bird: Transformers for Longer Sequences}, 
      author={Manzil Zaheer and Guru Guruganesh and Avinava Dubey and Joshua Ainslie and Chris Alberti and Santiago Ontanon and Philip Pham and Anirudh Ravula and Qifan Wang and Li Yang and Amr Ahmed},
      year={2021},
      eprint={2007.14062},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2007.14062}, 
}

@misc{kwon2023efficientmemorymanagementlarge,
      title={Efficient Memory Management for Large Language Model Serving with PagedAttention}, 
      author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
      year={2023},
      eprint={2309.06180},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.06180}, 
}

@misc{hu2025epicefficientpositionindependentcaching,
      title={EPIC: Efficient Position-Independent Caching for Serving Large Language Models}, 
      author={Junhao Hu and Wenrui Huang and Weidong Wang and Haoyi Wang and Tiancheng Hu and Qin Zhang and Hao Feng and Xusheng Chen and Yizhou Shan and Tao Xie},
      year={2025},
      eprint={2410.15332},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.15332}, 
}
