@article{hu2024pytorch,
  title={PyTorch Frame: A Modular Framework for Multi-Modal Tabular Learning},
  author={Hu, Weihua and Yuan, Yiwen and Zhang, Zecheng and Nitta, Akihiro and Cao, Kaidi and Kocijan, Vid and Leskovec, Jure and Fey, Matthias},
  journal={arXiv preprint arXiv:2404.00776},
  year={2024}
}
@misc{excelformer,
      title={ExcelFormer: A neural network surpassing GBDTs on tabular data}, 
      author={Jintai Chen and Jiahuan Yan and Qiyuan Chen and Danny Ziyi Chen and Jian Wu and Jimeng Sun},
      year={2024},
      eprint={2301.02819},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2301.02819}, 
}
@misc{arik2020tabnetattentiveinterpretabletabular,
      title={TabNet: Attentive Interpretable Tabular Learning}, 
      author={Sercan O. Arik and Tomas Pfister},
      year={2020},
      eprint={1908.07442},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1908.07442}, 
}
@misc{gorishniy2023revisitingdeeplearningmodels,
      title={Revisiting Deep Learning Models for Tabular Data}, 
      author={Yury Gorishniy and Ivan Rubachev and Valentin Khrulkov and Artem Babenko},
      year={2023},
      eprint={2106.11959},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.11959}, 
}
@misc{chen2023tromptbetterdeepneural,
      title={Trompt: Towards a Better Deep Neural Network for Tabular Data}, 
      author={Kuan-Yu Chen and Ping-Han Chiang and Hsin-Rung Chou and Ting-Wei Chen and Tien-Hao Chang},
      year={2023},
      eprint={2305.18446},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.18446}, 
}
@misc{tabred,
      title={TabReD: Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning Benchmarks}, 
      author={Ivan Rubachev and Nikolay Kartashev and Yury Gorishniy and Artem Babenko},
      year={2024},
      eprint={2406.19380},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.19380}, 
}
@misc{embeddings,
      title={On Embeddings for Numerical Features in Tabular Deep Learning}, 
      author={Yury Gorishniy and Ivan Rubachev and Artem Babenko},
      year={2023},
      eprint={2203.05556},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.05556}, 
}
@misc{realmlp,
      title={Better by Default: Strong Pre-Tuned MLPs and Boosted Trees on Tabular Data}, 
      author={David Holzmüller and Léo Grinsztajn and Ingo Steinwart},
      year={2025},
      eprint={2407.04491},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.04491}, 
}
@inproceedings{facebook,
  title={Practical Lessons from Predicting Clicks on Ads at Facebook},
  author={Xinran He and Junfeng Pan and Ou Jin and Tianbing Xu and Bo Liu and Tao Xu and Yanxin Shi and Antoine Atallah and Ralf Herbrich and Stuart Bowers and Joaquin Qui{\~n}onero Candela},
  booktitle={International Workshop on Data Mining for Online Advertising},
  year={2014},
  url={https://api.semanticscholar.org/CorpusID:2999385}
}
@misc{tabarena,
      title={TabArena: A Living Benchmark for Machine Learning on Tabular Data}, 
      author={Nick Erickson and Lennart Purucker and Andrej Tschalzev and David Holzmüller and Prateek Mutalik Desai and David Salinas and Frank Hutter},
      year={2025},
      eprint={2506.16791},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2506.16791}, 
}
@misc{stripe,
      title={How We Built It: Stripe Radar}, 
      author={Ryan Drapeau},
      year={2023},
      url={https://stripe.com/blog/how-we-built-it-stripe-radar}, 
}
@misc{swiggy,
      title={Learning to Rank Restaurants}, 
      author={Ashay Tamhane},
      year={2021},
      url={https://bytes.swiggy.com/learning-to-rank-restaurants-c6a69ba4b330}, 
}
@inproceedings{sharechat, series={FIRE 2023},
   title={On Gradient Boosted Decision Trees and Neural Rankers: A Case-Study on Short-Video Recommendations at ShareChat},
   url={http://dx.doi.org/10.1145/3632754.3632940},
   DOI={10.1145/3632754.3632940},
   booktitle={Proceedings of the 15th Annual Meeting of the Forum for Information Retrieval Evaluation},
   publisher={ACM},
   author={Jeunen, Olivier and Sagtani, Hitesh and Doi, Himanshu and Karimov, Rasul and Pokharna, Neeti and Kalim, Danish and Ustimenko, Aleksei and Green, Christopher and Mehrotra, Rishabh and Shi, Wenzhe},
   year={2023},
   month=dec, pages={136–141},
   collection={FIRE 2023} }


@inproceedings{grinsz,
author = {Grinsztajn, L\'{e}o and Oyallon, Edouard and Varoquaux, Ga\"{e}l},
title = {Why do tree-based models still outperform deep learning on typical tabular data?},
year = {2022},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {While deep learning has enabled tremendous progress on text and image datasets, its superiority on tabular data is not clear. We contribute extensive benchmarks of standard and novel deep learning methods as well as tree-based models such as XGBoost and Random Forests, across a large number of datasets and hyperparam-eter combinations. We define a standard set of 45 datasets from varied domains with clear characteristics of tabular data and a benchmarking methodology accounting for both fitting models and finding good hyperparameters. Results show that tree-based models remain state-of-the-art on medium-sized data (∼10K samples) even without accounting for their superior speed. To understand this gap, we conduct an empirical investigation into the differing inductive biases of tree-based models and neural networks. This leads to a series of challenges which should guide researchers aiming to build tabular-specific neural network: <b>1</b>. be robust to uninformative features, <b>2</b>. preserve the orientation of the data, and <b>3</b>. be able to easily learn irregular functions. To stimulate research on tabular architectures, we contribute a standard benchmark and raw data for baselines: every point of a 20 000 compute hours hyperparameter search for each learner.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {37},
numpages = {14},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@inproceedings{reg-is-all-you-need,
author = {Kadra, Arlind and Lindauer, Marius and Hutter, Frank and Grabocka, Josif},
title = {Well-tuned simple nets excel on tabular datasets},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Tabular datasets are the last "unconquered castle" for deep learning, with traditional ML methods like Gradient-Boosted Decision Trees still performing strongly even against recent specialized neural architectures. In this paper, we hypothesize that the key to boosting the performance of neural networks lies in rethinking the joint and simultaneous application of a large set of modern regularization techniques. As a result, we propose regularizing plain Multilayer Perceptron (MLP) networks by searching for the optimal combination/cocktail of 13 regularization techniques for each dataset using a joint optimization over the decision on which regularizers to apply and their subsidiary hyperparameters.We empirically assess the impact of these regularization cocktails for MLPs in a large-scale empirical study comprising 40 tabular datasets and demonstrate that (i) well-regularized plain MLPs significantly outperform recent state-of-the-art specialized neural network architectures, and (ii) they even outperform strong traditional ML methods, such as XGBoost.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1832},
numpages = {14},
series = {NIPS '21}
}

@inproceedings{tabm,
title={TabM: Advancing tabular deep learning with parameter-efficient ensembling},
author={Yury Gorishniy and Akim Kotelnikov and Artem Babenko},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=Sd4wYYOhmY}
}

