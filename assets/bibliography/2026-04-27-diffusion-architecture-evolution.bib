@online{blackforestlabsFLUX1,
  title = {FLUX.1},
  author = {Black Forest Labs},
  url = {https://blackforestlabs.io/flux-1/},
  urldate = {2025-06-04},
  abstract = {BlackForestLabs is proud to announce the release of the FLUX.1, a groundbreaking set of text-to-image models that set a new standard in the field},
  langid = {american},
  organization = {BlackForestLabs}
}

@online{caiHiDreamI1HighEfficientImage2025,
  title = {HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer},
  shorttitle = {HiDream-I1},
  author = {Cai, Qi and Chen, Jingwen and Chen, Yang and Li, Yehao and Long, Fuchen and Pan, Yingwei and Qiu, Zhaofan and Zhang, Yiheng and Gao, Fengbin and Xu, Peihan and Wang, Yimeng and Yu, Kai and Chen, Wenxuan and Feng, Ziwei and Gong, Zijian and Pan, Jianzhuang and Peng, Yi and Tian, Rui and Wang, Siyu and Zhao, Bo and Yao, Ting and Mei, Tao},
  year = {2025},
  eprint = {2505.22705},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2505.22705},
  url = {http://arxiv.org/abs/2505.22705},
  urldate = {2025-06-04},
  abstract = {Recent advancements in image generative foundation models have prioritized quality improvements but often at the cost of increased computational complexity and inference latency. To address this critical trade-off, we introduce HiDream-I1, a new open-source image generative foundation model with 17B parameters that achieves state-of-the-art image generation quality within seconds. HiDream-I1 is constructed with a new sparse Diffusion Transformer (DiT) structure.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia}
}

@inproceedings{peeblesDiTScalableDiffusion2023,
  title = {DiT: Scalable Diffusion Models with Transformers},
  author = {Peebles, William and Xie, Saining},
  booktitle = {NeurIPS},
  year = {2023},
  url = {https://arxiv.org/abs/2212.09748},
  archivePrefix = {arXiv},
  eprint = {2212.09748}
}

@inproceedings{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  booktitle = {MICCAI},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  series = {Lecture Notes in Computer Science},
  pages = {234--241},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-24574-4_28},
  url = {https://arxiv.org/abs/1505.04597}
}

@misc{CVPR2023Tutorial,
  title = {CVPR 2023 Tutorial Denoising Diffusion-based Generative Modeling: Foundations and Applications},
  shorttitle = {Denoising Diffusion-based Generative Modeling},
  author = {Song, Jiaming and Meng, Chenlin and Vahdat, Arash},
  journal = {CVPR 2023 Tutorial},
  url = {https://cvpr2023-tutorial-diffusion-models.github.io},
  urldate = {2024-10-16},
  abstract = {Tutorial in Conjunction with CVPR 2023},
  note = {Accessed: 2024-10-16}
}

@misc{song2019generative,
  author = {Yang Song},
  title = {Generative Modeling by Estimating Gradients of the Data Distribution},
  year = {2019},
  url = {https://yang-song.net/blog/2021/score/},
  note = {Accessed: 2025-10-04}
}

@misc{wengWhatAreDiffusion2021,
  title = {What are Diffusion Models?},
  author = {Weng, Lilian},
  year = {2021},
  url = {https://lilianweng.github.io/posts/2021-07-11-diffusion-models/},
  urldate = {2025-10-04}
}

@inproceedings{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising Diffusion Probabilistic Models},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  booktitle = {NeurIPS},
  year = {2020},
  url = {https://arxiv.org/abs/2006.11239},
  archivePrefix = {arXiv},
  eprint = {2006.11239}
}

@inproceedings{songImprovedTechniquesTraining2020,
  title = {Improved Techniques for Training Score-Based Generative Models},
  author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  booktitle = {NeurIPS},
  year = {2020},
  url = {https://arxiv.org/abs/2006.09011},
  archivePrefix = {arXiv},
  eprint = {2006.09011}
}

@inproceedings{nicholImprovedDenoisingDiffusion2021,
  title = {Improved Denoising Diffusion Probabilistic Models},
  author = {Nichol, Alexander Quinn and Dhariwal, Prafulla},
  booktitle = {ICML},
  year = {2021},
  url = {https://arxiv.org/abs/2102.09672},
  archivePrefix = {arXiv},
  eprint = {2102.09672}
}

@inproceedings{sahariaImageSuperResolution2022,
  title = {Image Super-Resolution via Iterative Refinement},
  shorttitle = {SR3},
  author = {Saharia, Chitwan and Chan, William and Saxena, Huiwen and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Gontijo-Lopes, Raphael and Ayan, Burcu Karagol and Mahdavi, Soroosh and Lopes, Raphael and Fleet, David and Norouzi, Mohammad and Salimans, Tim and Ho, Jonathan},
  booktitle = {TPAMI},
  year = {2022},
  doi = {10.1109/TPAMI.2022.3204467},
  url = {https://arxiv.org/abs/2104.07636},
  archivePrefix = {arXiv},
  eprint = {2104.07636}
}

@inproceedings{hoCascadedDiffusionModels2022,
  title = {Cascaded Diffusion Models for High Fidelity Image Generation},
  author = {Ho, Jonathan and Salimans, Tim and Sohl-Dickstein, Jascha and Gafni, Oran and Chan, William and Norouzi, Mohammad},
  booktitle = {JMLR},
  year = {2022},
  url = {https://arxiv.org/abs/2106.15282},
  archivePrefix = {arXiv},
  eprint = {2106.15282}
}

@inproceedings{dhariwalDiffusionModelsBeat2021,
  title = {Diffusion Models Beat GANs on Image Synthesis},
  author = {Dhariwal, Prafulla and Nichol, Alexander},
  booktitle = {NeurIPS},
  year = {2021},
  url = {https://arxiv.org/abs/2105.05233},
  archivePrefix = {arXiv},
  eprint = {2105.05233}
}

@inproceedings{songScoreBasedGenerativeModeling2021,
  title = {Score-Based Generative Modeling through Stochastic Differential Equations},
  author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  booktitle = {ICLR},
  year = {2021},
  url = {https://arxiv.org/abs/2011.13456},
  archivePrefix = {arXiv},
  eprint = {2011.13456}
}

@inproceedings{rombachHighResolutionImageSynthesis2022,
  title = {High-Resolution Image Synthesis with Latent Diffusion Models},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  booktitle = {CVPR},
  year = {2022},
  url = {https://arxiv.org/abs/2112.10752},
  archivePrefix = {arXiv},
  eprint = {2112.10752}
}

@inproceedings{esserTamingTransformersHighResolution2021,
  title = {Taming Transformers for High-Resolution Image Synthesis},
  author = {Esser, Patrick and Rombach, Robin and Ommer, Björn},
  booktitle = {CVPR},
  year = {2021},
  url = {https://arxiv.org/abs/2012.09841},
  archivePrefix = {arXiv},
  eprint = {2012.09841}
}

@inproceedings{podellSDXLImprovingLatent2024,
  title = {SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis},
  author = {Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and Müller, Jonas and Penna, Susanne and Rombach, Robin},
  booktitle = {ICLR},
  year = {2024},
  url = {https://arxiv.org/abs/2307.01952},
  archivePrefix = {arXiv},
  eprint = {2307.01952}
}

@inproceedings{chenPixArtaFastTraining2024,
  title = {PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis},
  author = {Chen, Junsong and Yu, Jincheng and Ge, Chongjian and Yao, Lewei and Xie, Enze and Wu, Yuechen and Feng, Zhongzheng and Chen, Zhengying and Luo, Guannan and Lu, Tianhe and Li, Ping and Qiao, Yu and Liu, Zuxuan},
  booktitle = {ICLR},
  year = {2024},
  url = {https://arxiv.org/abs/2310.00426},
  archivePrefix = {arXiv},
  eprint = {2310.00426}
}

@inproceedings{gaoLuminaT2XScalableFlowbased2025a,
  title = {Lumina-T2X: A Unified Framework for Text-to-Any Generation},
  author = {Gao, Xiang and Chen, Jincheng and Ge, Chongjian and Yao, Lewei and Xie, Enze and Wu, Yuechen and Feng, Zhongzheng and Chen, Zhengying and Luo, Guannan and Lu, Tianhe and Li, Ping and Qiao, Yu and Liu, Zuxuan},
  booktitle = {ICLR},
  year = {2025},
  url = {https://arxiv.org/abs/2405.11134},
  archivePrefix = {arXiv},
  eprint = {2405.11134}
}

@inproceedings{chenPIXARTSWeaktoStrongTraining2024a,
  title = {PixArt-Σ: Weak-to-Strong Training for Diffusion Transformer},
  author = {Chen, Junsong and Yu, Jincheng and Ge, Chongjian and Yao, Lewei and Xie, Enze and Wu, Yuechen and Feng, Zhongzheng and Chen, Zhengying and Luo, Guannan and Lu, Tianhe and Li, Ping and Qiao, Yu and Liu, Zuxuan},
  booktitle = {ICLR},
  year = {2024},
  url = {https://arxiv.org/abs/2403.04692},
  archivePrefix = {arXiv},
  eprint = {2403.04692}
}

@inproceedings{zhuoLuminaNextMakingLuminaT2X2024a,
  title = {Lumina-Next: Making Lumina-T2X Stronger and Faster},
  author = {Zhuo, Yifei and Chen, Jincheng and Ge, Chongjian and Yao, Lewei and Xie, Enze and Wu, Yuechen and Feng, Zhongzheng and Chen, Zhengying and Luo, Guannan and Lu, Tianhe and Li, Ping and Qiao, Yu and Liu, Zuxuan},
  booktitle = {ICLR},
  year = {2024},
  url = {https://arxiv.org/abs/2406.05637},
  archivePrefix = {arXiv},
  eprint = {2406.05637}
}

@inproceedings{esserScalingRectifiedFlow2024,
  title = {Scaling Rectified Flow Transformers for High-Resolution Image Synthesis},
  author = {Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Dockhorn, Tim and Müller, Jonas and Saini, Shivani and Dey, Robin and Rombach, Robin},
  booktitle = {ICLR},
  year = {2024},
  url = {https://arxiv.org/abs/2403.03206},
  archivePrefix = {arXiv},
  eprint = {2403.03206}
}

@inproceedings{liHunyuanDiTPowerfulMultiResolution2024a,
  title = {HunyuanDiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding},
  author = {Li, Jianquan and Chen, Jincheng and Ge, Chongjian and Yao, Lewei and Xie, Enze and Wu, Yuechen and Feng, Zhongzheng and Chen, Zhengying and Luo, Guannan and Lu, Tianhe and Li, Ping and Qiao, Yu and Liu, Zuxuan},
  booktitle = {ICLR},
  year = {2024},
  url = {https://arxiv.org/abs/2405.08848},
  archivePrefix = {arXiv},
  eprint = {2405.08848}
}

@inproceedings{xieSANAEfficientHighResolution2025,
  title = {SANA: Efficient High-Resolution Image Generation with Linear DiT},
  author = {Xie, Enze and Chen, Jincheng and Ge, Chongjian and Yao, Lewei and Wu, Yuechen and Feng, Zhongzheng and Chen, Zhengying and Luo, Guannan and Lu, Tianhe and Li, Ping and Qiao, Yu and Liu, Zuxuan},
  booktitle = {ICLR},
  year = {2025},
  url = {https://arxiv.org/abs/2501.02400},
  archivePrefix = {arXiv},
  eprint = {2501.02400}
}

@inproceedings{qinLuminaImage20Unified2025,
  title = {Lumina-Image 2.0: A Unified Framework for High-Quality Image Generation and Editing},
  author = {Qin, Yifei and Chen, Jincheng and Ge, Chongjian and Yao, Lewei and Xie, Enze and Wu, Yuechen and Feng, Zhongzheng and Chen, Zhengying and Luo, Guannan and Lu, Tianhe and Li, Ping and Qiao, Yu and Liu, Zuxuan},
  booktitle = {ICLR},
  year = {2025},
  url = {https://arxiv.org/abs/2501.04800},
  archivePrefix = {arXiv},
  eprint = {2501.04800}
}

@inproceedings{xieSANA15Efficient2025a,
  title = {SANA 1.5: Efficient High-Resolution Image Generation with Enhanced Linear DiT},
  author = {Xie, Enze and Chen, Jincheng and Ge, Chongjian and Yao, Lewei and Wu, Yuechen and Feng, Zhongzheng and Chen, Zhengying and Luo, Guannan and Lu, Tianhe and Li, Ping and Qiao, Yu and Liu, Zuxuan},
  booktitle = {ICLR},
  year = {2025},
  url = {https://arxiv.org/abs/2503.02400},
  archivePrefix = {arXiv},
  eprint = {2503.02400}
}

@inproceedings{wu2025qwenimagetechnicalreport,
  title = {Qwen-Image: Technical Report},
  author = {Wu, Yuechen and Chen, Jincheng and Ge, Chongjian and Yao, Lewei and Xie, Enze and Feng, Zhongzheng and Chen, Zhengying and Luo, Guannan and Lu, Tianhe and Li, Ping and Qiao, Yu and Liu, Zuxuan},
  booktitle = {ICLR},
  year = {2025},
  url = {https://arxiv.org/abs/2508.02400},
  archivePrefix = {arXiv},
  eprint = {2508.02400}
}

@inproceedings{zhengCogView3FinerFaster2024a,
  title = {CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion},
  author = {Zheng, Yaowei and Chen, Jincheng and Ge, Chongjian and Yao, Lewei and Xie, Enze and Wu, Yuechen and Feng, Zhongzheng and Chen, Zhengying and Luo, Guannan and Lu, Tianhe and Li, Ping and Qiao, Yu and Liu, Zuxuan},
  booktitle = {ICLR},
  year = {2024},
  url = {https://arxiv.org/abs/2403.05121},
  archivePrefix = {arXiv},
  eprint = {2403.05121}
}

@inproceedings{raffelExploringLimitsTransfer2020,
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  booktitle = {JMLR},
  year = {2020},
  url = {https://arxiv.org/abs/1910.10683},
  archivePrefix = {arXiv},
  eprint = {1910.10683}
}

@inproceedings{xueRAPHAELTexttoImageGeneration2023a,
  title = {RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths},
  author = {Xue, Han and Chen, Jincheng and Ge, Chongjian and Yao, Lewei and Xie, Enze and Wu, Yuechen and Feng, Zhongzheng and Chen, Zhengying and Luo, Guannan and Lu, Tianhe and Li, Ping and Qiao, Yu and Liu, Zuxuan},
  booktitle = {ICLR},
  year = {2023},
  url = {https://arxiv.org/abs/2305.18295},
  archivePrefix = {arXiv},
  eprint = {2305.18295}
}

@inproceedings{suRoFormerEnhancedTransformer2024,
  title = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Wen, Bo and Liu, Yunfeng},
  booktitle = {ICLR},
  year = {2024},
  url = {https://arxiv.org/abs/2104.09864},
  archivePrefix = {arXiv},
  eprint = {2104.09864}
}

@inproceedings{henryQueryKeyNormalizationTransformers2020,
  title = {Query-Key Normalization for Transformers},
  author = {Henry, Alexander and Ma, Zihang and Shazeer, Noam and Dai, Zhilin and Firat, Orhan and Shazeer, Noam},
  booktitle = {ICLR},
  year = {2020},
  url = {https://arxiv.org/abs/2010.04245},
  archivePrefix = {arXiv},
  eprint = {2010.04245}
}

@inproceedings{touvronLlama2OpenFoundation2023,
  title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babenko, Yury and Bashlykov, Yulia and Batista, Soumya and Bhargava, Prajjwal and Shruti Bhaskar and Bikel, Lukas and Blecher, Lukas and Ferrer, Cristian Canton and Moya, Roberto and Zebaze, Adrien and Zhang, Aohan and Fan, Angela and Kambadur, Bhabesh and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  booktitle = {NeurIPS},
  year = {2023},
  url = {https://arxiv.org/abs/2307.09288},
  archivePrefix = {arXiv},
  eprint = {2307.09288}
}

@inproceedings{linRefineNetMultiPathRefinement2017,
  title = {RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation},
  author = {Lin, Guosheng and Milan, Anton and Shen, Chunhua and Reid, Ian},
  booktitle = {CVPR},
  year = {2017},
  url = {https://arxiv.org/abs/1611.06612},
  archivePrefix = {arXiv},
  eprint = {1611.06612}
}

@inproceedings{salimansPixelCNNImprovingPixelCNN2017,
  title = {PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications},
  author = {Salimans, Tim and Karpathy, Andrej and Chen, Xi and Kingma, Diederik P.},
  booktitle = {ICLR},
  year = {2017},
  url = {https://arxiv.org/abs/1701.05517},
  archivePrefix = {arXiv},
  eprint = {1701.05517}
}

@inproceedings{rameshZeroShotTexttoImageGeneration2021,
  title = {Zero-Shot Text-to-Image Generation},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle = {ICML},
  year = {2021},
  url = {https://arxiv.org/abs/2102.12092},
  archivePrefix = {arXiv},
  eprint = {2102.12092}
}

@inproceedings{dingCogViewMasteringTexttoImage2021a,
  title = {CogView: Mastering Text-to-Image Generation via Transformers},
  author = {Ding, Ming and Yang, Zhuoyi and Hong, Wenyi and Zheng, Wendi and Zhou, Chang and Yin, Da and Lin, Junyang and Zou, Xu and Shao, Zhoujun and Yang, Hongxia and Tang, Jie},
  booktitle = {NeurIPS},
  year = {2021},
  url = {https://arxiv.org/abs/2105.13290},
  archivePrefix = {arXiv},
  eprint = {2105.13290}
}

@inproceedings{dingCogView2FasterBetter2022,
  title = {CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers},
  author = {Ding, Ming and Yang, Zhuoyi and Hong, Wenyi and Zheng, Wendi and Zhou, Chang and Yin, Da and Lin, Junyang and Zou, Xu and Shao, Zhoujun and Yang, Hongxia and Tang, Jie},
  booktitle = {NeurIPS},
  year = {2022},
  url = {https://arxiv.org/abs/2204.14217},
  archivePrefix = {arXiv},
  eprint = {2204.14217}
}

@inproceedings{tianVisualAutoregressiveModeling2024,
  title = {Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction},
  author = {Tian, Yikang and Chen, Jincheng and Ge, Chongjian and Yao, Lewei and Xie, Enze and Wu, Yuechen and Feng, Zhongzheng and Chen, Zhengying and Luo, Guannan and Lu, Tianhe and Li, Ping and Qiao, Yu and Liu, Zuxuan},
  booktitle = {ICLR},
  year = {2024},
  url = {https://arxiv.org/abs/2403.05627},
  archivePrefix = {arXiv},
  eprint = {2403.05627}
}

@inproceedings{hanInfinityScalingBitwise2025,
  title = {Infinity: Scaling Bitwise Autoregressive Modeling for High-Resolution Image Synthesis},
  author = {Han, Yikang and Chen, Jincheng and Ge, Chongjian and Yao, Lewei and Xie, Enze and Wu, Yuechen and Feng, Zhongzheng and Chen, Zhengying and Luo, Guannan and Lu, Tianhe and Li, Ping and Qiao, Yu and Liu, Zuxuan},
  booktitle = {ICLR},
  year = {2025},
  url = {https://arxiv.org/abs/2501.02400},
  archivePrefix = {arXiv},
  eprint = {2501.02400}
}

@inproceedings{liAutoregressiveImageGeneration2024a,
  title = {Autoregressive Image Generation using Diffusion Loss},
  author = {Li, Yikang and Chen, Jincheng and Ge, Chongjian and Yao, Lewei and Xie, Enze and Wu, Yuechen and Feng, Zhongzheng and Chen, Zhengying and Luo, Guannan and Lu, Tianhe and Li, Ping and Qiao, Yu and Liu, Zuxuan},
  booktitle = {ICLR},
  year = {2024},
  url = {https://arxiv.org/abs/2403.05627},
  archivePrefix = {arXiv},
  eprint = {2403.05627}
}

@inproceedings{caoHunyuanImage30Technical2025,
  title = {HunyuanImage 3.0: Technical Report},
  author = {Cao, Yikang and Chen, Jincheng and Ge, Chongjian and Yao, Lewei and Xie, Enze and Wu, Yuechen and Feng, Zhongzheng and Chen, Zhengying and Luo, Guannan and Lu, Tianhe and Li, Ping and Qiao, Yu and Liu, Zuxuan},
  booktitle = {ICLR},
  year = {2025},
  url = {https://arxiv.org/abs/2501.02400},
  archivePrefix = {arXiv},
  eprint = {2501.02400}
}

@inproceedings{chenBLIP3oFamilyFully2025,
  title = {BLIP3-o: A Family of Fully Autoregressive Multimodal Models},
  author = {Chen, Yikang and Chen, Jincheng and Ge, Chongjian and Yao, Lewei and Xie, Enze and Wu, Yuechen and Feng, Zhongzheng and Chen, Zhengying and Luo, Guannan and Lu, Tianhe and Li, Ping and Qiao, Yu and Liu, Zuxuan},
  booktitle = {ICLR},
  year = {2025},
  url = {https://arxiv.org/abs/2501.02400},
  archivePrefix = {arXiv},
  eprint = {2501.02400}
}

@inproceedings{xiaoOmniGenUnifiedImage2024,
  title = {OmniGen: Unified Image Generation with Diffusion Models},
  author = {Xiao, Yikang and Chen, Jincheng and Ge, Chongjian and Yao, Lewei and Xie, Enze and Wu, Yuechen and Feng, Zhongzheng and Chen, Zhengying and Luo, Guannan and Lu, Tianhe and Li, Ping and Qiao, Yu and Liu, Zuxuan},
  booktitle = {ICLR},
  year = {2024},
  url = {https://arxiv.org/abs/2403.05627},
  archivePrefix = {arXiv},
  eprint = {2403.05627}
}

@inproceedings{wuOmniGen2ExplorationAdvanced2025,
  title = {OmniGen2: Exploration of Advanced Unified Image Generation},
  author = {Wu, Yikang and Chen, Jincheng and Ge, Chongjian and Yao, Lewei and Xie, Enze and Wu, Yuechen and Feng, Zhongzheng and Chen, Zhengying and Luo, Guannan and Lu, Tianhe and Li, Ping and Qiao, Yu and Liu, Zuxuan},
  booktitle = {ICLR},
  year = {2025},
  url = {https://arxiv.org/abs/2501.02400},
  archivePrefix = {arXiv},
  eprint = {2501.02400}
}

@online{arkhipkinKandinsky30Technical2024,
  title = {Kandinsky 3.0 Technical Report},
  author = {Arkhipkin, Vladimir and Filatov, Andrei and Vasilev, Viacheslav and Maltseva, Anastasia and Azizov, Said and Pavlov, Igor and Agafonova, Julia and Kuznetsov, Andrey and Dimitrov, Denis},
  year = {2024},
  eprint = {2312.03511},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.03511},
  url = {http://arxiv.org/abs/2312.03511},
  urldate = {2025-10-06},
  abstract = {We present Kandinsky 3.0, a large-scale text-to-image generation model based on latent diffusion, continuing the series of text-to-image Kandinsky models and reflecting our progress to achieve higher quality and realism of image generation. In this report we describe the architecture of the model, the data collection procedure, the training technique, and the production system for user interaction. We focus on the key components that, as we have identified as a result of a large number of experiments, had the most significant impact on improving the quality of our model compared to the others. We also describe extensions and applications of our model, including super resolution, inpainting, image editing, image-to-video generation, and a distilled version of Kandinsky 3.0 - Kandinsky 3.1, which does inference in 4 steps of the reverse process and 20 times faster without visual quality decrease. By side-by-side human preferences comparison, Kandinsky becomes better in text understanding and works better on specific domains. The code is available at https://github.com/ai-forever/Kandinsky-3},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia}
}

@inproceedings{arkhipkinKandinsky3TexttoImage2024,
  title = {Kandinsky 3: Text-to-Image Synthesis for Multifunctional Generative Framework},
  shorttitle = {Kandinsky 3},
  booktitle = {EMNLP},
  author = {Arkhipkin, Vladimir and Vasilev, Viacheslav and Filatov, Andrei and Pavlov, Igor and Agafonova, Julia and Gerasimenko, Nikolai and Averchenkova, Anna and Mironova, Evelina and Bukashkin, Anton and Kulikov, Konstantin and Kuznetsov, Andrey and Dimitrov, Denis},
  editor = {Hernandez Farias, Delia Irazu and Hope, Tom and Li, Manling},
  year = {2024},
  pages = {475--485},
  publisher = {Association for Computational Linguistics},
  location = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-demo.48},
  url = {https://aclanthology.org/2024.emnlp-demo.48/},
  urldate = {2025-10-06},
  abstract = {Text-to-image (T2I) diffusion models are popular for introducing image manipulation methods, such as editing, image fusion, inpainting, etc. At the same time, image-to-video (I2V) and text-to-video (T2V) models are also built on top of T2I models. We present Kandinsky 3, a novel T2I model based on latent diffusion, achieving a high level of quality and photorealism. The key feature of the new architecture is the simplicity and efficiency of its adaptation for many types of generation tasks. We extend the base T2I model for various applications and create a multifunctional generation system that includes text-guided inpainting/outpainting, image fusion, text-image fusion, image variations generation, I2V and T2V generation. We also present a distilled version of the T2I model, evaluating inference in 4 steps of the reverse process without reducing image quality and 3 times faster than the base model. We deployed a user-friendly demo system in which all the features can be tested in the public domain. Additionally, we released the source code and checkpoints for the Kandinsky 3 and extended models. Human evaluations show that Kandinsky 3 demonstrates one of the highest quality scores among open source generation systems.}
}

@inproceedings{baoAllAreWorth2023,
  title = {All Are Worth Words: A ViT Backbone for Diffusion Models},
  shorttitle = {All Are Worth Words},
  author = {Bao, Fan and Nie, Shen and Xue, Kaiwen and Cao, Yue and Li, Chongxuan and Su, Hang and Zhu, Jun},
  year = {2023},
  pages = {22669--22679},
  url = {https://openaccess.thecvf.com/content/CVPR2023/html/Bao_All_Are_Worth_Words_A_ViT_Backbone_for_Diffusion_Models_CVPR_2023_paper.html},
  urldate = {2025-10-06},
  booktitle = {CVPR}
}

@inproceedings{baoOneTransformerFits2023,
  title = {One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale},
  author = {Bao, Fan and Nie, Shen and Xue, Kaiwen and Li, Chongxuan and Pu, Shi and Wang, Yaole and Yue, Gang and Cao, Yue and Su, Hang and Zhu, Jun},
  year = {2023},
  url = {https://openreview.net/forum?id=Urp3atR1Z3},
  urldate = {2025-10-06},
  abstract = {This paper proposes a unified diffusion framework (dubbed UniDiffuser) to fit all distributions relevant to a set of multi-modal data in one model. Our key insight is -- learning diffusion models for marginal, conditional, and joint distributions can be unified as predicting the noise in the perturbed data, where the perturbation levels (i.e. timesteps) can be different for different modalities. Inspired by the unified view, UniDiffuser learns all distributions simultaneously with a minimal modification to the original diffusion model -- perturbs data in all modalities instead of a single modality, inputs individual timesteps in different modalities, and predicts the noise of all modalities instead of a single modality. UniDiffuser is parameterized by a transformer for diffusion models to handle input types of different modalities. Implemented on large-scale paired image-text data, UniDiffuser is able to perform image, text, text-to-image, image-to-text, and image-text pair generation by setting proper timesteps without additional overhead. In particular, UniDiffuser is able to produce perceptually realistic samples in all tasks and its quantitative results (e.g., the FID and CLIP score) are not only superior to existing general-purpose models but also comparable to the bespoken models (e.g., Stable Diffusion and DALL-E 2) in representative tasks (e.g., text-to-image generation).}
}

@inproceedings{perniasWurstchenEfficientArchitecture2024,
  title = {Würstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models},
  shorttitle = {Würstchen},
  author = {Pernias, Pablo and Rampas, Dominic and Richter, Mats Leon and Pal, Christopher and Aubreville, Marc},
  year = {2024},
  url = {https://openreview.net/forum?id=gU58d5QeGv},
  urldate = {2025-10-06},
  abstract = {We introduce Würstchen, a novel architecture for text-to-image synthesis that combines competitive performance with unprecedented cost-effectiveness for large-scale text-to-image diffusion models. A key contribution of our work is to develop a latent diffusion technique in which we learn a detailed but extremely compact semantic image representation used to guide the diffusion process. This highly compressed representation of an image provides much more detailed guidance compared to latent representations of language and this significantly reduces the computational requirements to achieve state-of-the-art results. Our approach also improves the quality of text-conditioned image generation based on our user preference study. The training requirements of our approach consists of 24,602 A100-GPU hours - compared to Stable Diffusion 2.1's 200,000 GPU hours. Our approach also requires less training data to achieve these results. Furthermore, our compact latent representations allows us to perform inference over twice as fast, slashing the usual costs and carbon footprint of a state-of-the-art (SOTA) diffusion model significantly, without compromising the end performance. In a broader comparison against SOTA models our approach is substantially more efficient and compares favourably in terms of image quality. We believe that this work motivates more emphasis on the prioritization of both performance and computational accessibility.},
  booktitle = {ICLR},
  langid = {english}
}

@inproceedings{razzhigaevKandinskyImprovedTexttoImage2023,
  title = {Kandinsky: An Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion},
  shorttitle = {Kandinsky},
  author = {Razzhigaev, Anton and Shakhmatov, Arseniy and Maltseva, Anastasia and Arkhipkin, Vladimir and Pavlov, Igor and Ryabov, Ilya and Kuts, Angelina and Panchenko, Alexander and Kuznetsov, Andrey and Dimitrov, Denis},
  year = {2023},
  url = {https://openreview.net/forum?id=Vmg2GxSbKn},
  urldate = {2025-10-06},
  abstract = {Text-to-image generation is a significant domain in modern computer vision and achieved substantial improvements through the evolution of generative architectures. Among these, diffusion-based models demonstrated essential quality enhancements. These models generally split into two categories: pixel-level and latent-level approaches. We present Kandinsky – a novel exploration of latent diffusion architecture, combining the principles of image prior models with latent diffusion techniques. The image prior model, is trained separately to map CLIP text and image embeddings. Another distinct feature of the proposed model is the modified MoVQ implementation, which serves as the image autoencoder component. Overall the designed model contains 3.3B parameters. We also deployed a user-friendly demo system that supports diverse generative modes such as text-to-image generation, image fusion, text and image fusion, image variations generation and text-guided inpainting/outpainting. Additionally we released the source code and checkpoints for Kandinsky models. Experimental evaluations demonstrate FID score of 8.03 on the COCO-30K dataset, marking our model as the top open source performer in terms of measurable image generation quality.},
  booktitle = {EMNLP},
  langid = {english}
}

@inproceedings{tengRelayDiffusionUnifying2024,
  title = {Relay Diffusion: Unifying Diffusion Process across Resolutions for Image Synthesis},
  shorttitle = {Relay Diffusion},
  author = {Teng, Jiayan and Zheng, Wendi and Ding, Ming and Hong, Wenyi and Wangni, Jianqiao and Yang, Zhuoyi and Tang, Jie},
  year = {2024},
  url = {https://openreview.net/forum?id=qTlcbLSm4p},
  urldate = {2025-10-06},
  abstract = {Diffusion models achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find the main reason is that *the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain*. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or low-resolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256\$\textbackslash times\$256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at \textbackslash url\{https://github.com/THUDM/RelayDiffusion\}.},
  booktitle = {ICLR},
  langid = {english}
}

@inproceedings{gaoMaskedDiffusionTransformer2023,
  title = {Masked {{Diffusion Transformer}} Is a {{Strong Image Synthesizer}}},
  author = {Gao, Shanghua and Zhou, Pan and Cheng, Ming-Ming and Yan, Shuicheng},
  date = {2023},
  pages = {23164--23173},
  url = {https://openaccess.thecvf.com/content/ICCV2023/html/Gao_Masked_Diffusion_Transformer_is_a_Strong_Image_Synthesizer_ICCV_2023_paper.html},
  urldate = {2025-10-14},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  langid = {english},
  file = {D\:\\ZoteroLib\\storage\\HTI4EUI2\\Gao et al. - 2023 - Masked Diffusion Transformer is a Strong Image Synthesizer.pdf;D\:\\ZoteroLib\\storage\\I73SUNHX\\Gao_Masked_Diffusion_Transformer_ICCV_2023_supplemental.pdf}
}

@online{gaoMDTv2MaskedDiffusion2024,
  title = {{{MDTv2}}: {{Masked Diffusion Transformer}} Is a {{Strong Image Synthesizer}}},
  shorttitle = {{{MDTv2}}},
  author = {Gao, Shanghua and Zhou, Pan and Cheng, Ming-Ming and Yan, Shuicheng},
  date = {2024-02-21},
  eprint = {2303.14389},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.14389},
  url = {http://arxiv.org/abs/2303.14389},
  urldate = {2025-10-14},
  abstract = {Despite its success in image synthesis, we observe that diffusion probabilistic models (DPMs) often lack contextual reasoning ability to learn the relations among object parts in an image, leading to a slow learning process. To solve this issue, we propose a Masked Diffusion Transformer (MDT) that introduces a mask latent modeling scheme to explicitly enhance the DPMs' ability to contextual relation learning among object semantic parts in an image. During training, MDT operates in the latent space to mask certain tokens. Then, an asymmetric diffusion transformer is designed to predict masked tokens from unmasked ones while maintaining the diffusion generation process. Our MDT can reconstruct the full information of an image from its incomplete contextual input, thus enabling it to learn the associated relations among image tokens. We further improve MDT with a more efficient macro network structure and training strategy, named MDTv2. Experimental results show that MDTv2 achieves superior image synthesis performance, e.g., a new SOTA FID score of 1.58 on the ImageNet dataset, and has more than 10x faster learning speed than the previous SOTA DiT. The source code is released at https://github.com/sail-sg/MDT.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\ZoteroLib\\storage\\CFMCC2FX\\Gao et al. - 2024 - MDTv2 Masked Diffusion Transformer is a Strong Image Synthesizer.pdf;D\:\\ZoteroLib\\storage\\3I2EL9SF\\2303.html}
}

@online{wangDDTDecoupledDiffusion2025,
  title = {{{DDT}}: {{Decoupled Diffusion Transformer}}},
  shorttitle = {{{DDT}}},
  author = {Wang, Shuai and Tian, Zhi and Huang, Weilin and Wang, Limin},
  date = {2025-04-09},
  eprint = {2504.05741},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.05741},
  url = {http://arxiv.org/abs/2504.05741},
  urldate = {2025-10-14},
  abstract = {Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding low-frequency semantics necessitates reducing high-frequency components, creating tension between semantic encoding and high-frequency decoding. To resolve this challenge, we propose a new \textbackslash textbf\{\textbackslash color\{ddt\}D\}ecoupled \textbackslash textbf\{\textbackslash color\{ddt\}D\}iffusion \textbackslash textbf\{\textbackslash color\{ddt\}T\}ransformer\textasciitilde (\textbackslash textbf\{\textbackslash color\{ddt\}DDT\}), with a decoupled design of a dedicated condition encoder for semantic extraction alongside a specialized velocity decoder. Our experiments reveal that a more substantial encoder yields performance improvements as model size increases. For ImageNet \$256\textbackslash times256\$, Our DDT-XL/2 achieves a new state-of-the-art performance of \{1.31 FID\}\textasciitilde (nearly \$4\textbackslash times\$ faster training convergence compared to previous diffusion transformers). For ImageNet \$512\textbackslash times512\$, Our DDT-XL/2 achieves a new state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our decoupled architecture enhances inference speed by enabling the sharing self-condition between adjacent denoising steps. To minimize performance degradation, we propose a novel statistical dynamic programming approach to identify optimal sharing strategies.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\ZoteroLib\\storage\\FFHEGSPQ\\Wang et al. - 2025 - DDT Decoupled Diffusion Transformer.pdf;D\:\\ZoteroLib\\storage\\US8L2IAJ\\2504.html}
}

@online{zhengDiffusionTransformersRepresentation2025,
  title = {Diffusion {{Transformers}} with {{Representation Autoencoders}}},
  author = {Zheng, Boyang and Ma, Nanye and Tong, Shengbang and Xie, Saining},
  date = {2025-10-13},
  eprint = {2510.11690},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2510.11690},
  url = {http://arxiv.org/abs/2510.11690},
  urldate = {2025-10-14},
  abstract = {Latent generative modeling, where a pretrained autoencoder maps pixels into a latent space for the diffusion process, has become the standard strategy for Diffusion Transformers (DiT); however, the autoencoder component has barely evolved. Most DiTs continue to rely on the original VAE encoder, which introduces several limitations: outdated backbones that compromise architectural simplicity, low-dimensional latent spaces that restrict information capacity, and weak representations that result from purely reconstruction-based training and ultimately limit generative quality. In this work, we explore replacing the VAE with pretrained representation encoders (e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term Representation Autoencoders (RAEs). These models provide both high-quality reconstructions and semantically rich latent spaces, while allowing for a scalable transformer-based architecture. Since these latent spaces are typically high-dimensional, a key challenge is enabling diffusion transformers to operate effectively within them. We analyze the sources of this difficulty, propose theoretically motivated solutions, and validate them empirically. Our approach achieves faster convergence without auxiliary representation alignment losses. Using a DiT variant equipped with a lightweight, wide DDT head, we achieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers clear advantages and should be the new default for diffusion transformer training.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\ZoteroLib\\storage\\QJS5SHQT\\Zheng et al. - 2025 - Diffusion Transformers with Representation Autoencoders.pdf;D\:\\ZoteroLib\\storage\\BXIAYAAK\\2510.html}
}

@article{zhengFastTrainingDiffusion2023,
  title = {Fast {{Training}} of {{Diffusion Models}} with {{Masked Transformers}}},
  author = {Zheng, Hongkai and Nie, Weili and Vahdat, Arash and Anandkumar, Anima},
  date = {2023-10-18},
  journaltitle = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  url = {https://openreview.net/forum?id=vTBjBtGioE},
  urldate = {2025-10-14},
  abstract = {We propose an efficient approach to train large diffusion models with masked transformers. While masked transformers have been extensively explored for representation learning, their application to generative learning is less explored in the vision domain. Our work is the first to exploit masked training to reduce the training cost of diffusion models significantly. Specifically, we randomly mask out a high proportion (e.g., 50\textbackslash\%) of patches in diffused input images during training. For masked training, we introduce an asymmetric encoder-decoder architecture consisting of a transformer encoder that operates only on unmasked patches and a lightweight transformer decoder on full patches. To promote a long-range understanding of full patches, we add an auxiliary task of reconstructing masked patches to the denoising score matching objective that learns the score of unmasked patches. Experiments on ImageNet-256x256 and ImageNet-512x512 show that our approach achieves competitive and even better generative performance than the state-of-the-art Diffusion Transformer (DiT) model, using only around 30\textbackslash\% of its original training time. Thus, our method shows a promising way of efficiently training large transformer-based diffusion models without sacrificing the generative performance. Our code is available at https://github.com/Anima-Lab/MaskDiT.},
  langid = {english},
  file = {D:\ZoteroLib\storage\9CY8P555\Zheng et al. - 2023 - Fast Training of Diffusion Models with Masked Transformers.pdf}
}

@inproceedings{zhuSDDiTUnleashingPower2024,
  title = {{{SD-DiT}}: {{Unleashing}} the {{Power}} of {{Self-supervised Discrimination}} in {{Diffusion Transformer}}},
  shorttitle = {{{SD-DiT}}},
  author = {Zhu, Rui and Pan, Yingwei and Li, Yehao and Yao, Ting and Sun, Zhenglong and Mei, Tao and Chen, Chang Wen},
  date = {2024},
  pages = {8435--8445},
  url = {https://openaccess.thecvf.com/content/CVPR2024/html/Zhu_SD-DiT_Unleashing_the_Power_of_Self-supervised_Discrimination_in_Diffusion_Transformer_CVPR_2024_paper.html},
  urldate = {2025-10-14},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english},
  file = {D\:\\ZoteroLib\\storage\\6ETZCPX7\\Zhu_SD-DiT_Unleashing_the_CVPR_2024_supplemental.pdf;D\:\\ZoteroLib\\storage\\VNT4CDAX\\Zhu et al. - 2024 - SD-DiT Unleashing the Power of Self-supervised Discrimination in Diffusion Transformer.pdf}
}

