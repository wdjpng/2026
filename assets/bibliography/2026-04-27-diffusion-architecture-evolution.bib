



@online{arkhipkinKandinsky30Technical2024,
  title = {Kandinsky 3.0 Technical Report},
  author = {Arkhipkin, Vladimir and Filatov, Andrei and Vasilev, Viacheslav and Maltseva, Anastasia and Azizov, Said and Pavlov, Igor and Agafonova, Julia and Kuznetsov, Andrey and Dimitrov, Denis},
  date = {2024-06-28},
  year = {2024},
  eprint = {2312.03511},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.03511},
  url = {http://arxiv.org/abs/2312.03511},
  urldate = {2025-10-06},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia},
}

@inproceedings{arkhipkinKandinsky3TexttoImage2024,
  title = {Kandinsky 3: Text-to-Image Synthesis for Multifunctional Generative Framework},
  shorttitle = {Kandinsky 3},
  booktitle = {EMNLP},
  author = {Arkhipkin, Vladimir and Vasilev, Viacheslav and Filatov, Andrei and Pavlov, Igor and Agafonova, Julia and Gerasimenko, Nikolai and Averchenkova, Anna and Mironova, Evelina and Bukashkin, Anton and Kulikov, Konstantin and Kuznetsov, Andrey and Dimitrov, Denis},
  editor = {Hernandez Farias, Delia Irazu and Hope, Tom and Li, Manling},
  date = {2024-11},
  year = {2024},
  pages = {475--485},
  publisher = {Association for Computational Linguistics},
  location = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-demo.48},
  url = {https://aclanthology.org/2024.emnlp-demo.48/},
  urldate = {2025-10-06},
}

@inproceedings{avrahamiStableFlowVital2025,
  title = {Stable Flow: Vital Layers for Training-Free Image Editing},
  shorttitle = {Stable Flow},
  author = {Avrahami, Omri and Patashnik, Or and Fried, Ohad and Nemchinov, Egor and Aberman, Kfir and Lischinski, Dani and Cohen-Or, Daniel},
  date = {2025},
  year = {2025},
  pages = {7877--7888},
  url = {https://openaccess.thecvf.com/content/CVPR2025/html/Avrahami_Stable_Flow_Vital_Layers_for_Training-Free_Image_Editing_CVPR_2025_paper.html},
  urldate = {2025-06-23},
  booktitle = {CVPR},
  langid = {english},
}

@inproceedings{Bao_2023_CVPR,
  title = {All Are Worth Words: A ViT Backbone for Diffusion Models},
  shorttitle = {All Are Worth Words},
  booktitle = {CVPR},
  author = {Bao, Fan and Nie, Shen and Xue, Kaiwen and Cao, Yue and Li, Chongxuan and Su, Hang and Zhu, Jun},
  date = {2023},
  year = {2023},
  pages = {22669--22679},
  url = {https://openaccess.thecvf.com/content/CVPR2023/html/Bao_All_Are_Worth_Words_A_ViT_Backbone_for_Diffusion_Models_CVPR_2023_paper.html},
  urldate = {2024-06-19},
  booktitle = {CVPR},
  langid = {english},
}

@inproceedings{baoOneTransformerFits2023,
  title = {One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale},
  author = {Bao, Fan and Nie, Shen and Xue, Kaiwen and Li, Chongxuan and Pu, Shi and Wang, Yaole and Yue, Gang and Cao, Yue and Su, Hang and Zhu, Jun},
  date = {2023-06-15},
  year = {2023},
  booktitle = {ICML},
  url = {https://openreview.net/forum?id=Urp3atR1Z3},
  urldate = {2025-10-06},
}

@inproceedings{bar-talText2LIVETextDrivenLayered2022,
  title = {Text2LIVE: Text-Driven Layered Image and Video Editing},
  shorttitle = {Text2LIVE},
  booktitle = {ECCV},
  author = {Bar-Tal, Omer and Ofri-Amar, Dolev and Fridman, Rafail and Kasten, Yoni and Dekel, Tali},
  editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  date = {2022},
  year = {2022},
  volume = {13675},
  pages = {707--723},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-19784-0_41},
  url = {https://link.springer.com/10.1007/978-3-031-19784-0_41},
  urldate = {2024-11-26},
  isbn = {978-3-031-19783-3 978-3-031-19784-0},
  langid = {english},
}

@online{betkerImprovingImageGeneration2023,
  title = {Improving Image Generation with Better Captions},
  author = {Betker, James and Goh, Gabriel and Jing, Li and Brooks, Tim and Wang, Jianfeng and Li, Linjie and Ouyang, Long and Zhuang, Juntang and Lee, Joyce and Guo, Yufei and Manassra, Wesam and Dhariwal, Prafulla and Chu, Casey and Jiao, Yunxin and Ramesh, Aditya},
  date = {2023},
  year = {2023},
  langid = {english},
  pubstate = {prepublished},
  keywords = {DALL-E 3,OpenAI},
}

@inproceedings{brooksInstructPix2PixLearningFollow2023a,
  title = {InstructPix2Pix: Learning To Follow Image Editing Instructions},
  shorttitle = {InstructPix2Pix},
  author = {Brooks, Tim and Holynski, Aleksander and Efros, Alexei A.},
  date = {2023},
  year = {2023},
  pages = {18392--18402},
  url = {https://openaccess.thecvf.com/content/CVPR2023/html/Brooks_InstructPix2Pix_Learning_To_Follow_Image_Editing_Instructions_CVPR_2023_paper.html},
  urldate = {2024-07-20},
  booktitle = {CVPR},
  langid = {english},
  keywords = {InstructPix2Pix},
}

@online{caiHiDreamI1HighEfficientImage2025,
  title = {HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer},
  shorttitle = {HiDream-I1},
  author = {Cai, Qi and Chen, Jingwen and Chen, Yang and Li, Yehao and Long, Fuchen and Pan, Yingwei and Qiu, Zhaofan and Zhang, Yiheng and Gao, Fengbin and Xu, Peihan and Wang, Yimeng and Yu, Kai and Chen, Wenxuan and Feng, Ziwei and Gong, Zijian and Pan, Jianzhuang and Peng, Yi and Tian, Rui and Wang, Siyu and Zhao, Bo and Yao, Ting and Mei, Tao},
  date = {2025-05-28},
  year = {2025},
  eprint = {2505.22705},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2505.22705},
  url = {http://arxiv.org/abs/2505.22705},
  urldate = {2025-06-03},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Multimedia},
}

@online{caoHunyuanImage30Technical2025,
  title = {HunyuanImage 3.0 Technical Report},
  author = {Cao, Siyu and Chen, Hangting and Chen, Peng and Cheng, Yiji and Cui, Yutao and Deng, Xinchi and Dong, Ying and Gong, Kipper and Gu, Tianpeng and Gu, Xiusen and Hang, Tiankai and Huang, Duojun and Jiang, Jie and Jiang, Zhengkai and Kong, Weijie and Li, Changlin and Li, Donghao and Li, Junzhe and Li, Xin and Li, Yang and Li, Zhenxi and Li, Zhimin and Lin, Jiaxin and Linus and Liu, Lucaz and Liu, Shu and Liu, Songtao and Liu, Yu and Liu, Yuhong and Long, Yanxin and Lu, Fanbin and Lu, Qinglin and Peng, Yuyang and Peng, Yuanbo and Shen, Xiangwei and Shi, Yixuan and Tao, Jiale and Tao, Yangyu and Tian, Qi and Wan, Pengfei and Wang, Chunyu and Wang, Kai and Wang, Lei and Wang, Linqing and Wang, Lucas and Wang, Qixun and Wang, Weiyan and Wen, Hao and Wu, Bing and Wu, Jianbing and Wu, Yue and Xie, Senhao and Yang, Fang and Yang, Miles and Yang, Xiaofeng and Yang, Xuan and Yang, Zhantao and Yu, Jingmiao and Yuan, Zheng and Zhang, Chao and Zhang, Jian-Wei and Zhang, Peizhen and Zhang, Shi-Xue and Zhang, Tao and Zhang, Weigang and Zhang, Yepeng and Zhang, Yingfang and Zhang, Zihao and Zhang, Zijian and Zhao, Penghao and Zhao, Zhiyuan and Zhe, Xuefei and Zhu, Jianchen and Zhong, Zhao},
  date = {2025-09-28},
  year = {2025},
  eprint = {2509.23951},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2509.23951},
  url = {http://arxiv.org/abs/2509.23951},
  urldate = {2025-10-04},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@online{chaoMaskedUnmaskedDiscrete2025,
  title = {Beyond Masked and Unmasked: Discrete Diffusion Models via Partial Masking},
  shorttitle = {Beyond Masked and Unmasked},
  author = {Chao, Chen-Hao and Sun, Wei-Fang and Liang, Hanwen and Lee, Chun-Yi and Krishnan, Rahul G.},
  date = {2025-05-24},
  year = {2025},
  eprint = {2505.18495},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2505.18495},
  url = {http://arxiv.org/abs/2505.18495},
  urldate = {2025-10-05},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
}

@online{chenBLIP3oFamilyFully2025,
  title = {BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset},
  shorttitle = {BLIP3-o},
  author = {Chen, Jiuhai and Xu, Zhiyang and Pan, Xichen and Hu, Yushi and Qin, Can and Goldstein, Tom and Huang, Lifu and Zhou, Tianyi and Xie, Saining and Savarese, Silvio and Xue, Le and Xiong, Caiming and Xu, Ran},
  date = {2025-05-14},
  year = {2025},
  eprint = {2505.09568},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2505.09568},
  url = {http://arxiv.org/abs/2505.09568},
  urldate = {2025-05-17},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{chenGenerativePretrainingPixels2020,
  title = {Generative Pretraining From Pixels},
  booktitle = {ICML},
  author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  date = {2020-11-21},
  year = {2020},
  pages = {1691--1703},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/chen20s.html},
  urldate = {2024-08-20},
  booktitle = {ICML},
  langid = {english},
  keywords = {OpenAI},
}

@online{chenOpenGPT4oImageComprehensive2025,
  title = {OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing},
  shorttitle = {OpenGPT-4o-Image},
  author = {Chen, Zhihong and Bai, Xuehai and Shi, Yang and Fu, Chaoyou and Zhang, Huanyu and Wang, Haotian and Sun, Xiaoyan and Zhang, Zhang and Wang, Liang and Zhang, Yuanxing and Wan, Pengfei and Zhang, Yi-Fan},
  date = {2025-09-29},
  year = {2025},
  eprint = {2509.24900},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2509.24900},
  url = {http://arxiv.org/abs/2509.24900},
  urldate = {2025-10-03},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{chenPixArtaFastTraining2024,
  title = {PixArt-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis},
  shorttitle = {PixArt-\$\textbackslash alpha\$},
  author = {Chen, Junsong and Yu, Jincheng and Ge, Chongjian and Yao, Lewei and Xie, Enze and Wang, Zhongdao and Kwok, James and Luo, Ping and Lu, Huchuan and Li, Zhenguo},
  date = {2024},
  year = {2024},
  url = {https://openreview.net/forum?id=eAKmQPe3m1},
  urldate = {2025-01-11},
  booktitle = {ICLR},
  langid = {english},
}

@inproceedings{chenPIXARTdFastControllable2024a,
  title = {PIXART-δ: Fast and Controllable Image Generation with Latent Consistency Models},
  shorttitle = {PIXART-δ},
  author = {Chen, Junsong and Luo, Simian and Xie, Enze},
  date = {2024-07-03},
  year = {2024},
  url = {https://openreview.net/forum?id=Dyi0hXiQ9R},
  urldate = {2025-08-28},
  booktitle = {ICMLW},
  langid = {english},
}

@inproceedings{chenPIXARTSWeaktoStrongTraining2024a,
  title = {PIXART-Σ: Weak-to-Strong Training of~Diffusion Transformer for~4K Text-to-Image Generation},
  shorttitle = {PIXART-\$\$\textbackslashSigma \$\$},
  booktitle = {ECCV},
  author = {Chen, Junsong and Ge, Chongjian and Xie, Enze and Wu, Yue and Yao, Lewei and Ren, Xiaozhe and Wang, Zhongdao and Luo, Ping and Lu, Huchuan and Li, Zhenguo},
  editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
  date = {2024},
  year = {2024},
  pages = {74--91},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-73411-3_5},
  isbn = {978-3-031-73411-3},
  langid = {english},
  keywords = {Diffusion Transformer,Efficient Model,T2I Synthesis},
}

@online{chenSANASprintOneStepDiffusion2025,
  title = {SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation},
  shorttitle = {SANA-Sprint},
  author = {Chen, Junsong and Xue, Shuchen and Zhao, Yuyang and Yu, Jincheng and Paul, Sayak and Chen, Junyu and Cai, Han and Xie, Enze and Han, Song},
  date = {2025-03-23},
  year = {2025},
  eprint = {2503.09641},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.09641},
  url = {http://arxiv.org/abs/2503.09641},
  urldate = {2025-05-11},
  pubstate = {prepublished},
  keywords = {Computer Science - Graphics},
}

@online{comaniciGemini25Pushing2025,
  title = {Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities},
  shorttitle = {Gemini 2.5},
  author = {Gemini Team},
  date = {2025-07-07},
  year = {2025},
  eprint = {2507.06261},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2507.06261},
  url = {http://arxiv.org/abs/2507.06261},
  urldate = {2025-07-13},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
}

@inproceedings{couaironDiffEditDiffusionbasedSemantic2023,
  title = {DiffEdit: Diffusion-based Semantic Image Editing with Mask Guidance},
  shorttitle = {DiffEdit},
  author = {Couairon, Guillaume and Verbeek, Jakob and Schwenk, Holger and Cord, Matthieu},
  date = {2023},
  year = {2023},
  url = {https://openreview.net/forum?id=3lge0p5o-M-},
  urldate = {2024-07-20},
  booktitle = {ICLR},
}

@online{dengEmergingPropertiesUnified2025,
  title = {Emerging Properties in Unified Multimodal Pretraining},
  author = {Deng, Chaorui and Zhu, Deyao and Li, Kunchang and Gou, Chenhui and Li, Feng and Wang, Zeyu and Zhong, Shu and Yu, Weihao and Nie, Xiaonan and Song, Ziang and Shi, Guang and Fan, Haoqi},
  date = {2025-05-20},
  year = {2025},
  eprint = {2505.14683},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2505.14683},
  url = {http://arxiv.org/abs/2505.14683},
  urldate = {2025-05-25},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{dhariwalDiffusionModelsBeat2021,
  title = {Diffusion Models Beat GANs on Image Synthesis},
  booktitle = {NeurIPS},
  author = {Dhariwal, Prafulla and Nichol, Alexander},
  date = {2021},
  year = {2021},
  volume = {34},
  pages = {8780--8794},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html},
  urldate = {2024-05-15},
  keywords = {OpenAI},
}

@inproceedings{dingCogView2FasterBetter2022,
  title = {CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers},
  shorttitle = {CogView2},
  author = {Ding, Ming and Zheng, Wendi and Hong, Wenyi and Tang, Jie},
  date = {2022-10-31},
  year = {2022},
  url = {https://openreview.net/forum?id=GkDbQb6qu_r},
  urldate = {2025-10-04},
  booktitle = {NeurIPS},
  langid = {english},
}

@inproceedings{dingCogViewMasteringTexttoImage2021,
  title = {CogView: Mastering Text-to-Image Generation via Transformers},
  shorttitle = {CogView},
  author = {Ding, Ming and Yang, Zhuoyi and Hong, Wenyi and Zheng, Wendi and Zhou, Chang and Yin, Da and Lin, Junyang and Zou, Xu and Shao, Zhou and Yang, Hongxia and Tang, Jie},
  date = {2021-05-26},
  year = {2021},
  url = {https://www.semanticscholar.org/paper/CogView%3A-Mastering-Text-to-Image-Generation-via-Ding-Yang/1197ae4a62f0e0e4e3f3fb70396b5ff06ef371aa},
  urldate = {2024-09-01},
  booktitle = {NeurIPS},
  langid = {english},
}

@inproceedings{Du_2024_CVPR,
  title = {DemoFusion: Democratising High-Resolution Image Generation with No \$\$\$},
  shorttitle = {DemoFusion},
  booktitle = {CVPR},
  author = {Du, Ruoyi and Chang, Dongliang and Hospedales, Timothy and Song, Yi-Zhe and Ma, Zhanyu},
  date = {2024-06},
  year = {2024},
  pages = {6159--6168},
  url = {http://openaccess.thecvf.com//content/CVPR2024/papers/Du_DemoFusion_Democratising_High-Resolution_Image_Generation_With_No__CVPR_2024_paper.pdf},
  urldate = {2025-10-06},
  booktitle = {CVPR},
  langid = {english},
}

@inproceedings{esserScalingRectifiedFlow2024,
  title = {Scaling Rectified Flow Transformers for High-Resolution Image Synthesis},
  author = {Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and Müller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and Podell, Dustin and Dockhorn, Tim and English, Zion and Rombach, Robin},
  date = {2024-06-06},
  year = {2024},
  url = {https://openreview.net/forum?id=FPnUhsQJ5B},
  urldate = {2024-07-07},
  booktitle = {ICML},
  langid = {english},
  keywords = {Stable Diffusion},
}

@inproceedings{esserTamingTransformersHighResolution2021,
  title = {Taming Transformers for High-Resolution Image Synthesis},
  author = {Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
  date = {2021},
  year = {2021},
  pages = {12873--12883},
  url = {https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html?ref=},
  urldate = {2024-08-12},
  booktitle = {CVPR},
  langid = {english},
  keywords = {VGGAN},
}

@inproceedings{gaoLuminaT2XScalableFlowbased2025a,
  title = {Lumina-T2X: Scalable Flow-based Large Diffusion Transformer for Flexible Resolution Generation},
  shorttitle = {Lumina-T2X},
  author = {Gao, Peng and Zhuo, Le and Liu, Dongyang and Du, Ruoyi and Luo, Xu and Qiu, Longtian and Zhang, Yuhang and Huang, Rongjie and Geng, Shijie and Zhang, Renrui and Xie, Junlin and Shao, Wenqi and Jiang, Zhengkai and Yang, Tianshuo and Ye, Weicai and He, Tong and He, Jingwen and He, Junjun and Qiao, Yu and Li, Hongsheng},
  date = {2025},
  year = {2025},
  url = {https://openreview.net/forum?id=EbWf36quzd},
  urldate = {2025-05-11},
  booktitle = {ICLR},
  langid = {english},
}

@online{geminiteamgoogleNanoBananaImage2025,
  title = {Nano Banana! Image Editing in Gemini Just Got a Major Upgrade},
  author = {Gemini Team, Google},
  date = {2025-08-26},
  year = {2025},
  url = {https://blog.google/intl/en-mena/product-updates/explore-get-answers/nano-banana-image-editing-in-gemini-just-got-a-major-upgrade/},
  urldate = {2025-08-27},
  langid = {english},
  organization = {Google},
}

@inproceedings{ghoshGenEvalObjectfocusedFramework2023,
  title = {GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment},
  shorttitle = {GenEval},
  booktitle = {NeurIPS},
  author = {Ghosh, Dhruba and Hajishirzi, Hannaneh and Schmidt, Ludwig},
  date = {2023},
  year = {2023},
  volume = {36},
  pages = {52132--52152},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/a3bf71c7c63f0c3bcb7ff67c67b1e7b1-Abstract-Datasets_and_Benchmarks.html},
  urldate = {2024-04-15},
}

@online{gongSeedream20Native2025,
  title = {Seedream 2.0: A Native Chinese-English Bilingual Image Generation Foundation Model},
  shorttitle = {Seedream 2.0},
  author = {Gong, Lixue and Hou, Xiaoxia and Li, Fanshi and Li, Liang and Lian, Xiaochen and Liu, Fei and Liu, Liyang and Liu, Wei and Lu, Wei and Shi, Yichun and Sun, Shiqi and Tian, Yu and Tian, Zhi and Wang, Peng and Wang, Xun and Wang, Ye and Wu, Guofeng and Wu, Jie and Xia, Xin and Xiao, Xuefeng and Yang, Linjie and Zhai, Zhonghua and Zhang, Xinyu and Zhang, Qi and Zhang, Yuwei and Zhao, Shijia and Yang, Jianchao and Huang, Weilin},
  date = {2025-03-10},
  year = {2025},
  eprint = {2503.07703},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.07703},
  url = {http://arxiv.org/abs/2503.07703},
  urldate = {2025-10-04},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{hanInfinityScalingBitwise2025,
  title = {Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis},
  shorttitle = {Infinity},
  author = {Han, Jian and Liu, Jinlai and Jiang, Yi and Yan, Bin and Zhang, Yuqi and Yuan, Zehuan and Peng, Bingyue and Liu, Xiaobing},
  date = {2025},
  year = {2025},
  pages = {15733--15744},
  url = {https://openaccess.thecvf.com/content/CVPR2025/html/Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis_CVPR_2025_paper.html},
  urldate = {2025-06-14},
  booktitle = {CVPR},
}

@inproceedings{hertzPrompttoPromptImageEditing2022,
  title = {Prompt-to-Prompt Image Editing with Cross-Attention Control},
  author = {Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and Cohen-or, Daniel},
  date = {2022-09-29},
  year = {2022},
  url = {https://openreview.net/forum?id=_CDixzkzeyb},
  urldate = {2024-08-26},
  booktitle = {ICLR},
  langid = {english},
}

@inproceedings{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising Diffusion Probabilistic Models},
  booktitle = {NeurIPS},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  date = {2020},
  year = {2020},
  volume = {33},
  pages = {6840--6851},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
  urldate = {2024-06-12},
  keywords = {DNN},
  annotation = {CCF-A},
}

@inproceedings{Huang_2024_CVPR,
  title = {SmartEdit: Exploring Complex Instruction-Based Image Editing with Multimodal Large Language Models},
  shorttitle = {SmartEdit},
  booktitle = {CVPR},
  author = {Huang, Yuzhou and Xie, Liangbin and Wang, Xintao and Yuan, Ziyang and Cun, Xiaodong and Ge, Yixiao and Zhou, Jiantao and Dong, Chao and Huang, Rui and Zhang, Ruimao and Shan, Ying},
  date = {2024-06},
  year = {2024},
  pages = {8362--8371},
  url = {http://openaccess.thecvf.com//content/CVPR2024/papers/Huang_SmartEdit_Exploring_Complex_Instruction-based_Image_Editing_with_Multimodal_Large_Language_CVPR_2024_paper.pdf},
  urldate = {2024-12-19},
  booktitle = {CVPR},
  langid = {english},
}

@article{huangDiffusionModelBasedImage2025,
  title = {Diffusion Model-Based Image Editing: A Survey},
  shorttitle = {Diffusion Model-Based Image Editing},
  author = {Huang, Yi and Huang, Jiancheng and Liu, Yifan and Yan, Mingfu and Lv, Jiaxi and Liu, Jianzhuang and Xiong, Wei and Zhang, He and Cao, Liangliang and Chen, Shifeng},
  date = {2025},
  year = {2025},
  journal = {TPAMI},
  pages = {1--27},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2025.3541625},
  url = {https://ieeexplore.ieee.org/document/10884879},
  urldate = {2025-05-06},
  keywords = {AIGC,diffusion model,Diffusion models,Diffusion processes,image editing,Image synthesis,Mathematical models,Noise,Noise measurement,Reviews,Surveys,Training,Visualization},
}

@inproceedings{Kawar_2023_CVPR,
  title = {Imagic: Text-based Real Image Editing with Diffusion Models},
  shorttitle = {Imagic},
  booktitle = {CVPR},
  author = {Kawar, Bahjat and Zada, Shiran and Lang, Oran and Tov, Omer and Chang, Huiwen and Dekel, Tali and Mosseri, Inbar and Irani, Michal},
  date = {2023-06},
  year = {2023},
  pages = {6007--6017},
  url = {http://openaccess.thecvf.com//content/CVPR2023/papers/Kawar_Imagic_Text-Based_Real_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf},
  urldate = {2024-10-18},
  booktitle = {CVPR},
  langid = {english},
}

@inproceedings{kingmaAutoEncodingVariationalBayes2014,
  title = {Auto-Encoding Variational Bayes},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2014},
  year = {2014},
  url = {https://openreview.net/forum?id=33X9fd2-9FyZd},
  urldate = {2024-09-09},
  booktitle = {ICLR},
  langid = {english},
}

@online{labsFLUX1KontextFlow2025,
  title = {FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space},
  shorttitle = {FLUX.1 Kontext},
  author = {Labs, Black Forest and Batifol, Stephen and Blattmann, Andreas and Boesel, Frederic and Consul, Saksham and Diagne, Cyril and Dockhorn, Tim and English, Jack and English, Zion and Esser, Patrick and Kulal, Sumith and Lacey, Kyle and Levi, Yam and Li, Cheng and Lorenz, Dominik and Müller, Jonas and Podell, Dustin and Rombach, Robin and Saini, Harry and Sauer, Axel and Smith, Luke},
  date = {2025-06-24},
  year = {2025},
  eprint = {2506.15742},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2506.15742},
  url = {http://arxiv.org/abs/2506.15742},
  urldate = {2025-06-26},
  pubstate = {prepublished},
  keywords = {Computer Science - Graphics},
}

@inproceedings{leeDiffusionExplainerVisual2024,
  title = {Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion},
  shorttitle = {Diffusion Explainer},
  booktitle = {VIS},
  author = {Lee, Seongmin and Hoover, Benjamin and Strobelt, Hendrik and Wang, Zijie J. and Peng, ShengYun and Wright, Austin and Li, Kevin and Park, Haekyu and Yang, Haoyang and Chau, Duen Horng Polo},
  date = {2024-10},
  year = {2024},
  pages = {96--100},
  issn = {2771-9553},
  doi = {10.1109/VIS55277.2024.00027},
  url = {https://ieeexplore.ieee.org/document/10771097},
  urldate = {2025-10-12},
  booktitle = {VIS},
  keywords = {Animation,Artificial Intelligence,Blogs,Generative AI,Image synthesis,Interactive visualization,Machine Learning,Text to image,Text-to-image generative AI,Transforms,User study,Visual analytics,Visualization},
}

@inproceedings{Li_2024_CVPR,
  title = {On the Scalability of Diffusion-Based Text-to-Image Generation},
  booktitle = {CVPR},
  author = {Li, Hao and Zou, Yang and Wang, Ying and Majumder, Orchid and Xie, Yusheng and Manmatha, R. and Swaminathan, Ashwin and Tu, Zhuowen and Ermon, Stefano and Soatto, Stefano},
  date = {2024-06},
  year = {2024},
  pages = {9400--9409},
  url = {http://openaccess.thecvf.com//content/CVPR2024/papers/Li_On_the_Scalability_of_Diffusion-based_Text-to-Image_Generation_CVPR_2024_paper.pdf},
  urldate = {2025-05-11},
  booktitle = {CVPR},
  langid = {english},
}

@inproceedings{liAutoregressiveImageGeneration2024a,
  title = {Autoregressive Image Generation without Vector Quantization},
  booktitle = {NeurIPS},
  author = {Li, Tianhong and Tian, Yonglong and Li, He and Deng, Mingyang and He, Kaiming},
  date = {2024-01-01},
  year = {2024},
  url = {https://openreview.net/forum?id=VHbCa8NWpt},
  urldate = {2024-12-03},
  langid = {english},
}

@inproceedings{liBalancingPreservationModification2025,
  title = {Balancing Preservation and Modification: A Region and Semantic Aware Metric for Instruction-Based Image Editing},
  shorttitle = {Balancing Preservation and Modification},
  author = {Li, Zhuoying and Xu, Zhu and Peng, Yuxin and Liu, Yang},
  date = {2025-06-18},
  year = {2025},
  url = {https://openreview.net/forum?id=Nrs6csi52N},
  urldate = {2025-07-17},
  booktitle = {ICML},
  langid = {english},
  annotation = {https://joyli-x.github.io/BPM/},
}

@online{liHunyuanDiTPowerfulMultiResolution2024,
  title = {Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding},
  shorttitle = {Hunyuan-DiT},
  author = {Li, Zhimin and Zhang, Jianwei and Lin, Qin and Xiong, Jiangfeng and Long, Yanxin and Deng, Xinchi and Zhang, Yingfang and Liu, Xingchao and Huang, Minbin and Xiao, Zedong and Chen, Dayou and He, Jiajun and Li, Jiahao and Li, Wenyue and Zhang, Chen and Quan, Rongwei and Lu, Jianxiang and Huang, Jiabin and Yuan, Xiaoyan and Zheng, Xiaoxiao and Li, Yixuan and Zhang, Jihong and Zhang, Chao and Chen, Meng and Liu, Jie and Fang, Zheng and Wang, Weiyan and Xue, Jinbao and Tao, Yangyu and Zhu, Jianchen and Liu, Kai and Lin, Sihuan and Sun, Yifu and Li, Yun and Wang, Dongdong and Chen, Mingtao and Hu, Zhichao and Xiao, Xiao and Chen, Yan and Liu, Yuhong and Liu, Wei and Wang, Di and Yang, Yong and Jiang, Jie and Lu, Qinglin},
  date = {2024-05-14},
  year = {2024},
  eprint = {2405.08748},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.08748},
  url = {http://arxiv.org/abs/2405.08748},
  urldate = {2025-05-26},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{linRefineNetMultiPathRefinement2017,
  title = {RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation},
  shorttitle = {RefineNet},
  author = {Lin, Guosheng and Milan, Anton and Shen, Chunhua and Reid, Ian},
  date = {2017},
  year = {2017},
  pages = {1925--1934},
  url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Lin_RefineNet_Multi-Path_Refinement_CVPR_2017_paper.html},
  urldate = {2025-01-18},
  booktitle = {CVPR},
}

@inproceedings{linSPHINXMixerWeights2024,
  title = {SPHINX: A Mixer of~Weights, Visual Embeddings and~Image Scales for~Multi-modal Large Language Models},
  shorttitle = {SPHINX},
  booktitle = {ECCV},
  author = {Lin, Ziyi and Liu, Dongyang and Zhang, Renrui and Gao, Peng and Qiu, Longtian and Xiao, Han and Qiu, Han and Shao, Wenqi and Chen, Keqin and Han, Jiaming and Huang, Siyuan and Zhang, Yichi and He, Xuming and Qiao, Yu and Li, Hongsheng},
  editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
  date = {2024},
  year = {2024},
  pages = {36--55},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-73033-7_3},
  isbn = {978-3-031-73033-7},
  langid = {english},
}

@inproceedings{liuConvNet2020s2022,
  title = {A ConvNet for the 2020s},
  author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  date = {2022},
  year = {2022},
  pages = {11976--11986},
  url = {https://openaccess.thecvf.com/content/CVPR2022/html/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.html},
  urldate = {2025-06-15},
  booktitle = {CVPR},
}

@online{liuMedEBenchDiagnosingReliability2025,
  title = {MedEBench: Diagnosing Reliability in Text-Guided Medical Image Editing},
  shorttitle = {MedEBench},
  author = {Liu, Minghao and He, Zhitao and Fan, Zhiyuan and Wang, Qingyun and Fung, Yi R.},
  date = {2025-09-23},
  year = {2025},
  eprint = {2506.01921},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2506.01921},
  url = {http://arxiv.org/abs/2506.01921},
  urldate = {2025-10-03},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
}

@online{liuStep1XEditPracticalFramework2025,
  title = {Step1X-Edit: A Practical Framework for General Image Editing},
  shorttitle = {Step1X-Edit},
  author = {Liu, Shiyu and Han, Yucheng and Xing, Peng and Yin, Fukun and Wang, Rui and Cheng, Wei and Liao, Jiaqi and Wang, Yingming and Fu, Honghao and Han, Chunrui and Li, Guopeng and Peng, Yuang and Sun, Quan and Wu, Jingwei and Cai, Yan and Ge, Zheng and Ming, Ranchen and Xia, Lei and Zeng, Xianfang and Zhu, Yibo and Jiao, Binxing and Zhang, Xiangyu and Yu, Gang and Jiang, Daxin},
  date = {2025-05-06},
  year = {2025},
  eprint = {2504.17761},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.17761},
  url = {http://arxiv.org/abs/2504.17761},
  urldate = {2025-05-21},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{liuText2EarthUnlockingTextdriven2025,
  title = {Text2Earth: Unlocking Text-Driven Remote Sensing Image Generation with a Global-Scale Dataset and a Foundation Model},
  shorttitle = {Text2Earth},
  author = {Liu, Chenyang and Chen, Keyan and Zhao, Rui and Zou, Zhengxia and Shi, Zhenwei},
  date = {2025},
  year = {2025},
  journal = {GRSM},
  pages = {2--23},
  issn = {2168-6831},
  doi = {10.1109/MGRS.2025.3560455},
  url = {https://ieeexplore.ieee.org/document/10988859},
  urldate = {2025-05-10},
  keywords = {Diffusion models,Foundation models,Image resolution,Image synthesis,Metadata,Noise reduction,Remote sensing,Spatial resolution,Training,Visualization},
}

@online{luEasyTextControllableDiffusion2025,
  title = {EasyText: Controllable Diffusion Transformer for Multilingual Text Rendering},
  shorttitle = {EasyText},
  author = {Lu, Runnan and Zhang, Yuxuan and Liu, Jiaming and Wang, Haofan and Song, Yiren},
  date = {2025-05-30},
  year = {2025},
  eprint = {2505.24417},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2505.24417},
  url = {http://arxiv.org/abs/2505.24417},
  urldate = {2025-10-18},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{mengSDEditGuidedImage2022,
  title = {SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations},
  shorttitle = {SDEdit},
  author = {Meng, Chenlin and He, Yutong and Song, Yang and Song, Jiaming and Wu, Jiajun and Zhu, Jun-Yan and Ermon, Stefano},
  date = {2022},
  year = {2022},
  url = {https://openreview.net/forum?id=aBsCjcPu_tE},
  urldate = {2024-07-22},
  booktitle = {ICLR},
}

@inproceedings{Mokady_2023_CVPR,
  title = {NULL-text Inversion for Editing Real Images Using Guided Diffusion Models},
  booktitle = {CVPR},
  author = {Mokady, Ron and Hertz, Amir and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},
  date = {2023-06},
  year = {2023},
  pages = {6038--6047},
  url = {http://openaccess.thecvf.com//content/CVPR2023/papers/Mokady_NULL-Text_Inversion_for_Editing_Real_Images_Using_Guided_Diffusion_Models_CVPR_2023_paper.pdf},
  urldate = {2024-10-30},
  booktitle = {CVPR},
}

@inproceedings{nicholImprovedDenoisingDiffusion2021,
  title = {Improved Denoising Diffusion Probabilistic Models},
  booktitle = {ICML},
  author = {Nichol, Alexander Quinn and Dhariwal, Prafulla},
  date = {2021-07-01},
  year = {2021},
  pages = {8162--8171},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/nichol21a.html},
  urldate = {2024-06-19},
  booktitle = {ICML},
  langid = {english},
}

@online{niuWISEWorldKnowledgeInformed2025,
  title = {WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation},
  shorttitle = {WISE},
  author = {Niu, Yuwei and Ning, Munan and Zheng, Mengren and Jin, Weiyang and Lin, Bin and Jin, Peng and Liao, Jiaqi and Feng, Chaoran and Ning, Kunpeng and Zhu, Bin and Yuan, Li},
  date = {2025-05-27},
  year = {2025},
  eprint = {2503.07265},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.07265},
  url = {http://arxiv.org/abs/2503.07265},
  urldate = {2025-10-10},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
}

@online{openaiIntroducing4oImage2025,
  title = {Introducing 4o Image Generation},
  author = {OpenAI},
  date = {2025},
  year = {2025},
  url = {https://openai.com/index/introducing-4o-image-generation/},
  urldate = {2025-05-10},
  langid = {american}
}

@inproceedings{Peebles_2023_ICCV,
  title = {Scalable Diffusion Models with Transformers},
  booktitle = {ICCV},
  author = {Peebles, William and Xie, Saining},
  date = {2023},
  year = {2023},
  pages = {4195--4205},
  url = {https://openaccess.thecvf.com/content/ICCV2023/html/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.html},
  urldate = {2024-08-12},
  booktitle = {ICCV},
  langid = {english},
  keywords = {DiT},
}

@inproceedings{gaoMaskedDiffusionTransformer2023,
  title = {Masked Diffusion Transformer Is a Strong Image Synthesizer},
  author = {Gao, Shanghua and Zhou, Pan and Cheng, Ming-Ming and Yan, Shuicheng},
  date = {2023},
  year = {2023},
  booksubtitle = {ICCV},
  pages = {23164--23173},
  url = {https://openaccess.thecvf.com/content/ICCV2023/html/Gao_Masked_Diffusion_Transformer_is_a_Strong_Image_Synthesizer_ICCV_2023_paper.html},
  urldate = {2025-10-13},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  langid = {english},
  file = {D\:\\ZoteroLib\\storage\\HTI4EUI2\\Gao et al. - 2023 - Masked Diffusion Transformer is a Strong Image Synthesizer.pdf;D\:\\ZoteroLib\\storage\\I73SUNHX\\Gao_Masked_Diffusion_Transformer_ICCV_2023_supplemental.pdf}
}

@article{zhengFastTrainingDiffusion2024,
  title = {Fast Training of Diffusion Models with Masked Transformers},
  author = {Zheng, Hongkai and Nie, Weili and Vahdat, Arash and Anandkumar, Anima},
  date = {2024},
  year = {2024},
  journal = {TMLR},
  issn = {2835-8856},
  url = {https://openreview.net/forum?id=vTBjBtGioE},
  urldate = {2025-10-13},
  abstract = {We propose an efficient approach to train large diffusion models with masked transformers. While masked transformers have been extensively explored for representation learning, their application to generative learning is less explored in the vision domain. Our work is the first to exploit masked training to reduce the training cost of diffusion models significantly. Specifically, we randomly mask out a high proportion (e.g., 50\textbackslash\%) of patches in diffused input images during training. For masked training, we introduce an asymmetric encoder-decoder architecture consisting of a transformer encoder that operates only on unmasked patches and a lightweight transformer decoder on full patches. To promote a long-range understanding of full patches, we add an auxiliary task of reconstructing masked patches to the denoising score matching objective that learns the score of unmasked patches. Experiments on ImageNet-256x256 and ImageNet-512x512 show that our approach achieves competitive and even better generative performance than the state-of-the-art Diffusion Transformer (DiT) model, using only around 30\textbackslash\% of its original training time. Thus, our method shows a promising way of efficiently training large transformer-based diffusion models without sacrificing the generative performance. Our code is available at https://github.com/Anima-Lab/MaskDiT.},
  langid = {english},
  file = {D:\ZoteroLib\storage\9CY8P555\Zheng et al. - 2023 - Fast Training of Diffusion Models with Masked Transformers.pdf}
}


@online{gaoMDTv2MaskedDiffusion2024,
  title = {MDTv2: Masked Diffusion Transformer Is a Strong Image Synthesizer},
  author = {Gao, Shanghua and Zhou, Pan and Cheng, Ming-Ming and Yan, Shuicheng},
  date = {2024-02-21},
  year = {2024},
  eprint = {2303.14389},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.14389},
  url = {http://arxiv.org/abs/2303.14389},
  urldate = {2025-10-13},
  abstract = {Despite its success in image synthesis, we observe that diffusion probabilistic models (DPMs) often lack contextual reasoning ability to learn the relations among object parts in an image, leading to a slow learning process. To solve this issue, we propose a Masked Diffusion Transformer (MDT) that introduces a mask latent modeling scheme to explicitly enhance the DPMs' ability to contextual relation learning among object semantic parts in an image. During training, MDT operates in the latent space to mask certain tokens. Then, an asymmetric diffusion transformer is designed to predict masked tokens from unmasked ones while maintaining the diffusion generation process. Our MDT can reconstruct the full information of an image from its incomplete contextual input, thus enabling it to learn the associated relations among image tokens. We further improve MDT with a more efficient macro network structure and training strategy, named MDTv2. Experimental results show that MDTv2 achieves superior image synthesis performance, e.g., a new SOTA FID score of 1.58 on the ImageNet dataset, and has more than 10x faster learning speed than the previous SOTA DiT. The source code is released at https://github.com/sail-sg/MDT.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\ZoteroLib\\storage\\CFMCC2FX\\Gao et al. - 2024 - MDTv2 Masked Diffusion Transformer is a Strong Image Synthesizer.pdf;D\:\\ZoteroLib\\storage\\3I2EL9SF\\2303.html}
}


@inproceedings{perniasWurstchenEfficientArchitecture2024,
  title = {Würstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models},
  author = {Pernias, Pablo and Rampas, Dominic and Richter, Mats Leon and Pal, Christopher and Aubreville, 
  Marc},
  date = {2024-02-07},
  year = {2024},
  url = {https://openreview.net/forum?id=gU58d5QeGv},
  urldate = {2025-10-06},
  booktitle = {ICLR},
  langid = {english},
}

@inproceedings{podellSDXLImprovingLatent2024,
  title = {SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis},
  shorttitle = {SDXL},
  author = {Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and Müller, Jonas and Penna, Joe and Rombach, Robin},
  date = {2024},
  year = {2024},
  url = {https://openreview.net/forum?id=di52zR8xgf},
  urldate = {2024-07-07},
  booktitle = {ICLR},
  langid = {english},
  keywords = {Stable Diffusion},
  annotation = {https://github.com/Stability-AI/generative-models},
}

@online{qianGIEBenchGroundedEvaluation2025,
  title = {GIE-Bench: Towards Grounded Evaluation for Text-Guided Image Editing},
  shorttitle = {GIE-Bench},
  author = {Qian, Yusu and Lu, Jiasen and Fu, Tsu-Jui and Wang, Xinze and Chen, Chen and Yang, Yinfei and Hu, Wenze and Gan, Zhe},
  date = {2025-07-25},
  year = {2025},
  eprint = {2505.11493},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2505.11493},
  url = {http://arxiv.org/abs/2505.11493},
  urldate = {2025-10-03},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@online{qinLuminaImage20Unified2025,
  title = {Lumina-Image 2.0: A Unified and Efficient Image Generative Framework},
  shorttitle = {Lumina-Image 2.0},
  author = {Qin, Qi and Zhuo, Le and Xin, Yi and Du, Ruoyi and Li, Zhen and Fu, Bin and Lu, Yiting and Yuan, Jiakang and Li, Xinyue and Liu, Dongyang and Zhu, Xiangyang and Zhang, Manyuan and Beddow, Will and Millon, Erwann and Perez, Victor and Wang, Wenhai and He, Conghui and Zhang, Bo and Liu, Xiaohong and Li, Hongsheng and Qiao, Yu and Xu, Chang and Gao, Peng},
  date = {2025-03-27},
  year = {2025},
  eprint = {2503.21758},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.21758},
  url = {http://arxiv.org/abs/2503.21758},
  urldate = {2025-05-11},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@online{rameshHierarchicalTextConditionalImage2022,
  title = {Hierarchical Text-Conditional Image Generation with CLIP Latents},
  author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  date = {2022-04-12},
  year = {2022},
  eprint = {2204.06125},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2204.06125},
  url = {http://arxiv.org/abs/2204.06125},
  urldate = {2024-04-16},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,DALL-E 2,OpenAI,unCLIP},
  annotation = {arXiv},
}

@inproceedings{rameshZeroShotTexttoImageGeneration2021,
  title = {Zero-Shot Text-to-Image Generation},
  booktitle = {ICML},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  date = {2021-07-01},
  year = {2021},
  pages = {8821--8831},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/ramesh21a.html},
  urldate = {2024-08-20},
  booktitle = {ICML},
  langid = {english},
  keywords = {DALL-E},
}

@inproceedings{razaviGeneratingDiverseHighfidelity2019,
  title = {Generating Diverse High-Fidelity Images with VQ-VAE-2},
  booktitle = {NeurIPS},
  author = {Razavi, Ali and family=Oord, given=Aäron, prefix=van den, useprefix=true and Vinyals, Oriol},
  date = {2019-12-08},
  year = {2019},
  pages = {14866--14876},
  publisher = {Curran Associates Inc.},
  location = {Red Hook, NY, USA},
  keywords = {VQ-VAE},
}

@inproceedings{razzhigaevKandinskyImprovedTexttoImage2023,
  title = {Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and Conditional Diffusion Models},
  booktitle = {EMNLP},
  author = {Razzhigaev, Anton and Shakhmatov, Arseniy and Maltseva, Anastasia and Arkhipkin, Vladimir and Pavlov, Igor and Ryabov, Ilya and Kuts, Angelina and Panchenko, Alexander and Kuznetsov, Andrey and Dimitrov, Denis},
  date = {2023-01-01},
  year = {2023},
  url = {https://openreview.net/forum?id=Vmg2GxSbKn},
  urldate = {2025-10-06},
  langid = {english},
}

@inproceedings{rombachHighResolutionImageSynthesis2022,
  title = {High-Resolution Image Synthesis With Latent Diffusion Models},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  date = {2022},
  year = {2022},
  pages = {10684--10695},
  url = {https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html},
  urldate = {2024-06-28},
  booktitle = {CVPR},
  langid = {english},
  keywords = {Stable Diffusion},
  annotation = {https://github.com/CompVis/latent-diffusion\\
https://github.com/CompVis/stable-diffusion},
}

@inproceedings{routSemanticImageInversion2025,
  title = {Semantic Image Inversion and Editing Using Rectified Stochastic Differential Equations},
  author = {Rout, Litu and Chen, Yujia and Ruiz, Nataniel and Caramanis, Constantine and Shakkottai, Sanjay and Chu, Wen-Sheng},
  date = {2025},
  year = {2025},
  url = {https://openreview.net/forum?id=Hu0FSOSEyS},
  urldate = {2025-10-06},
  booktitle = {ICLR},
  langid = {english},
}

@inproceedings{sabourAlignYourSteps2024,
  title = {Align Your Steps: Optimizing Sampling Schedules in Diffusion Models},
  shorttitle = {Align Your Steps},
  author = {Sabour, Amirmojtaba and Fidler, Sanja and Kreis, Karsten},
  date = {2024-06-06},
  year = {2024},
  url = {https://openreview.net/forum?id=nBGBzV4It3},
  urldate = {2025-10-12},
  booktitle = {ICML},
  langid = {english},
}

@inproceedings{sahooSimpleEffectiveMasked2024,
  title = {Simple and Effective Masked Diffusion Language Models},
  author = {Sahoo, Subham Sekhar and Arriola, Marianne and Schiff, Yair and Gokaslan, Aaron and Marroquin, Edgar Mariano and Chiu, Justin T. and Rush, Alexander M. and Kuleshov, Volodymyr},
  date = {2024-11-06},
  year = {2024},
  url = {https://openreview.net/forum?id=L4uaAR4ArM},
  urldate = {2025-10-05},
  booktitle = {NeurIPS},
  langid = {english},
}

@inproceedings{salimansPixelCNNImprovingPixelCNN2017,
  title = {PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications},
  shorttitle = {PixelCNN++},
  author = {Salimans, Tim and Karpathy, Andrej and Chen, Xi and Kingma, Diederik P.},
  date = {2017-02-06},
  year = {2017},
  url = {https://openreview.net/forum?id=BJrFC6ceg},
  urldate = {2025-01-18},
  booktitle = {ICLR},
  langid = {english},
}

@inproceedings{sauerAdversarialDiffusionDistillation2024,
  title = {Adversarial Diffusion Distillation},
  booktitle = {ECCV},
  author = {Sauer, Axel and Lorenz, Dominik and Blattmann, Andreas and Rombach, Robin},
  editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
  date = {2024},
  year = {2024},
  pages = {87--103},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-73016-0_6},
  isbn = {978-3-031-73016-0},
  langid = {english},
}

@online{sauerFastHighResolutionImage2024,
  title = {Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation},
  author = {Sauer, Axel and Boesel, Frederic and Dockhorn, Tim and Blattmann, Andreas and Esser, Patrick and Rombach, Robin},
  date = {2024-03-18},
  year = {2024},
  eprint = {2403.12015},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.12015},
  url = {http://arxiv.org/abs/2403.12015},
  urldate = {2025-08-30},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@online{seedreamSeedream40Nextgeneration2025,
  title = {Seedream 4.0: Toward Next-generation Multimodal Image Generation},
  shorttitle = {Seedream 4.0},
  author = {Seedream Team},
  date = {2025-09-28},
  year = {2025},
  eprint = {2509.20427},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2509.20427},
  url = {http://arxiv.org/abs/2509.20427},
  urldate = {2025-10-04},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@online{shiSeedEditAlignImage2024,
  title = {SeedEdit: Align Image Re-Generation to Image Editing},
  shorttitle = {SeedEdit},
  author = {Shi, Yichun and Wang, Peng and Huang, Weilin},
  date = {2024-11-11},
  year = {2024},
  eprint = {2411.06686},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2411.06686},
  url = {http://arxiv.org/abs/2411.06686},
  urldate = {2024-11-12},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{songGenerativeModelingEstimating2019,
  title = {Generative Modeling by Estimating Gradients of the Data Distribution},
  booktitle = {NeurIPS},
  author = {Song, Yang and Ermon, Stefano},
  date = {2019-12-08},
  year = {2019},
  url = {https://openreview.net/forum?id=B1lcYrBgLH},
  urldate = {2024-11-18},
  langid = {english},
}

@inproceedings{songImprovedTechniquesTraining2020,
  title = {Improved Techniques for Training Score-Based Generative Models},
  booktitle = {NeurIPS},
  author = {Song, Yang and Ermon, Stefano},
  date = {2020},
  year = {2020},
  series = {NIPS '20},
  pages = {12438--12448},
  publisher = {Curran Associates Inc.},
  location = {Red Hook, NY, USA},
  isbn = {978-1-7138-2954-6},
}

@inproceedings{songScoreBasedGenerativeModeling2021,
  title = {Score-Based Generative Modeling through Stochastic Differential Equations},
  author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  date = {2021},
  year = {2021},
  url = {https://openreview.net/forum?id=PxTIG12RRHS&utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter},
  urldate = {2024-08-17},
  booktitle = {ICLR},
  langid = {english},
}

@inproceedings{tayUL2UnifyingLanguage2023,
  title = {UL2: Unifying Language Learning Paradigms},
  shorttitle = {UL2},
  author = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q. and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Bahri, Dara and Schuster, Tal and Zheng, Steven and Zhou, Denny and Houlsby, Neil and Metzler, Donald},
  date = {2023},
  year = {2023},
  url = {https://openreview.net/forum?id=6ruVLB727MC},
  urldate = {2025-10-06},
  booktitle = {ICLR},
  langid = {english},
}

@inproceedings{tengRelayDiffusionUnifying2024,
  title = {Relay Diffusion: Unifying diffusion process for coarse-to-fine generation},
  author = {Teng, Jiayan and Zheng, Wendi and Ding, Ming and Hong, Wenyi and Wangni, Jianqiao and Yang, 
  Zhuoyi and Tang, Jie},
  date = {2024-03-27},
  year = {2024},
  url = {https://openreview.net/forum?id=qTlcbLSm4p},
  urldate = {2025-10-06},
  booktitle = {ICLR},
  langid = {english},
}

@inproceedings{tianVisualAutoregressiveModeling2024,
  title = {Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction},
  shorttitle = {Visual Autoregressive Modeling},
  author = {Tian, Keyu and Jiang, Yi and Yuan, Zehuan and Peng, Bingyue and Wang, Liwei},
  date = {2024-11-06},
  year = {2024},
  url = {https://openreview.net/forum?id=gojL67CfS8&noteId=GKzcnWCV21},
  urldate = {2024-12-03},
  booktitle = {NeurIPS},
}

@inproceedings{vandenoordNeuralDiscreteRepresentation2017,
  title = {Neural Discrete Representation Learning},
  booktitle = {NeurIPS},
  author = {family=Oord, given=Aaron, prefix=van den, useprefix=true and Vinyals, Oriol and Kavukcuoglu, Koray},
  date = {2017-12-04},
  year = {2017},
  series = {NIPS'17},
  pages = {6309--6318},
  publisher = {Curran Associates Inc.},
  location = {Red Hook, NY, USA},
  isbn = {978-1-5108-6096-4},
  keywords = {VQ-VAE},
}

@inproceedings{wangDiffusionDBLargescalePrompt2023,
  title = {DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models},
  shorttitle = {DiffusionDB},
  booktitle = {ACL},
  author = {Wang, Zijie J. and Montoya, Evan and Munechika, David and Yang, Haoyang and Hoover, Benjamin and Chau, Duen Horng},
  editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  date = {2023-07},
  year = {2023},
  pages = {893--911},
  publisher = {Association for Computational Linguistics},
  location = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-long.51},
  url = {https://aclanthology.org/2023.acl-long.51},
  urldate = {2024-06-19},
  booktitle = {ACL},
}

@online{wangGPTIMAGEEDIT15MMillionScaleGPTGenerated2025,
  title = {GPT-IMAGE-EDIT-1.5M: A Million-Scale, GPT-Generated Image Dataset},
  shorttitle = {GPT-IMAGE-EDIT-1.5M},
  author = {Wang, Yuhan and Yang, Siwei and Zhao, Bingchen and Zhang, Letian and Liu, Qing and Zhou, Yuyin and Xie, Cihang},
  date = {2025-07-28},
  year = {2025},
  eprint = {2507.21033},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2507.21033},
  url = {http://arxiv.org/abs/2507.21033},
  urldate = {2025-10-22},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@online{wangSeedEdit30Fast2025,
  title = {SeedEdit 3.0: Fast and High-Quality Generative Image Editing},
  shorttitle = {SeedEdit 3.0},
  author = {Wang, Peng and Shi, Yichun and Lian, Xiaochen and Zhai, Zhonghua and Xia, Xin and Xiao, Xuefeng and Huang, Weilin and Yang, Jianchao},
  date = {2025-06-06},
  year = {2025},
  eprint = {2506.05083},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2506.05083},
  url = {http://arxiv.org/abs/2506.05083},
  urldate = {2025-06-13},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{wangTamingRectifiedFlow2025,
  title = {Taming Rectified Flow for Inversion and Editing},
  author = {Wang, Jiangshan and Pu, Junfu and Qi, Zhongang and Guo, Jiayi and Ma, Yue and Huang, Nisha and Chen, Yuxin and Li, Xiu and Shan, Ying},
  date = {2025-06-18},
  year = {2025},
  url = {https://openreview.net/forum?id=uDreZphNky},
  urldate = {2025-07-17},
  booktitle = {ICML},
  langid = {english},
}

@online{wengDiffusionModelsVideo2024,
  title = {Diffusion Models for Video Generation},
  author = {Weng, Lilian},
  date = {2024-04-12T00:00:00+00:00},
  year = {2024},
  url = {https://lilianweng.github.io/posts/2024-04-12-diffusion-video/},
  urldate = {2025-06-17},
  langid = {english},
}

@online{song2019generative,
  title = {Generative Modeling by Estimating Gradients of the Data Distribution},
  url = {https://yang-song.net/blog/2021/score/},
  year = {2019},
 author={Song Yang}
}


@online{wengWhatAreDiffusion2021,
  title = {What Are Diffusion Models?},
  author = {Weng, Lilian},
  date = {2021-07-11},
  year = {2021},
  url = {https://lilianweng.github.io/posts/2021-07-11-diffusion-models/},
  langid = {english},
}

@inproceedings{xueRAPHAELTexttoImageGeneration2023,
  title = {RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths},
  shorttitle = {RAPHAEL},
  author = {Xue, Zeyue and Song, Guanglu and Guo, Qiushan and Liu, Boxiao and Zong, Zhuofan and Liu, Yu and Luo, Ping},
  date = {2023},
  year = {2023},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/821655c7dc4836838cd8524d07f9d6fd-Abstract-Conference.html},
  urldate = {2024-04-15},
  booktitle = {NeurIPS},
  langid = {english},
}

@article{raffelExploringLimitsTransfer2020,
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  date = {2020},
  year = {2020},
  journal = {JMLR},
  volume = {21},
  number = {140},
  pages = {1--67},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v21/20-074.html},
  urldate = {2024-05-27},
  langid = {english},
  keywords = {T5},
}



@inproceedings{Woo_2023_CVPR,
  title = {ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders},
  shorttitle = {ConvNeXt V2},
  booktitle = {CVPR},
  author = {Woo, Sanghyun and Debnath, Shoubhik and Hu, Ronghang and Chen, Xinlei and Liu, Zhuang and Kweon, In So and Xie, Saining},
  date = {2023-06},
  year = {2023},
  pages = {16133--16142},
  url = {http://openaccess.thecvf.com//content/CVPR2023/papers/Woo_ConvNeXt_V2_Co-Designing_and_Scaling_ConvNets_With_Masked_Autoencoders_CVPR_2023_paper.pdf},
  urldate = {2025-06-15},
  booktitle = {CVPR},
}

@online{wuEditRewardHumanAlignedReward2025,
  title = {EditReward: A Human-Aligned Reward Model for Instruction-Guided Image Editing},
  shorttitle = {EditReward},
  author = {Wu, Keming and Jiang, Sicong and Ku, Max and Nie, Ping and Liu, Minghao and Chen, Wenhu},
  date = {2025-09-30},
  year = {2025},
  eprint = {2509.26346},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2509.26346},
  url = {http://arxiv.org/abs/2509.26346},
  urldate = {2025-10-04},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
}

@online{wuOmniGen2ExplorationAdvanced2025,
  title = {OmniGen2: Exploration to Advanced Multimodal Generation},
  shorttitle = {OmniGen2},
  author = {Wu, Chenyuan and Zheng, Pengfei and Yan, Ruiran and Xiao, Shitao and Luo, Xin and Wang, Yueze and Li, Wanli and Jiang, Xiyan and Liu, Yexin and Zhou, Junjie and Liu, Ze and Xia, Ziyi and Li, Chaofan and Deng, Haoge and Wang, Jiahao and Luo, Kun and Zhang, Bo and Lian, Defu and Wang, Xinlong and Wang, Zhongyuan and Huang, Tiejun and Liu, Zheng},
  date = {2025-06-25},
  year = {2025},
  eprint = {2506.18871},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2506.18871},
  url = {http://arxiv.org/abs/2506.18871},
  urldate = {2025-08-29},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
}

@article{suRoFormerEnhancedTransformer2024,
  title = {{{RoFormer}}: {{Enhanced}} Transformer with {{Rotary Position Embedding}}},
  shorttitle = {{{RoFormer}}},
  author = {Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  date = {2024},
  year = {2024},
  journal = {Neurocomput.},
  shortjournal = {Neurocomputing},
  volume = {568},
  number = {C},
  pages = {127063},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2023.127063},
  url = {https://doi.org/10.1016/j.neucom.2023.127063},
  urldate = {2024-10-01},
  abstract = {Position encoding has recently been shown to be effective in transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding (RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in the self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: https://huggingface.co/docs/transformers/model\_doc/roformer.},
  keywords = {Natural language processing,Position information encoding,Pre-trained language models,Pre-training},
  file = {D\:\\ZoteroLib\\storage\\3ZZHBU9K\\Su et al. - 2024 - RoFormer Enhanced transformer with Rotary Position Embedding.pdf;D\:\\ZoteroLib\\storage\\SS2ESLF2\\Su et al. - 2024 - RoFormer Enhanced transformer with Rotary Position Embedding.pdf;D\:\\ZoteroLib\\storage\\2SCUV7GJ\\S0925231223011864.html;D\:\\ZoteroLib\\storage\\47ANGSHX\\S0925231223011864.html}
}

@inproceedings{henryQueryKeyNormalizationTransformers2020,
  title = {Query-Key Normalization for Transformers},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
  author = {Henry, Alex and Dachapally, Prudhvi Raj and Pawar, Shubham Shantaram and Chen, Yuxuan},
  editor = {Cohn, Trevor and He, Yulan and Liu, Yang},
  date = {2020-11},
  year = {2020},
  pages = {4246--4253},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.findings-emnlp.379},
  url = {https://aclanthology.org/2020.findings-emnlp.379/},
  urldate = {2025-08-30},
  abstract = {Low-resource language translation is a challenging but socially valuable NLP task. Building on recent work adapting the Transformer's normalization to this setting, we propose QKNorm, a normalization technique that modifies the attention mechanism to make the softmax function less prone to arbitrary saturation without sacrificing expressivity. Specifically, we apply l2-normalization along the head dimension of each query and key matrix prior to multiplying them and then scale up by a learnable parameter instead of dividing by the square root of the embedding dimension. We show improvements averaging 0.928 BLEU over state-of-the-art bilingual benchmarks for 5 low-resource translation pairs from the TED Talks corpus and IWSLT'15.},
  eventtitle = {Findings 2020},
  file = {D:\ZoteroLib\storage\ANSNV2T9\Henry et al. - 2020 - Query-Key Normalization for Transformers.pdf}
}

@online{touvronLlama2Open2023,
  title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  date = {2023-07-19},
  year = {2023},
  eprint = {2307.09288},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.09288},
  url = {http://arxiv.org/abs/2307.09288},
  urldate = {2024-06-24},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,LLaMA,Meta AI},
  file = {D\:\\ZoteroLib\\storage\\KXBYPDRP\\Touvron et al_2023_Llama 2.pdf;D\:\\ZoteroLib\\storage\\VSMWICFW\\2307.html}
}

@inproceedings{changConvolutionsSelfAttentionReinterpreting2021,
  title = {Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models},
  booktitle = {ACL},
  author = {Chang, Tyler A. and Xu, Yifan and Xu, Weijian and Tu, Zhuowen},
  editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
  date = {2021-08},
  year = {2021},
  pages = {4322--4333},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2021.acl-long.333},
  url = {https://aclanthology.org/2021.acl-long.333/},
  urldate = {2025-10-25},
  abstract = {In this paper, we detail the relationship between convolutions and self-attention in natural language tasks. We show that relative position embeddings in self-attention layers are equivalent to recently-proposed dynamic lightweight convolutions, and we consider multiple new ways of integrating convolutions into Transformer self-attention. Specifically, we propose composite attention, which unites previous relative position encoding methods under a convolutional framework. We conduct experiments by training BERT with composite attention, finding that convolutions consistently improve performance on multiple downstream tasks, replacing absolute position embeddings. To inform future work, we present results comparing lightweight convolutions, dynamic convolutions, and depthwise-separable convolutions in language model pre-training, considering multiple injection points for convolutions in self-attention layers.},
  eventtitle = {{{ACL-IJCNLP}} 2021},
  file = {D:\ZoteroLib\storage\VIVRP2DJ\Chang et al. - 2021 - Convolutions and Self-Attention Re-interpreting Relative Positions in Pre-trained Language Models.pdf}
}

@inproceedings{choiGraphConvolutionsEnrich2024,
  title = {Graph Convolutions Enrich the Self-Attention in Transformers!},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Choi, Jeongwhan and Wi, Hyowon and Kim, Jayoung and Shin, Yehjin and Lee, Kookjin and Trask, Nathaniel and Park, Noseong},
  date = {2024-12-16},
  year = {2024},
  volume = {37},
  pages = {52891--52936},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/5eceb48c3bc8b5d936c05ff8e2ece65e-Abstract-Conference.html},
  urldate = {2025-10-25},
  langid = {english},
  file = {D:\ZoteroLib\storage\EGI7P9QT\Choi et al. - 2024 - Graph Convolutions Enrich the Self-Attention in Transformers!.pdf}
}

@inproceedings{cordonnierRelationshipSelfAttentionConvolutional2020,
  title = {On the Relationship between Self-Attention and Convolutional Layers},
  author = {Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
  date = {2020},
  year = {2020},
  url = {https://openreview.net/forum?id=HJlnC1rKPB},
  urldate = {2025-10-25},
  abstract = {Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available.},
  eventtitle = {ICLR},
  langid = {english},
  file = {D:\ZoteroLib\storage\652MBQJR\Cordonnier et al. - 2019 - On the Relationship between Self-Attention and Convolutional Layers.pdf}
}

@article{goyalInductiveBiasesDeep2022a,
  title = {Inductive Biases for Deep Learning of Higher-Level Cognition},
  author = {Goyal, Anirudh and Bengio, Yoshua},
  date = {2022-10-12},
  year = {2022},
  journaltitle = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {478},
  number = {2266},
  pages = {20210068},
  publisher = {Royal Society},
  doi = {10.1098/rspa.2021.0068},
  url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2021.0068},
  urldate = {2025-07-18},
  abstract = {A fascinating hypothesis is that human and animal intelligence could be explained by a few principles (rather than an encyclopaedic list of heuristics). If that hypothesis was correct, we could more easily both understand our own intelligence and build intelligent machines. Just like in physics, the principles themselves would not be sufficient to predict the behaviour of complex systems like brains, and substantial computation might be needed to simulate human-like intelligence. This hypothesis would suggest that studying the kind of inductive biases that humans and animals exploit could help both clarify these principles and provide inspiration for AI research and neuroscience theories. Deep learning already exploits several key inductive biases, and this work considers a larger list, focusing on those which concern mostly higher-level and sequential conscious processing. The objective of clarifying these particular principles is that they could potentially help us build AI systems benefiting from humans’ abilities in terms of flexible out-of-distribution and systematic generalization, which is currently an area where a large gap exists between state-of-the-art machine learning and human intelligence.},
  keywords = {causality,deep learning,reasoning,system 2,systematic and out-of-distribution generalization},
  file = {D:\ZoteroLib\storage\E2MINIBR\Goyal and Bengio - 2022 - Inductive biases for deep learning of higher-level cognition.pdf}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long Short-Term Memory},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date = {1997-11},
  year = {1997},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  url = {https://ieeexplore.ieee.org/abstract/document/6795963},
  urldate = {2024-05-14},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  eventtitle = {Neural Computation},
  file = {D:\ZoteroLib\storage\MXHEJSF7\neco.1997.9.8.html}
}

@article{hookerHardwareLottery2021,
  title = {The Hardware Lottery},
  author = {Hooker, Sara},
  date = {2021},
  year = {2021},
  journaltitle = {Commun. ACM},
  volume = {64},
  number = {12},
  pages = {58--65},
  issn = {0001-0782},
  doi = {10.1145/3467017},
  url = {https://dl.acm.org/doi/10.1145/3467017},
  urldate = {2025-10-25},
  abstract = {After decades of incentivizing the isolation of hardware, software, and algorithm development, the catalysts for closer collaboration are changing the paradigm.},
  file = {D:\ZoteroLib\storage\KKZ4EA4B\Hooker - 2021 - The hardware lottery.pdf}
}

@online{joshiTransformersAreGraph2025,
  title = {Transformers Are Graph Neural Networks},
  author = {Joshi, Chaitanya K.},
  date = {2025-06-27},
  year = {2025},
  eprint = {2506.22084},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2506.22084},
  url = {http://arxiv.org/abs/2506.22084},
  urldate = {2025-10-25},
  abstract = {We establish connections between the Transformer architecture, originally introduced for natural language processing, and Graph Neural Networks (GNNs) for representation learning on graphs. We show how Transformers can be viewed as message passing GNNs operating on fully connected graphs of tokens, where the self-attention mechanism capture the relative importance of all tokens w.r.t. each-other, and positional encodings provide hints about sequential ordering or structure. Thus, Transformers are expressive set processing networks that learn relationships among input elements without being constrained by apriori graphs. Despite this mathematical connection to GNNs, Transformers are implemented via dense matrix operations that are significantly more efficient on modern hardware than sparse message passing. This leads to the perspective that Transformers are GNNs currently winning the hardware lottery.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {D\:\\ZoteroLib\\storage\\232UNHF3\\Joshi - 2025 - Transformers are Graph Neural Networks.pdf;D\:\\ZoteroLib\\storage\\2THDIFQZ\\2506.html}
}

@online{hornTranslationalEquivarianceKernelizable2021,
  title = {Translational Equivariance in Kernelizable Attention},
  author = {Horn, Max and Shridhar, Kumar and Groenewald, Elrich and Baumann, Philipp F. M.},
  date = {2021-02-15},
  year = {2021},
  eprint = {2102.07680},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2102.07680},
  url = {http://arxiv.org/abs/2102.07680},
  urldate = {2025-10-25},
  abstract = {While Transformer architectures have show remarkable success, they are bound to the computation of all pairwise interactions of input element and thus suffer from limited scalability. Recent work has been successful by avoiding the computation of the complete attention matrix, yet leads to problems down the line. The absence of an explicit attention matrix makes the inclusion of inductive biases relying on relative interactions between elements more challenging. An extremely powerful inductive bias is translational equivariance, which has been conjectured to be responsible for much of the success of Convolutional Neural Networks on image recognition tasks. In this work we show how translational equivariance can be implemented in efficient Transformers based on kernelizable attention - Performers. Our experiments highlight that the devised approach significantly improves robustness of Performers to shifts of input images compared to their naive application. This represents an important step on the path of replacing Convolutional Neural Networks with more expressive Transformer architectures and will help to improve sample efficiency and robustness in this realm.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {D\:\\ZoteroLib\\storage\\H8YM68RF\\Horn et al. - 2021 - Translational Equivariance in Kernelizable Attention.pdf;D\:\\ZoteroLib\\storage\\QVQZ5QJ6\\2102.html}
}

@inproceedings{tianUDiTsDownsampleTokens2024,
  title = {U-DiTs: Downsample Tokens in U-Shaped Diffusion Transformers},
  booktitle = {NeurIPS},
  author = {Tian, Yuchuan and Tu, Zhijun and Chen, Hanting and Hu, Jie and Xu, Chao and Wang, Yunhe},
  date = {2024-12-16},
  year = {2024},
  volume = {37},
  pages = {51994--52013},
  url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/5d2e24df9cfaad3189833b819c40b392-Abstract-Conference.html},
  urldate = {2025-10-25},
  eventtitle = {NeurIPS},
  langid = {english},
  file = {D:\ZoteroLib\storage\2STI8RL4\Tian et al. - 2024 - U-DiTs Downsample Tokens in U-Shaped Diffusion Transformers.pdf}
}


@online{taiMathematicalExplanationUNet2024,
  title = {A Mathematical Explanation of UNet},
  author = {Tai, Xue-Cheng and Liu, Hao and Chan, Raymond H. and Li, Lingfeng},
  date = {2024-10-06},
  year = {2024},
  eprint = {2410.04434},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2410.04434},
  url = {http://arxiv.org/abs/2410.04434},
  urldate = {2025-10-25},
  abstract = {The UNet architecture has transformed image segmentation. UNet's versatility and accuracy have driven its widespread adoption, significantly advancing fields reliant on machine learning problems with images. In this work, we give a clear and concise mathematical explanation of UNet. We explain what is the meaning and function of each of the components of UNet. We will show that UNet is solving a control problem. We decompose the control variables using multigrid methods. Then, operator-splitting techniques is used to solve the problem, whose architecture exactly recovers the UNet architecture. Our result shows that UNet is a one-step operator-splitting algorithm for the control problem.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\ZoteroLib\\storage\\FRVQRXGW\\Tai et al. - 2024 - A Mathematical Explanation of UNet.pdf;D\:\\ZoteroLib\\storage\\A28NZNAB\\2410.html}
}


@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  date = {1998-11},
  year = {1998},
  journa = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  url = {https://ieeexplore.ieee.org/abstract/document/726791},
  urldate = {2024-06-10},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  eventtitle = {Proceedings of the IEEE},
  keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {D\:\\ZoteroLib\\storage\\EJZBFHSG\\Lecun et al_1998_Gradient-based learning applied to document recognition.pdf;D\:\\ZoteroLib\\storage\\NGQ53GZY\\Lecun et al. - 1998 - Gradient-based learning applied to document recognition.pdf;D\:\\ZoteroLib\\storage\\IELWADJE\\726791.html}
}

@inproceedings{ramachandranStandAloneSelfAttentionVision2019,
  title = {Stand-Alone Self-Attention in Vision Models},
  booktitle = {NeurIPS},
  author = {Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish and Bello, Irwan and Levskaya, Anselm and Shlens, Jon},
  date = {2019},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2019/hash/3416a75f4cea9109507cacd8e2f2aefc-Abstract.html},
  urldate = {2025-10-25},
  abstract = {Convolutions are a fundamental building block of modern  computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention to ResNet-50 produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12\% fewer FLOPS and 29\% fewer parameters. On COCO object detection, a fully self-attention model matches the mAP of a baseline RetinaNet while having 39\% fewer FLOPS and 34\% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox.},
  eventtitle = {NeurIPS},
  file = {D\:\\ZoteroLib\\storage\\6Q57RSMF\\Ramachandran et al. - 2019 - Stand-Alone Self-Attention in Vision Models.pdf;D\:\\ZoteroLib\\storage\\IZMA3EC4\\Stand_Alone_Self_Attention_in_Vision_Models_Appendix.pdf}
}

@online{richsuttonBitterLesson2019,
  title = {The Bitter Lesson},
  author = {Rich Sutton},
  date = {2019},
  year = {2019},
  url = {https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf},
  urldate = {2025-10-25},
  file = {D:\ZoteroLib\storage\WBC6XE4Y\cs.utexas.edu~eunsolcoursesdatabitter_lesson.pdf.pdf}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is All You Need},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
  date = {2017-12-04},
  year = {2017},
  series = {{{NIPS}}'17},
  pages = {6000--6010},
  publisher = {Curran Associates Inc.},
  location = {Red Hook, NY, USA},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  urldate = {2024-07-01},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  isbn = {978-1-5108-6096-4},
  file = {D:\ZoteroLib\storage\THUFLBGR\Vaswani et al_2017_Attention is all you need.pdf}
}

@inproceedings{velickovicGraphAttentionNetworks2018,
  title = {Graph Attention Networks},
  author = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
  date = {2018-02-15},
  year = {2018},
  url = {https://openreview.net/forum?id=rJXMpikCZ},
  urldate = {2025-10-25},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of computationally intensive matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  eventtitle = {ICLR},
  langid = {english},
  file = {D:\ZoteroLib\storage\CM9PSHVP\Veličković et al. - 2018 - Graph Attention Networks.pdf}
}

@online{wuQwenImageTechnicalReport2025,
  title = {Qwen-Image Technical Report},
  author = {Qwen Team},
  date = {2025-08-04},
  year = {2025},
  eprint = {2508.02324},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2508.02324},
  url = {http://arxiv.org/abs/2508.02324},
  urldate = {2025-08-12},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@online{xiaoOmniGenUnifiedImage2024,
  title = {OmniGen: Unified Image Generation},
  shorttitle = {OmniGen},
  author = {Xiao, Shitao and Wang, Yueze and Zhou, Junjie and Yuan, Huaying and Xing, Xingrun and Yan, Ruiran and Wang, Shuting and Huang, Tiejun and Liu, Zheng},
  date = {2024-09-17},
  year = {2024},
  eprint = {2409.11340},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2409.11340},
  url = {http://arxiv.org/abs/2409.11340},
  urldate = {2024-11-12},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{xieSANA15Efficient2025a,
  title = {SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute in Linear Diffusion Transformer},
  shorttitle = {SANA 1.5},
  author = {Xie, Enze and Chen, Junsong and Zhao, Yuyang and Yu, Jincheng and Zhu, Ligeng and Lin, Yujun and Zhang, Zhekai and Li, Muyang and Chen, Junyu and Cai, Han and Liu, Bingchen and Zhou, Daquan and Han, Song},
  date = {2025-06-18},
  year = {2025},
  url = {https://openreview.net/forum?id=27hOkXzy9e},
  urldate = {2025-07-17},
  booktitle = {ICML},
  langid = {english},
}

@inproceedings{xieSANAEfficientHighResolution2025,
  title = {SANA: Efficient High-Resolution Text-to-Image Synthesis with Linear Diffusion Transformers},
  shorttitle = {SANA},
  author = {Xie, Enze and Chen, Junsong and Chen, Junyu and Cai, Han and Tang, Haotian and Lin, Yujun and Zhang, Zhekai and Li, Muyang and Zhu, Ligeng and Lu, Yao and Han, Song},
  date = {2025},
  year = {2025},
  url = {https://openreview.net/forum?id=N8Oj1XhtYZ},
  urldate = {2025-05-11},
  booktitle = {ICLR},
  langid = {english},
}

@inproceedings{wangLiTDelvingSimple2025,
  title = {LiT: Delving into a Simple Linear Diffusion Transformer for Image Generation},
  shorttitle = {LiT},
  author = {Wang, Jiahao and Kang, Ning and Yao, Lewei and Chen, Mengzhao and Wu, Chengyue and Zhang, Songyang and Xue, Shuchen and others},
  date = {2025},
  year = {2025},
  pages = {16068--16078},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  url = {https://openaccess.thecvf.com/content/ICCV2025/html/Wang_LiT_Delving_into_a_Simple_Linear_Diffusion_Transformer_for_Image_ICCV_2025_paper.html},
  urldate = {2025-11-16},
  publisher = {IEEE},
  langid = {english},
}

@inproceedings{gao2023masked,
  title={Masked diffusion transformer is a strong image synthesizer},
  author={Gao, Shanghua and Zhou, Pan and Cheng, Ming-Ming and Yan, Shuicheng},
  booktitle={ICCV},
  pages={23164--23173},
  year = {2023},
}


@inproceedings{xueRAPHAELTexttoImageGeneration2023,
  title = {RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths},
  shorttitle = {RAPHAEL},
  booktitle = {NeurIPS},
  author = {Xue, Zeyue and Song, Guanglu and Guo, Qiushan and Liu, Boxiao and Zong, Zhuofan and Liu, Yu and Luo, Ping},
  date = {2023},
  year = {2023},
  volume = {36},
  pages = {41693--41706},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/821655c7dc4836838cd8524d07f9d6fd-Abstract-Conference.html},
  urldate = {2024-04-15},
  booktitle = {NeurIPS},
  langid = {english},
}

@online{yanGPTImgEvalComprehensiveBenchmark2025,
  title = {GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation},
  shorttitle = {GPT-ImgEval},
  author = {Yan, Zhiyuan and Ye, Junyan and Li, Weijia and Huang, Zilong and Yuan, Shenghai and He, Xiangyang and Lin, Kaiqing and He, Jun and He, Conghui and Yuan, Li},
  date = {2025-04-03},
  year = {2025},
  eprint = {2504.02782},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.02782},
  url = {http://arxiv.org/abs/2504.02782},
  urldate = {2025-04-13},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@online{yeImgEditUnifiedImage2025,
  title = {ImgEdit: A Unified Image Editing Dataset and Benchmark},
  shorttitle = {ImgEdit},
  author = {Ye, Yang and He, Xianyi and Li, Zongjian and Lin, Bin and Yuan, Shenghai and Yan, Zhiyuan and Hou, Bohan and Yuan, Li},
  date = {2025-05-26},
  year = {2025},
  eprint = {2505.20275},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2505.20275},
  url = {http://arxiv.org/abs/2505.20275},
  urldate = {2025-10-03},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{zhanConditionalImageSynthesis2025,
  title = {Conditional Image Synthesis with Diffusion Models: A Survey},
  shorttitle = {Conditional Image Synthesis with Diffusion Models},
  author = {Zhan, Zheyuan and Chen, Defang and Mei, Jian-Ping and Zhao, Zhenghe and Chen, Jiawei and Chen, Chun and Lyu, Siwei and Wang, Can},
  date = {2025},
  year = {2025},
  journal = {TMLR},
  issn = {2835-8856},
  url = {https://openreview.net/forum?id=ewwNKwh6SK},
  urldate = {2025-10-01},
  langid = {english},
}

@online{zhangInContextEditEnabling2025,
  title = {In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer},
  shorttitle = {In-Context Edit},
  author = {Zhang, Zechuan and Xie, Ji and Lu, Yu and Yang, Zongxin and Yang, Yi},
  date = {2025-04-29},
  year = {2025},
  eprint = {2504.20690},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.20690},
  url = {http://arxiv.org/abs/2504.20690},
  urldate = {2025-05-01},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{zhangMagicBrushManuallyAnnotated2023,
  title = {MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing},
  shorttitle = {MagicBrush},
  booktitle = {NeurIPS},
  author = {Zhang, Kai and Mo, Lingbo and Chen, Wenhu and Sun, Huan and Su, Yu},
  date = {2023},
  year = {2023},
  volume = {36},
  pages = {31428--31449},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/64008fa30cba9b4d1ab1bd3bd3d57d61-Abstract-Datasets_and_Benchmarks.html},
  urldate = {2024-04-15},
  langid = {english},
}

@online{zhangTexttoimageDiffusionModels2024,
  title = {Text-to-Image Diffusion Models in Generative AI: A Survey},
  shorttitle = {Text-to-Image Diffusion Models in Generative AI},
  author = {Zhang, Chenshuang and Zhang, Chaoning and Zhang, Mengchun and Kweon, In So and Kim, Junmo},
  date = {2024-11-08},
  year = {2024},
  eprint = {2303.07909},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.07909},
  url = {http://arxiv.org/abs/2303.07909},
  urldate = {2025-10-06},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
}

@online{zhangTexttoImageSynthesisDecade2024,
  title = {Text-to-Image Synthesis: A Decade Survey},
  shorttitle = {Text-to-Image Synthesis},
  author = {Zhang, Nonghai and Tang, Hao},
  date = {2024-11-25},
  year = {2024},
  eprint = {2411.16164},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2411.16164},
  url = {http://arxiv.org/abs/2411.16164},
  urldate = {2024-12-26},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@online{zhaoEnvisioningPixelsBenchmarking2025,
  title = {Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing},
  shorttitle = {Envisioning Beyond the Pixels},
  author = {Zhao, Xiangyu and Zhang, Peiyuan and Tang, Kexian and Zhu, Xiaorong and Li, Hao and Chai, Wenhao and Zhang, Zicheng and Xia, Renqiu and Zhai, Guangtao and Yan, Junchi and Yang, Hua and Yang, Xue and Duan, Haodong},
  date = {2025-05-27},
  year = {2025},
  eprint = {2504.02826},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.02826},
  url = {http://arxiv.org/abs/2504.02826},
  urldate = {2025-09-18},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{zhaoUltraEditInstructionbasedFineGrained2024b,
  title = {UltraEdit: Instruction-based Fine-Grained Image Editing at Scale},
  shorttitle = {UltraEdit},
  author = {Zhao, Haozhe and Ma, Xiaojian and Chen, Liang and Si, Shuzheng and Wu, Rujie and An, Kaikai and Yu, Peiyu and Zhang, Minjia and Li, Qing and Chang, Baobao},
  date = {2024-11-13},
  year = {2024},
  url = {https://openreview.net/forum?id=9ZDdlgH6O8#discussion},
  urldate = {2025-05-06},
  booktitle = {NeurIPS},
  langid = {english},
}

@inproceedings{zhengCogView3FinerFaster2024a,
  title = {CogView3: Finer and~Faster Text-to-Image Generation via~Relay Diffusion},
  shorttitle = {CogView3},
  booktitle = {ECCV},
  author = {Zheng, Wendi and Teng, Jiayan and Yang, Zhuoyi and Wang, Weihan and Chen, Jidong and Gu, Xiaotao and Dong, Yuxiao and Ding, Ming and Tang, Jie},
  editor = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
  date = {2024},
  year = {2024},
  pages = {1--22},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-72980-5_1},
  isbn = {978-3-031-72980-5},
  langid = {english},
  keywords = {Diffusion Models,Text-to-Image Generation},
}

@online{zhengContinuouslyAugmentedDiscrete2025,
  title = {Continuously Augmented Discrete Diffusion Model for Categorical Generative Modeling},
  author = {Zheng, Huangjie and Gong, Shansan and Zhang, Ruixiang and Chen, Tianrong and Gu, Jiatao and Zhou, Mingyuan and Jaitly, Navdeep and Zhang, Yizhe},
  date = {2025-10-01},
  year = {2025},
  eprint = {2510.01329},
  eprinttype = {arXiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.2510.01329},
  url = {http://arxiv.org/abs/2510.01329},
  urldate = {2025-10-05},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
}

@online{zhuoFactualityMattersWhen2025,
  title = {Factuality Matters: When Image Generation and Editing Meet Structured Visuals},
  shorttitle = {Factuality Matters},
  author = {Zhuo, Le and Han, Songhao and Pu, Yuandong and Qiu, Boxiang and Paul, Sayak and Liao, Yue and Liu, Yihao and Shao, Jie and Chen, Xi and Liu, Si and Li, Hongsheng},
  date = {2025-10-06},
  year = {2025},
  eprint = {2510.05091},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2510.05091},
  url = {http://arxiv.org/abs/2510.05091},
  urldate = {2025-10-10},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@online{CVPR2023Tutorial,
  title = {CVPR 2023 Tutorial Denoising Diffusion-based Generative Modeling: Foundation and Applications},
  shorttitle = {Denoising {{Diffusion-based Generative Modeling}}},
  url = {https://cvpr2023-tutorial-diffusion-models.github.io},
  urldate = {2024-10-15},
  year = {2023},
  abstract = {Tutorial in Conjunction with CVPR 2023},
  author={Vahdat, Arash and Song, Jiaming and Meng, Chenlin}
}


@inproceedings{zhuoLuminaNextMakingLuminaT2X2024a,
  title = {Lumina-Next : Making Lumina-T2X Stronger and Faster with Next-DiT},
  shorttitle = {Lumina-Next},
  author = {Zhuo, Le and Du, Ruoyi and Xiao, Han and Li, Yangguang and Liu, Dongyang and Huang, Rongjie and Liu, Wenze and Zhu, Xiangyang and Wang, Fu-Yun and Ma, Zhanyu and Luo, Xu and Wang, Zehan and Zhang, Kaipeng and Zhao, Lirui and Liu, Si and Yue, Xiangyu and Ouyang, Wanli and Qiao, Yu and Li, Hongsheng and Gao, Peng},
  date = {2024-11-06},
  year = {2024},
  url = {https://openreview.net/forum?id=ieYdf9TZ2u},
  urldate = {2025-05-19},
  booktitle = {NeurIPS},
  langid = {english},
}

@article{hoCascadedDiffusionModels2022,
  title = {Cascaded Diffusion Models for High Fidelity Image Generation},
  author = {Ho, Jonathan and Saharia, Chitwan and Chan, William and Fleet, David J. and Norouzi, Mohammad and Salimans, Tim},
  date = {2022-01-01},
  year = {2022},
  journal = {JMLR},
  shortjournal = {J. Mach. Learn. Res.},
  volume = {23},
  number = {1},
  pages = {47:2249--47:2281},
  issn = {1532-4435},
  keywords = {diffusion models,generative models,iterative refinement,score matching,super-resolution},
  annotation = {CCF-A},
  file = {D:\Zotero\storage(sakura)\storage\6C2HY2P3\Ho et al_2022_Cascaded diffusion models for high fidelity image generation.pdf}
}

@article{sahariaImageSuperResolution2022,
  title={Image super-resolution via iterative refinement},
  author={Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J and Norouzi, Mohammad},
  journal={TPAMI},
  volume={45},
  number={4},
  pages={4713--4726},
  year = {2022},
  publisher={IEEE}
}

@online{wuRepresentationEntanglementGeneration2025,
  title     = {Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think},
  shorttitle= {Representation {{Entanglement}} for {{Generation}}},
  author    = {Wu, Ge and Zhang, Shen and Shi, Ruijing and Gao, Shanghua and Chen, Zhenyuan and Wang, Lei and Chen, Zhaowei and Gao, Hongcheng and Tang, Yao and Yang, Jian and Cheng, Ming-Ming and Li, Xiang},
  date      = {2025-07-02},
  year = {2025},
  eprint    = {2507.01467},
  eprinttype= {arXiv},
  eprintclass= {cs},
  doi       = {10.48550/arXiv.2507.01467},
  url       = {http://arxiv.org/abs/2507.01467},
  urldate   = {2025-09-21},
  pubstate  = {prepublished},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition},
  file      = {D:\Zotero\storage(sakura)\storage\IUBFLYA8\Wu et al. - 2025 - Representation Entanglement for GenerationTraining Diffusion Transformers Is Much Easier Than You T.pdf}
}

@inproceedings{yuRepresentationAlignmentGeneration2025,
  title     = {Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think},
  shorttitle= {Representation {{Alignment}} for {{Generation}}},
  author    = {Yu, Sihyun and Kwak, Sangkyung and Jang, Huiwon and Jeong, Jongheon and Huang, Jonathan and Shin, Jinwoo and Xie, Saining},
  date      = {2025},
  year = {2025},
  url       = {https://openreview.net/forum?id=DJSZGGZYVi},
  urldate   = {2025-09-21},
  booktitle= {ICLR},
  file      = {D:\Zotero\storage(sakura)\storage\P9MC6S6M\Yu et al. - 2024 - Representation Alignment for Generation Training Diffusion Transformers Is Easier Than You Think.pdf}
}

@online{blackforestlabsFLUX1,
  title = {FLUX.1},
  author = {Black Forest Labs},
  url = {https://blackforestlabs.io/flux-1/},
  year = {2025},
  urldate = {2025-06-03},
  abstract = {BlackForestLabs is proud to announce the release of the FLUX.1, a groundbreaking set of text-to-image models that set a new standard in the field},
  langid = {american},
  organization = {BlackForestLabs},
  file = {D:\ZoteroLib\storage\CRDCIFTE\flux-1.html}
}


@inproceedings{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  booktitle = {MICCAI},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  date = {2015},
  year = {2015},
  volume = {9351},
  pages = {234--241},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-24574-4_28},
  url = {http://link.springer.com/10.1007/978-3-319-24574-4_28},
  urldate = {2024-04-28},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  eventtitle = {MICCAI},
  isbn = {978-3-319-24573-7 978-3-319-24574-4},
  langid = {english},
  file = {D:\ZoteroLib\storage\XYRVIIVK\Ronneberger et al_2015_U-Net.pdf}
}

@online{wangDDTDecoupledDiffusion2025,
  title = {DDT: Decoupled Diffusion Transformer},
  shorttitle = {DDT},
  author = {Wang, Shuai and Tian, Zhi and Huang, Weilin and Wang, Limin},
  date = {2025-04-09},
  year = {2025},
  eprint = {2504.05741},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.05741},
  url = {http://arxiv.org/abs/2504.05741},
  urldate = {2025-10-13},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition}
}

@online{zhengDiffusionTransformersRepresentation2025,
  title = {Diffusion Transformers with Representation Autoencoders},
  author = {Zheng, Boyang and Ma, Nanye and Tong, Shengbang and Xie, Saining},
  date = {2025-10-13},
  year = {2025},
  eprint = {2510.11690},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2510.11690},
  url = {http://arxiv.org/abs/2510.11690},
  urldate = {2025-10-13}
}

@online{chenAligningVisualFoundation2025,
  title = {Aligning Visual Foundation Encoders to Tokenizers for Diffusion Models},
  author = {Chen, Bowei and Bi, Sai and Tan, Hao and Zhang, He and Zhang, Tianyuan and Li, Zhengqi and Xiong, Yuanjun and Zhang, Jianming and Zhang, Kai},
  date = {2025-09-29},
  year = {2025},
  eprint = {2509.25162},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2509.25162},
  url = {http://arxiv.org/abs/2509.25162},
  urldate = {2025-10-15}
}

@inproceedings{hoogeboomSimpleDiffusionEndtoend2023,
  title = {Simple Diffusion: End-to-end Diffusion for High Resolution Images},
  shorttitle = {Simple Diffusion},
  author = {Hoogeboom, Emiel and Heek, Jonathan and Salimans, Tim},
  year = {2023},
  month = {jun},
  urldate = {2025-11-19}
}

@inproceedings{hoogeboomSimplerDiffusion152025,
  title = {Simpler Diffusion: 1.5 FID on ImageNet512 with Pixel-Space Diffusion},
  shorttitle = {Simpler Diffusion},
  booktitle = {2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  author = {Hoogeboom, Emiel and Mensink, Thomas and Heek, Jonathan and Lamerigts, Kay and Gao, Ruiqi and Salimans, Tim},
  year = {2025},
  month = {jun},
  pages = {18062--18071},
  issn = {2575-7075},
  doi = {10.1109/CVPR52734.2025.01683},
  urldate = {2025-11-19},
  keywords = {Computer vision,diffusion models,Diffusion models,Image quality,Image resolution,image synthesis,Image synthesis,Memory architecture,Pattern recognition,pixel diffusion}
}

@misc{liBackBasicsLet2025a,
  title = {Back to Basics: Let Denoising Generative Models Denoise},
  shorttitle = {Back to Basics},
  author = {Li, Tianhong and He, Kaiming},
  year = {2025},
  month = {nov},
  urldate = {2025-11-19}
}

@inproceedings{luFiTFlexibleVision2024,
  title = {FiT: Flexible Vision Transformer for Diffusion Model},
  shorttitle = {FiT},
  booktitle = {Forty-First International Conference on Machine Learning},
  author = {Lu, Zeyu and Wang, ZiDong and Huang, Di and Wu, Chengyue and Liu, Xihui and Ouyang, Wanli and Bai, Lei},
  year = {2024},
  month = {jun},
  urldate = {2025-08-03}
}

@inproceedings{maSiTExploringFlow2024,
  title = {SiT: Exploring Flow and Diffusion-Based Generative Models with Scalable Interpolant Transformers},
  shorttitle = {SiT},
  booktitle = {Computer Vision -- ECCV 2024},
  author = {Ma, Nanye and Goldstein, Mark and Albergo, Michael S. and Boffi, Nicholas M. and Vanden-Eijnden, Eric and Xie, Saining},
  editor = {Leonardis, Ales and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gul},
  year = {2024},
  volume = {15135},
  pages = {23--40},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-72980-5_2},
  urldate = {2024-11-20},
  isbn = {978-3-031-72979-9 978-3-031-72980-5}
}

@inproceedings{nguyenImageWorthMore2024a,
  title = {An Image Is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels},
  shorttitle = {An Image Is Worth More Than 16x16 Patches},
  booktitle = {ICLR},
  author = {Nguyen, Duy Kien and Assran, Mido and Jain, Unnat and Oswald, Martin R. and Snoek, Cees G. M. and Chen, Xinlei},
  year = {2025},
  month = {may},
  urldate = {2025-11-19}
}

@inproceedings{wangNativeResolutionImageSynthesis2025,
  title = {Native-Resolution Image Synthesis},
  booktitle = {The Thirty-ninth Annual Conference on Neural Information Processing Systems},
  author = {Wang, ZiDong and Bai, Lei and Yue, Xiangyu and Ouyang, Wanli and Zhang, Yiyuan},
  year = {2025},
  month = {oct},
  urldate = {2025-11-19}
}

@inproceedings{yaoReconstructionVsGeneration2025,
  title = {Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models},
  shorttitle = {Reconstruction vs. Generation},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  author = {Yao, Jingfeng and Yang, Bin and Wang, Xinggang},
  year = {2025},
  pages = {15703--15712},
  urldate = {2025-11-19}
}

@inproceedings{zhuDiGScalableEfficient2025,
  title = {DiG: Scalable and Efficient Diffusion Models with Gated Linear Attention},
  shorttitle = {DiG},
  booktitle = {2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  author = {Zhu, Lianghui and Huang, Zilong and Liao, Bencheng and Liew, Jun Hao and Yan, Hanshu and Feng, Jiashi and Wang, Xinggang},
  year = {2025},
  month = {jun},
  pages = {7664--7674},
  issn = {2575-7075},
  doi = {10.1109/CVPR52734.2025.00718},
  urldate = {2025-11-19},
  keywords = {Computer vision,Diffusion models,Graphics processing units,Image resolution,Image synthesis,linear attention,Logic gates,Pattern recognition,Transformers,Videos,Visualization}
}

@misc{chenDiPTamingDiffusion2025,
  title = {DiP: Taming Diffusion Models in Pixel Space},
  shorttitle = {DiP},
  author = {Chen, Zhennan and Zhu, Junwei and Chen, Xu and Zhang, Jiangning and Hu, Xiaobin and Zhao, Hanzhen and Wang, Chengjie and Yang, Jian and Tai, Ying},
  year = {2025},
  month = {nov},
  number = {arXiv:2511.18822},
  eprint = {2511.18822},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2511.18822},
  urldate = {2025-11-26},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{maDeCoFrequencyDecoupledPixel2025,
  title = {DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation},
  shorttitle = {DeCo},
  author = {Ma, Zehong and Wei, Longhui and Wang, Shuai and Zhang, Shiliang and Tian, Qi},
  year = {2025},
  month = {nov},
  number = {arXiv:2511.19365},
  eprint = {2511.19365},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2511.19365},
  urldate = {2025-11-26},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition}
}

@misc{yuPixelDiTPixelDiffusion2025,
  title = {PixelDiT: Pixel Diffusion Transformers for Image Generation},
  shorttitle = {PixelDiT},
  author = {Yu, Yongsheng and Xiong, Wei and Nie, Weili and Sheng, Yichen and Liu, Shiqiu and Luo, Jiebo},
  year = {2025},
  month = {nov},
  number = {arXiv:2511.20645},
  eprint = {2511.20645},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2511.20645},
  urldate = {2025-11-26},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{z-image-2025,
  title = {Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer},
  author = {Tongyi Lab},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/Tongyi-MAI/Z-Image}}
}

@misc{flux-2-2025,
  author = {Black Forest Labs},
  title = {FLUX.2: Frontier Visual Intelligence},
  year = {2025},
  howpublished = {\url{https://bfl.ai/blog/flux-2}}
}

