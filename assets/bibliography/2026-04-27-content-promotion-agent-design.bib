@misc{zhou_wpo_2024,
	title = {{WPO}: Enhancing {RLHF} with Weighted Preference Optimization},
	url = {http://arxiv.org/abs/2406.11827},
	shorttitle = {{WPO}},
	abstract = {Reinforcement learning from human feedback ({RLHF}) is a promising solution to align large language models ({LLMs}) more closely with human values. Off-policy preference optimization, where the preference data is obtained from other models, is widely adopted due to its cost efficiency and scalability. However, off-policy preference optimization often suffers from a distributional gap between the policy used for data collection and the target policy, leading to suboptimal optimization. In this paper, we propose a novel strategy to mitigate this problem by simulating on-policy learning with off-policy preference data. Our Weighted Preference Optimization ({WPO}) method adapts off-policy data to resemble on-policy data more closely by reweighting preference pairs according to their probability under the current policy. This method not only addresses the distributional gap problem but also enhances the optimization process without incurring additional costs. We validate our method on instruction following benchmarks including Alpaca Eval 2 and {MT}-bench. {WPO} not only outperforms Direct Preference Optimization ({DPO}) by up to 5.6\% on Alpaca Eval 2 but also establishes a remarkable length-controlled winning rate against {GPT}-4-turbo of 76.7\% based on Gemma-2-9b-it. We release the code and models at https://github.com/wzhouad/{WPO}.},
	number = {{arXiv}:2406.11827},
	publisher = {{arXiv}},
	author = {Zhou, Wenxuan and Agrawal, Ravi and Zhang, Shujian and Indurthi, Sathish Reddy and Zhao, Sanqiang and Song, Kaiqiang and Xu, Silei and Zhu, Chenguang},
	urldate = {2024-11-23},
	date = {2024-10-03},
	eprinttype = {arxiv},
	eprint = {2406.11827 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\G4SCPXN3\\Zhou et al. - 2024 - WPO Enhancing RLHF with Weighted Preference Optim.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\RSX4KUA6\\2406.html:text/html},
}

@article{kalai_commitment_2010,
	title = {A commitment folk theorem},
	volume = {69},
	issn = {0899-8256},
	url = {https://www.sciencedirect.com/science/article/pii/S0899825609001912},
	doi = {10.1016/j.geb.2009.09.008},
	series = {Special Issue In Honor of Robert Aumann},
	abstract = {Real world players often increase their payoffs by voluntarily committing to play a fixed strategy, prior to the start of a strategic game. In fact, the players may further benefit from commitments that are conditional on the commitments of others. This paper proposes a model of conditional commitments that unifies earlier models while avoiding circularities that often arise in such models. A commitment folk theorem shows that the potential of voluntary conditional commitments is essentially unlimited. All feasible and individually rational payoffs of a two-person strategic game can be attained at the equilibria of one (universal) commitment game that uses simple commitment devices. The commitments are voluntary in the sense that each player maintains the option of playing the game without commitment, as originally defined.},
	pages = {127--137},
	number = {1},
	journaltitle = {Games and Economic Behavior},
	shortjournal = {Games and Economic Behavior},
	author = {Kalai, Adam Tauman and Kalai, Ehud and Lehrer, Ehud and Samet, Dov},
	urldate = {2025-10-17},
	date = {2010-05-01},
	file = {ScienceDirect Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\7G682T7H\\S0899825609001912.html:text/html},
}

@misc{li_meta_game_2024,
	title = {A Meta-Game Evaluation Framework for Deep Multiagent Reinforcement Learning},
	url = {http://arxiv.org/abs/2405.00243},
	doi = {10.48550/arXiv.2405.00243},
	abstract = {Evaluating deep multiagent reinforcement learning ({MARL}) algorithms is complicated by stochasticity in training and sensitivity of agent performance to the behavior of other agents. We propose a meta-game evaluation framework for deep {MARL}, by framing each {MARL} algorithm as a meta-strategy, and repeatedly sampling normal-form empirical games over combinations of meta-strategies resulting from different random seeds. Each empirical game captures both self-play and cross-play factors across seeds. These empirical games provide the basis for constructing a sampling distribution, using bootstrapping, over a variety of game analysis statistics. We use this approach to evaluate state-of-the-art deep {MARL} algorithms on a class of negotiation games. From statistics on individual payoffs, social welfare, and empirical best-response graphs, we uncover strategic relationships among self-play, population-based, model-free, and model-based {MARL} methods.We also investigate the effect of run-time search as a meta-strategy operator, and find via meta-game analysis that the search version of a meta-strategy generally leads to improved performance.},
	number = {{arXiv}:2405.00243},
	publisher = {{arXiv}},
	author = {Li, Zun and Wellman, Michael P.},
	urldate = {2025-10-17},
	date = {2024-04-30},
	eprinttype = {arxiv},
	eprint = {2405.00243 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\J7US49VV\\Li and Wellman - 2024 - A Meta-Game Evaluation Framework for Deep Multiage.pdf:application/pdf},
}

@misc{hajiaghayi_ad_2024,
	title = {Ad Auctions for {LLMs} via Retrieval Augmented Generation},
	url = {http://arxiv.org/abs/2406.09459},
	abstract = {In the field of computational advertising, the integration of ads into the outputs of large language models ({LLMs}) presents an opportunity to support these services without compromising content integrity. This paper introduces novel auction mechanisms for ad allocation and pricing within the textual outputs of {LLMs}, leveraging retrieval-augmented generation ({RAG}). We propose a segment auction where an ad is probabilistically retrieved for each discourse segment (paragraph, section, or entire output) according to its bid and relevance, following the {RAG} framework, and priced according to competing bids. We show that our auction maximizes logarithmic social welfare, a new notion of welfare that balances allocation efficiency and fairness, and we characterize the associated incentive-compatible pricing rule. These results are extended to multi-ad allocation per segment. An empirical evaluation validates the feasibility and effectiveness of our approach over several ad auction scenarios, and exhibits inherent tradeoffs in metrics as we allow the {LLM} more flexibility to allocate ads.},
	number = {{arXiv}:2406.09459},
	publisher = {{arXiv}},
	author = {Hajiaghayi, {MohammadTaghi} and Lahaie, Sébastien and Rezaei, Keivan and Shin, Suho},
	urldate = {2024-10-23},
	date = {2024-06-12},
	eprinttype = {arxiv},
	eprint = {2406.09459 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\4MKQ68RW\\Hajiaghayi et al. - 2024 - Ad Auctions for LLMs via Retrieval Augmented Gener.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\ARF39VC6\\2406.html:text/html},
}

@misc{kovarik_ai_2025,
	title = {{AI} Testing Should Account for Sophisticated Strategic Behaviour},
	url = {http://arxiv.org/abs/2508.14927},
	doi = {10.48550/arXiv.2508.14927},
	abstract = {This position paper argues for two claims regarding {AI} testing and evaluation. First, to remain informative about deployment behaviour, evaluations need account for the possibility that {AI} systems understand their circumstances and reason strategically. Second, game-theoretic analysis can inform evaluation design by formalising and scrutinising the reasoning in evaluation-based safety cases. Drawing on examples from existing {AI} systems, a review of relevant research, and formal strategic analysis of a stylised evaluation scenario, we present evidence for these claims and motivate several research directions.},
	number = {{arXiv}:2508.14927},
	publisher = {{arXiv}},
	author = {Kovarik, Vojtech and Chen, Eric Olav and Petersen, Sami and Ghersengorin, Alexis and Conitzer, Vincent},
	urldate = {2025-10-17},
	date = {2025-08-19},
	eprinttype = {arxiv},
	eprint = {2508.14927 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\U68B8P2T\\Kovarik et al. - 2025 - AI Testing Should Account for Sophisticated Strate.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\UBV4MDLX\\2508.html:text/html},
}

@misc{feige_chasing_2014,
	title = {Chasing Ghosts: Competing with Stateful Policies},
	url = {http://arxiv.org/abs/1407.7635},
	doi = {10.48550/arXiv.1407.7635},
	shorttitle = {Chasing Ghosts},
	abstract = {We consider sequential decision making in a setting where regret is measured with respect to a set of stateful reference policies, and feedback is limited to observing the rewards of the actions performed (the so called "bandit" setting). If either the reference policies are stateless rather than stateful, or the feedback includes the rewards of all actions (the so called "expert" setting), previous work shows that the optimal regret grows like \${\textbackslash}Theta({\textbackslash}sqrt\{T\})\$ in terms of the number of decision rounds \$T\$. The difficulty in our setting is that the decision maker unavoidably loses track of the internal states of the reference policies, and thus cannot reliably attribute rewards observed in a certain round to any of the reference policies. In fact, in this setting it is impossible for the algorithm to estimate which policy gives the highest (or even approximately highest) total reward. Nevertheless, we design an algorithm that achieves expected regret that is sublinear in \$T\$, of the form \$O( T/{\textbackslash}log{\textasciicircum}\{1/4\}\{T\})\$. Our algorithm is based on a certain local repetition lemma that may be of independent interest. We also show that no algorithm can guarantee expected regret better than \$O( T/{\textbackslash}log{\textasciicircum}\{3/2\} T)\$.},
	number = {{arXiv}:1407.7635},
	publisher = {{arXiv}},
	author = {Feige, Uriel and Koren, Tomer and Tennenholtz, Moshe},
	urldate = {2025-10-17},
	date = {2014-07-29},
	eprinttype = {arxiv},
	eprint = {1407.7635 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\FKXVR5WB\\Feige et al. - 2014 - Chasing Ghosts Competing with Stateful Policies.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\EWP4PG7I\\1407.html:text/html},
}

@inproceedings{kurland_competitive_2022,
	location = {Madrid Spain},
	title = {Competitive Search},
	isbn = {978-1-4503-8732-3},
	url = {https://dl.acm.org/doi/10.1145/3477495.3532771},
	doi = {10.1145/3477495.3532771},
	eventtitle = {{SIGIR} '22: The 45th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	pages = {2838--2849},
	booktitle = {Proceedings of the 45th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Kurland, Oren and Tennenholtz, Moshe},
	urldate = {2024-10-21},
	date = {2022-07-06},
	langid = {english},
	file = {Kurland and Tennenholtz - 2022 - Competitive Search.pdf:C\:\\Users\\tommy\\Zotero\\storage\\4PCI8CUG\\Kurland and Tennenholtz - 2022 - Competitive Search.pdf:application/pdf},
}

@misc{liu_datasentinel_2025,
	title = {{DataSentinel}: A Game-Theoretic Detection of Prompt Injection Attacks},
	url = {http://arxiv.org/abs/2504.11358},
	doi = {10.48550/arXiv.2504.11358},
	shorttitle = {{DataSentinel}},
	abstract = {{LLM}-integrated applications and agents are vulnerable to prompt injection attacks, where an attacker injects prompts into their inputs to induce attacker-desired outputs. A detection method aims to determine whether a given input is contaminated by an injected prompt. However, existing detection methods have limited effectiveness against state-of-the-art attacks, let alone adaptive ones. In this work, we propose {DataSentinel}, a game-theoretic method to detect prompt injection attacks. Specifically, {DataSentinel} fine-tunes an {LLM} to detect inputs contaminated with injected prompts that are strategically adapted to evade detection. We formulate this as a minimax optimization problem, with the objective of fine-tuning the {LLM} to detect strong adaptive attacks. Furthermore, we propose a gradient-based method to solve the minimax optimization problem by alternating between the inner max and outer min problems. Our evaluation results on multiple benchmark datasets and {LLMs} show that {DataSentinel} effectively detects both existing and adaptive prompt injection attacks.},
	number = {{arXiv}:2504.11358},
	publisher = {{arXiv}},
	author = {Liu, Yupei and Jia, Yuqi and Jia, Jinyuan and Song, Dawn and Gong, Neil Zhenqiang},
	urldate = {2025-10-17},
	date = {2025-09-14},
	eprinttype = {arxiv},
	eprint = {2504.11358 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\HKBEJ3YF\\Liu et al. - 2025 - DataSentinel A Game-Theoretic Detection of Prompt.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\N97F8BFG\\2504.html:text/html},
}

@misc{gemp_developing_2022,
	title = {Developing, Evaluating and Scaling Learning Agents in Multi-Agent Environments},
	url = {http://arxiv.org/abs/2209.10958},
	doi = {10.48550/arXiv.2209.10958},
	abstract = {The Game Theory \& Multi-Agent team at {DeepMind} studies several aspects of multi-agent learning ranging from computing approximations to fundamental concepts in game theory to simulating social dilemmas in rich spatial environments and training 3-d humanoids in difficult team coordination tasks. A signature aim of our group is to use the resources and expertise made available to us at {DeepMind} in deep reinforcement learning to explore multi-agent systems in complex environments and use these benchmarks to advance our understanding. Here, we summarise the recent work of our team and present a taxonomy that we feel highlights many important open challenges in multi-agent research.},
	number = {{arXiv}:2209.10958},
	publisher = {{arXiv}},
	author = {Gemp, Ian and Anthony, Thomas and Bachrach, Yoram and Bhoopchand, Avishkar and Bullard, Kalesha and Connor, Jerome and Dasagi, Vibhavari and Vylder, Bart De and Duenez-Guzman, Edgar and Elie, Romuald and Everett, Richard and Hennes, Daniel and Hughes, Edward and Khan, Mina and Lanctot, Marc and Larson, Kate and Lever, Guy and Liu, Siqi and Marris, Luke and {McKee}, Kevin R. and Muller, Paul and Perolat, Julien and Strub, Florian and Tacchetti, Andrea and Tarassov, Eugene and Wang, Zhe and Tuyls, Karl},
	urldate = {2025-11-08},
	date = {2022-09-22},
	eprinttype = {arxiv},
	eprint = {2209.10958 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\LKARG9UH\\Gemp et al. - 2022 - Developing, Evaluating and Scaling Learning Agents.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\QGHARHIM\\2209.html:text/html},
}

@inproceedings{conitzer_foundations_2023,
	title = {Foundations of cooperative {AI}},
	volume = {37},
	isbn = {978-1-57735-880-0},
	url = {https://doi.org/10.1609/aaai.v37i13.26791},
	doi = {10.1609/aaai.v37i13.26791},
	series = {{AAAI}'23/{IAAI}'23/{EAAI}'23},
	abstract = {{AI} systems can interact in unexpected ways, sometimes with disastrous consequences. As {AI} gets to control more of our world, these interactions will become more common and have higher stakes. As {AI} becomes more advanced, these interactions will become more sophisticated, and game theory will provide the tools for analyzing these interactions. However, {AI} agents are in some ways unlike the agents traditionally studied in game theory, introducing new challenges as well as opportunities. We propose a research agenda to develop the game theory of highly advanced {AI} agents, with a focus on achieving cooperation.},
	pages = {15359--15367},
	booktitle = {Proceedings of the Thirty-Seventh {AAAI} Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
	publisher = {{AAAI} Press},
	author = {Conitzer, Vincent and Oesterheld, Caspar},
	urldate = {2025-11-08},
	date = {2023-02-07},
	file = {Full Text:C\:\\Users\\tommy\\Zotero\\storage\\WK28D3M9\\Conitzer and Oesterheld - 2023 - Foundations of cooperative AI.pdf:application/pdf},
}

@misc{kovarik_game_2024,
	title = {Game Theory with Simulation of Other Players},
	url = {http://arxiv.org/abs/2305.11261},
	doi = {10.48550/arXiv.2305.11261},
	abstract = {Game-theoretic interactions with {AI} agents could differ from traditional human-human interactions in various ways. One such difference is that it may be possible to simulate an {AI} agent (for example because its source code is known), which allows others to accurately predict the agent's actions. This could lower the bar for trust and cooperation. In this paper, we formalize games in which one player can simulate another at a cost. We first derive some basic properties of such games and then prove a number of results for them, including: (1) introducing simulation into generic-payoff normal-form games makes them easier to solve; (2) if the only obstacle to cooperation is a lack of trust in the possibly-simulated agent, simulation enables equilibria that improve the outcome for both agents; and however (3) there are settings where introducing simulation results in strictly worse outcomes for both players.},
	number = {{arXiv}:2305.11261},
	publisher = {{arXiv}},
	author = {Kovarik, Vojtech and Oesterheld, Caspar and Conitzer, Vincent},
	urldate = {2025-10-17},
	date = {2024-03-19},
	eprinttype = {arxiv},
	eprint = {2305.11261 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\9RKZ6YAA\\Kovarik et al. - 2024 - Game Theory with Simulation of Other Players.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\ZUBFBFIC\\2305.html:text/html},
}

@misc{aggarwal_geo_2024,
	title = {{GEO}: Generative Engine Optimization},
	url = {http://arxiv.org/abs/2311.09735},
	doi = {10.48550/arXiv.2311.09735},
	shorttitle = {{GEO}},
	abstract = {The advent of large language models ({LLMs}) has ushered in a new paradigm of search engines that use generative models to gather and summarize information to answer user queries. This emerging technology, which we formalize under the unified framework of generative engines ({GEs}), can generate accurate and personalized responses, rapidly replacing traditional search engines like Google and Bing. Generative Engines typically satisfy queries by synthesizing information from multiple sources and summarizing them using {LLMs}. While this shift significantly improves \${\textbackslash}textit\{user\}\$ utility and \${\textbackslash}textit\{generative search engine\}\$ traffic, it poses a huge challenge for the third stakeholder -- website and content creators. Given the black-box and fast-moving nature of generative engines, content creators have little to no control over \${\textbackslash}textit\{when\}\$ and \${\textbackslash}textit\{how\}\$ their content is displayed. With generative engines here to stay, we must ensure the creator economy is not disadvantaged. To address this, we introduce Generative Engine Optimization ({GEO}), the first novel paradigm to aid content creators in improving their content visibility in generative engine responses through a flexible black-box optimization framework for optimizing and defining visibility metrics. We facilitate systematic evaluation by introducing {GEO}-bench, a large-scale benchmark of diverse user queries across multiple domains, along with relevant web sources to answer these queries. Through rigorous evaluation, we demonstrate that {GEO} can boost visibility by up to \$40{\textbackslash}\%\$ in generative engine responses. Moreover, we show the efficacy of these strategies varies across domains, underscoring the need for domain-specific optimization methods. Our work opens a new frontier in information discovery systems, with profound implications for both developers of generative engines and content creators.},
	number = {{arXiv}:2311.09735},
	publisher = {{arXiv}},
	author = {Aggarwal, Pranjal and Murahari, Vishvak and Rajpurohit, Tanmay and Kalyan, Ashwin and Narasimhan, Karthik and Deshpande, Ameet},
	urldate = {2025-10-17},
	date = {2024-06-28},
	eprinttype = {arxiv},
	eprint = {2311.09735 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\NCTVDF6T\\Aggarwal et al. - 2024 - GEO Generative Engine Optimization.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\I7TK5TTB\\2311.html:text/html},
}

@misc{guo_large_2024,
	title = {Large Language Model based Multi-Agents: A Survey of Progress and Challenges},
	url = {http://arxiv.org/abs/2402.01680},
	doi = {10.48550/arXiv.2402.01680},
	shorttitle = {Large Language Model based Multi-Agents},
	abstract = {Large Language Models ({LLMs}) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of {LLMs}, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one {LLM} as a single planning or decision-making agent, {LLM}-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on {LLMs}, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do {LLM}-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize the commonly used datasets or benchmarks for them to have convenient access. To keep researchers updated on the latest studies, we maintain an open-source {GitHub} repository, dedicated to outlining the research on {LLM}-based multi-agent systems.},
	number = {{arXiv}:2402.01680},
	publisher = {{arXiv}},
	author = {Guo, Taicheng and Chen, Xiuying and Wang, Yaqi and Chang, Ruidi and Pei, Shichao and Chawla, Nitesh V. and Wiest, Olaf and Zhang, Xiangliang},
	urldate = {2025-11-08},
	date = {2024-04-19},
	eprinttype = {arxiv},
	eprint = {2402.01680 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Multiagent Systems},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\JLD9EVE2\\Guo et al. - 2024 - Large Language Model based Multi-Agents A Survey .pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\8PF3NHM8\\2402.html:text/html},
}

@article{monderer_learning_2007,
	title = {Learning equilibrium as a generalization of learning to optimize},
	volume = {171},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370207000069},
	doi = {10.1016/j.artint.2007.01.002},
	series = {Foundations of Multi-Agent Learning},
	abstract = {We argue that learning equilibrium is an appropriate generalization to multi-agent systems of the concept of learning to optimize in single-agent setting. We further define and discuss the concept of weak learning equilibrium.},
	pages = {448--452},
	number = {7},
	journaltitle = {Artificial Intelligence},
	shortjournal = {Artificial Intelligence},
	author = {Monderer, Dov and Tennenholtz, Moshe},
	urldate = {2025-10-17},
	date = {2007-05-01},
	keywords = {Learning, Machine learning, Learning equilibrium},
	file = {ScienceDirect Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\JVDTLFHH\\S0004370207000069.html:text/html},
}

@inproceedings{mordo_lemss_2025,
	location = {New York, {NY}, {USA}},
	title = {{LEMSS}: {LLM}-Based Platform for Multi-Agent Competitive Search Simulation},
	isbn = {9798400715921},
	url = {https://dl.acm.org/doi/10.1145/3726302.3730312},
	doi = {10.1145/3726302.3730312},
	series = {{SIGIR} '25},
	shorttitle = {{LEMSS}},
	abstract = {In competitive search settings, document publishers (authors) respond to rankings induced for queries of interest: they modify the documents to improve their future ranking. Hence, for some queries there is an on-going ranking competition. Prior empirical studies of competitive search were based on controlled ranking competitions between humans. Large Language Models ({LLMs}), capable of generating high quality content, provide new opportunities for studying ranking competitions. Furthermore, there is a significant amount of content on the Web, which is a canonical example of a competitive search setting, generated by {LLMs}. In this paper, we introduce {LEMSS}: a multi-agent platform that leverages {LLMs} as publishers in competitive search settings. In addition to enabling the execution of large-scale and highly configurable ranking competitions, {LEMSS} includes tools to analyze and compare the competitions using a wide range of measures. We use these tools to analyze examples of datasets that result from ranking competitions executed using {LEMSS}. The analysis reveals, for example, that using {LLMs} as publishers reduced content diversity in the corpus to a larger extent than having human publishers.},
	pages = {3595--3605},
	booktitle = {Proceedings of the 48th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {Association for Computing Machinery},
	author = {Mordo, Tommy and Kordonsky, Tomer and Nachimovsky, Haya and Tennenholtz, Moshe and Kurland, Oren},
	urldate = {2025-10-17},
	date = {2025-07-13},
	file = {Full Text PDF:C\:\\Users\\tommy\\Zotero\\storage\\Z82KIDWJ\\Mordo et al. - 2025 - LEMSS LLM-Based Platform for Multi-Agent Competit.pdf:application/pdf},
}

@misc{zhou_multi_agent_2025,
	title = {Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies},
	url = {http://arxiv.org/abs/2502.02533},
	doi = {10.48550/arXiv.2502.02533},
	shorttitle = {Multi-Agent Design},
	abstract = {Large language models, employed as multiple agents that interact and collaborate with each other, have excelled at solving complex tasks. The agents are programmed with prompts that declare their functionality, along with the topologies that orchestrate interactions across agents. Designing prompts and topologies for multi-agent systems ({MAS}) is inherently complex. To automate the entire design process, we first conduct an in-depth analysis of the design space aiming to understand the factors behind building effective {MAS}. We reveal that prompts together with topologies play critical roles in enabling more effective {MAS} design. Based on the insights, we propose Multi-Agent System Search ({MASS}), a {MAS} optimization framework that efficiently exploits the complex {MAS} design space by interleaving its optimization stages, from local to global, from prompts to topologies, over three stages: 1) block-level (local) prompt optimization; 2) workflow topology optimization; 3) workflow-level (global) prompt optimization, where each stage is conditioned on the iteratively optimized prompts/topologies from former stages. We show that {MASS}-optimized multi-agent systems outperform a spectrum of existing alternatives by a substantial margin. Based on the {MASS}-found systems, we finally propose design principles behind building effective multi-agent systems.},
	number = {{arXiv}:2502.02533},
	publisher = {{arXiv}},
	author = {Zhou, Han and Wan, Xingchen and Sun, Ruoxi and Palangi, Hamid and Iqbal, Shariq and Vulić, Ivan and Korhonen, Anna and Arık, Sercan Ö},
	urldate = {2025-10-17},
	date = {2025-02-04},
	eprinttype = {arxiv},
	eprint = {2502.02533 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Multiagent Systems},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\PQCGSBBL\\Zhou et al. - 2025 - Multi-Agent Design Optimizing Agents with Better .pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\MFCQA3FI\\2502.html:text/html},
}

@book{howard_paradoxes_2003,
	location = {Cambridge, {MA}, {USA}},
	title = {Paradoxes of Rationality: Theory of Metagames and Political Behavior},
	isbn = {978-0-262-58237-7},
	shorttitle = {Paradoxes of Rationality},
	abstract = {The aim of his work is to produce a technique that can be used to resolve real-life, real-time conflict situations and to investigate political and social interactions between decision makers.},
	pagetotal = {272},
	publisher = {{MIT} Press},
	author = {Howard, Nigel},
	date = {2003-03-17},
	langid = {english},
}

@article{tennenholtz_program_2004,
	title = {Program equilibrium},
	volume = {49},
	issn = {0899-8256},
	url = {https://www.sciencedirect.com/science/article/pii/S0899825604000314},
	doi = {10.1016/j.geb.2004.02.002},
	abstract = {In a computerized setting, players' strategies can be implemented by computer programs, to be executed on a shared computational devise. This situation becomes typical to new Internet economies, where agent technologies play a major role. This allows the definition of a program equilibrium. Following the fundamental ideas introduced by von Neumann in the 1940s (in parallel to his seminal contribution to game theory), a computer program can be used both as a set of instructions, as well as a file that can be read and compared with other files. We show that this idea implies that in a program equilibrium of the one-shot prisoners dilemma mutual cooperation is obtained. More generally, we show that the set of program equilibrium payoffs of a game coincides with the set of feasible and individually rational payoffs of it.},
	pages = {363--373},
	number = {2},
	journaltitle = {Games and Economic Behavior},
	shortjournal = {Games and Economic Behavior},
	author = {Tennenholtz, Moshe},
	urldate = {2025-10-17},
	date = {2004-11-01},
	file = {ScienceDirect Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\WK5T7AFV\\S0899825604000314.html:text/html},
}

@misc{mordo_rlrf_2025,
	title = {{RLRF}: Competitive Search Agent Design via Reinforcement Learning from Ranker Feedback},
	url = {http://arxiv.org/abs/2510.04096},
	doi = {10.48550/arXiv.2510.04096},
	shorttitle = {{RLRF}},
	abstract = {Competitive search is a setting where document publishers modify them to improve their ranking in response to a query. Recently, publishers have increasingly leveraged {LLMs} to generate and modify competitive content. We introduce Reinforcement Learning from Ranker Feedback ({RLRF}), a framework that trains {LLMs} using preference datasets derived from ranking competitions. The goal of a publisher ({LLM}-based) agent is to optimize content for improved ranking while accounting for the strategies of competing agents. We generate the datasets using approaches that do not rely on human-authored data. We show that our proposed agents consistently and substantially outperform previously suggested approaches for {LLM}-based competitive document modification. We further show that our agents are effective with ranking functions they were not trained for (i.e., out of distribution) and they adapt to strategic opponents. These findings provide support to the significant potential of using reinforcement learning in competitive search.},
	number = {{arXiv}:2510.04096},
	publisher = {{arXiv}},
	author = {Mordo, Tommy and Dekel, Sagie and Madmon, Omer and Tennenholtz, Moshe and Kurland, Oren},
	urldate = {2025-10-17},
	date = {2025-10-05},
	eprinttype = {arxiv},
	eprint = {2510.04096 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Science and Game Theory, Computer Science - Information Retrieval},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\W7SKW27A\\Mordo et al. - 2025 - RLRF Competitive Search Agent Design via Reinforc.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\AYA9GAUY\\2510.html:text/html},
}

@article{tennenholtz_artificial_nodate,
	title = {Artificial Social Intelligence},
	author = {Tennenholtz, Moshe},
	langid = {english},
	file = {Tennenholtz - Artificial Social Intelligence.pdf:C\:\\Users\\tommy\\Zotero\\storage\\YNHS43NA\\Tennenholtz - Artificial Social Intelligence.pdf:application/pdf},
}

@misc{fang_serl_2025,
	title = {{SeRL}: Self-Play Reinforcement Learning for Large Language Models with Limited Data},
	url = {http://arxiv.org/abs/2505.20347},
	doi = {10.48550/arXiv.2505.20347},
	shorttitle = {{SeRL}},
	abstract = {Recent advances have demonstrated the effectiveness of Reinforcement Learning ({RL}) in improving the reasoning capabilities of Large Language Models ({LLMs}). However, existing works inevitably rely on high-quality instructions and verifiable rewards for effective training, both of which are often difficult to obtain in specialized domains. In this paper, we propose Self-play Reinforcement Learning({SeRL}) to bootstrap {LLM} training with limited initial data. Specifically, {SeRL} comprises two complementary modules: self-instruction and self-rewarding. The former module generates additional instructions based on the available data at each training step, employing robust online filtering strategies to ensure instruction quality, diversity, and difficulty. The latter module introduces a simple yet effective majority-voting mechanism to estimate response rewards for additional instructions, eliminating the need for external annotations. Finally, {SeRL} performs conventional {RL} based on the generated data, facilitating iterative self-play learning. Extensive experiments on various reasoning benchmarks and across different {LLM} backbones demonstrate that the proposed {SeRL} yields results superior to its counterparts and achieves performance on par with those obtained by high-quality data with verifiable rewards. Our code is available at https://github.com/wantbook-book/{SeRL}.},
	number = {{arXiv}:2505.20347},
	publisher = {{arXiv}},
	author = {Fang, Wenkai and Liu, Shunyu and Zhou, Yang and Zhang, Kongcheng and Zheng, Tongya and Chen, Kaixuan and Song, Mingli and Tao, Dacheng},
	urldate = {2025-11-17},
	date = {2025-05-25},
	eprinttype = {arxiv},
	eprint = {2505.20347 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\5LYTQC5A\\Fang et al. - 2025 - SeRL Self-Play Reinforcement Learning for Large L.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\NBIP8RKA\\2505.html:text/html},
}

@misc{zhang_survey_2025,
	title = {A Survey on Self-play Methods in Reinforcement Learning},
	url = {http://arxiv.org/abs/2408.01072},
	doi = {10.48550/arXiv.2408.01072},
	abstract = {Self-play, a learning paradigm where agents iteratively refine their policies by interacting with historical or concurrent versions of themselves or other evolving agents, has shown remarkable success in solving complex non-cooperative multi-agent tasks. Despite its growing prominence in multi-agent reinforcement learning ({MARL}), such as Go, poker, and video games, a comprehensive and structured understanding of self-play remains lacking. This survey fills this gap by offering a comprehensive roadmap to the diverse landscape of self-play methods. We begin by introducing the necessary preliminaries, including the {MARL} framework and basic game theory concepts. Then, it provides a unified framework and classifies existing self-play algorithms within this framework. Moreover, the paper bridges the gap between the algorithms and their practical implications by illustrating the role of self-play in different non-cooperative scenarios. Finally, the survey highlights open challenges and future research directions in self-play.},
	number = {{arXiv}:2408.01072},
	publisher = {{arXiv}},
	author = {Zhang, Ruize and Xu, Zelai and Ma, Chengdong and Yu, Chao and Tu, Wei-Wei and Tang, Wenhao and Huang, Shiyu and Ye, Deheng and Ding, Wenbo and Yang, Yaodong and Wang, Yu},
	urldate = {2025-11-17},
	date = {2025-10-18},
	eprinttype = {arxiv},
	eprint = {2408.01072 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\HRHHZG9T\\Zhang et al. - 2025 - A Survey on Self-play Methods in Reinforcement Lea.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\Y5WUIRWY\\2408.html:text/html},
}

@online{noauthor_just_nodate,
	title = {Just a moment...},
	url = {https://chatgpt.com/},
	urldate = {2025-11-18},
	file = {Just a moment...:C\:\\Users\\tommy\\Zotero\\storage\\HHBQB7NT\\chatgpt.com.html:text/html},
}

@article{metzler_rethinking_2021,
	title = {Rethinking search: making domain experts out of dilettantes},
	volume = {55},
	issn = {0163-5840},
	url = {https://dl.acm.org/doi/10.1145/3476415.3476428},
	doi = {10.1145/3476415.3476428},
	shorttitle = {Rethinking search},
	abstract = {When experiencing an information need, users want to engage with a domain expert, but often turn to an information retrieval system, such as a search engine, instead. Classical information retrieval systems do not answer information needs directly, but instead provide references to (hopefully authoritative) answers. Successful question answering systems offer a limited corpus created on-demand by human experts, which is neither timely nor scalable. Pre-trained language models, by contrast, are capable of directly generating prose that may be responsive to an information need, but at present they are dilettantes rather than domain experts - they do not have a true understanding of the world, they are prone to hallucinating, and crucially they are incapable of justifying their utterances by referring to supporting documents in the corpus they were trained over. This paper examines how ideas from classical information retrieval and pre-trained language models can be synthesized and evolved into systems that truly deliver on the promise of domain expert advice.},
	pages = {1--27},
	number = {1},
	journaltitle = {{ACM} {SIGIR} Forum},
	shortjournal = {{SIGIR} Forum},
	author = {Metzler, Donald and Tay, Yi and Bahri, Dara and Najork, Marc},
	urldate = {2025-11-20},
	date = {2021-06},
	langid = {english},
	file = {Submitted Version:C\:\\Users\\tommy\\Zotero\\storage\\UDRSUQQF\\Metzler et al. - 2021 - Rethinking search making domain experts out of di.pdf:application/pdf},
}

@book{aumann_repeated_1995,
	title = {Repeated Games with Incomplete Information},
	isbn = {978-0-262-01147-1},
	abstract = {During the height of the Cold War, between 1965 and 1968, Robert Aumann, Michael Maschler and Richard Stearns collaborated on research on the dynamics of arms control negotiations that has since become foundational to work on repeated games. These five seminal papers are collected in this text, with the addition of postscripts describing many of the developments since the papers were written. The basic model studied throughout the book is one in which players ignorant about the game being played must learn what they can from the actions of the others.},
	pagetotal = {372},
	publisher = {{MIT} Press},
	author = {Aumann, Robert J. and Maschler, Michael and Stearns, Richard E.},
	date = {1995},
	langid = {english},
	keywords = {Mathematics / Game Theory, Business \& Economics / Economics / General, Business \& Economics / Finance / General, Political Science / International Relations / Arms Control},
}

@misc{shapira_glee_2025,
	title = {{GLEE}: A Unified Framework and Benchmark for Language-based Economic Environments},
	url = {http://arxiv.org/abs/2410.05254},
	doi = {10.48550/arXiv.2410.05254},
	shorttitle = {{GLEE}},
	abstract = {Large Language Models ({LLMs}) show significant potential in economic and strategic interactions, where communication via natural language is often prevalent. This raises key questions: Do {LLMs} behave rationally? How do they perform compared to humans? Do they tend to reach an efficient and fair outcome? What is the role of natural language in strategic interaction? How do characteristics of the economic environment influence these dynamics? These questions become crucial concerning the economic and societal implications of integrating {LLM}-based agents into real-world data-driven systems, such as online retail platforms and recommender systems. To answer these questions, we introduce a benchmark for standardizing research on two-player, sequential, language-based games. Inspired by the economic literature, we define three base families of games with consistent parameterization, degrees of freedom and economic measures to evaluate agents' performance (self-gain), as well as the game outcome (efficiency and fairness). We develop an open-source framework for interaction simulation and analysis, and utilize it to collect a dataset of {LLM} vs. {LLM} interactions across numerous game configurations and an additional dataset of human vs. {LLM} interactions. Through extensive experimentation, we demonstrate how our framework and dataset can be used to: (i) compare the behavior of {LLM}-based agents in various economic contexts; (ii) evaluate agents in both individual and collective performance measures; and (iii) quantify the effect of the economic characteristics of the environments on the behavior of agents. Our results suggest that the market parameters, as well as the choice of the {LLMs}, tend to have complex and interdependent effects on the economic outcome, which calls for careful design and analysis of the language-based economic ecosystem.},
	number = {{arXiv}:2410.05254},
	publisher = {{arXiv}},
	author = {Shapira, Eilam and Madmon, Omer and Reinman, Itamar and Amouyal, Samuel Joseph and Reichart, Roi and Tennenholtz, Moshe},
	urldate = {2025-11-23},
	date = {2025-05-22},
	eprinttype = {arxiv},
	eprint = {2410.05254 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Computers and Society, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\NI66LM2C\\Shapira et al. - 2025 - GLEE A Unified Framework and Benchmark for Langua.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\2JZW34N8\\2410.html:text/html},
}

@inproceedings{ye_llm_empowered_2025,
	location = {Padua Italy},
	title = {{LLM}-Empowered Creator Simulation for Long-Term Evaluation of Recommender Systems Under Information Asymmetry},
	isbn = {9798400715921},
	url = {https://dl.acm.org/doi/10.1145/3726302.3730026},
	doi = {10.1145/3726302.3730026},
	eventtitle = {{SIGIR} '25: The 48th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	pages = {201--211},
	booktitle = {Proceedings of the 48th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Ye, Xiaopeng and Xu, Chen and Sun, Zhongxiang and Xu, Jun and Wang, Gang and Dong, Zhenhua and Wen, Ji-Rong},
	urldate = {2025-11-23},
	date = {2025-07-13},
	langid = {english},
}

@article{shoham_if_2025,
	title = {If multi-agent learning is the answer, what is the question?},
	url = {https://www.researchgate.net/publication/222675630_If_multi-agent_learning_is_the_answer_what_is_the_question},
	doi = {10.1016/j.artint.2006.02.006},
	abstract = {Download Citation {\textbar} If multi-agent learning is the answer, what is the question? {\textbar} The area of learning in multi-agent systems is today one of the most fertile grounds for interaction between game theory and artificial... {\textbar} Find, read and cite all the research you need on {ResearchGate}},
	journaltitle = {{ResearchGate}},
	author = {Shoham, Yoav and Powers, Rob and Grenager, Trond},
	urldate = {2025-11-23},
	date = {2025-08-09},
	langid = {english},
	file = {Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\A7PPTEX2\\222675630_If_multi-agent_learning_is_the_answer_what_is_the_question.html:text/html},
}

@misc{spennemann_delving_2025,
	title = {Delving into: the quantification of Ai-generated content on the internet (synthetic data)},
	url = {http://arxiv.org/abs/2504.08755},
	doi = {10.48550/arXiv.2504.08755},
	shorttitle = {Delving into},
	abstract = {While it is increasingly evident that the internet is becoming saturated with content created by generated Ai large language models, accurately measuring the scale of this phenomenon has proven challenging. By analyzing the frequency of specific keywords commonly used by {ChatGPT}, this paper demonstrates that such linguistic markers can effectively be used to esti-mate the presence of generative {AI} content online. The findings suggest that at least 30\% of text on active web pages originates from {AI}-generated sources, with the actual proportion likely ap-proaching 40\%. Given the implications of autophagous loops, this is a sobering realization.},
	number = {{arXiv}:2504.08755},
	publisher = {{arXiv}},
	author = {Spennemann, Dirk {HR}},
	urldate = {2025-11-23},
	date = {2025-03-29},
	eprinttype = {arxiv},
	eprint = {2504.08755 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, Computer Science - Human-Computer Interaction},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\495U92EA\\Spennemann - 2025 - Delving into the quantification of Ai-generated c.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\DWB8RGIE\\2504.html:text/html},
}

@misc{fu_improving_2023,
	title = {Improving Language Model Negotiation with Self-Play and In-Context Learning from {AI} Feedback},
	url = {http://arxiv.org/abs/2305.10142},
	doi = {10.48550/arXiv.2305.10142},
	abstract = {We study whether multiple large language models ({LLMs}) can autonomously improve each other in a negotiation game by playing, reflecting, and criticizing. We are interested in this question because if {LLMs} were able to improve each other, it would imply the possibility of creating strong {AI} agents with minimal human intervention. We ask two {LLMs} to negotiate with each other, playing the roles of a buyer and a seller, respectively. They aim to reach a deal with the buyer targeting a lower price and the seller a higher one. A third language model, playing the critic, provides feedback to a player to improve the player's negotiation strategies. We let the two agents play multiple rounds, using previous negotiation history and {AI} feedback as in-context demonstrations to improve the model's negotiation strategy iteratively. We use different {LLMs} ({GPT} and Claude) for different roles and use the deal price as the evaluation metric. Our experiments reveal multiple intriguing findings: (1) Only a subset of the language models we consider can self-play and improve the deal price from {AI} feedback, weaker models either do not understand the game's rules or cannot incorporate {AI} feedback for further improvement. (2) Models' abilities to learn from the feedback differ when playing different roles. For example, it is harder for Claude-instant to improve as the buyer than as the seller. (3) When unrolling the game to multiple rounds, stronger agents can consistently improve their performance by meaningfully using previous experiences and iterative {AI} feedback, yet have a higher risk of breaking the deal. We hope our work provides insightful initial explorations of having models autonomously improve each other with game playing and {AI} feedback.},
	number = {{arXiv}:2305.10142},
	publisher = {{arXiv}},
	author = {Fu, Yao and Peng, Hao and Khot, Tushar and Lapata, Mirella},
	urldate = {2025-11-23},
	date = {2023-05-17},
	eprinttype = {arxiv},
	eprint = {2305.10142 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\Y7TSZ5R3\\Fu et al. - 2023 - Improving Language Model Negotiation with Self-Pla.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\2M4DMBQ4\\2305.html:text/html},
}

@misc{gao_retrieval_augmented_2024,
	title = {Retrieval-Augmented Generation for Large Language Models: A Survey},
	url = {http://arxiv.org/abs/2312.10997},
	doi = {10.48550/arXiv.2312.10997},
	shorttitle = {Retrieval-Augmented Generation for Large Language Models},
	abstract = {Large Language Models ({LLMs}) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation ({RAG}) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. {RAG} synergistically merges {LLMs}' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of {RAG} paradigms, encompassing the Naive {RAG}, the Advanced {RAG}, and the Modular {RAG}. It meticulously scrutinizes the tripartite foundation of {RAG} frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in {RAG} systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
	number = {{arXiv}:2312.10997},
	publisher = {{arXiv}},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
	urldate = {2025-11-24},
	date = {2024-03-27},
	eprinttype = {arxiv},
	eprint = {2312.10997 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\CFTCIPYZ\\Gao et al. - 2024 - Retrieval-Augmented Generation for Large Language .pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\84ASLYY4\\2312.html:text/html},
}

@inproceedings{gyongyi_web_2005,
	title = {Web Spam Taxonomy},
	pages = {39--47},
	booktitle = {Proceedings of {AIRWeb} 2005, First International Workshop on Adversarial Information Retrieval on the Web},
	author = {Gyöngyi, Zoltán and Garcia-Molina, Hector},
	date = {2005},
}

@inproceedings{mordo_ameliorating_2025,
	location = {Padua Italy},
	title = {Ameliorating the Herding Effect Driven by Search Engines using Diversity-Based Ranking},
	isbn = {9798400718618},
	url = {https://dl.acm.org/doi/10.1145/3731120.3744600},
	doi = {10.1145/3731120.3744600},
	eventtitle = {{ICTIR} '25: International {ACM} {SIGIR} Conference on Innovative Concepts and Theories in Information Retrieval},
	pages = {1--11},
	booktitle = {Proceedings of the 2025 International {ACM} {SIGIR} Conference on Innovative Concepts and Theories in Information Retrieval ({ICTIR})},
	publisher = {{ACM}},
	author = {Mordo, Tommy and Reinman, Itamar and Tennenholtz, Moshe and Kurland, Oren},
	urldate = {2025-08-14},
	date = {2025-07-18},
	langid = {english},
}

@inproceedings{yao_how_2023,
	title = {How Bad is Top-\$K\$ Recommendation under Competing Content Creators?},
	url = {https://proceedings.mlr.press/v202/yao23b.html},
	abstract = {This study explores the impact of content creators’ competition on user welfare in recommendation platforms, as well as the long-term dynamics of relevance-driven recommendations. We establish a model of creator competition, under the setting where the platform uses a top-{KKK} recommendation policy, user decisions are guided by the Random Utility model, and creators, in absence of explicit utility functions, employ arbitrary no-regret learning algorithms for strategy updates. We study the user welfare guarantee through the lens of Price of Anarchy and show that the fraction of user welfare loss due to creator competition is always upper bounded by a small constant depending on {KKK} and randomness in user decisions; we also prove the tightness of this bound. Our result discloses an intrinsic merit of the relevance-driven recommendation policy, as long as users’ decisions involve randomness and the platform provides reasonably many alternatives to its users.},
	eventtitle = {International Conference on Machine Learning},
	pages = {39674--39701},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Yao, Fan and Li, Chuanhao and Nekipelov, Denis and Wang, Hongning and Xu, Haifeng},
	urldate = {2025-11-24},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:C\:\\Users\\tommy\\Zotero\\storage\\C6RXTZHB\\Yao et al. - 2023 - How Bad is Top-\$K\$ Recommendation under Competing .pdf:application/pdf},
}

@inproceedings{yao_human_2024,
	title = {Human vs. Generative {AI} in Content Creation Competition: Symbiosis or Conflict?},
	url = {https://proceedings.mlr.press/v235/yao24b.html},
	shorttitle = {Human vs. Generative {AI} in Content Creation Competition},
	abstract = {The advent of generative {AI} ({GenAI}) technology produces a transformative impact on the content creation landscape, offering alternative approaches to produce diverse, good-quality content across media, thereby reshaping online ecosystems but also raising concerns about market over-saturation and the potential marginalization of human creativity. Our work introduces a competition model generalized from the Tullock contest to analyze the tension between human creators and {GenAI}. Our theory and simulations suggest that despite challenges, a stable equilibrium between human and {AI}-generated content is possible. Our work contributes to understanding the competitive dynamics in the content creation industry, offering insights into the future interplay between human creativity and technological advancements in {GenAI}.},
	eventtitle = {International Conference on Machine Learning},
	pages = {56885--56913},
	booktitle = {Proceedings of the 41st International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Yao, Fan and Li, Chuanhao and Nekipelov, Denis and Wang, Hongning and Xu, Haifeng},
	urldate = {2025-11-24},
	date = {2024-07-08},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@inproceedings{yao_user_2024,
	location = {Barcelona Spain},
	title = {User Welfare Optimization in Recommender Systems with Competing Content Creators},
	isbn = {9798400704901},
	url = {https://dl.acm.org/doi/10.1145/3637528.3672021},
	doi = {10.1145/3637528.3672021},
	eventtitle = {{KDD} '24: The 30th {ACM} {SIGKDD} Conference on Knowledge Discovery and Data Mining},
	pages = {3874--3885},
	booktitle = {Proceedings of the 30th {ACM} {SIGKDD} Conference on Knowledge Discovery and Data Mining},
	publisher = {{ACM}},
	author = {Yao, Fan and Liao, Yiming and Wu, Mingzhe and Li, Chuanhao and Zhu, Yan and Yang, James and Liu, Jingzhou and Wang, Qifan and Xu, Haifeng and Wang, Hongning},
	urldate = {2025-11-24},
	date = {2024-08-25},
	langid = {english},
	file = {Full Text:C\:\\Users\\tommy\\Zotero\\storage\\LK582JEP\\Yao et al. - 2024 - User Welfare Optimization in Recommender Systems w.pdf:application/pdf},
}

@misc{madmon_search_2024,
	title = {The Search for Stability: Learning Dynamics of Strategic Publishers with Initial Documents},
	url = {http://arxiv.org/abs/2305.16695},
	shorttitle = {The Search for Stability},
	abstract = {We study a game-theoretic information retrieval model in which strategic publishers aim to maximize their chances of being ranked first by the search engine while maintaining the integrity of their original documents. We show that the commonly used Probability Ranking Principle ({PRP}) ranking scheme results in an unstable environment where games often fail to reach pure Nash equilibrium. We propose two families of ranking functions that do not adhere to the {PRP} principle. We provide both theoretical and empirical evidence that these methods lead to a stable search ecosystem, by providing positive results on the learning dynamics convergence. We also define the publishers' and users' welfare, demonstrate a possible publisher-user trade-off, and provide means for a search system designer to control it. Finally, we show how instability harms long-term users' welfare.},
	number = {{arXiv}:2305.16695},
	publisher = {{arXiv}},
	author = {Madmon, Omer and Pipano, Idan and Reinman, Itamar and Tennenholtz, Moshe},
	urldate = {2024-12-08},
	date = {2024-05-19},
	eprinttype = {arxiv},
	eprint = {2305.16695 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Information Retrieval},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\D3XPHRJL\\Madmon et al. - 2024 - The Search for Stability Learning Dynamics of Str.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\J4P4LQNG\\2305.html:text/html},
}

@article{madmon_convergence_2024,
	title = {On the Convergence of No-Regret Dynamics in Information Retrieval Games with Proportional Ranking Functions},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2405.11517},
	doi = {10.48550/ARXIV.2405.11517},
	abstract = {Publishers who publish their content on the web act strategically, in a behavior that can be modeled within the online learning framework. Regret, a central concept in machine learning, serves as a canonical measure for assessing the performance of learning agents within this framework. We prove that any proportional content ranking function with a concave activation function induces games in which no-regret learning dynamics converge. Moreover, for proportional ranking functions, we prove the equivalence of the concavity of the activation function, the social concavity of the induced games and the concavity of the induced games. We also study the empirical trade-offs between publishers' and users' welfare, under different choices of the activation function, using a state-of-the-art no-regret dynamics algorithm. Furthermore, we demonstrate how the choice of the ranking function and changes in the ecosystem structure affect these welfare measures, as well as the dynamics' convergence rate.},
	author = {Madmon, Omer and Pipano, Idan and Reinman, Itamar and Tennenholtz, Moshe},
	urldate = {2025-11-24},
	date = {2024},
	note = {Publisher: {arXiv}
Version Number: 3},
	keywords = {Computer Science and Game Theory (cs.{GT}), {FOS}: Computer and information sciences, Information Retrieval (cs.{IR})},
	file = {Full Text PDF:C\:\\Users\\tommy\\Zotero\\storage\\JQ3ATCKX\\Madmon et al. - 2024 - On the Convergence of No-Regret Dynamics in Inform.pdf:application/pdf},
}

@misc{rafailov_direct_2024,
	title = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
	url = {http://arxiv.org/abs/2305.18290},
	doi = {10.48550/arXiv.2305.18290},
	shorttitle = {Direct Preference Optimization},
	abstract = {While large-scale unsupervised language models ({LMs}) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised {LM} to align with these preferences, often with reinforcement learning from human feedback ({RLHF}). However, {RLHF} is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised {LM} using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in {RLHF} that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard {RLHF} problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization ({DPO}), is stable, performant, and computationally lightweight, eliminating the need for sampling from the {LM} during fine-tuning or performing significant hyperparameter tuning. Our experiments show that {DPO} can fine-tune {LMs} to align with human preferences as well as or better than existing methods. Notably, fine-tuning with {DPO} exceeds {PPO}-based {RLHF} in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
	number = {{arXiv}:2305.18290},
	publisher = {{arXiv}},
	author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
	urldate = {2025-11-24},
	date = {2024-07-29},
	eprinttype = {arxiv},
	eprint = {2305.18290 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\Y5XFCCQM\\Rafailov et al. - 2024 - Direct Preference Optimization Your Language Mode.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\ZID927ZD\\2305.html:text/html},
}

@misc{schulman_proximal_2017,
	title = {Proximal Policy Optimization Algorithms},
	url = {http://arxiv.org/abs/1707.06347},
	doi = {10.48550/arXiv.1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization ({PPO}), have some of the benefits of trust region policy optimization ({TRPO}), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test {PPO} on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that {PPO} outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	number = {{arXiv}:1707.06347},
	publisher = {{arXiv}},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	urldate = {2025-11-24},
	date = {2017-08-28},
	eprinttype = {arxiv},
	eprint = {1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\KT8HUVWT\\Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\EN92NIXQ\\1707.html:text/html},
}

@inproceedings{calandriello_multi-turn_2024,
	location = {Vancouver, {BC}, Canada},
	title = {Multi-turn Reinforcement Learning with Preference Human Feedback},
	isbn = {9798331314385},
	url = {http://www.proceedings.com/079017-3779.html},
	doi = {10.52202/079017-3779},
	eventtitle = {Advances in Neural Information Processing Systems 37},
	pages = {118953--118993},
	booktitle = {Advances in Neural Information Processing Systems 37},
	publisher = {Neural Information Processing Systems Foundation, Inc. ({NeurIPS})},
	author = {Calandriello, Daniele and Cassel, Asaf and Hassidim, Avinatan and Keller, Orgad and Lang, Oran and Matias, Yossi and Munos, Rémi and Noga, Hila and Piot, Bilal and Rosenberg, Aviv and Shani, Lior and Szpektor, Idan and Zipori, Avital},
	urldate = {2025-11-24},
	date = {2024},
}

@misc{shao_deepseekmath_2024,
	title = {{DeepSeekMath}: Pushing the Limits of Mathematical Reasoning in Open Language Models},
	url = {http://arxiv.org/abs/2402.03300},
	doi = {10.48550/arXiv.2402.03300},
	shorttitle = {{DeepSeekMath}},
	abstract = {Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce {DeepSeekMath} 7B, which continues pre-training {DeepSeek}-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. {DeepSeekMath} 7B has achieved an impressive score of 51.7\% on the competition-level {MATH} benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and {GPT}-4. Self-consistency over 64 samples from {DeepSeekMath} 7B achieves 60.9\% on {MATH}. The mathematical reasoning capability of {DeepSeekMath} is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization ({GRPO}), a variant of Proximal Policy Optimization ({PPO}), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of {PPO}.},
	number = {{arXiv}:2402.03300},
	publisher = {{arXiv}},
	author = {Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, Y. K. and Wu, Y. and Guo, Daya},
	urldate = {2025-11-24},
	date = {2024-04-27},
	eprinttype = {arxiv},
	eprint = {2402.03300 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\9FTYIDJP\\Shao et al. - 2024 - DeepSeekMath Pushing the Limits of Mathematical R.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\GDSV2WPJ\\2402.html:text/html},
}

@misc{liu_agentlite_2024,
	title = {{AgentLite}: A Lightweight Library for Building and Advancing Task-Oriented {LLM} Agent System},
	url = {http://arxiv.org/abs/2402.15538},
	doi = {10.48550/arXiv.2402.15538},
	shorttitle = {{AgentLite}},
	abstract = {The booming success of {LLMs} initiates rapid development in {LLM} agents. Though the foundation of an {LLM} agent is the generative model, it is critical to devise the optimal reasoning strategies and agent architectures. Accordingly, {LLM} agent research advances from the simple chain-of-thought prompting to more complex {ReAct} and Reflection reasoning strategy; agent architecture also evolves from single agent generation to multi-agent conversation, as well as multi-{LLM} multi-agent group chat. However, with the existing intricate frameworks and libraries, creating and evaluating new reasoning strategies and agent architectures has become a complex challenge, which hinders research investigation into {LLM} agents. Thus, we open-source a new {AI} agent library, {AgentLite}, which simplifies this process by offering a lightweight, user-friendly platform for innovating {LLM} agent reasoning, architectures, and applications with ease. {AgentLite} is a task-oriented framework designed to enhance the ability of agents to break down tasks and facilitate the development of multi-agent systems. Furthermore, we introduce multiple practical applications developed with {AgentLite} to demonstrate its convenience and flexibility. Get started now at: {\textbackslash}url\{https://github.com/{SalesforceAIResearch}/{AgentLite}\}.},
	number = {{arXiv}:2402.15538},
	publisher = {{arXiv}},
	author = {Liu, Zhiwei and Yao, Weiran and Zhang, Jianguo and Yang, Liangwei and Liu, Zuxin and Tan, Juntao and Choubey, Prafulla K. and Lan, Tian and Wu, Jason and Wang, Huan and Heinecke, Shelby and Xiong, Caiming and Savarese, Silvio},
	urldate = {2025-01-11},
	date = {2024-02-23},
	eprinttype = {arxiv},
	eprint = {2402.15538 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\QALZUVMW\\Liu et al. - 2024 - AgentLite A Lightweight Library for Building and .pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\3HXEBC6X\\2402.html:text/html},
}

@misc{feng_agile_2024,
	title = {{AGILE}: A Novel Reinforcement Learning Framework of {LLM} Agents},
	url = {http://arxiv.org/abs/2405.14751},
	doi = {10.48550/arXiv.2405.14751},
	shorttitle = {{AGILE}},
	abstract = {We introduce a novel reinforcement learning framework of {LLM} agents named {AGILE} ({AGent} that Interacts and Learns from Environments) designed to perform complex conversational tasks with users, leveraging {LLMs}, memory, tools, and interactions with experts. The agent possesses capabilities beyond conversation, including reflection, tool usage, and expert consultation. We formulate the construction of such an {LLM} agent as a reinforcement learning ({RL}) problem, in which the {LLM} serves as the policy model. We fine-tune the {LLM} using labeled data of actions and the {PPO} algorithm. We focus on question answering and release a dataset for agents called {ProductQA}, comprising challenging questions in online shopping. Our extensive experiments on {ProductQA}, {MedMCQA} and {HotPotQA} show that {AGILE} agents based on 7B and 13B {LLMs} trained with {PPO} can outperform {GPT}-4 agents. Our ablation study highlights the indispensability of memory, tools, consultation, reflection, and reinforcement learning in achieving the agent's strong performance. Datasets and code are available at https://github.com/bytarnish/{AGILE}.},
	number = {{arXiv}:2405.14751},
	publisher = {{arXiv}},
	author = {Feng, Peiyuan and He, Yichen and Huang, Guanhua and Lin, Yuan and Zhang, Hanchong and Zhang, Yuchen and Li, Hang},
	urldate = {2025-01-11},
	date = {2024-11-05},
	eprinttype = {arxiv},
	eprint = {2405.14751 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\3ZERZ9QC\\Feng et al. - 2024 - AGILE A Novel Reinforcement Learning Framework of.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\TSMUVZXA\\2405.html:text/html},
}

@misc{xi_riprag_2025,
	title = {{RIPRAG}: Hack a Black-box Retrieval-Augmented Generation Question-Answering System with Reinforcement Learning},
	url = {http://arxiv.org/abs/2510.10008},
	doi = {10.48550/arXiv.2510.10008},
	shorttitle = {{RIPRAG}},
	abstract = {Retrieval-Augmented Generation ({RAG}) systems based on Large Language Models ({LLMs}) have become a core technology for tasks such as question-answering ({QA}) and content generation. However, by injecting poisoned documents into the database of {RAG} systems, attackers can manipulate {LLMs} to generate text that aligns with their intended preferences. Existing research has primarily focused on white-box attacks against simplified {RAG} architectures. In this paper, we investigate a more complex and realistic scenario: the attacker lacks knowledge of the {RAG} system's internal composition and implementation details, and the {RAG} system comprises components beyond a mere retriever. Specifically, we propose the {RIPRAG} attack framework, an end-to-end attack pipeline that treats the target {RAG} system as a black box, where the only information accessible to the attacker is whether the poisoning succeeds. Our method leverages Reinforcement Learning ({RL}) to optimize the generation model for poisoned documents, ensuring that the generated poisoned document aligns with the target {RAG} system's preferences. Experimental results demonstrate that this method can effectively execute poisoning attacks against most complex {RAG} systems, achieving an attack success rate ({ASR}) improvement of up to 0.72 compared to baseline methods. This highlights prevalent deficiencies in current defensive methods and provides critical insights for {LLM} security research.},
	number = {{arXiv}:2510.10008},
	publisher = {{arXiv}},
	author = {Xi, Meng and Lv, Sihan and Jin, Yechen and Cheng, Guanjie and Wang, Naibo and Li, Ying and Yin, Jianwei},
	urldate = {2025-11-25},
	date = {2025-10-11},
	eprinttype = {arxiv},
	eprint = {2510.10008 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\5GT5FL7F\\Xi et al. - 2025 - RIPRAG Hack a Black-box Retrieval-Augmented Gener.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\S5K9MKSN\\2510.html:text/html},
}

@misc{zou_poisonedrag_2024,
	title = {{PoisonedRAG}: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models},
	url = {http://arxiv.org/abs/2402.07867},
	doi = {10.48550/arXiv.2402.07867},
	shorttitle = {{PoisonedRAG}},
	abstract = {Large language models ({LLMs}) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation ({RAG}) is a state-of-the-art technique to mitigate these limitations. The key idea of {RAG} is to ground the answer generation of an {LLM} on external knowledge retrieved from a knowledge database. Existing studies mainly focus on improving the accuracy or efficiency of {RAG}, leaving its security largely unexplored. We aim to bridge the gap in this work. We find that the knowledge database in a {RAG} system introduces a new and practical attack surface. Based on this attack surface, we propose {PoisonedRAG}, the first knowledge corruption attack to {RAG}, where an attacker could inject a few malicious texts into the knowledge database of a {RAG} system to induce an {LLM} to generate an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge corruption attacks as an optimization problem, whose solution is a set of malicious texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on a {RAG} system, we propose two solutions to solve the optimization problem, respectively. Our results show {PoisonedRAG} could achieve a 90\% attack success rate when injecting five malicious texts for each target question into a knowledge database with millions of texts. We also evaluate several defenses and our results show they are insufficient to defend against {PoisonedRAG}, highlighting the need for new defenses.},
	number = {{arXiv}:2402.07867},
	publisher = {{arXiv}},
	author = {Zou, Wei and Geng, Runpeng and Wang, Binghui and Jia, Jinyuan},
	urldate = {2025-11-25},
	date = {2024-08-13},
	eprinttype = {arxiv},
	eprint = {2402.07867 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\LYEC88HN\\Zou et al. - 2024 - PoisonedRAG Knowledge Corruption Attacks to Retri.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\LXKL8I7T\\2402.html:text/html},
}

@misc{wu_what_2025,
	title = {What Generative Search Engines Like and How to Optimize Web Content Cooperatively},
	url = {http://arxiv.org/abs/2510.11438},
	doi = {10.48550/arXiv.2510.11438},
	abstract = {By employing large language models ({LLMs}) to retrieve documents and generate natural language responses, Generative Engines, such as Google {AI} overview and {ChatGPT}, provide significantly enhanced user experiences and have rapidly become the new form of search. Their rapid adoption also drives the needs of Generative Engine Optimization ({GEO}), as content providers are eager to gain more traction from them. In this paper, we introduce {AutoGEO}, a framework to automatically learn generative engine preferences when using retrieved contents for response generation, and rewrite web contents for more such traction. {AutoGEO} first prompts frontier {LLMs} to explain generative engine preferences and extract meaningful preference rules from these explanations. Then it uses preference rules as context engineering for {AutoGEO}\$\_{\textbackslash}text\{{API}\}\$, a prompt-based {GEO} system, and as rule-based rewards to train {AutoGEO}\$\_{\textbackslash}text\{Mini\}\$, a cost-effective {GEO} model. Experiments on the standard {GEO}-Bench and two newly constructed benchmarks using real user queries demonstrate the effectiveness of {AutoGEO} in enhancing content traction while preserving search utility. Analyses confirm the learned rules' robustness and abilities to capture unique preferences in variant domains, and {AutoGEO} systems' ability to embed them in content optimization. The code is released at https://github.com/cxcscmu/{AutoGEO}.},
	number = {{arXiv}:2510.11438},
	publisher = {{arXiv}},
	author = {Wu, Yujiang and Zhong, Shanshan and Kim, Yubin and Xiong, Chenyan},
	urldate = {2025-11-25},
	date = {2025-10-13},
	eprinttype = {arxiv},
	eprint = {2510.11438 [cs]},
	keywords = {Computer Science - Information Retrieval},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\NT2RRWIU\\Wu et al. - 2025 - What Generative Search Engines Like and How to Opt.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\5KEKNQMS\\2510.html:text/html},
}

@inproceedings{raifer_information_2017,
	location = {Shinjuku Tokyo Japan},
	title = {Information Retrieval Meets Game Theory: The Ranking Competition Between Documents' Authors},
	isbn = {978-1-4503-5022-8},
	url = {https://dl.acm.org/doi/10.1145/3077136.3080785},
	doi = {10.1145/3077136.3080785},
	shorttitle = {Information Retrieval Meets Game Theory},
	eventtitle = {{SIGIR} '17: The 40th International {ACM} {SIGIR} conference on research and development in Information Retrieval},
	pages = {465--474},
	booktitle = {Proceedings of the 40th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Raifer, Nimrod and Raiber, Fiana and Tennenholtz, Moshe and Kurland, Oren},
	urldate = {2024-10-21},
	date = {2017-08-07},
	langid = {english},
	file = {Raifer et al. - 2017 - Information Retrieval Meets Game Theory The Ranki.pdf:C\:\\Users\\tommy\\Zotero\\storage\\GN6RYDGF\\Raifer et al. - 2017 - Information Retrieval Meets Game Theory The Ranki.pdf:application/pdf},
}

@inproceedings{nachimovsky_power_2025,
	title = {On the power of strategic corpus enrichment in content creation games},
	volume = {39},
	isbn = {978-1-57735-897-8},
	url = {https://doi.org/10.1609/aaai.v39i13.33534},
	doi = {10.1609/aaai.v39i13.33534},
	series = {{AAAI}'25/{IAAI}'25/{EAAI}'25},
	abstract = {Search and recommendation ecosystems exhibit competition among content creators. This competition has been tackled in a variety of game-theoretic frameworks. Content creators generate documents with the aim of being recommended by a content ranker for various information needs. In order for the ecosystem, modeled as a content ranking game, to be effective and maximize user welfare, it should guarantee stability, where stability is associated with the existence of pure Nash equilibrium in the corresponding game. Moreover, if the contents' ranking algorithm possesses a game in which any best-response learning dynamics of the content creators converge to equilibrium of high welfare, the system is considered highly attractive. However, as classical content ranking algorithms, employed by search and recommendation systems, rank documents by their distance to information needs, it has been shown that they fail to provide such stability properties. As a result, novel content ranking algorithms have been devised. In this work, we offer an alternative approach: corpus enrichment with a small set of fixed dummy documents. It turns out that, with the right design, such enrichment can lead to pure Nash equilibrium and even to the convergence of any best-response dynamics to a high welfare result, where we still employ the classical/current content ranking approach. We show two such corpus enrichment techniques with tight bounds on the number of documents needed to obtain the desired results. Interestingly, our study is a novel extension of Borel's Colonel Blotto game.},
	pages = {14019--14026},
	booktitle = {Proceedings of the Thirty-Ninth {AAAI} Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence},
	publisher = {{AAAI} Press},
	author = {Nachimovsky, Haya and Tennenholtz, Moshe},
	urldate = {2025-11-25},
	date = {2025-02-25},
}

@inproceedings{mordo_sponsored_2024,
	location = {Washington {DC} {USA}},
	title = {Sponsored Question Answering},
	isbn = {9798400706813},
	url = {https://dl.acm.org/doi/10.1145/3664190.3672517},
	doi = {10.1145/3664190.3672517},
	eventtitle = {{ICTIR} '24: The 2024 {ACM} {SIGIR} International Conference on the Theory of Information Retrieval},
	pages = {167--173},
	booktitle = {Proceedings of the 2024 {ACM} {SIGIR} International Conference on Theory of Information Retrieval},
	publisher = {{ACM}},
	author = {Mordo, Tommy and Tennenholtz, Moshe and Kurland, Oren},
	urldate = {2025-11-25},
	date = {2024-08-02},
	langid = {english},
	file = {Full Text:C\:\\Users\\tommy\\Zotero\\storage\\9R4HAWXQ\\Mordo et al. - 2024 - Sponsored Question Answering.pdf:application/pdf},
}

@misc{nachimovsky_multi_agent_2025,
	title = {A Multi-Agent Perspective on Modern Information Retrieval},
	url = {http://arxiv.org/abs/2502.14796},
	doi = {10.48550/arXiv.2502.14796},
	abstract = {The rise of large language models ({LLMs}) has introduced a new era in information retrieval ({IR}), where queries and documents that were once assumed to be generated exclusively by humans can now also be created by automated agents. These agents can formulate queries, generate documents, and perform ranking. This shift challenges some long-standing {IR} paradigms and calls for a reassessment of both theoretical frameworks and practical methodologies. We advocate for a multi-agent perspective to better capture the complex interactions between query agents, document agents, and ranker agents. Through empirical exploration of various multi-agent retrieval settings, we reveal the significant impact of these interactions on system performance. Our findings underscore the need to revisit classical {IR} paradigms and develop new frameworks for more effective modeling and evaluation of modern retrieval systems.},
	number = {{arXiv}:2502.14796},
	publisher = {{arXiv}},
	author = {Nachimovsky, Haya and Tennenholtz, Moshe and Kurland, Oren},
	urldate = {2025-11-25},
	date = {2025-02-20},
	eprinttype = {arxiv},
	eprint = {2502.14796 [cs]},
	keywords = {Computer Science - Information Retrieval},
	file = {Preprint PDF:C\:\\Users\\tommy\\Zotero\\storage\\4L6HR2BT\\Nachimovsky et al. - 2025 - A Multi-Agent Perspective on Modern Information Re.pdf:application/pdf;Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\KPAGGKQE\\2502.html:text/html},
}

@misc{feizi_online_2024,
	title = {Online Advertisements with {LLMs}: Opportunities and Challenges},
	url = {http://arxiv.org/abs/2311.07601},
	shorttitle = {Online Advertisements with {LLMs}},
	abstract = {This paper explores the potential for leveraging Large Language Models ({LLM}) in the realm of online advertising systems. We delve into essential requirements including privacy, latency, reliability, users and advertisers’ satisfaction, which such a system must fulfill. We further introduce a general framework for {LLM} advertisement, consisting of modification, bidding, prediction, and auction modules. Different design considerations for each module is presented, with an in-depth examination of their practicality and the technical challenges inherent to their implementation.},
	number = {{arXiv}:2311.07601},
	publisher = {{arXiv}},
	author = {Feizi, Soheil and Hajiaghayi, {MohammadTaghi} and Rezaei, Keivan and Shin, Suho},
	urldate = {2024-02-28},
	date = {2024-02-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2311.07601 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {Feizi et al. - 2024 - Online Advertisements with LLMs Opportunities and.pdf:C\:\\Users\\tommy\\Zotero\\storage\\TAJVAP9G\\Feizi et al. - 2024 - Online Advertisements with LLMs Opportunities and.pdf:application/pdf},
}

@misc{bohnet_attributed_2023,
	title = {Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models},
	author = {Bohnet, Bernd and Tran, Vinh Q. and Verga, Pat and Aharoni, Roee and Andor, Daniel and Soares, Livio Baldini and Ciaramita, Massimiliano and Eisenstein, Jacob and Ganchev, Kuzman and Herzig, Jonathan and Hui, Kai and Kwiatkowski, Tom and Ma, Ji and Ni, Jianmo and Saralegui, Lierni Sestorain and Schuster, Tal and Cohen, William W. and Collins, Michael and Das, Dipanjan and Metzler, Donald and Petrov, Slav and Webster, Kellie},
	date = {2023},
}

@online{page_pagerank_1999,
	title = {The {PageRank} Citation Ranking: Bringing Order to the Web.},
	url = {http://ilpubs.stanford.edu:8090/422/},
	shorttitle = {The {PageRank} Citation Ranking},
	abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes {PageRank}, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare {PageRank} to an idealized random Web surfer. We show how to efficiently compute {PageRank} for large numbers of pages. And, we show how to apply {PageRank} to search and to user navigation.},
	type = {Techreport},
	author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
	urldate = {2025-11-29},
	date = {1999-11-11},
	note = {Publisher: Stanford {InfoLab}},
	file = {Snapshot:C\:\\Users\\tommy\\Zotero\\storage\\P9MRK8SH\\422.html:text/html},
}

@inproceedings{bardas_automatic_2025,
    address = {New York, NY, USA},
    series = {{SIGIR} '25},
    title = {Automatic {Document} {Editing} for {Improved} {Ranking}},
    isbn = {9798400715921},
    url = {https://dl.acm.org/doi/10.1145/3726302.3730168},
    doi = {10.1145/3726302.3730168},
    abstract = {We present a study of using large language models (LLMs) to modify a document so as to have it highly ranked for a query by an undisclosed ranking function. We present different prompting methods inspired by work on using LLMs to induce ranking. Empirical evaluation attests to the merits of the best performing methods with respect to human modifications and a highly effective feature-based modification method.},
    urldate = {2025-07-15},
    booktitle = {Proceedings of the 48th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
    publisher = {Association for Computing Machinery},
    author = {Bardas, Niv and Mordo, Tommy and Kurland, Oren and Tennenholtz, Moshe},
    month = jul,
    year = {2025},
    pages = {2779--2783},
    file = {Full Text PDF:C\:\\Users\\97252\\Zotero\\storage\\VD3HNHD9\\Bardas et al. - 2025 - Automatic Document Editing for Improved Ranking.pdf:application/pdf},
}