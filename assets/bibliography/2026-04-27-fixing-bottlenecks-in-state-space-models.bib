@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@misc{xiao2023introductiontransformersnlpperspective,
      title={Introduction to Transformers: an NLP Perspective}, 
      author={Tong Xiao and Jingbo Zhu},
      year={2023},
      eprint={2311.17633},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.17633}, 
}

@misc{dao2024transformersssmsgeneralizedmodels,
      title={Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality}, 
      author={Tri Dao and Albert Gu},
      year={2024},
      eprint={2405.21060},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.21060}, 
}

@misc{katharopoulos2020transformersrnnsfastautoregressive,
      title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention}, 
      author={Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and François Fleuret},
      year={2020},
      eprint={2006.16236},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.16236}, 
}

@misc{de2024griffinmixinggatedlinear,
      title={Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models}, 
      author={Soham De and Samuel L. Smith and Anushan Fernando and Aleksandar Botev and George Cristian-Muraru and Albert Gu and Ruba Haroun and Leonard Berrada and Yutian Chen and Srivatsan Srinivasan and Guillaume Desjardins and Arnaud Doucet and David Budden and Yee Whye Teh and Razvan Pascanu and Nando De Freitas and Caglar Gulcehre},
      year={2024},
      eprint={2402.19427},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.19427}, 
}

@misc{chung2014empiricalevaluationgatedrecurrent,
      title={Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}, 
      author={Junyoung Chung and Caglar Gulcehre and KyungHyun Cho and Yoshua Bengio},
      year={2014},
      eprint={1412.3555},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1412.3555}, 
}

@misc{gu2024mambalineartimesequencemodeling,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
      author={Albert Gu and Tri Dao},
      year={2024},
      eprint={2312.00752},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.00752}, 
}

@misc{gu2022efficientlymodelinglongsequences,
      title={Efficiently Modeling Long Sequences with Structured State Spaces}, 
      author={Albert Gu and Karan Goel and Christopher Ré},
      year={2022},
      eprint={2111.00396},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2111.00396}, 
}

@misc{gupta2022diagonalstatespaceseffective,
      title={Diagonal State Spaces are as Effective as Structured State Spaces}, 
      author={Ankit Gupta and Albert Gu and Jonathan Berant},
      year={2022},
      eprint={2203.14343},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.14343}, 
}

@misc{gupta2023simplifyingunderstandingstatespace,
      title={Simplifying and Understanding State Space Models with Diagonal Linear RNNs}, 
      author={Ankit Gupta and Harsh Mehta and Jonathan Berant},
      year={2023},
      eprint={2212.00768},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.00768}, 
}

@misc{smith2023simplifiedstatespacelayers,
      title={Simplified State Space Layers for Sequence Modeling}, 
      author={Jimmy T. H. Smith and Andrew Warrington and Scott W. Linderman},
      year={2023},
      eprint={2208.04933},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2208.04933}, 
}

@misc{ramachandran2017searchingactivationfunctions,
      title={Searching for Activation Functions}, 
      author={Prajit Ramachandran and Barret Zoph and Quoc V. Le},
      year={2017},
      eprint={1710.05941},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1710.05941}, 
}


@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}


@misc{sun2023retentivenetworksuccessortransformer,
      title={Retentive Network: A Successor to Transformer for Large Language Models}, 
      author={Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},
      year={2023},
      eprint={2307.08621},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.08621}, 
}

@article{10.1093/imamci/dnac005,
    author = {Pechlivanidou, Georgia and Karampetakis, Nicholas},
    title = {Zero-order hold discretization of general state space systems with input delay},
    journal = {IMA Journal of Mathematical Control and Information},
    volume = {39},
    number = {2},
    pages = {708-730},
    year = {2022},
    month = {04},
    abstract = {Plethora of applications in physics and engineering are dealing with systems that are subject to input delays. Despite the scientific focus, previous research investigated only state space systems with input delays. Here, we investigate the discretization of generalized state space system with input delay by using the zero-order hold method. Firstly, the solution of a continuous time, generalized state space system with input delay is addressed. By applying the appropriate zero-order hold sampling method, we transform a continuous time linear system into the equivalent discrete time system, while the discrete time solution of the equivalent system is analytically presented. Finally, we use local error metric to estimate the difference between the continuous time and discrete time solutions.},
    issn = {1471-6887},
    doi = {10.1093/imamci/dnac005},
    url = {https://doi.org/10.1093/imamci/dnac005},
    eprint = {https://academic.oup.com/imamci/article-pdf/39/2/708/48613741/dnac005.pdf},
}


@misc{gu2020hipporecurrentmemoryoptimal,
      title={HiPPO: Recurrent Memory with Optimal Polynomial Projections}, 
      author={Albert Gu and Tri Dao and Stefano Ermon and Atri Rudra and Christopher Re},
      year={2020},
      eprint={2008.07669},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2008.07669}, 
}
@misc{gu2021combiningrecurrentconvolutionalcontinuoustime,
      title={Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers}, 
      author={Albert Gu and Isys Johnson and Karan Goel and Khaled Saab and Tri Dao and Atri Rudra and Christopher Ré},
      year={2021},
      eprint={2110.13985},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.13985}, 
}

@misc{gu2022parameterizationinitializationdiagonalstate,
      title={On the Parameterization and Initialization of Diagonal State Space Models}, 
      author={Albert Gu and Ankit Gupta and Karan Goel and Christopher Ré},
      year={2022},
      eprint={2206.11893},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2206.11893}, 
}

@misc{wang2025understandingmitigatingbottlenecksstate,
      title={Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing}, 
      author={Peihao Wang and Ruisi Cai and Yuehao Wang and Jiajun Zhu and Pragya Srivastava and Zhangyang Wang and Pan Li},
      year={2025},
      eprint={2501.00658},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.00658}, 
}