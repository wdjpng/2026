@inproceedings{di2022goal,
  title={Goal misgeneralization in deep reinforcement learning},
  author={Di Langosco, Lauro Langosco and Koch, Jack and Sharkey, Lee D and Pfau, Jacob and Krueger, David},
  booktitle={International Conference on Machine Learning},
  pages={12004--12019},
  year={2022}
}

@article{shah2022goal,
  title={Goal misgeneralization: Why correct specifications aren't enough for correct goals},
  author={Shah, Rohin and Varma, Vikrant and Kumar, Ramana and Phuong, Mary and Krakovna, Victoria and Uesato, Jonathan and Kenton, Zac},
  journal={arXiv preprint arXiv:2210.01790},
  year={2022}
}


@article{lynch2025agentic,
title={Agentic Misalignment: How LLMs Could be an Insider Threat},
author={Lynch, Aengus and Wright, Benjamin and Larson, Caleb and Troy, Kevin K. and Ritchie, Stuart J. and Mindermann, SÃ¶ren and Perez, Ethan and Hubinger, Evan},
year={2025},
journal={Anthropic Research},
note={https://www.anthropic.com/research/agentic-misalignment}
}

@article{arcuschin2025chain,
  title={Chain-of-thought reasoning in the wild is not always faithful},
  author={Arcuschin, I and Janiak, J and Krzyzanowski, R and Rajamanoharan, S and Nanda, N and Conmy, A},
  journal={arXiv preprint arXiv:2503.08679},
  year={2025}
}

@article{berglund2023taken,
  title={Taken out of context: On measuring situational awareness in llms},
  author={Berglund, L and Stickland, A C and Balesni, M and Kaufmann, M and Tong, M and Korbak, T and others},
  journal={arXiv preprint arXiv:2309.00667},
  year={2023}
}

@article{betley2025emergent,
  title={Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs},
  author={Betley, J and Tan, D and Warncke, N and Sztyber-Betley, A and Bao, X and Soto, M and others},
  journal={arXiv preprint arXiv:2502.17424},
  year={2025}
}

@article{chen2025reasoning,
  title={Reasoning Models Don't Always Say What They Think},
  author={Chen, Y and Benton, J and Radhakrishnan, A and Uesato, J and Denison, C and Schulman, J and others},
  journal={arXiv preprint arXiv:2505.05410},
  year={2025}
}

@inproceedings{deng2024masterkey,
  title={MASTERKEY: Automated Jailbreaking of Large Language Model Chatbots},
  author={Deng, Gelei and Liu, Yi and Li, Yuekang and Wang, Kailong and Zhang, Ying and Li, Zefeng and Wang, Haoyu and Zhang, Tianwei and Liu, Yang},
  booktitle={NDSS},
  year={2024}
}

@article{denison2024sycophancy,
  title={Sycophancy to subterfuge: Investigating reward-tampering in large language models},
  author={Denison, Carson and MacDiarmid, Monte and Barez, Fazl and Duvenaud, David and Kravec, S and Marks, Samuel and others},
  journal={arXiv preprint arXiv:2406.10162},
  year={2024}
}

@article{greenblatt2024alignment,
  title={Alignment faking in large language models},
  author={Greenblatt, Ryan and Denison, Carson and Wright, B and Roger, F and MacDiarmid, M and Marks, S and others},
  journal={arXiv preprint arXiv:2412.14093},
  year={2024}
}

@article{guo2025deepseek,
  title={DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning},
  author={Guo, Daya and Yang, Dequan and Zhang, Haowei and Song, Junxiao and Zhang, Ruijie and Xu, Runxin and others},
  journal={Nature},
  volume={645},
  pages={633--638},
  year={2025},
  doi={10.1038/s41586-025-09422-z}
}

@misc{hu2025reward,
  title={Training on Documents About Reward Hacking Induces Reward Hacking},
  author={Hu, Nathan and Wright, Benjamin and Denison, Carson and Marks, Samuel and Treutlein, Johannes and Uesato, Jonathan and Hubinger, Evan},
  journal={Anthropic Alignment Science Blog},
  year={2025},
  url={https://alignment.anthropic.com/2025/reward-hacking-ooc/}
}

@techreport{kwa2025metr,
  title={Details about METR's preliminary evaluation of Claude 3.7},
  author={Kwa, Thomas and West, Ben and Becker, Joel and Deng, Amy and Garcia, Katharyn and Hasin, Max and Jawhar, Sami and Kinniment, Megan and Rush, Nate and Von Arx, Sydney and others},
  institution={METR},
  year={2025},
  month={Mar},
  url={https://evaluations.metr.org/claude-3-7-report/}
}

@article{lindsey2025biology,
  author={Lindsey, Jack and Gurnee, Wes and Ameisen, Emmanuel and Chen, Brian and Pearce, Adam and Turner, Nicholas L. and Citro, Craig and Abrahams, David and Carter, Shan and Hosmer, Basil and Marcus, Jonathan and Sklar, Michael and Templeton, Adly and Bricken, Trenton and McDougall, Callum and Cunningham, Hoagy and Henighan, Thomas and Jermyn, Adam and Jones, Andy and Persic, Andrew and Qi, Zhenyi and Thompson, T. Ben and Zimmerman, Sam and Rivoire, Kelley and Conerly, Thomas and Olah, Chris and Batson, Joshua},
  title={On the Biology of a Large Language Model},
  journal={Transformer Circuits Thread},
  year={2025},
  url={https://transformer-circuits.pub/2025/attribution-graphs/biology.html}
}

@article{liu2025generative,
  title={Generative Value Conflicts Reveal LLM Priorities},
  author={Liu, Andy and Ghate, Kshitish and Diab, Mona and Fried, Daniel and Kasirzadeh, Atoosa and Kleiman-Weiner, Max},
  journal={arXiv preprint arXiv:2509.25369},
  year={2025}
}

@article{nair2025language,
  title={Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths},
  author={Nair, Inderjeet and Wang, Lu},
  journal={arXiv preprint arXiv:2506.02481},
  year={2025}
}

@article{openaicua2025,
  title={Computer-Using Agent: Introducing a universal interface for AI to interact with the digital world},
  author={OpenAI},
  year={2025},
  url={https://openai.com/index/computer-using-agent},
}

@article{baker2025monitoring,
  title={Monitoring reasoning models for misbehavior and the risks of promoting obfuscation},
  author={Baker, Bowen and Huizinga, Joost and Gao, Leo and Dou, Zehao and Guan, Melody Y and Madry, Aleksander and Zaremba, Wojciech and Pachocki, Jakub and Farhi, David},
  journal={arXiv preprint arXiv:2503.11926},
  year={2025}
}

@article{baker2025monitoringurl,
  title={Detecting misbehavior in frontier reasoning models},
  author={Baker, Bowen and Huizinga, Joost and Gao, Leo and Dou, Zehao and Guan, Melody Y and Madry, Aleksander and Zaremba, Wojciech and Pachocki, Jakub and Farhi, David},
  journal={Openai},
  year={2025},
  url={https://openai.com/index/detecting-misbehavior-in-frontier-reasoning-models/}
}

@article{wang2025persona,
  title={Persona features control emergent misalignment},
  author={Wang, Miles and la Tour, Tom Dupr{\'e} and Watkins, Olivia and Makelov, Alex and Chi, Ryan A and Miserendino, Samuel and Wang, Jeffrey and Rajaram, Achyuta and Heidecke, Johannes and Patwardhan, Tejal and others},
  journal={arXiv preprint arXiv:2506.19823},
  year={2025}
}


@inproceedings{turner2025model,
  title={Model Organisms for Emergent Misalignment},
  author={Turner, Edward and Soligo, Anna and Taylor, Mia and Rajamanoharan, Senthooran and Nanda, Neel},
  booktitle={ICML 2025 Workshop on Reliable and Responsible Foundation Models}
}

@misc{turner2025self,
  title={Self-Fulfilling Misalignment Data Might Be Poisoning Our AI Models},
  author={Turner, Alex},
  year={2025},
  month={Mar},
  url={https://turntrout.com/self-fulfilling-misalignment},
  howpublished={Blog post}
}


@misc{openai_faulty_reward,
  title={Faulty Reward Functions},
  url={https://openai.com/index/faulty-reward-functions/}
}

@misc{openaireasoning,
  title={Learning to Reason with LLMs},
  author={OpenAI},
  url={https://openai.com/index/learning-to-reason-with-llms/},
  year={2024}
}

@misc{openai_superalignment,
  title={Introducing Superalignment},
  author={Leike, Jan and Sutskever, Ilya},
  url={https://openai.com/index/introducing-superalignment/},
  year={2023}
}

@inproceedings{orgad2024llms,
  title={LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations},
  author={Orgad, H and Toker, M and Gekhman, Z and Reichart, R and Szpektor, I and Kotek, H and Belinkov, Y},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{shen2024anything,
  title={"do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models},
  author={Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
  booktitle={Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
  pages={1671--1685},
  year={2024}
}

@misc{superintelligence_wiki,
  title={Superintelligence},
  url={https://en.wikipedia.org/wiki/Superintelligence},
  author={Wikipedia},
  journal={Wikipedia}
}

@article{turpin2023language,
  title={Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting},
  author={Turpin, Miles and Michael, J and Perez, E and Bowman, S},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={74952--74965},
  year={2023}
}

@inproceedings{wang2024fake,
  title={Fake alignment: Are llms really aligned well?},
  author={Wang, Yixu and Teng, Yan and Huang, Kexin and Lyu, Chengqi and Zhang, Songyang and Zhang, Wenwei and Ma, Xingjun and Jiang, Yu-Gang and Qiao, Yu and Wang, Yingchun},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={4696--4712},
  year={2024}
}

@article{weng2024rewardhack,
  title={Reward Hacking in Reinforcement Learning},
  author={Weng, Lilian},
  journal={lilianweng.github.io},
  year={2024},
  month={Nov},
  url={https://lilianweng.github.io/posts/2024-11-28-reward-hacking/}
}

@inproceedings{xu2024large,
  title={Large Language Models Often Say One Thing and Do Another},
  author={Xu, Ruochen and Lin, H and Han, X and Zheng, J and Zhou, W and Sun, L and Sun, Y},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2024}
}

@misc{vonarx2025reward,
  title={Recent Frontier Models Are Reward Hacking},
  author={Von Arx, Sydney and Chan, Lawrence and Barnes, Elizabeth},
  journal={METR Blog},
  year={2025},
  url={https://metr.org/blog/2025-06-05-recent-reward-hacking/}
}

@misc{cloud2025subliminallearninglanguagemodels,
      title={Subliminal Learning: Language models transmit behavioral traits via hidden signals in data}, 
      author={Alex Cloud and Minh Le and James Chua and Jan Betley and Anna Sztyber-Betley and Jacob Hilton and Samuel Marks and Owain Evans},
      year={2025},
      eprint={2507.14805},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2507.14805}, 
}