@article{Huang_2025,
title={A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
volume={43},
ISSN={1558-2868},
url={http://dx.doi.org/10.1145/3703155},
DOI={10.1145/3703155},
number={2},
journal={ACM Transactions on Information Systems},
publisher={Association for Computing Machinery (ACM)},
author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
year={2025},
month=jan, pages={1–55} }
@misc{bai2025hallucinationmultimodallargelanguage,
      title={Hallucination of Multimodal Large Language Models: A Survey}, 
      author={Zechen Bai and Pichao Wang and Tianjun Xiao and Tong He and Zongbo Han and Zheng Zhang and Mike Zheng Shou},
      year={2025},
      eprint={2404.18930},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.18930}, 
}
@misc{su2025largelanguagemodelsreally,
      title={Do Large Language Models (Really) Need Statistical Foundations?}, 
      author={Weijie Su},
      year={2025},
      eprint={2505.19145},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2505.19145}, 
}
@misc{dai2023plausiblefaithfulprobingobject,
      title={Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training}, 
      author={Wenliang Dai and Zihan Liu and Ziwei Ji and Dan Su and Pascale Fung},
      year={2023},
      eprint={2210.07688},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.07688}, 
}
@misc{agrawal2024languagemodelsknowtheyre,
      title={Do Language Models Know When They're Hallucinating References?}, 
      author={Ayush Agrawal and Mirac Suzgun and Lester Mackey and Adam Tauman Kalai},
      year={2024},
      eprint={2305.18248},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.18248}, 
}
@misc{kandpal2023largelanguagemodelsstruggle,
      title={Large Language Models Struggle to Learn Long-Tail Knowledge}, 
      author={Nikhil Kandpal and Haikang Deng and Adam Roberts and Eric Wallace and Colin Raffel},
      year={2023},
      eprint={2211.08411},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.08411}, 
}
@misc{damani2025binaryrewardstraininglms,
      title={Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty}, 
      author={Mehul Damani and Isha Puri and Stewart Slocum and Idan Shenfeld and Leshem Choshen and Yoon Kim and Jacob Andreas},
      year={2025},
      eprint={2507.16806},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2507.16806}, 
}
@misc{ji2024llminternalstatesreveal,
      title={LLM Internal States Reveal Hallucination Risk Faced With a Query}, 
      author={Ziwei Ji and Delong Chen and Etsuko Ishii and Samuel Cahyawijaya and Yejin Bang and Bryan Wilie and Pascale Fung},
      year={2024},
      eprint={2407.03282},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.03282}, 
}
@misc{wei2025truthrlincentivizingtruthfulllms,
      title={TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning}, 
      author={Zhepei Wei and Xiao Yang and Kai Sun and Jiaqi Wang and Rulin Shao and Sean Chen and Mohammad Kachuee and Teja Gollapudi and Tony Liao and Nicolas Scheffer and Rakesh Wanga and Anuj Kumar and Yu Meng and Wen-tau Yih and Xin Luna Dong},
      year={2025},
      eprint={2509.25760},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2509.25760}, 
}
@misc{kang2023deficiencylargelanguagemodels,
      title={Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination}, 
      author={Haoqiang Kang and Xiao-Yang Liu},
      year={2023},
      eprint={2311.15548},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.15548}, 
}
@misc{alansari2025largelanguagemodelshallucination,
      title={Large Language Models Hallucination: A Comprehensive Survey}, 
      author={Aisha Alansari and Hamzah Luqman},
      year={2025},
      eprint={2510.06265},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2510.06265}, 
}
@article{Ji_2023,
   title={Survey of Hallucination in Natural Language Generation},
   volume={55},
   ISSN={1557-7341},
   url={http://dx.doi.org/10.1145/3571730},
   DOI={10.1145/3571730},
   number={12},
   journal={ACM Computing Surveys},
   publisher={Association for Computing Machinery (ACM)},
   author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
   year={2023},
   month=mar, pages={1–38} }
@misc{kalai2025languagemodelshallucinate,
      title={Why Language Models Hallucinate}, 
      author={Adam Tauman Kalai and Ofir Nachum and Santosh S. Vempala and Edwin Zhang},
      year={2025},
      eprint={2509.04664},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2509.04664}, 
}
@inproceedings{longpre-etal-2021-entity,
    title = "Entity-Based Knowledge Conflicts in Question Answering",
    author = "Longpre, Shayne  and
      Perisetla, Kartik  and
      Chen, Anthony  and
      Ramesh, Nikhil  and
      DuBois, Chris  and
      Singh, Sameer",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.565/",
    doi = "10.18653/v1/2021.emnlp-main.565",
    pages = "7052--7063",
    abstract = "Knowledge-dependent tasks typically use two sources of knowledge: parametric, learned at training time, and contextual, given as a passage at inference time. To understand how models use these sources together, we formalize the problem of knowledge conflicts, where the contextual information contradicts the learned information. Analyzing the behaviour of popular models, we measure their over-reliance on memorized information (the cause of hallucinations), and uncover important factors that exacerbate this behaviour. Lastly, we propose a simple method to mitigate over-reliance on parametric knowledge, which minimizes hallucination, and improves out-of-distribution generalization by 4{\%} - 7{\%}. Our findings demonstrate the importance for practitioners to evaluate model tendency to hallucinate rather than read, and show that our mitigation strategy encourages generalization to evolving information (i.e. time-dependent queries). To encourage these practices, we have released our framework for generating knowledge conflicts."
}

@misc{mohsin2025fundamentallimitsllmsscale,
      title={On the Fundamental Limits of LLMs at Scale}, 
      author={Muhammad Ahmed Mohsin and Muhammad Umer and Ahsan Bilal and Zeeshan Memon and Muhammad Ibtsaam Qadir and Sagnik Bhattacharya and Hassan Rizwan and Abhiram R. Gorle and Maahe Zehra Kazmi and Ayesha Mohsin and Muhammad Usman Rafique and Zihao He and Pulkit Mehta and Muhammad Ali Jamshed and John M. Cioffi},
      year={2025},
      eprint={2511.12869},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2511.12869}, 
}
@misc{pandya2025compasscontextmodulatedpidattention,
      title={COMPASS: Context-Modulated PID Attention Steering System for Hallucination Mitigation}, 
      author={Snigdha Pandya and Rohan Nagale and Kenji Sahay and Anna Lin and Shikhar Shiromani and Kevin Zhu and Dev Sunishchal},
      year={2025},
      eprint={2511.14776},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2511.14776}, 
}
@misc{karpowicz2025fundamentalimpossibilityhallucinationcontrol,
      title={On the Fundamental Impossibility of Hallucination Control in Large Language Models}, 
      author={Michał P. Karpowicz},
      year={2025},
      eprint={2506.06382},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2506.06382}, 
}