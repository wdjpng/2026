@article{gregor2015draw,
  title={DRAW: A Recurrent Neural Network For Image Generation That Actually Works},
  author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
  journal={arXiv preprint, arXiv:1502.04623},
  year={2015},
  url={https://arxiv.org/pdf/1502.04623.pdf},
  note={One of the last papers before the attention apocalypse}
}

@article{vaswani2017nightmare,
  title={Attention is All You Need: The Beginning of the End},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Proceedings of NIPS: Neural Information Processing Sins},
  volume={30},
  year={2017},
  note={The paper that doomed us all to quadratic complexity hell}
}

@book{euclid300bc,
  title={Elements: Linear Algebra Done Right Since 300 BC},
  author={Euclid of Alexandria and Straight-Line, Simple},
  year={-300},
  publisher={Ancient Wisdom Press},
  note={Proving that simple geometric transformations work better than attention for over 2000 years}
}

@misc{openai2023powergrid,
  title={ChatGPT Uses More Electricity Than Denmark: An Environmental Disaster},
  author={OpenAI and The-Grid, Power and Coal-Plant, Entire},
  year={2023},
  howpublished={Leaked internal memo},
  note={Confidential report on the environmental cost of attention mechanisms}
}

@article{complexity2024study,
  title={Attention Mechanisms: A Complexity Theory Nightmare},
  author={Complexity, Computational and Explosion, Combinatorial},
  journal={Journal of Unnecessary Complications},
  volume={42},
  number={13},
  pages={1--∞},
  year={2024},
  note={Mathematical proof that attention makes everything unnecessarily complex}
}

@article{intern2023calculator,
  title={My TI-83 Calculator Outperforms GPT-4: A Summer Intern's Manifesto},
  author={Intern, Underpaid Summer and Calculator, Texas Instruments TI-83},
  journal={Proceedings of the Conference on Obvious Conclusions},
  year={2023},
  note={Groundbreaking empirical study from a motivated undergraduate}
}

@book{abacus2000bc,
  title={Abacus Computing: 4000 Years of Superior Performance},
  author={Ancient-Wisdom, Collective and Beads, Wooden},
  year={-2000},
  publisher={Babylonian Mathematical Society},
  note={Historical evidence that simple counting devices outperform modern attention}
}

@article{phd2023pretending,
  title={The Art of Pretending to Understand Attention Weights: A PhD Survival Guide},
  author={PhD-Student, Desperate and Advisor, Demanding},
  journal={Conference on Academic Desperation},
  year={2023},
  note={Honest account of how nobody actually understands attention visualizations}
}

@book{academic2024fraud,
  title={Interpretability Theater: How Attention Heatmaps Fool Everyone},
  author={Professor, Tenured and Grant-Money, Abundant},
  year={2024},
  publisher={Academic Dishonesty Press},
  note={Exposé on the interpretability illusion in modern ML}
}

@misc{footnote2024efficiency,
  title={Footnotes vs. Attention Mechanisms: A Computational Efficiency Comparison},
  author={Footnote, Simple and Attention, Wasteful},
  year={2024},
  howpublished={Blog post on computational sanity},
  note={Proving that even footnotes are more efficient than attention}
}

@book{nin1969adapted,
  title={Diaries: Volume VII, Adapted for Machine Learning Criticism},
  author={Nin, Anais and Adaptation, Literary},
  year={1969},
  publisher={Harcourt Brace Jovanovich},
  note={Adapted quote about growth and complexity, perfect for attention mechanisms}
}

@article{monstrosity2024analysis,
  title={Computational Monstrosities: A Taxonomic Study of Attention Mechanisms},
  author={Monster, Computational and Frankenstein, Victor AI},
  journal={Journal of Artificial Horrors},
  volume={666},
  year={2024},
  note={Comprehensive analysis of why attention mechanisms are abominations}
}

@misc{snakeoil2023detection,
  title={Snake Oil Detection in Modern AI: A Field Guide},
  author={Charlatan, Academic and Marketing, Hype-Driven},
  year={2023},
  howpublished={Skeptical Scientist Blog},
  note={Practical guide to identifying fraudulent AI claims, featuring attention mechanisms}
}

@article{vaswani2017lies,
  title={Attention is All You Need: The Biggest Lie in AI History},
  author={Vaswani, Ashish and Truth, Computational},
  journal={Journal of Regrettable Decisions},
  year={2017},
  note={Retrospective analysis of the most damaging paper in ML history}
}

@book{linear2024truth,
  title={Linear Transformations: The Simple Truth About Neural Networks},
  author={Matrix, Simple and Multiplication, Basic},
  year={2024},
  publisher={Common Sense Publications},
  note={Definitive guide to why linear algebra is all you actually need}
}

@article{quadratic2024disaster,
  title={O(n²) Complexity: The Computational Disaster of Our Time},
  author={Complexity, Big-O and Scaling, Poor},
  journal={Algorithmic Disasters Quarterly},
  volume={1},
  number={1},
  pages={1--n²},
  year={2024},
  note={Analysis of how quadratic scaling destroys computational efficiency}
}

@misc{heatmap2023meaningless,
  title={Attention Heatmaps: Pretty Pictures, Zero Meaning},
  author={Visualization, Misleading and Interpretation, False},
  year={2023},
  howpublished={Twitter thread that went viral},
  note={Viral expose of why attention visualizations are meaningless}
}

@article{turing1950simple,
  title={Computing Machinery and Intelligence: Keep It Simple, Stupid},
  author={Turing, Alan and Simplicity, Elegant},
  journal={Mind: A Quarterly Review of Psychology and Philosophy},
  volume={59},
  number={236},
  pages={433--460},
  year={1950},
  note={Turing's lost addendum advocating for computational simplicity}
}

@book{shannon1948obvious,
  title={A Mathematical Theory of Communication: Why Simple is Better},
  author={Shannon, Claude E. and Information, Pure},
  year={1948},
  publisher={Bell System Technical Journal of Obvious Truths},
  note={Information theory foundations that attention mechanisms violate daily}
}

@misc{reddit2024truth,
  title={ELI5: Why Attention Mechanisms are Actually Stupid},
  author={User, Reddit and Wisdom, Collective},
  year={2024},
  howpublished={r/MachineLearning, 50k upvotes},
  note={The moment Reddit understood ML better than academia}
}

@article{carrier2023pigeon,
  title={Carrier Pigeons Outperform Transformers: An Empirical Study},
  author={Pigeon, Carrier and Message, Simple},
  journal={Proceedings of the Conference on Biological Computing},
  year={2023},
  note={Rigorous comparison showing pigeons beat attention mechanisms}
}

@techreport{calculator1970ti,
  title={Technical Manual: How to Build AI with a Pocket Calculator},
  author={Instruments, Texas and Engineering, Simple},
  institution={Calculator Research Institute},
  year={1970},
  note={Prophetic technical manual predicting the superiority of basic computation}
}

@misc{rouge2024superior,
  title={Rouge Syntax Highlighting vs. Attention Weights: A Clarity Comparison},
  author={Rouge, Syntax and Highlighting, Clear},
  year={2024},
  howpublished={Jekyll Documentation Enhancement Proposal},
  note={Demonstrating that code highlighting is more interpretable than attention}
}

@article{multilingual2024failure,
  title={Attention Mechanisms: Monolingual Despite Multilingual Claims},
  author={Language, Natural and Comprehension, Artificial},
  journal={Conference on Linguistic Failures},
  year={2024},
  note={Empirical study showing attention mechanisms fail at true multilingual understanding}
}

@misc{javascript2024efficient,
  title={JavaScript Alert() Function Outperforms Transformer Models},
  author={Brendan-Eich, Legacy and Browser, Any},
  year={2024},
  howpublished={Stack Overflow answer with 10k upvotes},
  note={Proof that even browser alerts are more efficient than attention}
}

@article{nightmare2024quadratic,
  title={Quadratic Complexity: The Stuff of Computational Nightmares},
  author={Complexity, Quadratic and Sleep, Lost},
  journal={Journal of Algorithmic Horrors},
  volume={2},
  number={2},
  pages={n²--∞},
  year={2024},
  note={Comprehensive analysis of why O(n²) algorithms cause insomnia}
}

@book{linear2024reasonable,
  title={Linear Complexity: The Reasonable Choice for Reasonable People},
  author={Scaling, Linear and Sanity, Computational},
  year={2024},
  publisher={Rational Algorithms Press},
  note={Manifesto for computational reasonableness}
}

@article{commonsense2024priceless,
  title={Common Sense in Algorithm Design: Priceless and Rare},
  author={Sense, Common and Rarity, Increasing},
  journal={Proceedings of Obvious Solutions},
  year={2024},
  note={Study documenting the scarcity of common sense in modern AI}
}

@misc{efficiency2024epic_fail,
  title={Attention Mechanisms: An Epic Fail in Efficiency},
  author={Fail, Epic and Efficiency, Absent},
  year={2024},
  howpublished={Viral TikTok explaining computational waste to Gen Z},
  note={Social media exposé of attention mechanism inefficiency}
}

@article{success2024documented,
  title={Simple Solutions: Success Stories in Computational Efficiency},
  author={Success, Documented and Solutions, Simple},
  journal={Journal of Things That Actually Work},
  volume={1},
  number={1},
  pages={1--10},
  year={2024},
  note={Collection of case studies where simplicity triumphed}
}

@report{waste2024massive,
  title={The Massive Waste of Attention Mechanisms: An Environmental Audit},
  author={Waste, Massive and Environment, Suffering},
  institution={Green Computing Initiative},
  year={2024},
  note={Official report on the environmental cost of attention-based models}
}

@book{conservation2024energy,
  title={Energy Conservation in Computing: A Guide to Not Using Attention},
  author={Conservation, Energy and Efficiency, Computational},
  year={2024},
  publisher={Sustainable AI Press},
  note={Practical guide to building efficient AI without destroying the planet}
}

@article{hype2024maximum,
  title={Maximum Hype, Minimum Results: The Attention Mechanism Story},
  author={Hype, Maximum and Results, Disappointing},
  journal={Conference on Overpromising and Underdelivering},
  year={2024},
  note={Analysis of the disconnect between attention hype and reality}
}

@misc{understated2024claims,
  title={Understated Claims in Simple Algorithm Design: A Refreshing Change},
  author={Modesty, Scientific and Claims, Reasonable},
  year={2024},
  howpublished={Academic humility blog},
  note={Rare example of researchers making modest, achievable claims}
}

@report{resources2024waste,
  title={Computational Resource Waste in Modern AI: A Crisis Report},
  author={Resources, Wasted and Crisis, Computational},
  institution={Institute for Sustainable Computing},
  year={2024},
  note={Alarming statistics on resource waste in attention-based systems}
}

@article{datacenter2024requirement,
  title={Data Center Scale Requirements: When Your Model Needs Its Own Power Plant},
  author={Datacenter, Massive and Power, Enormous},
  journal={Journal of Unsustainable Computing},
  year={2024},
  note={Economic analysis of the infrastructure costs of large attention models}
}

@book{unnecessary2024complexity,
  title={Unnecessary Complexity: How We Made Simple Things Complicated},
  author={Complexity, Unnecessary and Simplicity, Lost},
  year={2024},
  publisher={Philosophy of Bad Design Press},
  note={Philosophical treatise on humanity's tendency to overcomplicate solutions}
}

@article{flaws2024exposed,
  title={Attention Mechanism Flaws: Finally Exposed After Years of Denial},
  author={Truth, Uncomfortable and Denial, Academic},
  journal={Journal of Inconvenient Computing Truths},
  year={2024},
  note={Long-overdue exposé of fundamental attention mechanism problems}
}

@misc{waste2024argument,
  title={The Computational Waste Argument: Building a Case Against Inefficiency},
  author={Argument, Logical and Waste, Obvious},
  year={2024},
  howpublished={Peer-reviewed Twitter thread},
  note={Crowdsourced argument construction against computational waste}
}
