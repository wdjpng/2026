{"UNet2DConditionModel":{"file":"diffusers/models/unets/unet_2d_condition.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unets/unet_2d_condition.py","description":"2D UNet model with conditional inputs for diffusion models","key_methods":["forward","__init__"]},"Transformer2DModel":{"file":"diffusers/models/transformers/transformer_2d.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformers/transformer_2d.py","description":"2D Transformer model for DiT-based architectures","key_methods":["forward","__init__"]},"SD3Transformer2DModel":{"file":"diffusers/models/transformers/transformer_sd3.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformers/transformer_sd3.py","description":"Stable Diffusion 3 Transformer model","key_methods":["forward","__init__"]},"FluxTransformer2DModel":{"file":"diffusers/models/transformers/transformer_flux.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformers/transformer_flux.py","description":"FLUX Transformer model","key_methods":["forward","__init__"]},"HunyuanDiT2DModel":{"file":"diffusers/models/transformers/hunyuandit_2d.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformers/hunyuandit_2d.py","description":"HunyuanDiT 2D model","key_methods":["forward","__init__"]},"SanaTransformer2DModel":{"file":"diffusers/models/transformers/transformer_sana.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformers/transformer_sana.py","description":"SANA Transformer model","key_methods":["forward","__init__"]},"LuminaNextTransformer2DModel":{"file":"diffusers/models/transformers/lumina_nextdit2d.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformers/lumina_nextdit2d.py","description":"Lumina-Next DiT model","key_methods":["forward","__init__"]},"CogView3PlusTransformer2DModel":{"file":"diffusers/models/transformers/cogview3plus_transformer_2d.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformers/cogview3plus_transformer_2d.py","description":"CogView3+ Transformer model","key_methods":["forward","__init__"]},"AutoencoderKL":{"file":"diffusers/models/autoencoders/autoencoder_kl.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/autoencoders/autoencoder_kl.py","description":"Variational Autoencoder with KL divergence","key_methods":["encode","decode"]},"AutoencoderDC":{"file":"diffusers/models/autoencoders/autoencoder_dc.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/autoencoders/autoencoder_dc.py","description":"Deep Compression Autoencoder","key_methods":["encode","decode"]},"CLIPTextModel":{"file":"transformers/models/clip/modeling_clip.py","github_url":"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_clip.py","description":"CLIP text encoder model","key_methods":["forward"]},"CLIPTextModelWithProjection":{"file":"transformers/models/clip/modeling_clip.py","github_url":"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_clip.py","description":"CLIP text encoder with projection layer","key_methods":["forward"]},"T5EncoderModel":{"file":"transformers/models/t5/modeling_t5.py","github_url":"https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py","description":"T5 encoder-only model","key_methods":["forward"]},"StableDiffusionPipeline":{"file":"diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py","description":"Stable Diffusion text-to-image pipeline","key_methods":["__call__"]},"StableDiffusionXLPipeline":{"file":"diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py","description":"Stable Diffusion XL text-to-image pipeline","key_methods":["__call__"]},"StableDiffusion3Pipeline":{"file":"diffusers/pipelines/stable_diffusion_3/pipeline_stable_diffusion_3.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion_3/pipeline_stable_diffusion_3.py","description":"Stable Diffusion 3 text-to-image pipeline","key_methods":["__call__"]},"FluxPipeline":{"file":"diffusers/pipelines/flux/pipeline_flux.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/flux/pipeline_flux.py","description":"FLUX text-to-image pipeline","key_methods":["__call__"]},"PixArtAlphaPipeline":{"file":"diffusers/pipelines/pixart_alpha/pipeline_pixart_alpha.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/pixart_alpha/pipeline_pixart_alpha.py","description":"PixArt-α text-to-image pipeline","key_methods":["__call__"]},"PixArtSigmaPipeline":{"file":"diffusers/pipelines/pixart_alpha/pipeline_pixart_sigma.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/pixart_alpha/pipeline_pixart_sigma.py","description":"PixArt-Σ text-to-image pipeline","key_methods":["__call__"]},"HunyuanDiTPipeline":{"file":"diffusers/pipelines/hunyuandit/pipeline_hunyuandit.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/hunyuandit/pipeline_hunyuandit.py","description":"HunyuanDiT text-to-image pipeline","key_methods":["__call__"]},"SanaPipeline":{"file":"diffusers/pipelines/sana/pipeline_sana.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/sana/pipeline_sana.py","description":"SANA text-to-image pipeline","key_methods":["__call__"]},"LuminaNextPipeline":{"file":"diffusers/pipelines/lumina/pipeline_lumina_next.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/lumina/pipeline_lumina_next.py","description":"Lumina-Next text-to-image pipeline","key_methods":["__call__"]},"CogView3PlusPipeline":{"file":"diffusers/pipelines/cogview3/pipeline_cogview3_plus.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/cogview3/pipeline_cogview3_plus.py","description":"CogView3+ text-to-image pipeline","key_methods":["__call__"]},"CogView4Pipeline":{"file":"diffusers/pipelines/cogview4/pipeline_cogview4.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/cogview4/pipeline_cogview4.py","description":"CogView4 text-to-image pipeline with advanced Chinese text rendering","key_methods":["__call__"]},"CogView4Transformer2DModel":{"file":"diffusers/models/transformers/cogview4_transformer_2d.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformers/cogview4_transformer_2d.py","description":"CogView4 Transformer model with Chinese text rendering capabilities","key_methods":["forward","__init__"]},"DDIMScheduler":{"file":"diffusers/schedulers/scheduling_ddim.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py","description":"Denoising Diffusion Implicit Models scheduler","key_methods":["step"]},"EulerDiscreteScheduler":{"file":"diffusers/schedulers/scheduling_euler_discrete.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py","description":"Euler discrete scheduler for diffusion models","key_methods":["step"]},"DPMSolverMultistepScheduler":{"file":"diffusers/schedulers/scheduling_dpmsolver_multistep.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py","description":"DPM-Solver++ multistep scheduler","key_methods":["step"]},"FlowMatchEulerDiscreteScheduler":{"file":"diffusers/schedulers/scheduling_flow_match_euler_discrete.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_flow_match_euler_discrete.py","description":"Flow matching Euler discrete scheduler","key_methods":["step"]},"DDPMScheduler":{"file":"diffusers/schedulers/scheduling_ddpm.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py","description":"Denoising Diffusion Probabilistic Models scheduler","key_methods":["step"]},"CogVideoXDDIMScheduler":{"file":"diffusers/schedulers/scheduling_cogvideox_ddim.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_cogvideox_ddim.py","description":"CogVideoX DDIM scheduler","key_methods":["step"]},"CLIPTokenizer":{"file":"transformers/models/clip/tokenization_clip.py","github_url":"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/tokenization_clip.py","description":"CLIP tokenizer for text processing","key_methods":["tokenize","encode"]},"T5Tokenizer":{"file":"transformers/models/t5/tokenization_t5.py","github_url":"https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5.py","description":"T5 tokenizer for text processing","key_methods":["tokenize","encode"]},"T5TokenizerFast":{"file":"transformers/models/t5/tokenization_t5_fast.py","github_url":"https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5_fast.py","description":"Fast T5 tokenizer using Rust backend","key_methods":["tokenize","encode"]},"BertTokenizer":{"file":"transformers/models/bert/tokenization_bert.py","github_url":"https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/tokenization_bert.py","description":"BERT tokenizer for text processing","key_methods":["tokenize","encode"]},"BertModel":{"file":"transformers/models/bert/modeling_bert.py","github_url":"https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py","description":"BERT model for text encoding","key_methods":["forward"]},"CLIPImageProcessor":{"file":"transformers/models/clip/image_processing_clip.py","github_url":"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/image_processing_clip.py","description":"CLIP image processor for preprocessing images","key_methods":["preprocess"]},"Gemma2Model":{"file":"transformers/models/gemma2/modeling_gemma2.py","github_url":"https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma2/modeling_gemma2.py","description":"Gemma 2 language model","key_methods":["forward"]},"GemmaTokenizerFast":{"file":"transformers/models/gemma/tokenization_gemma_fast.py","github_url":"https://github.com/huggingface/transformers/blob/main/src/transformers/models/gemma/tokenization_gemma_fast.py","description":"Fast Gemma tokenizer","key_methods":["tokenize","encode"]},"QwenImagePipeline":{"file":"diffusers/pipelines/qwenimage/pipeline_qwenimage.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/qwenimage/pipeline_qwenimage.py","description":"Qwen-Image text-to-image pipeline with advanced text rendering","key_methods":["__call__"]},"QwenImageTransformer2DModel":{"file":"diffusers/models/transformers/transformer_qwenimage.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformers/transformer_qwenimage.py","description":"Qwen-Image Transformer model with text rendering capabilities","key_methods":["forward","__init__"]},"AutoencoderKLQwenImage":{"file":"diffusers/models/autoencoders/autoencoder_kl_qwenimage.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/autoencoders/autoencoder_kl_qwenimage.py","description":"Qwen-Image specialized VAE for text rendering","key_methods":["encode","decode"]},"Qwen2_5_VLForConditionalGeneration":{"file":"transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py","github_url":"https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py","description":"Qwen 2.5 Vision-Language model for conditional generation","key_methods":["forward"]},"Qwen2Tokenizer":{"file":"transformers/models/qwen2/tokenization_qwen2.py","github_url":"https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2/tokenization_qwen2.py","description":"Qwen2 tokenizer for text processing","key_methods":["tokenize","encode"]},"UniDiffuserModel":{"file":"diffusers/models/unets/unet_unidiffuser.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unets/unet_unidiffuser.py","description":"UniDiffuser unified transformer model for multi-modal generation","key_methods":["forward","__init__"]},"UniDiffuserTextDecoder":{"file":"diffusers/models/transformers/unidiffuser_text_decoder.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/transformers/unidiffuser_text_decoder.py","description":"UniDiffuser text decoder for image-to-text generation","key_methods":["forward"]},"UniDiffuserPipeline":{"file":"diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py","description":"UniDiffuser unified pipeline for text-to-image and image-to-text generation","key_methods":["__call__"]},"PaellaVQModel":{"file":"diffusers/models/vq_model.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/vq_model.py","description":"Paella VQ-VAE model for Stable Cascade","key_methods":["encode","decode"]},"StableCascadeUNet":{"file":"diffusers/models/unets/unet_stable_cascade.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unets/unet_stable_cascade.py","description":"Stable Cascade UNet decoder model","key_methods":["forward","__init__"]},"StableCascadeDecoderPipeline":{"file":"diffusers/pipelines/stable_cascade/pipeline_stable_cascade.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_cascade/pipeline_stable_cascade.py","description":"Stable Cascade decoder pipeline for text-to-image generation","key_methods":["__call__"]},"DDPMWuerstchenScheduler":{"file":"diffusers/schedulers/scheduling_ddpm_wuerstchen.py","github_url":"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm_wuerstchen.py","description":"DDPM scheduler for Würstchen/Stable Cascade","key_methods":["step"]},"CLIPVisionModelWithProjection":{"file":"transformers/models/clip/modeling_clip.py","github_url":"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_clip.py","description":"CLIP vision encoder with projection layer","key_methods":["forward"]},"GPT2Tokenizer":{"file":"transformers/models/gpt2/tokenization_gpt2.py","github_url":"https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/tokenization_gpt2.py","description":"GPT-2 tokenizer for text processing","key_methods":["tokenize","encode"]},"CLIPTokenizerFast":{"file":"transformers/models/clip/tokenization_clip_fast.py","github_url":"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/tokenization_clip_fast.py","description":"Fast CLIP tokenizer using Rust backend","key_methods":["tokenize","encode"]},"GlmModel":{"file":"transformers/models/glm/modeling_glm.py","github_url":"https://github.com/huggingface/transformers/blob/main/src/transformers/models/glm/modeling_glm.py","description":"GLM (General Language Model) for text encoding","key_methods":["forward"]},"PreTrainedTokenizerFast":{"file":"transformers/tokenization_utils_fast.py","github_url":"https://github.com/huggingface/transformers/blob/main/src/transformers/tokenization_utils_fast.py","description":"Fast tokenizer base class using Rust backend","key_methods":["tokenize","encode"]}}