<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://iclr-blogposts.github.io/2026/feed.xml" rel="self" type="application/atom+xml"/><link href="https://iclr-blogposts.github.io/2026/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-07T15:32:47+00:00</updated><id>https://iclr-blogposts.github.io/2026/feed.xml</id><title type="html">ICLR Blogposts 2026</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The Adversarial Conditioning Paradox: Why Attacked Inputs Are More Stable, Not Less</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/adversarial-conditioning-paradox/" rel="alternate" type="text/html" title="The Adversarial Conditioning Paradox: Why Attacked Inputs Are More Stable, Not Less"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/adversarial-conditioning-paradox</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/adversarial-conditioning-paradox/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Adversarial attacks on NLP systems pose a growing security concern. Attacks such as TextFooler <d-cite key="jin2020bert"></d-cite>, BERT-Attack <d-cite key="li2020bert"></d-cite>, and PWWS <d-cite key="ren2019generating"></d-cite> generate inputs that appear semantically similar to clean text yet cause misclassification. These attacks are specifically optimized to minimize embedding distance while maximizing prediction change—making them invisible to similarity-based detection methods.</p> <p>We set out to investigate whether <strong>Jacobian conditioning analysis</strong> could provide an alternative detection signal. The condition number κ of a layer’s Jacobian measures the ratio of maximum to minimum singular values, capturing how much the layer amplifies perturbations in different directions.</p> <div class="l-body"> <div style="background-color: #f0f0f0; padding: 15px; border-left: 4px solid #333; margin: 20px 0;"> <strong>Initial Hypothesis:</strong> Adversarial inputs should exhibit <em>high</em> condition numbers, indicating they occupy ill-conditioned regions where small perturbations cause disproportionately large output changes. </div> </div> <p>This hypothesis seemed natural. Adversarial attacks succeed by finding perturbations that cause large prediction shifts. High conditioning (large κ) would indicate sensitivity to perturbation—exactly what adversarial examples exploit.</p> <p><strong>We found the opposite.</strong></p> <h2 id="the-adversarial-conditioning-paradox">The Adversarial Conditioning Paradox</h2> <p>Across three different attack types—word-level substitution (TextFooler, PWWS) and character-level perturbation (DeepWordBug)—adversarial inputs show systematically <em>lower</em> condition numbers at Layer 1 of BERT compared to clean inputs.</p> <div class="l-body"> <figure> <img src="/2026/assets/img/2026-04-27-adversarial-conditioning-paradox/layer_conditioning.png" alt="Layer-wise condition numbers"/> <figcaption>Figure 1: Average condition numbers across transformer layers for clean vs adversarial inputs. Adversarial examples show consistently lower κ at early layers.</figcaption> </figure> </div> <p>The effect is statistically significant:</p> <ul> <li><strong>TextFooler</strong>: AUC = 0.72, p = 0.001</li> <li><strong>DeepWordBug</strong>: AUC = 0.75, p = 0.001</li> <li><strong>PWWS</strong>: AUC = 0.59, p = 0.29 (directionally consistent)</li> </ul> <p>Meanwhile, cosine distance—the metric attacks explicitly minimize—fails completely (AUC ≈ 0.25).</p> <p>This <strong>adversarial conditioning paradox</strong> demands explanation. Why would adversarial inputs be <em>more</em> numerically stable, not less? And why does this pattern hold across fundamentally different attack strategies?</p> <p>We propose a geometric interpretation: adversarial attacks succeed not by exploiting instability, but by finding <strong>well-conditioned perturbation directions that happen to cross decision boundaries</strong>. Ill-conditioned directions would make the attack optimization unstable—small changes in the perturbation would cause unpredictable output swings, making it difficult to reliably flip predictions. Instead, attacks implicitly select for smooth, stable paths to misclassification.</p> <h2 id="background-and-related-work">Background and Related Work</h2> <h3 id="adversarial-attacks-on-nlp">Adversarial Attacks on NLP</h3> <p>Adversarial attacks on text classifiers seek to find inputs that cause misclassification while preserving semantic content. We study three attack families:</p> <p><strong>TextFooler</strong> <d-cite key="jin2020bert"></d-cite> uses a greedy search that identifies important words via deletion and replaces them with semantically similar alternatives from a counter-fitted embedding space. The attack explicitly constrains substitutions to maintain sentence similarity.</p> <p><strong>PWWS</strong> <d-cite key="ren2019generating"></d-cite> combines word importance ranking with WordNet-based synonym substitution, using probability-weighted saliency to prioritize replacements. Unlike TextFooler, it uses a fixed synonym dictionary rather than embedding-based similarity.</p> <p><strong>DeepWordBug</strong> <d-cite key="gao2018black"></d-cite> operates at the character level, introducing typos, character swaps, and insertions. This attack is geometrically distinct from word-level attacks—it perturbs within the token embedding space rather than substituting between discrete tokens.</p> <h3 id="detection-methods">Detection Methods</h3> <p>Prior work on adversarial detection in NLP includes:</p> <ul> <li>Perplexity-based methods <d-cite key="mozes2021frequency"></d-cite></li> <li>Frequency-based analysis <d-cite key="pruthi2019combating"></d-cite></li> <li>Certified robustness <d-cite key="jia2019certified"></d-cite></li> <li>Ensemble disagreement approaches</li> </ul> <p>These methods operate on <strong>external properties</strong> of inputs. Our approach differs: we analyze <strong>internal geometric properties</strong> of how the model processes inputs, specifically the conditioning of layer-wise Jacobians.</p> <h3 id="jacobian-conditioning">Jacobian Conditioning</h3> <p>The condition number κ of a matrix J is defined as:</p> \[\kappa(J) = \frac{\sigma_{\max}(J)}{\sigma_{\min}(J)}\] <p>where σ_max and σ_min are the maximum and minimum singular values. For the Jacobian of a neural layer, κ captures how uniformly the layer responds to perturbations:</p> <ul> <li>High κ indicates ill-conditioning: some directions are amplified much more than others</li> <li>Low κ indicates well-conditioning: all directions are treated more uniformly</li> </ul> <h2 id="methods">Methods</h2> <h3 id="attack-generation">Attack Generation</h3> <p>We generate adversarial examples on the SST-2 sentiment classification task using:</p> <ol> <li><strong>TextFooler</strong>: Word substitution via embedding similarity</li> <li><strong>PWWS</strong>: WordNet-based synonym replacement</li> <li><strong>DeepWordBug</strong>: Character-level perturbations</li> </ol> <p>All attacks use default parameters from TextAttack <d-cite key="morris2020textattack"></d-cite> library. We generate 1000 successful adversarial examples per attack type, requiring:</p> <ul> <li>Successful label flip</li> <li>Semantic similarity &gt; 0.8 (for word-level attacks)</li> <li>Edit distance &lt; 30 characters (for character-level attacks)</li> </ul> <h3 id="conditioning-analysis">Conditioning Analysis</h3> <p>For each input (clean or adversarial), we extract:</p> <ol> <li> <p><strong>Layer-wise Jacobians</strong>: For transformer layer l with function f_l, we compute: \(J_l = \frac{\partial f_l(x)}{\partial x}\)</p> </li> <li> <p><strong>Condition numbers</strong>: Using randomized SVD for efficiency: \(\kappa_l = \frac{\sigma_{\max}(J_l)}{\sigma_{\min}(J_l) + \epsilon}\) where ε = 1e-10 for numerical stability.</p> </li> <li> <p><strong>Statistics</strong>: We compute κ for layers {1, 3, 6, 9, 12} of BERT-base.</p> </li> </ol> <h3 id="spectral-conditioning-monitor">Spectral Conditioning Monitor</h3> <p>We implement the Spectral Conditioning Monitor (SCM) algorithm for efficient condition number estimation:</p> <div class="l-body"> <figure> <img src="/2026/assets/img/2026-04-27-adversarial-conditioning-paradox/scm_algorithm.png" alt="SCM Algorithm"/> <figcaption>Figure 2: The SCM algorithm efficiently estimates condition numbers using randomized SVD and power iteration.</figcaption> </figure> </div> <h2 id="results">Results</h2> <h3 id="layer-wise-analysis">Layer-wise Analysis</h3> <div class="l-page"> <figure> <img src="/2026/assets/img/2026-04-27-adversarial-conditioning-paradox/kappa_distribution.png" alt="Distribution of condition numbers"/> <figcaption>Figure 3: Distribution of condition numbers at Layer 1 for clean vs adversarial inputs across three attack types.</figcaption> </figure> </div> <p><strong>Key findings:</strong></p> <ol> <li><strong>Layer 1 shows strongest signal</strong>: Adversarial κ consistently lower than clean</li> <li><strong>Effect diminishes with depth</strong>: By Layer 12, distributions overlap substantially</li> <li><strong>Cross-attack consistency</strong>: All three attacks show same directional effect</li> </ol> <div class="l-body"> <table> <thead> <tr> <th>Attack</th> <th>Layer 1 κ (Clean)</th> <th>Layer 1 κ (Adv)</th> <th>p-value</th> <th>AUC</th> </tr> </thead> <tbody> <tr> <td>TextFooler</td> <td>23.45 ± 8.32</td> <td>18.73 ± 6.21</td> <td>0.001</td> <td>0.72</td> </tr> <tr> <td>DeepWordBug</td> <td>24.12 ± 9.15</td> <td>17.89 ± 5.43</td> <td>0.001</td> <td>0.75</td> </tr> <tr> <td>PWWS</td> <td>22.78 ± 7.94</td> <td>20.91 ± 7.12</td> <td>0.29</td> <td>0.59</td> </tr> </tbody> </table> <figcaption>Table 1: Condition number statistics at Layer 1 for clean vs adversarial inputs.</figcaption> </div> <h3 id="attack-specific-patterns">Attack-specific Patterns</h3> <p>Different attacks show distinct conditioning signatures:</p> <div class="l-body"> <figure> <img src="/2026/assets/img/2026-04-27-adversarial-conditioning-paradox/attack_patterns.png" alt="Attack-specific patterns"/> <figcaption>Figure 4: Attack-specific conditioning patterns across layers reveal different perturbation strategies.</figcaption> </figure> </div> <ul> <li><strong>TextFooler</strong>: Smooth decay from Layer 1 to 12</li> <li><strong>DeepWordBug</strong>: Sharp drop at Layer 1, then stabilizes</li> <li><strong>PWWS</strong>: Gradual change, weakest signal</li> </ul> <h3 id="detection-performance">Detection Performance</h3> <p>ROC analysis shows strong detection capability using Layer 1 conditioning alone:</p> <div class="l-body"> <figure> <img src="/2026/assets/img/2026-04-27-adversarial-conditioning-paradox/roc_curves.png" alt="ROC curves"/> <figcaption>Figure 5: ROC curves for adversarial detection using conditioning vs cosine distance.</figcaption> </figure> </div> <p>Remarkably, cosine distance—which attacks explicitly minimize—provides no discriminative signal (AUC ≈ 0.25), while conditioning achieves AUC = 0.72-0.75.</p> <h2 id="discussion">Discussion</h2> <h3 id="geometric-interpretation">Geometric Interpretation</h3> <p>Why do adversarial inputs show <em>lower</em> condition numbers? We propose three complementary explanations:</p> <p><strong>1. Optimization stability:</strong> Ill-conditioned directions would destabilize attack optimization. Small adjustments during the attack search would cause unpredictable output changes, making it difficult to reliably flip predictions. Attacks implicitly select for well-conditioned paths.</p> <p><strong>2. Semantic preservation:</strong> Well-conditioned directions may better preserve semantic content. High-κ directions could correspond to linguistically meaningful variations that attacks must avoid to maintain similarity.</p> <p><strong>3. Decision boundary geometry:</strong> The model’s decision boundaries may be smoother (lower curvature) in well-conditioned regions. Attacks find these smooth crossings rather than sharp, unstable transitions.</p> <h3 id="implications-for-defense">Implications for Defense</h3> <p>Our findings suggest new defense strategies:</p> <ol> <li><strong>Conditioning-based detection:</strong> Monitor Layer 1 conditioning as a real-time detection signal</li> <li><strong>Adversarial training:</strong> Include conditioning regularization to eliminate well-conditioned attack paths</li> <li><strong>Architecture design:</strong> Engineer models with uniform conditioning to reduce attack surface</li> </ol> <p>The paradox also reveals a fundamental trade-off: making models more stable (lower κ) may inadvertently create smoother attack surfaces.</p> <h2 id="conclusion">Conclusion</h2> <p>We document an unexpected phenomenon: adversarial inputs to NLP models exhibit <em>lower</em> Jacobian condition numbers at early layers, contradicting the intuitive hypothesis that attacks exploit unstable regions. This “adversarial conditioning paradox” holds across word-level and character-level attacks, providing a strong detection signal where embedding-based methods fail.</p> <p>Our findings suggest that adversarial attacks succeed not through chaos but through stability—finding well-conditioned directions that smoothly cross decision boundaries. This geometric insight opens new avenues for both understanding and defending against adversarial examples in NLP systems.</p> <p>Future work should investigate:</p> <ul> <li>Whether the paradox extends to other architectures (GPT, RoBERTa)</li> <li>How conditioning evolves during adversarial training</li> <li>Whether attacks can be modified to maintain high conditioning while preserving effectiveness</li> </ul> <p>The code and data for reproducing our experiments are available at [anonymous repository link].</p> <h2 id="acknowledgments">Acknowledgments</h2> <p>We thank the anonymous reviewers for their valuable feedback. This work was supported by [anonymized funding source].</p>]]></content><author><name>Anonymous Authors</name></author><category term="adversarial-ml"/><category term="nlp-security"/><summary type="html"><![CDATA[Adversarial inputs exhibit systematically lower Jacobian condition numbers at early transformer layers—the opposite of our initial hypothesis that attacks exploit unstable regions. This paradox reveals that adversarial attacks succeed by finding well-conditioned directions that cross decision boundaries.]]></summary></entry><entry><title type="html">(LLM-)Judges on autopilot</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/auto-calibration/" rel="alternate" type="text/html" title="(LLM-)Judges on autopilot"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/auto-calibration</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/auto-calibration/"><![CDATA[<div class="row mt-4"> <div class="col text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-auto-calibration/judge_funny-480.webp 480w,/2026/assets/img/2026-04-27-auto-calibration/judge_funny-800.webp 800w,/2026/assets/img/2026-04-27-auto-calibration/judge_funny-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-auto-calibration/judge_funny.png" class="img-fluid d-block mx-auto" width="100%" height="auto" style=" max-width: 90%; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="what-is-an-llm-as-a-judge">What is an LLM-as-a-judge?</h2> <p>Modern evaluation pipelines increasingly rely on LLM-as-a-judge <d-cite key="zheng2023llmjudge"></d-cite> to assess the quality of AI-generated responses. This approach uses an LLM to act as a judge, assessing the quality of the generated outputs against specific criteria. The judge can be either a different model or the same one that generated the response. An example prompt for the <i>answer relevance</i> metric is shown below <d-cite key="opik_answer_relevance"></d-cite>.</p> <blockquote> <p>You are an expert in NLP evaluation metrics, specifically trained to assess the relevance of answers. Your role is to evaluate the relevance of a given answer based on the user’s input. Follow these steps to complete the evaluation. […]</p> </blockquote> <p>However, this approach is inherently self-referential: an LLM produces the answer and another LLM evaluates it. What assures us that the judge is trustworthy? The primary way to establish trust is by verifying its alignment with known cases. For example, one could ask a pool of experts to rate a small subset of responses; if the LLM-as-a-judge and the experts agree on those evaluations, it’s reasonable to assume the automated judge will also perform reliably on the remaining examples.</p> <p>Typically, adjusting the LLM-as-a-judge prompt to align with human feedback is cumbersome and largely manual: ground-truth collection requires human experts, and prompt calibration requires engineering expertise. This reliance on human judgment makes the process less reproducible and prone to inconsistency.</p> <p>In this article, we investigate how to automate LLM-as-a-judge prompt calibration. Recent work introduced the use of a “cheatsheet”, a set of reference instructions that guides the LLM’s reasoning. Methods such as Dynamic Cheatsheet (DC) <d-cite key="suzgun2025dynamiccheatsheet"></d-cite> and Agentic Context Engineering (ACE) <d-cite key="zhang2025agenticcontextengineering"></d-cite> iteratively refine the cheatsheet based on observed examples, treating prompt optimization as an online learning problem.</p> <p>However, when processing samples one at a time and updating the cheatsheet sequentially, important details are lost and performance degrades over time—a phenomenon known as <i>context collapse</i> <d-cite key="zhang2025agenticcontextengineering"></d-cite>. We observe this issue in our experimental evaluation, and address it with a batch-based approach inspired by classical machine learning training pipelines. Before diving into the methods, let’s first discuss what we’re evaluating and the data used.</p> <h2 id="what-are-we-evaluating">What are we evaluating?</h2> <p>Our reference use case is assessing <i>answer relevance</i> for responses generated by an internal AI assistant for network troubleshooting. Answer relevance is a metric that evaluates how pertinent an AI system’s response is to the user’s query <d-cite key="opik_answer_relevance"></d-cite>. To measure answer relevance, we provide the LLM-as-a-judge with an input query and the corresponding output response. The judge produces a score between 0 and 1, along with a justification for its rating. Example (anonymized):</p> <table style="border: none; border-collapse: collapse;"> <tr style="border: none;"> <td style="border: none; vertical-align: top; padding: 8px;"><strong>user</strong></td> <td style="border: none; padding: 8px;">"Show me the latest issues"</td> </tr> <tr style="border: none;"> <td style="border: none; vertical-align: top; padding: 8px;"><strong>system</strong></td> <td style="border: none; padding: 8px;">"Fetching issues from the past 24 hours…We detected 5 alerts in the last 24 hours, 5 of which are major. The latest major alert was 'Packet Loss Internet - Alert - XYZ' applied on the target 'xyz'. The alert started at Dec 1, 2025 05:36:00 and lasted for 145 seconds impacting 1 test and 1 agent across the following 1 location: XYZ. The alert triggered because the packet loss, 15.0%, was outside of the threshold of 3%. We also detected 4 application outages. The latest app outage started at November 30, 2025 21:15:02 &amp; lasted for 180 seconds impacting 5 servers &amp; 2 locations. The outage occurred in these locations: London, England, UK; New York, New York, US."</td> </tr> <tr style="border: none;"> <td style="border: none; vertical-align: top; padding: 8px;"><strong>score</strong></td> <td style="border: none; padding: 8px;">1.0</td> </tr> <tr style="border: none;"> <td style="border: none; vertical-align: top; padding: 8px;"><strong>reason</strong></td> <td style="border: none; padding: 8px;">"The answer addresses the question exhaustively and clearly."</td> </tr> </table> <blockquote> <p><b><i>Note:</i></b> Unlike traditional metrics with clear mathematical definitions, this score represents a qualitative judgment guided by high-level criteria.</p> </blockquote> <h2 id="what-about-the-data">What about the data?</h2> <p>Let’s establish some notation that we’ll use throughout this article. Given a dataset of input/output pairs with associated scores and reasons, for a sample $i$ we denote:</p> <table> <thead> <tr> <th>Symbol</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>$x_i$</td> <td>User input query</td> </tr> <tr> <td>$y_i$</td> <td>System output</td> </tr> <tr> <td>$s_i$</td> <td>LLM-as-a-judge generated score</td> </tr> <tr> <td>$r_i$</td> <td>LLM-as-a-judge generated reason</td> </tr> <tr> <td>$\tilde{s}_i$</td> <td>Ground-truth score (human-annotated)</td> </tr> <tr> <td>$\tilde{r}_i$</td> <td>Ground-truth reason (human-annotated)</td> </tr> </tbody> </table> <p>Later in this article, we will denote the cheatsheet at iteration $t$ as $M_t$ (where $M$ stands for memory). In addition, when discussing batch-based approaches to prompt calibration, we will use $B$ to denote the set of sample indices belonging to a batch.</p> <h3 id="the-challenge-of-high-quality-ground-truth">The challenge of high-quality ground truth</h3> <p>To assess the performance of LLM-as-a-judge metrics, we need ground-truth scores and reasons. Obtaining reliable scores is far from trivial. For our purposes, we relied on human annotations: experts followed rating guidelines (e.g., “subtract 0.1 to 0.3 for unnecessary verbosity or repetition”) to produce scores with quantitative justifications. In other words, for each input/output pair $(x_i, y_i)$, a human annotator provided a ground-truth score and reason, which we denote as $\tilde{s}_i$ and $\tilde{r}_i$.</p> <p>However, upon careful review, we found that many scores didn’t align with expected ratings. Subjectivity is inherent when evaluating LLM-generated content. To reduce this bias, we had multiple annotators perform ratings independently and retained only those records where they showed strong agreement.</p> <p>Another challenge is sample diversity. LLMs can produce multiple valid responses for a given input, making random train/test splits potentially problematic. To better represent a realistic production scenario, we split the training and test sets based on a temporal cutoff: samples before a certain date were used for training, and those after for testing.</p> <h2 id="who-are-you-to-judge-me">Who are you to judge me?</h2> <p>Now that we have established our evaluation metric and collected ground truth annotations, the key question becomes: how do we calibrate the judge prompt to align its evaluations with human judgments? In this section, we explore different approaches to calibration.</p> <h3 id="our-first-attempt-manual-calibration">Our first attempt: manual calibration</h3> <p>Traditional calibration relies on manually crafted prompts. The workflow depends on human annotation samples and uses techniques such as few-shot prompting to address problematic queries. This process is typically iterative:</p> <ol> <li>Collect ground-truth scores from human annotators</li> <li>Compute alignment between human annotations and LLM-as-a-judge metrics</li> <li>Identify discordant samples where human and model scores diverge</li> <li>Refine the judge prompt to minimize score discrepancy</li> </ol> <p>Engineering teams perform multiple rounds of manual calibration over time to ensure that human annotators and the LLM-as-a-judge metrics converge. Beyond being time-consuming and error-prone, this approach raises several technical concerns: (a) step 4 typically involves adding few-shot examples, which results in extremely long and over-engineered prompts; (b) ensuring that existing functionalities are preserved at each calibration round requires careful selection and analysis of reference samples. This process requires an expert engineer and can take several days. These limitations motivated us to explore more automated approaches.</p> <h3 id="take-your-cheatsheet-out-the-game-begins">Take your cheatsheet out, the game begins</h3> <p>Dynamic Cheatsheet Cumulative (DC-Cu) <d-cite key="suzgun2025dynamiccheatsheet"></d-cite> was introduced in April 2025 by researchers at <i>Stanford University</i> and <i>Together AI</i>. The key idea is to treat prompt optimization as an online learning problem where an LLM processes samples sequentially, updating the cheatsheet after each one.</p> <p>DC-Cu distinguishes between two roles: a <i>Generator</i> LLM and a memory <i>Curator</i> LLM. The Generator takes the input query $x_i$ and the current cheatsheet $M_i$ to produce an output $y_i$:</p> \[y_i = \text{Generator}(x_i, M_i)\] <p>The Curator then evaluates the output and refines the cheatsheet, keeping only the most useful and generalizable strategies:</p> \[M_{i+1} = \text{Curator}(M_i, x_i, y_i)\] <p>Since no ground truth is available, the Curator itself judges response quality. In practice, this self-verification approach is achievable for math questions and puzzles where the correctness of the solution can be verified using tools such as a calculator or code-execution environment.</p> <blockquote> <p><b><i>Note:</i></b> Unlike math questions and puzzles, LLM-as-a-judge evaluations are not self-verifiable. There is no objective tool to verify whether a relevance score is correct. Therefore, we need ground-truth human annotations.</p> </blockquote> <p>We adapted DC-Cu for supervised auto-calibration by providing the Memory Curator with ground truth scores $\tilde{s}_i$ and reasoning $\tilde{r}_i$ alongside the input/output pairs $(x_i, y_i)$ (see Fig. 1). We also modified the prompts to ensure these human annotations guide the cheatsheet generation process.</p> <div class="row mt-4"> <div class="col text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-auto-calibration/2025_12_01_DC.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-auto-calibration/2025_12_01_DC.svg" class="img-fluid d-block mx-auto" width="100%" height="auto" style=" max-width: 90%; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-left"> Fig. 1: Overview of the DC‑Cu method adapted to Judge auto‑calibration. Input–output pairs $(x_i, y_i)$ are retrieved from the database and evaluated by the Judge, which assigns a relevance score and reason $(s_i, r_i)$. The predicted score–reason pair is concatenated with the original input–output pair and the ground‑truth score–reason pair $(\tilde{s}_i, \tilde{r}_i)$, and passed to the Curator, which updates the cheatsheet $M_i$ to align the Judge's scoring behavior with the ground truth. </div> <p>In our adapted DC-Cu framework, the process alternates between two phases. In the <strong>Judgment Phase</strong>, the Judge evaluates the input/output pair using the current cheatsheet $M_i$ to produce a score and reasoning:</p> \[s_i, r_i = \text{Judge}(x_i, y_i, M_i)\] <p>In the <strong>Curation Phase</strong>, the Curator updates the cheatsheet by comparing the Judge’s predictions with the ground truth annotations ($\tilde{s}_i$, $\tilde{r}_i$):</p> \[M_{i+1} = \text{Curator}(M_i, x_i, y_i, \tilde{s}_i, \tilde{r}_i, s_i, r_i)\] <p>The Curator uses the discrepancy between predicted and ground truth values to refine the evaluation criteria stored in the cheatsheet.</p> <h3 id="ace-up-your-sleeve-agentic-context-engineering">ACE Up Your Sleeve: Agentic Context Engineering</h3> <p>Agentic Context Engineering (ACE) <d-cite key="zhang2025agenticcontextengineering"></d-cite> builds upon DC-Cu by further refining its architectural structure. In ACE, the Memory Curator role is decomposed into two specialized components: a <i>Reflector</i>, which synthesizes insights from both correct and erroneous outputs, and a <i>Curator</i>, which integrates these insights into context updates. This separation prevents overburdening a single agent with the dual responsibilities of quality assessment and cheatsheet evolution. Additionally, ACE introduces a grow-and-refine mechanism that implements incremental updates to avoid full cheatsheet rewrites, pruning redundant entries through semantic analysis to ensure the cheatsheet remains both comprehensive and concise.</p> <p>Inspired by ACE, we developed a customized implementation for the LLM-as-a-Judge supervised use case by extending DC-Cu with three key enhancements: (i) incorporating ground-truth scores and reasons during training, (ii) introducing a Reflector LLM, and (iii) enabling batching and epoch-based training. Further details are provided in the following section.</p> <h3 id="any-baci-please">Any BACI, please?</h3> <p>In this section, we introduce BACI (Batching Agentic Context Iteratively), our proposed strategy for automated judge calibration. The overall architecture is illustrated in Fig. 2.</p> <div class="row mt-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-auto-calibration/2025_12_01_BACI.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-auto-calibration/2025_12_01_BACI.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption text-left"> Fig. 2: Overview of the BACI method. A batch $B$ of annotated samples is retrieved from the database, where each sample contains an input–output pair $(x_i, y_i)$ and the corresponding ground‑truth score–reason $(\tilde{s}_i, \tilde{r}_i)$. The Judge uses the cheatsheet rules to evaluate each input–output pair $(x_i, y_i)$ individually, producing a predicted score and reason $(s_i, r_i)$. Each prediction is then concatenated with its input-output pair and ground-truth, and the batch is forwarded to the Reflector. The Reflector compares ground-truth and predicted scores along with their reasons, identifying relationships among samples to extract insights $I_t$ about the Judge's errors. The Curator incorporates these insights into the cheatsheet $M_t$, aligning the Judge's scores with the ground truth. This process repeats for $k$ epochs to progressively optimize the cheatsheet. </div> <p>BACI incorporates <strong>batching</strong> as a core component to <strong>iteratively</strong> optimize the <strong>agentic context</strong>. During training, the Judge individually evaluates each sample $i$ within a batch $B$ using the current cheatsheet $M_t$ (where $t$ denotes the iteration number):</p> \[s_i, r_i = \text{Judge}(x_i, y_i, M_t) \quad \forall i \in B\] <p>The batch is then passed to the Reflector, which extracts insights $I_t$ by comparing the Judge’s predictions with the human-provided ground truth across all samples in the batch:</p> \[I_{t} = \text{Reflector}(\{(x_i, y_i, \tilde{s}_i, \tilde{r}_i, s_i, r_i)\}_{i \in B})\] <p>These insights are fed to the Curator, which updates the cheatsheet accordingly:</p> \[M_{t+1} = \text{Curator}(M_t, I_t)\] <p>In subsequent iterations, the Judge uses the updated cheatsheet to generate new scores and reasons. This iterative process is repeated for all batches, with the cheatsheet being continuously refined at each step. The entire cycle is run for $k$ epochs, like a standard machine learning pipeline but employing gradient-free optimization.</p> <p>At test time, we provide the Judge with the final version of the cheatsheet, refined during training. The Judge uses this cheatsheet to evaluate new, unseen samples by generating scores and reasons based on the accumulated knowledge. The final cheatsheet serves as a distilled summary of the most relevant evaluation patterns learned during training, guiding the Judge’s evaluations in the test phase.</p> <p>Compared to ACE, our Curator is responsible for both adding new evaluation instructions and de-duplicating entries. This design makes our method more lightweight than the original ACE approach, which maintains embeddings for each instruction in the cheatsheet.</p> <p>In BACI, the Reflector extracts insights, and the Curator is instructed to perform updates by adding instructions (i) only if they are sufficiently different from existing ones, (ii) refining entries that lack important aspects, and (iii) discarding items that are similar to those already present. The combination of batching and the Reflector-Curator architecture helps us avoid context collapse and redundancy of instructions. In particular, the batch size plays a crucial role in this process — as we will demonstrate in the following section.</p> <blockquote> <p><b><i>Note:</i></b> we use Claude Sonnet 4.5, which has a nearly unlimited context window (200K base, can be extended up to 1 million tokens) for the LLM Judge, Reflector, and Curator. When using a model with a smaller context window, a trade-off in the batch size might be needed.</p> </blockquote> <h2 id="what-did-we-learn">What did we learn?</h2> <h3 id="starting-with-the-basics-when-less-is-more">Starting with the basics: when less is more</h3> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-auto-calibration/score_comparison_other_methods.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <div class="caption text-left"> Fig. 3: Score comparison on test set for the baseline methods. The score for each sample has been computed as the average across 10 predictions. The standard deviaton for each score can be seen by hovering over the point. </div> <p>We compared the score distributions across our test set for different baselines (see Fig. 3). We first evaluated what happens when the Judge receives no special instructions, using only the basic definition of Answer Relevance, with no training, evolution, or use case specific guidelines. Surprisingly, on average, this “empty cheatsheet” approach performs on par with our manually calibrated prompt. This is remarkable given that an empty cheatsheet contains no domain-specific details.</p> <p>Even more unexpectedly, the DC-Cu method performs worse than the previous baselines. As noted in <d-cite key="zhang2025agenticcontextengineering"></d-cite>, the main issue is context collapse: over time, the LLM Curator tends to generate shorter, less informative summaries, leading to a sharp decline in performance.</p> <p>These observations lead to our first key insight:</p> <blockquote> <p>It is better to provide no instructions in the cheatsheet than to include suboptimal instructions that may cause confusion or conflicting behavior.</p> </blockquote> <p>This also explains why the carefully calibrated prompt did not outperform the baselines on the test set, despite meticulous fine-tuning, adjusted dataset scoring, and multiple few-shot examples.</p> <h3 id="learning-in-batches-the-missing-ingredient">Learning in batches: the missing ingredient</h3> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-auto-calibration/score_comparison_BACI.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <div class="caption text-left"> Fig. 4: Score comparison on test set for BACI with different batch sizes and DC-Cu. The score for each sample has been computed as the average across 10 predictions. The standard deviaton for each score can be seen by hovering over the point. </div> <p>We evaluated two BACI configurations: BACI-1 uses a batch size of one with a single training epoch, while BACI-32 uses a batch size of 32 and trains for five epochs. Figure 4 compares their performance against ground truth and DC-Cu. Here are the key observations:</p> <ul> <li> <p>BACI-1 significantly outperforms DC-Cu. This improvement stems primarily from the separation of concerns between the Curator and Reflector components. As noted in the ACE work, this architectural separation — where insight extraction (Reflector) and cheatsheet updating (Curator) are distinct processes — helps mitigate context collapse across iterations.</p> </li> <li> <p>BACI-32 outperforms BACI-1. The larger batch size and multiple training epochs enable the system to observe the entire training dataset repeatedly, refining the cheatsheet iteratively. Crucially, processing samples in larger batches allows the Reflector to identify more generalizable patterns rather than overfitting individual examples.</p> </li> </ul> <h3 id="the-final-verdict">The final verdict</h3> <p>Bringing it all together: Table 1 summarizes all experimental results, showing average Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) <d-cite key="bishop2006pattern"></d-cite> across 10 test runs. BACI-32 (bold) clearly outperforms all competing methods on our dataset.</p> <div align="center"> <table> <thead> <tr> <th>Method</th> <th>MAE</th> <th>RMSE</th> </tr> </thead> <tbody> <tr> <td>Empty Cheatsheet</td> <td>0.134 ± 0.002</td> <td>0.201 ± 0.004</td> </tr> <tr> <td>Manually calibrated</td> <td>0.139 ± 0.010</td> <td>0.219 ± 0.016</td> </tr> <tr> <td>DC-Cu</td> <td>0.272 ± 0.006</td> <td>0.308 ± 0.007</td> </tr> <tr> <td>BACI-1</td> <td>0.147 ± 0.003</td> <td>0.212 ± 0.002</td> </tr> <tr> <td>BACI-32</td> <td><strong>0.111</strong> ± 0.003</td> <td><strong>0.198</strong> ± 0.004</td> </tr> </tbody> </table> </div> <div class="caption text-left"> Table 1: Experimental results comparing all methods. MAE and RMSE averaged across 10 test runs. </div> <p>Statistical validation using the Wilcoxon signed-rank test <d-cite key="wilcoxon1945individual"></d-cite> confirms: (1) manual calibration provided no significant improvement over an empty cheatsheet ($p = 0.62$); (2) BACI-32 significantly outperforms both BACI-1 and the empty cheatsheet ($p &lt; 0.01$).</p> <h2 id="key-takeaways">Key Takeaways</h2> <p><strong>Context collapse is real.</strong> Our empirical analysis confirms that iterative, sample-by-sample approaches to prompt calibration suffer from a critical issue: context collapse. The solution lies in applying proper machine learning strategies—processing samples in batches and tuning hyperparameters such as batch size and number of epochs.</p> <p><strong>Manual calibration faces similar challenges.</strong> Even manually crafted prompts can suffer from analogous issues, as they’re typically adjusted iteratively on a static set of examples. Moreover, human bandwidth limits the number of samples that can be examined, making the process tedious, error-prone, and nearly impossible to scale.</p> <p><strong>Sometimes, less is more.</strong> Perhaps our most surprising finding: the strong performance of the empty cheatsheet baseline. This serves as a clear warning that wrong guidance can be worse than no guidance at all. When a simple solution works, there’s no need to overcomplicate it.</p> <p><strong>Data quality matters — a lot.</strong> The quality of training data is just as important as the calibration method itself. While this principle applies to all machine learning, it is especially critical for generative AI: we cannot expect an LLM to generate meaningful insights from inconsistent or low-quality input data. In our work, we observed a substantial subjectivity bias in human annotations. Data cleaning was crucial and required time and resources. Despite the impressive capabilities of modern LLMs, human judgment remains indispensable—at least for now, AI cannot fully replace expert reviewers.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[How do you evaluate Large Language Model (LLM)-based systems in production at scale? Most teams turn to an LLM-as-a-judge: an approach that grasps the nuances of natural language where classical metrics fall short. But these judge models have their own “will”: sometimes they follow instructions precisely, sometimes they don't. To address this inconsistency, the judge prompt is calibrated to align with known, trusted cases. The problem? Manual calibration is time-consuming and error-prone. In this blog post, we explore auto-calibration techniques inspired by recent prompt-optimization research. We tackle context collapse by iteratively processing data in batches, similarly to a machine learning training pipeline. Along the way, we share some surprising findings about what works and what doesn't—including cases where simpler approaches outperform more sophisticated ones.]]></summary></entry><entry><title type="html">The 99% Success Paradox: When Near-Perfect Retrieval Equals Random Selection</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/bits-over-random/" rel="alternate" type="text/html" title="The 99% Success Paradox: When Near-Perfect Retrieval Equals Random Selection"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/bits-over-random</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/bits-over-random/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>For most of the history of information retrieval (IR), search results were designed for human consumers who could scan, filter, and discard irrelevant content on their own. This shaped retrieval systems to optimize for finding and ranking more relevant documents, but not for keeping results clean and minimal, as the human was the final filter.</p> <p><strong>Retrieval-augmented generation (RAG)</strong> and tool-using agents flip these assumptions. Now the consumer is often an LLM, not a person, and the model does not skim. In practice, introducing excessive or irrelevant context into the input can dilute the model’s ability to identify and focus on the most critical information. When you pass retrieved documents to an LLM:</p> <ul> <li>It can’t ignore irrelevant results. Every irrelevant chunk dilutes the model’s attention.</li> <li>Noise has a cost. Extra chunks cost tokens, latency, and computation. They also increase the odds that irrelevant or misleading content pulls attention away from what actually matters.</li> </ul> <h2 id="the-million-token-trap">The Million-Token Trap</h2> <p>You might be thinking: <em>“But modern LLMs have million-token context windows. Why care?”</em></p> <p>The real question isn’t whether a model can fit more context, but whether more context is actually helpful. Beyond a certain point, adding retrieved material (and the accompanying noise) can actively increase computational cost and degrade the quality of output.</p> <p>In our 20 Newsgroups classification case study, we increased the retrieval depth <strong>K</strong> from 10 to 100 items. This caused LLM accuracy to drop from 66% to 50%, even though the success metric (<strong>Success@K:</strong> the percentage of queries returning at least one relevant item) remained close to 100%. In other words, more retrieved content led to worse results, not better.</p> <p>This problem is especially severe for agentic systems that use tool-based retrieval, because context quality directly affects downstream decisions. A chatbot might give you a mediocre answer, however, an autonomous agent might call the wrong API, delete the wrong file, or execute the wrong command.</p> <p>We need a measure that asks: <em>“Given that I’m retrieving K items and my LLM will consume all of them, how much <strong>selective signal</strong> am I actually getting?”</em></p> <p>That’s what Bits-over-Random (BoR) measures. The rest of this post explains how.</p> <h2 id="what-traditional-metrics-miss">What Traditional Metrics Miss</h2> <p>Recall rewards finding more relevant documents, but is blind to how many irrelevant items you had to pull into the context window to get them. Over-retrieval is actually rewarded. As Manning et al.<d-cite key="manning2008introduction"></d-cite> note, “recall is a non-decreasing function of the number of documents retrieved”. Yet the choice of retrieval depth K is often an empirical, application-dependent choice <d-cite key="webber2010similarity"></d-cite>.</p> <p>Precision measures the relevance of retrieved results and helps limit excessive retrieval. However, it fails to account for the inherent difficulty of the retrieval task. For instance, achieving a 10% precision means something different if the corpus contains 10 relevant items out of 100 versus 10 relevant items out of 10,000. Same precision, very different selectivity.</p> <p>Ranking metrics (nDCG, RBP, MAP, ERR) penalize burying relevant items, but they do not penalize the presence of irrelevant items when the relevant item is also ranked highly. If you retrieve 100 items and the relevant one is at rank 1, nDCG can be perfect. Yet, RAG systems typically concatenate the top-K results into a single prompt. The LLM still has to read the other 99 items. Rankers optimize ordering, not volume. They don’t reduce the token cost of stuffing <strong>K</strong> documents into the context.</p> <p>In practice, teams end up juggling recall, precision, and ranking metrics. Each captures a different slice of behavior but none reflects the whole picture. There is no single framework that simultaneously accounts for how many items you retrieve, how big the corpus is, and how many items in the corpus are actually relevant to the query.</p> <h2 id="the-librarian-problem">The Librarian Problem</h2> <p>Consider a library of \(N = 1{,}000\) books, with \(R_q = 10\) books relevant to your query. Two librarians respond:</p> <ul> <li><strong>Librarian A</strong> retrieves \(K = 20\) books, 6 of which are relevant (precision 30%, recall 60%, F1 40%).</li> <li><strong>Librarian B</strong> retrieves \(K = 12\) books, 4 of which are relevant (precision 33%, recall 40%, F1 36%).</li> </ul> <p>Traditional IR metrics tend to favor Librarian A (higher recall and F1, similar precision). But Librarian A handed you 14 irrelevant books, versus B’s 8. If the librarians are retrievers or tools in an agent workflow and the consumer is an LLM, it must read everything it was given. Those 6 extra unhelpful books retrieved by Librarian A over Librarian B cost tokens, add noise, and waste computational resources.</p> <h2 id="the-new-baseline-random-chance">The New Baseline: Random Chance</h2> <p>And here’s the deeper question: <em>Beyond comparing A and B, is either of them an objectively skillful librarian? What is the baseline?</em></p> <p>If we compare each librarian to a random baseline (<em>“what if I picked K books uniformly at random?”</em>), we can ask which one is actually more selective than chance. Plugging these numbers into the chance-corrected formulas we introduce below shows that Librarian B is more selective than A. For an LLM consuming a fixed-size bundle of text, that selectivity per token is what matters.</p> <p>This is the key insight: every retrieval problem has a built-in baseline. If you picked <strong>K</strong> items completely at random, you’d still sometimes get lucky and grab something relevant, especially if relevant items are common.</p> <p>That random success rate is your floor. It tells you how much of your <em>“success”</em> is just dumb luck. Bits-over-Random (BoR) measures how far above random success you’ve climbed.</p> <p>In today’s RAG, agentic, and LLM workflows, we care less about who retrieved the most documents and more about who delivered the most signal with the least noise. By comparing a chosen success metric to random chance, BoR measures true selectivity: how much better is our retrieval bundle than random selection?</p> <p>Let’s break down how it works, step by step.</p> <h2 id="the-math">The Math</h2> <p>Evaluating a retriever shouldn’t require juggling incompatible metrics. To make sense of how well a system is actually performing, we need a baseline. Not just any baseline, but the most honest one possible: pure randomness. The framework below walks through a simple, quantitative way to express <em>“how much better than random”</em> your retrieval system really is.</p> <p>By measuring observed success, computing the expected success of random guessing, and comparing the two on a logarithmic scale, we end up with a clean, intuitive metric: <strong>Bits-over-Random (BoR)</strong>. This gives retrieval performance a natural, information-theoretic interpretation, each bit representing one doubling in effectiveness over chance.</p> <h2 id="the-quick-reference-version">The Quick-Reference Version</h2> <p>Here’s everything you need to remember:</p> <table> <thead> <tr> <th style="text-align: left">Symbol</th> <th style="text-align: left">Meaning</th> <th style="text-align: left">Example</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">\(N\)</td> <td style="text-align: left">Total items in corpus. Unit must be defined (e.g., documents, passages)</td> <td style="text-align: left">10,000 passages or 700 documents</td> </tr> <tr> <td style="text-align: left">\(K\)</td> <td style="text-align: left">How many items you retrieve per query (top-K)</td> <td style="text-align: left">\(K=10\) or \(K=100\)</td> </tr> <tr> <td style="text-align: left">\(R_q\)</td> <td style="text-align: left">Relevant items in the corpus for a certain query q</td> <td style="text-align: left">\(R_q=1\) (sparse) or \(R_q=20\) (many)</td> </tr> <tr> <td style="text-align: left">\(\bar{R}_q\)</td> <td style="text-align: left">Average relevant items in the corpus per query</td> <td style="text-align: left">≈1.1 on SciFact, ≈572 on 20 Newsgroups</td> </tr> <tr> <td style="text-align: left">\(P_{obs}(K)\)</td> <td style="text-align: left">Your observed success rate at K (Note: any success rate can be used here.)</td> <td style="text-align: left">60% of queries succeed</td> </tr> <tr> <td style="text-align: left">\(P_{rand}(K)\)</td> <td style="text-align: left">Random-chance success at K</td> <td style="text-align: left">What luck would give you</td> </tr> <tr> <td style="text-align: left">\(\lambda\)</td> <td style="text-align: left">Heuristic: expected random hits = \(K \cdot \bar{R}_q / N\)</td> <td style="text-align: left">\(\lambda\) in the 3–5 range signals collapse</td> </tr> </tbody> </table> <h3 id="step-1-measure-your-success-rate">Step 1. Measure Your Success Rate</h3> <p>First, pick a success condition. For most RAG systems, the natural rule is: <em>“Did I get at least one relevant item in my top-K results?”</em></p> <p>This is called <strong>Success@K</strong> (or coverage). For a batch of queries:</p> \[P_{\text{obs}}(K) = \frac{\text{number of queries with } \geq \text{ 1 relevant result in top-}K}{\text{total queries}}\] <p><strong>Note:</strong> The threshold doesn’t have to be 1. You can require at least m relevant documents if your system needs multiple pieces of evidence, for example, “at least 3 supporting passages.”</p> <p>If you retrieved K=10 items for 100 queries, and 60 queries got at least one relevant hit, then \(P_{\text{obs}}(10) = 60 / 100 = 0.60\).</p> <h3 id="step-2-calculate-the-random-baseline">Step 2. Calculate the Random Baseline</h3> <p>What if you picked <strong>K</strong> items completely at random? That’s your baseline.</p> <p>For a query where \(R_q\) items in the corpus are relevant, and the corpus has <strong>N</strong> total items, the hypergeometric distribution tells you the probability of randomly hitting at least one relevant item when picking <strong>K</strong> items:</p> <p>The probability of picking no relevant items in <strong>K</strong> picks is:</p> \[P_{\text{none}} = \frac{\binom{N-R_q}{K}}{\binom{N}{K}}\] <p>So, the probability of picking at least one relevant item is:</p> \[P_{\text{rand}} = 1 - P_{\text{none}} = 1 - \frac{\binom{N-R_q}{K}}{\binom{N}{K}}\] <p><strong>Special case:</strong> If every query has exactly one relevant item (\(R_q = 1\)), this simplifies to:</p> \[P_{\text{rand}}(K) = \frac{K}{N}\] <p>For example:</p> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>\(N\)</td> <td>10,000</td> </tr> <tr> <td>\(R_q\)</td> <td>10</td> </tr> <tr> <td>\(K\)</td> <td>20</td> </tr> </tbody> </table> \[P_{\text{rand}} = 1 - \frac{\binom{9990}{20}}{\binom{10000}{20}} \approx 0.02\] <p>This means random selection works <strong>~2%</strong> of the time.</p> <p>Because we evaluate over many queries, we average these random baselines:</p> \[\overline{P}_{\text{rand}}(K) = \text{average random success across all queries}\] \[\overline{P}_{\text{rand}}(K) = \frac{1}{|Q|} \sum\nolimits_{q} P_{\text{rand}}(K; R_q)\] <h3 id="step-3-enrichment-factor-how-many-times-are-we-better-than-random-chance">Step 3: Enrichment factor: how many times are we better than random chance?</h3> <p><strong>Enrichment Factor (EF)</strong> is defined as</p> \[\text{EF} = \frac{P_{\text{obs}}}{P_{\text{rand}}}\] <p>For a batch of queries, we use the averaged random baseline:</p> \[\text{EF}(K) = \frac{P_{\text{obs}}(K)}{\overline{P}_{\text{rand}}(K)}\] <p>An EF of 5 means you succeed 5× more often than random selection. An EF of 100 means you are 100× better. This formulation is consistent with enrichment metrics used in drug discovery screening <d-cite key="truchon2007evaluating"></d-cite>.</p> <h3 id="step-4-bits-over-random-bor-log-scale-conversion-of-ef">Step 4: Bits-over-Random (BoR): Log Scale conversion of EF</h3> \[\text{BoR} = \log_2(\text{EF}) = \log_2\left(\frac{P_{\text{obs}}}{P_{\text{rand}}}\right)\] <p>And similarly for averaging:</p> \[\text{BoR}(K) = \log_2(\text{EF}) = \log_2\left(\frac{P_{\text{obs}}(K)}{\overline{P}_{\text{rand}}(K)}\right)\] <p>Why \(\log_2\)? Bits are how information theory counts halvings, the same reason why binary search uses powers of 2. Each bit represents one halving of the search space. <strong>BoR = 10</strong> means <strong>10 halvings → 1,024× reduction</strong>.</p> <ul> <li><strong>BoR = 0</strong> → You’re no better than random</li> <li><strong>BoR = 1</strong> → <strong>2×</strong> better than random</li> <li><strong>BoR = 3</strong> → <strong>8×</strong> better than random</li> <li><strong>BoR = 10</strong> → <strong>1,024×</strong> better than random</li> </ul> <p>Each bit also represents a doubling of selectivity. Our definition follows.</p> <p><strong>Selectivity (n.):</strong> The ability of a retrieval system to surface relevant items while excluding irrelevant ones, measured relative to random chance. A system with high selectivity finds needles without bringing along the haystack.</p> <h2 id="a-concrete-example">A Concrete Example</h2> <p>Let’s assume you have 10,000 documents. Each query has exactly ten relevant documents (\(R_q = 1\)). <strong>Note:</strong> Many standard benchmarks such as MS MARCO have \(R_q ≈ 1\) on average, even sparser than this example.</p> <p>You are testing two different retriever systems against the same dataset:</p> <table> <thead> <tr> <th>Metric</th> <th>System A (K=20, 60% success)</th> <th>System B (K=100, 70% success)</th> </tr> </thead> <tbody> <tr> <td>P_obs</td> <td>0.60</td> <td>0.70</td> </tr> <tr> <td>P_rand</td> <td>0.01983</td> <td>0.09566</td> </tr> <tr> <td>EF (Enrichment Factor)</td> <td>0.60/0.01983 = 30.257</td> <td>0.70/0.09566 = 7.318</td> </tr> <tr> <td>BoR</td> <td>4.92 bits</td> <td>2.87 bits</td> </tr> </tbody> </table> <p><strong>System B</strong> has a higher raw success rate (70% vs. 60%) but a BoR score about 2 bits lower than <strong>System A</strong>. This lower score shows <strong>System B</strong> is less selective. It achieves higher coverage by expanding the retrieved set, which reduces informational efficiency. From an information-theoretic view, System B creates a larger <em>“haystack”</em> that delivers fewer useful bits of discrimination per query.</p> <h2 id="the-ceiling-problem">The Ceiling Problem</h2> <p>There’s a maximum BoR you can possibly achieve. If your system is perfect, achieving \(P_{\text{obs}}(K) = 1.0\) (every single query succeeds), the best you can do is:</p> \[\text{BoR}_{\text{max}}(K) = -\log_2(\overline{P}_{\text{rand}}(K))\] <p>This ceiling is determined entirely by the random baseline. Using our toy example:</p> <ul> <li><strong>System A:</strong> \(\text{BoR}_{\text{max}} = -\log_2(0.01983) = 5.66\) bits</li> <li><strong>System B:</strong> \(\text{BoR}_{\text{max}} = -\log_2(0.09566) = 3.39\) bits</li> </ul> <p>System A, even at 60% success, achieves 4.92 bits, already higher than System B’s ceiling. No amount of model improvement can help System B catch up. Given its success rate, it chose a retrieval depth K that limits its maximum possible selectivity.</p> <p><strong>When the random baseline is already high, even perfection gets you almost nothing.</strong></p> <h2 id="bor-optimistic-upper-bound">BoR optimistic upper bound</h2> <p>When you don’t know how many relevant items \(R_q\) exist in the corpus for each query, BoR enables you to define an optimistic upper bound by assuming each query has exactly one relevant item. In that case:</p> \[P_{\text{rand}}(K) \approx \frac{K}{N}\] <p>And:</p> \[\text{BoR}_{\text{opt}}(K) = \log_2\left(\frac{N}{K}\right)\] <p>It’s useful to compute the upper bound if calculating exact BoR is not feasible. \(\text{BoR}_{\text{opt}}(K)\) is an optimistic ceiling: no system on that corpus at depth <strong>K</strong> can have more than about \(\log_2(N / K)\) bits of selectivity under this assumption.</p> <p>Note that \(\text{BoR}_{\text{max}}\) uses actual \(R_q\) values while \(\text{BoR}_{\text{opt}}\) assumes \(R_q = 1\) throughout.</p> <h2 id="the-collapse-zone">The Collapse Zone</h2> <iframe src="/2026/assets/html/2026-04-27-bits-over-random/calculator.html" frameborder="0" scrolling="no" height="580px" width="100%" class="l-body rounded z-depth-1"></iframe> <p>Consider what happens when retrieval becomes <strong>“too easy”</strong>:</p> <ul> <li>If \(P_{\text{rand}} = 0.95\) (random selection succeeds 95% of the time), then even a perfect system only gets \(\text{BoR}_{\text{max}} \approx 0.07\) bits</li> <li>If \(P_{\text{rand}} = 0.99\) (random succeeds 99% of the time), then \(\text{BoR}_{\text{max}} \approx 0.01\) bits</li> </ul> <p>We call this the <em>“collapse zone.”</em> When you enter it, selectivity becomes mathematically impossible, even if your success rate looks great.</p> <p>The boundary is determined by:</p> \[\lambda = \frac{K \cdot \bar{R}_q}{N}\] <p>Where \(\bar{R}_q\) is the average number of relevant items per query.</p> <p>When \(\lambda\) reaches 3–5, you’ve entered the collapse zone. Random selection is already solving most queries, so even a perfect system can’t demonstrate meaningful skill.</p> <h2 id="what-happens-when-you-retrieve-more">What Happens When You Retrieve More?</h2> <p>Now that we have formulated a measure that evaluates an IR system with respect to random selection at a given K, what happens when you increase K (K₁ to K₂)? Typically, we expect the following:</p> <ol> <li>Your success rate improves (usually)</li> <li>Random selection also gets easier (always)</li> </ol> <p>The change in BoR is:</p> \[\Delta\text{BoR} = \log_2\left(\frac{P_2}{P_1}\right) - \log_2\left(\frac{\overline{P}_{\text{rand}}(K_2)}{\overline{P}_{\text{rand}}(K_1)}\right)\] <p>Translation:</p> <ul> <li><strong>First term:</strong> <em>“How much better did I actually do?”</em></li> <li><strong>Second term:</strong> <em>“How much easier did the task get for random guessing?”</em></li> </ul> <h2 id="the-doubling-rule">The Doubling Rule</h2> <p>In typical sparse-relevance scenarios (\(R_q \ll N\) and \(K \ll N\)), the hypergeometric baseline behaves like repeated independent draws. For small values of \(K \cdot R_q / N\), we can use standard approximations \((1 - x)^n \approx e^{-nx}\) and \(e^{-y} \approx 1 - y\) for \(y \to 0\).</p> <p>So, because: \(P_{\text{rand}}(K; R_q) \approx \frac{K \cdot R_q}{N}\) and averaging over queries yields \(\overline{P}_{\text{rand}}(K) \approx \frac{K \cdot \bar{R}_q}{N}\)</p> <p>We now have:</p> \[\Delta\text{BoR} \approx \log_2\left(\frac{P_2}{P_1}\right) - \log_2\left(\frac{K_2}{K_1}\right)\] <p>What does this mean in practice?</p> <p><strong>If you double K, but your success rate doesn’t improve, you lose about 1 bit of selectivity.</strong></p> <p>When you hear <strong>“just retrieve more,”</strong> remember: it’s not free. Once your success rate has plateaued:</p> <ul> <li>Double <strong>K</strong> and you lose \(\sim 1\) bit of selectivity</li> <li>\(10\times\) <strong>K</strong> and you lose \(\sim 3.3\) bits of selectivity</li> </ul> <p>To maintain selectivity when doubling <strong>K</strong>, you’d need \(P_{\text{obs}}\) to also double. But since \(P_{\text{obs}} \leq 1\), this becomes impossible once you’re above 50% success.</p> <p><strong>That’s why BoR inevitably degrades at larger depths once your success curve flattens.</strong></p> <h3 id="extensions-to-stricter-rules">Extensions to Stricter Rules</h3> <p>The BoR framework extends to stricter success rules. For example, requiring at least <strong>m</strong> relevant documents in the top-K:</p> \[\Delta\text{BoR} \approx -m \cdot \log_2\left(\frac{K_2}{K_1}\right)\] <p>Doubling K costs about <strong>m bits</strong> of selectivity. We focus on \(m=1\) in this post because it matches common single-evidence RAG scenarios.</p> <h2 id="case-studies-when-theory-meets-reality">Case Studies: When Theory Meets Reality</h2> <p>Let’s see how BoR behaves in the wild. We tested three different scenarios:</p> <table> <thead> <tr> <th style="text-align: left">Dataset</th> <th style="text-align: left">Corpus Size</th> <th style="text-align: left">Relevant Items per Query</th> <th style="text-align: left">Why Test It?</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>BEIR SciFact</strong></td> <td style="text-align: left">5,183 abstracts (1,409 queries/claims)</td> <td style="text-align: left">Sparse (\(R_q \approx 1\)–2)</td> <td style="text-align: left">Baseline: typical RAG scenario</td> </tr> <tr> <td style="text-align: left"><strong>MS MARCO</strong></td> <td style="text-align: left">~8.8M passages</td> <td style="text-align: left">Sparse (\(R_q \approx 1\))</td> <td style="text-align: left">Large scale: does BoR work at production size?</td> </tr> <tr> <td style="text-align: left"><strong>20 Newsgroups</strong></td> <td style="text-align: left">11,314 docs (training set) class-based setup</td> <td style="text-align: left">Dense (\(\bar{R}_q \approx 572\))</td> <td style="text-align: left">Stress test: what happens when selectivity collapses?</td> </tr> </tbody> </table> <p>We tested two retrievers representing different eras and approaches.</p> <ul> <li><strong>BM25:</strong> The classic lexical baseline</li> <li><strong>SPLADE:</strong> Modern neural sparse retriever (<a href="https://huggingface.co/naver/splade-cocondenser-ensembledistil">naver/splade-cocondenser-ensembledistil</a>): document top-k = 60, query top-k = 60, max sequence length = 256, batch size = 64 for documents and queries</li> </ul> <p>All results use exact hypergeometric baselines and 95% confidence intervals from bootstrap resampling (n=5,000, seed=7).</p> <h3 id="test-1-scifact-the-benchmark-case">Test 1: SciFact (The Benchmark Case)</h3> <p>This is what most people expect: sparse relevance, the kind you see in real RAG systems.</p> <p><strong>The results:</strong></p> <p>Both systems maintain strong selectivity even at <strong>K=100</strong>, with BoR staying above 5 bits. Predicted ΔBoR values match observed changes to within <strong>0.01</strong> bits across all configurations.</p> <p>This confirms that when \(\lambda = \frac{K \cdot \bar{R}_q}{N} \ll 1\) (well outside the collapse zone), retrieval systems can demonstrate meaningful selectivity over random chance.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-bits-over-random/bor_analysis_scifact-480.webp 480w,/2026/assets/img/2026-04-27-bits-over-random/bor_analysis_scifact-800.webp 800w,/2026/assets/img/2026-04-27-bits-over-random/bor_analysis_scifact-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-bits-over-random/bor_analysis_scifact.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Figure 1:</strong> <em>BoR analysis on the SciFact dataset shows sustained selectivity across retrieval depths. Both BM25 and SPLADE maintain high BoR values (5–11 bits), reflecting the dataset’s sparse relevance structure.</em></p> <p>But both BM25 and SPLADE operate very close to the theoretical ceiling. A 30-year-old algorithm nearly matches the modern neural system.</p> <p>Is SciFact just too easy? To investigate, we turn to literature and examine a much larger benchmark. On a corpus with millions of passages, how much headroom exists between top-performing systems and the theoretical ceiling?</p> <h3 id="test-2-ms-marco-the-industrial-scale-test">Test 2: MS MARCO (The Industrial Scale Test)</h3> <p>8.84 million passages. This is where large real-world systems operate.</p> <p>We computed BoR for <strong>41 different systems</strong> from the literature, from lexical baselines to state-of-the-art neural retrievers.</p> <p>At <strong>K=1000</strong>, the theoretical ceiling is:</p> \[\text{BoR}_{\text{opt}} \approx \log_2\left(\frac{8.84\text{M}}{1000}\right) \approx 13.11 \text{ bits}\] <p><strong>All 41 systems cluster within 0.2 bits of this ceiling.</strong> Indicatively, to show the range:</p> <table> <thead> <tr> <th style="text-align: left">System</th> <th style="text-align: left">Recall@1000</th> <th style="text-align: left">BoR (bits)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">BM25</td> <td style="text-align: left">85.7%</td> <td style="text-align: left">12.89</td> </tr> <tr> <td style="text-align: left">SPLADE</td> <td style="text-align: left">97.9%</td> <td style="text-align: left">13.08</td> </tr> <tr> <td style="text-align: left">ColBERTv2</td> <td style="text-align: left">98.5%</td> <td style="text-align: left">13.09</td> </tr> <tr> <td style="text-align: left">SimLM</td> <td style="text-align: left">98.7%</td> <td style="text-align: left">13.09</td> </tr> </tbody> </table> <p>BM25 gets 85.7% recall. SimLM (state-of-the-art) gets 98.7% recall. That’s a <strong>13-point recall gap.</strong></p> <p>But the BoR difference? <strong>Only 0.20 bits.</strong></p> <p>A three-decade-old lexical algorithm and cutting-edge neural systems are very close in chance-corrected selectivity (BoR) at this depth, for this dataset, and success rule (in this case, recall). This suggests diminishing returns from retriever improvements alone.</p> <p>Systems examined include: SimLM, AR2, uniCOIL, ColBERTv2, SPLADE (multiple versions), I3 Retriever, TCT-ColBERTv2, RoDR w/ ANCE, DPR-CLS, ColBERTer, ANCE, SLIM/SLIM++, and BM25.</p> <p>But both still show meaningful selectivity: BoR is above 12 bits. To really see what collapse looks like, we need an extreme test: a dataset where relevance is abundant, not rare.</p> <h3 id="test-3-20-newsgroups-the-stress-test">Test 3: 20 Newsgroups (The Stress Test)</h3> <p>The 20 Newsgroups dataset has 20 topical categories. We set up an extreme scenario: treat all documents in the same category as “relevant.”</p> <p>With <strong>11,314</strong> documents split across <strong>20</strong> classes, that’s about \(\bar{R}_q \approx 572\) relevant documents per query (over <strong>5%</strong> of the corpus).</p> <p>Why test something so unrealistic? Because, as you’ll see later, this can happen in LLM agent tool selection.</p> <p>This scenario pushes us directly into the collapse zone. At <strong>K = 100</strong>:</p> \[\lambda = \frac{K \cdot \bar{R}_q}{N} = \frac{100 \times 572}{11{,}314} \approx 5.1\] <p>Random selection alone would succeed ~99% of the time. The ceiling for any retrieval system is essentially zero. To make the contrast as clear as possible, here is 20NG vs SciFact against both systems.</p> <p><strong>Watch what happens:</strong></p> <table> <thead> <tr> <th style="text-align: left">Dataset</th> <th style="text-align: left">K</th> <th style="text-align: left">BoR Ceiling</th> <th style="text-align: left">BM25 Success</th> <th style="text-align: left">BM25 BoR</th> <th style="text-align: left">SPLADE Success</th> <th style="text-align: left">SPLADE BoR</th> <th style="text-align: left">ΔBoR (10→100)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>20NG</strong></td> <td style="text-align: left">10</td> <td style="text-align: left">1.31 bits</td> <td style="text-align: left">94%</td> <td style="text-align: left">1.22</td> <td style="text-align: left">95%</td> <td style="text-align: left">1.23</td> <td style="text-align: left">−1.22</td> </tr> <tr> <td style="text-align: left"><strong>20NG</strong></td> <td style="text-align: left">100</td> <td style="text-align: left">0.01 bits</td> <td style="text-align: left">100%</td> <td style="text-align: left">0.01</td> <td style="text-align: left">100%</td> <td style="text-align: left">0.01</td> <td style="text-align: left">—</td> </tr> <tr> <td style="text-align: left"><em>SciFact</em></td> <td style="text-align: left">10</td> <td style="text-align: left">8.84 bits</td> <td style="text-align: left">80%</td> <td style="text-align: left">8.52</td> <td style="text-align: left">81%</td> <td style="text-align: left">8.53</td> <td style="text-align: left">−3.12</td> </tr> <tr> <td style="text-align: left"><em>SciFact</em></td> <td style="text-align: left">100</td> <td style="text-align: left">5.52 bits</td> <td style="text-align: left">89%</td> <td style="text-align: left">5.36</td> <td style="text-align: left">93%</td> <td style="text-align: left">5.41</td> <td style="text-align: left">—</td> </tr> </tbody> </table> <p>At K=100 on 20 Newsgroups:</p> <ul> <li>Both systems achieve <strong>100% success</strong></li> <li>Both provide <strong>0.01 bits of selectivity</strong></li> </ul> <p>Perfect success rate. Essentially zero selectivity. <strong>The ceiling has collapsed.</strong></p> <p>The predicted <strong>ΔBoR</strong> from theory matches reality within <strong>0.01</strong> bits. The math is working exactly as expected.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-bits-over-random/selectivity_collapse_paradox_newsgroups-480.webp 480w,/2026/assets/img/2026-04-27-bits-over-random/selectivity_collapse_paradox_newsgroups-800.webp 800w,/2026/assets/img/2026-04-27-bits-over-random/selectivity_collapse_paradox_newsgroups-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-bits-over-random/selectivity_collapse_paradox_newsgroups.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Figure 2:</strong> <em>The selectivity collapse paradox on 20 Newsgroups. Left: BoR declines sharply with depth, converging to the theoretical ceiling (dashed line). Right: As Success@K approaches 100%, BoR approaches zero.</em></p> <p>But here’s the real question: <strong>Does this theoretical collapse actually hurt downstream performance?</strong></p> <h2 id="when-perfect-success-fails">When Perfect Success Fails</h2> <p>We tested this directly with a modern instruction-tuned LLM on the 20 Newsgroups collapsed scenario.</p> <p>Setup: Multiple-choice classification task, 50 queries per configuration, temperature=0.0.</p> <p><strong>The results:</strong></p> <table> <thead> <tr> <th style="text-align: left">System</th> <th style="text-align: left">Accuracy at K=10</th> <th style="text-align: left">Accuracy at K=100</th> <th style="text-align: left">Success@K</th> <th style="text-align: left">Token Cost</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>BM25</strong></td> <td style="text-align: left">66%</td> <td style="text-align: left"><strong>50%</strong></td> <td style="text-align: left">94% → 100%</td> <td style="text-align: left">10x increase</td> </tr> <tr> <td style="text-align: left"><strong>SPLADE</strong></td> <td style="text-align: left">68%</td> <td style="text-align: left"><strong>58%</strong></td> <td style="text-align: left">95% → 100%</td> <td style="text-align: left">10x increase</td> </tr> </tbody> </table> <p>Read that again:</p> <ul> <li>Success rate increased to 100% ✓</li> <li>Accuracy <strong>dropped</strong> by 10–16 percentage points ✗</li> <li>Token cost increased 10x ✗</li> </ul> <p><strong>This is the failure mode BoR detects.</strong> You’re paying 10x the tokens for random-level selectivity, and your AI is drowning in noise.</p> <p>When selectivity collapses, high success rates become meaningless or worse, misleading.</p> <h2 id="ai-agent-tool-selection">AI Agent Tool Selection</h2> <p>“That 20 Newsgroups test seems artificial,” you might be thinking. “Who retrieves documents where 5% of the corpus is relevant?”</p> <p>Fair Point. Let’s extend our testing to what happens with AI agents everyday.</p> <h3 id="when-agents-choose-tools">When Agents Choose Tools</h3> <p>Consider what Anthropic published in 2025<d-cite key="anthropic2025toolselection"></d-cite>:</p> <p><em>“Tool definitions can sometimes consume 50,000+ tokens before an agent reads a request. Agents should discover and load tools on-demand, keeping only what’s relevant for the current task.”</em></p> <p>Their example: 58 tools consuming ~55K tokens. Add integrations like Jira and you’re at 100K+ tokens. They’ve seen setups with tool definitions consuming 134K tokens before optimization.</p> <p>Now, let’s apply the same math as document retrieval:</p> <table> <thead> <tr> <th style="text-align: left">Parameter</th> <th style="text-align: left">Document Retrieval</th> <th style="text-align: left">Tool Selection</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>N</strong></td> <td style="text-align: left">Corpus size (thousands to millions)</td> <td style="text-align: left">Available tools (50–500)</td> </tr> <tr> <td style="text-align: left"><strong>K</strong></td> <td style="text-align: left">Documents shown to LLM</td> <td style="text-align: left">Tools shown to LLM</td> </tr> <tr> <td style="text-align: left"><strong>\(R_q\)</strong></td> <td style="text-align: left">Relevant documents</td> <td style="text-align: left">Applicable tools for task</td> </tr> </tbody> </table> <p>The critical difference: <strong>N is small for tools.</strong> And small N means you hit the collapse boundary much faster.</p> <h3 id="the-tool-selection-collapse">The Tool Selection Collapse</h3> <p>Let’s run the numbers for Anthropic’s 58-tool example. Assume 3–5 tools are typically relevant:</p> <table> <thead> <tr> <th style="text-align: left">Configuration</th> <th style="text-align: left">K</th> <th style="text-align: left">\(R_q\)</th> <th style="text-align: left">\(\lambda = \frac{K \cdot R_q}{N}\)</th> <th style="text-align: left">\(\text{BoR}_{\text{max}}\) (Poisson)</th> <th style="text-align: left">\(\text{BoR}_{\text{max}}\) (Exact)</th> <th style="text-align: left">What This Means</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Show 5 tools</td> <td style="text-align: left">5</td> <td style="text-align: left">4</td> <td style="text-align: left">0.34</td> <td style="text-align: left">~1.6 bits</td> <td style="text-align: left">~1.7</td> <td style="text-align: left">Meaningful selectivity</td> </tr> <tr> <td style="text-align: left">Show 20 tools</td> <td style="text-align: left">20</td> <td style="text-align: left">4</td> <td style="text-align: left">1.38</td> <td style="text-align: left">~0.4 bits</td> <td style="text-align: left">0.28</td> <td style="text-align: left">Degraded</td> </tr> <tr> <td style="text-align: left">Show all 58</td> <td style="text-align: left">58</td> <td style="text-align: left">4</td> <td style="text-align: left"><strong>4.0</strong></td> <td style="text-align: left">~0.02 bits</td> <td style="text-align: left">0</td> <td style="text-align: left">Collapse</td> </tr> </tbody> </table> <p>When all tool definitions are introduced simultaneously into the model’s context, the system operates at \(\lambda \approx 4\). This is deep into the collapse zone.</p> <p><strong>Even a perfect tool selector achieves only ~0.02 bits of selectivity over random chance.</strong></p> <p>The LLM is essentially guessing. And as Anthropic notes: <strong>“The most common failures are wrong tool selection and incorrect parameters, especially when tools have similar names.”</strong> This perfectly reflects the 20 Newsgroups scenario when \(R_q\) (relevant-per query items) was large.</p> <h3 id="the-pattern-extends-beyond-tools">The Pattern Extends Beyond Tools</h3> <p>The collapse boundary doesn’t care what you’re selecting. It’s a property of the selection problem itself: \(\lambda = \frac{K \cdot \bar{R}_q}{N}\)</p> <p>When \(\lambda\) hits 3–5, selectivity collapses, whether you’re selecting:</p> <ul> <li>Documents from a corpus</li> <li>Tools from an API library</li> <li>Agentic “skills”</li> <li>Functions from hundreds of endpoints</li> <li>Context from multi-hop retrieval chains</li> </ul> <table> <thead> <tr> <th style="text-align: left">Scenario</th> <th style="text-align: left">N</th> <th style="text-align: left">\(R_q\)</th> <th style="text-align: left">K</th> <th style="text-align: left">\(\lambda = \frac{K \cdot R_q}{N}\)</th> <th style="text-align: left">Regime</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">RAG (typical)</td> <td style="text-align: left">10,000</td> <td style="text-align: left">1–2</td> <td style="text-align: left">10</td> <td style="text-align: left">~0.002</td> <td style="text-align: left">Healthy</td> </tr> <tr> <td style="text-align: left">Tool selection (filtered)</td> <td style="text-align: left">20</td> <td style="text-align: left">3</td> <td style="text-align: left">5</td> <td style="text-align: left">0.75</td> <td style="text-align: left">Healthy</td> </tr> <tr> <td style="text-align: left">Tool selection (show all)</td> <td style="text-align: left">20</td> <td style="text-align: left">3</td> <td style="text-align: left">20</td> <td style="text-align: left">3.0</td> <td style="text-align: left">Collapse</td> </tr> <tr> <td style="text-align: left">API endpoints (show half)</td> <td style="text-align: left">100</td> <td style="text-align: left">8</td> <td style="text-align: left">50</td> <td style="text-align: left">4.0</td> <td style="text-align: left">Collapse</td> </tr> <tr> <td style="text-align: left">Anthropic’s 58-tool example</td> <td style="text-align: left">58</td> <td style="text-align: left">4</td> <td style="text-align: left">58</td> <td style="text-align: left">4.0</td> <td style="text-align: left">Collapse</td> </tr> </tbody> </table> <p><strong>This is why agentic systems struggle with tool selection far more than RAG systems struggle with document retrieval.</strong> The math is unforgiving when N is small.</p> <h2 id="what-you-should-do-about-this">What You Should Do About This</h2> <p>BoR gives you a new lens for evaluating retrieval systems. It reveals when high success rates are actually warning signs.</p> <p><strong>1. Monitor the collapse boundary</strong></p> <p>Calculate \(\lambda = \frac{K \cdot \bar{R}_q}{N}\) for your system. When \(\lambda\) approaches 3–5, you’re entering the danger zone. This single number tells you whether selectivity is even possible.</p> <p><strong>2. Use BoR to guide your K selection</strong></p> <p>Don’t just crank up K to boost success metrics. Instead:</p> <ul> <li>Stop increasing K when \(\text{BoR}_{\text{max}}\) drops below ~0.1 bits.</li> <li>If \(\text{BoR} \approx \text{BoR}_{\text{max}}\), you’ve saturated and more K won’t help.</li> <li>If \(\Delta\text{BoR}\) becomes negative or negligible, you’re adding noise, not signal.</li> </ul> <p><strong>3. For tool-based agents: Be aggressive about filtering</strong></p> <p>With small N (50–500 tools), you can’t afford to dump everything into context. Use:</p> <ul> <li>Two-stage retrieval (filter, then select)</li> <li>Dynamic tool loading based on task context</li> <li>Clustering by function domain</li> </ul> <p><strong>4. Remember the core insight</strong></p> <p><strong>More context is not always better.</strong> High Success@K can coexist with zero selectivity.</p> <table> <thead> <tr> <th style="text-align: left">Scenario</th> <th style="text-align: left">Calculations</th> <th style="text-align: left">Conclusion</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>K → N</strong> (K tends to N)</td> <td style="text-align: left">N = 100, K = 100, \(R_q = 1\)<br/><br/>\(P_{\text{obs}} = 1.0\) (retrieve everything, guaranteed success)<br/><br/>\(P_{\text{rand}} = 1.0\) (random selection of all 100 items → also guaranteed success)<br/><br/>\(\text{BoR} = \log_2(1.0 / 1.0) = 0\) bits exactly</td> <td style="text-align: left"><strong>BoR → 0 when K → N</strong> (K is closer to N)<br/><br/>Both Recall and Success@K are perfect. But BoR approaches zero asymptotically.<br/><br/>At K = N, BoR = 0.</td> </tr> <tr> <td style="text-align: left"><strong>Bad retriever</strong> - deliberately omits relevant results</td> <td style="text-align: left">N = 100, K = 10, \(R_q = 1\)<br/><br/>\(P_{\text{rand}} = 10/100 = 0.10\) (random succeeds 10% of the time)<br/><br/>retriever is adversarially bad: \(P_{\text{obs}} = 0.05\)<br/><br/>\(\text{BoR} = \log_2(0.05 / 0.10) = \log_2(0.5) = -1\) bit</td> <td style="text-align: left"><strong>BoR &lt; 0</strong> means we are actively avoiding relevant documents, doing worse than chance.</td> </tr> </tbody> </table> <h2 id="sidebar-successk-vs-recallk">Sidebar: Success@K vs Recall@K</h2> <p>Some readers might wonder: this post focuses on Success@K (coverage), but what about Recall@K?</p> <table> <thead> <tr> <th style="text-align: left">Metric</th> <th style="text-align: left">What It Measures</th> <th style="text-align: left">Per-Query Behavior</th> <th style="text-align: left">Best For</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Success@K</strong></td> <td style="text-align: left">Did you get \(\geq 1\) relevant item?</td> <td style="text-align: left">Binary: success or fail</td> <td style="text-align: left">RAG/QA where one good context suffices</td> </tr> <tr> <td style="text-align: left"><strong>Recall@K</strong></td> <td style="text-align: left">What fraction of all relevant items did you get?</td> <td style="text-align: left">Graded: 0% to 100%</td> <td style="text-align: left">Tasks needing comprehensive coverage</td> </tr> </tbody> </table> <p>The good news: <strong>BoR works with both.</strong></p> <h3 id="bor-for-recallk">BoR for Recall@K</h3> <p>The same framework applies. Instead of measuring “probability of \(\geq 1\) hit,” you measure “expected fraction retrieved”:</p> \[\text{BoR}_{\text{recall@K}} = \log_2\left(\frac{\text{observed_recall@K}}{\text{expected_recall@K_random}}\right)\] <p>For sparse relevance: \(\text{expected_recall@K_random} \approx \frac{K}{N}\)</p> <p><strong>Example:</strong> A query has 10 relevant items in a 1,000-document corpus. You retrieve 4 in top-20:</p> <ul> <li>Observed recall = \(\frac{4}{10} = 0.4\)</li> <li>Random baseline = \(\frac{20}{1{,}000} = 0.02\)</li> <li>\(\text{BoR}_{\text{recall@K}} = \log_2\left(\frac{0.4}{0.02}\right) = \log_2(20) \approx\) <strong>4.32 bits</strong></li> </ul> <h3 id="math-and-bor-interpretation">Math and BoR Interpretation</h3> <table> <thead> <tr> <th style="text-align: left">Metric</th> <th style="text-align: left">Definition</th> <th style="text-align: left">Formula</th> <th style="text-align: left">Observed Rate</th> <th style="text-align: left">Expected Rate (Random)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>BoR for Success@K</strong></td> <td style="text-align: left">Bits-over-Random for coverage (\(\geq 1\) relevant)</td> <td style="text-align: left">\(\log_2\left(\frac{\text{observed_success}}{\text{expected_success_random}}\right)\)</td> <td style="text-align: left">Fraction of queries with \(\geq 1\) relevant in top-K</td> <td style="text-align: left">Probability of \(\geq 1\) hit by random selection</td> </tr> <tr> <td style="text-align: left"><strong>BoR for Recall@K</strong></td> <td style="text-align: left">Bits-over-Random for recall (fraction retrieved)</td> <td style="text-align: left">\(\log_2\left(\frac{\text{observed_recall@K}}{\text{expected_recall@K_random}}\right)\)</td> <td style="text-align: left">Average fraction of relevant items in top-K</td> <td style="text-align: left">Expected fraction if picking K random (usually \(\frac{K}{N}\))</td> </tr> </tbody> </table> <p>The depth-calibrated identity also extends to Recall@K, with minor adjustments for the different success rule.</p> <p>We focus on Success@K in this post because it matches the most common RAG use case: you just need <em>one</em> good grounding passage.</p> <h2 id="final-thoughts">Final Thoughts</h2> <p>Retrieval evaluation has been stuck with metrics designed for human consumers. RAG and agentic AI systems need something different, something that accounts for the fact that every retrieved item imposes a cost, and random chance sets a floor.</p> <p><strong>Bits-over-Random provides that measure.</strong></p> <p>It makes three things visible that were previously hidden:</p> <ol> <li><strong>The ceiling:</strong> Even perfect systems have limited selectivity when random baselines are high</li> <li><strong>The collapse zone:</strong> When \(\lambda = \frac{K \cdot \bar{R}_q}{N}\) reaches 3–5, selectivity becomes impossible</li> <li><strong>The depth trade-off:</strong> Retrieving more doesn’t always help and it can actively hurt</li> </ol> <p>The math is simple but the implications are profound.</p> <p>When your tool-based agent has 50 functions available, and you dump all 50 into context, you’re not being thorough, you’re operating in the collapse zone. BoR reveals that.</p> <p>When you boost Success@K from 95% to 100% by tripling K, traditional metrics celebrate. BoR shows you just lost 1.5 bits of selectivity.</p> <p>The systems that win in the next era of AI won’t be the ones that retrieve the most. They’ll be the ones that retrieve the most <strong>selectively</strong>.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[For most of the history of information retrieval (IR), search results were designed for human consumers who could scan, filter, and discard irrelevant information on their own. This shaped retrieval systems to optimize for finding and ranking more relevant documents, but not keeping results clean and minimal, as the human was the final filter. However, LLMs have changed that by lacking this filtering ability. To address this, we introduce Bits-over-Random (BoR), a chance-corrected measure of retrieval selectivity that reveals when high success rates mask random-level performance.]]></summary></entry><entry><title type="html">Destruction is a General Strategy to Learn Generation; Diffusion’s Strength is to Take it Seriously; Exploration is the Future</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/destruction/" rel="alternate" type="text/html" title="Destruction is a General Strategy to Learn Generation; Diffusion’s Strength is to Take it Seriously; Exploration is the Future"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/destruction</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/destruction/"><![CDATA[<blockquote> <p>Pourquoi faire simple quand on peut faire compliqué?</p> <p>–French locution, usually ironic.</p> </blockquote> <p>This blogpost is composed of two main sections, tied together by their unusual information-theoretic viewpoint. <a href="#the-thesis">The Thesis</a> is an opinion/perspective/speculative piece on diffusion models. <a href="#the-tutorial">The Tutorial</a> is a diagrammatic presentation of diffusion models.</p> <h1 id="the-thesis">The Thesis</h1> <h2 id="learning-by-destroying">Learning by Destroying</h2> <p>Yang <em>et al.</em><d-cite key="yang2023diffusion"></d-cite> defined generative diffusion models as “a family of probabilistic generative models that progressively destruct data by injecting noise, then learn to reverse this process for sample generation.” I think that it is a good definition, except for the “by injecting noise” part. Indeed, there is a paper that is literally called <em>Cold Diffusion: Inverting Arbitrary Image Transforms <strong>Without Noise</strong></em><d-cite key="bansal2023cold"></d-cite> (emphasis mine), and calling “noise” the action of the <code class="language-plaintext highlighter-rouge">[MASK]</code> token in Mask Diffusion Language Models (MDLMs)<d-cite key="sahoo2024simple"></d-cite> is certainly debatable. For the sake of this blogpost, I will define generative diffusion models as follows.</p> <ol> <li> <p><strong>Data distribution.</strong> We have a training dataset sampled from a data distribution, and our goal is to generate new samples from the same data distribution. The only allowed “control mechanism” is <em>conditioning</em>.<d-footnote>Concretely, the condition can be specified as a predicate: a function accepting a data sample and returning either "true" or "false". When such a condition is provided, the model must only return samples for which the predicate returns "true", otherwise keeping the data distribution unaltered. Notice that this could be achieved by wrapping the model in a loop, and returning the first samples that passes the predicate check.</d-footnote></p> </li> <li> <p><strong>Destroying process.</strong> We also have access to a procedure that can gradually destroy the information in data samples. This procedure <strong>may</strong> involve randomness. Like the training dataset, this procedure if fully specified <em>a priori</em>, before any training takes place.</p> </li> <li> <p><strong>Generative process.</strong> We train a generative process so that the samples it generates approximate the sought data distribution. This training leverages both the training dataset, which specifies <em>what</em> the generative process should ultimately generate, and the destroying process, which specifies <em>how</em> to chunk generation into manageable pieces.<d-footnote>The loss function being minimized is typically an expectation of an expectation of an expectation, with those expectations respectively taken over the training data sample, the "level of destruction" to be applied on this training data sample, and the exact stochastic realization of this destruction (if applicable).</d-footnote></p> </li> </ol> <p>Using this definition, diffusion models can be seen as a special case of the tried-and-true machine learning technique consisting of withholding some information from a model’s input, then training the model to generate the withheld information.</p> <ul> <li>In supervised learning, the withheld (then generated) information is called “label”.</li> <li>In an autoregressive language model, it is the next token.</li> <li>For BERT-like language models, it is the <code class="language-plaintext highlighter-rouge">[MASK]</code>ed tokens.</li> <li>Some autoencoders rely on a representational bottleneck to withhold information.</li> <li>Contrastive autoencoders withhold whether pairs of samples are somehow related (positive pairs) or not (negative pairs).</li> <li>Denoising autoencoders add “noise” to samples, which destroys some of their informational content (i.e., withholding it).</li> <li>Diffusion models use the destroying process to withhold information.<d-footnote>This typically involves some form of randomness, but here we do not make it a requirement.</d-footnote></li> </ul> <p>You may notice a connection between the last two: that connection has already been made long ago.<d-cite key="kingma2021variational"></d-cite> In fact, such connections have been made between diffusion and other points in that list, <d-cite key="zheng2025masked,fathi2025unifying"></d-cite> and I think that, given enough coffee, we could get them all covered. <em>Should we</em>? That’s a good question, and the answer may depend in part on how data-starved we are.</p> <h2 id="diffusions-advantages">Diffusion’s Advantages</h2> <p>In <em>Diffusion Language Models are Super Data Learners</em>, Ni <em>et al.</em><d-cite key="ni2025diffusion"></d-cite> observe that</p> <blockquote> <p>when unique data is limited, diffusion language models (DLMs) consistently surpass autoregressive (AR) models by training for more epochs. The crossover shifts later with more or higher-quality data, earlier with larger models, and persists across dense and sparse architectures. We attribute the gains to three compounding factors: (1) any-order modeling, (2) super-dense compute from iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation […]</p> </blockquote> <p>I think that this makes a lot of sense, and that <strong>this phenomenon is likely general</strong>. We have devised simple (non-diffusion) ways to withhold information from our models, breaking the data into chunks that “make sense” to us, emphasizing aspects that must obviously be learned sooner or later. And when we train models to generate that withheld information, these simple approaches turn out to be locally optimal for the sake of training as quickly as possible on humongous datasets. But when the fresh data gets scarce, when you have to train from the same sample for the $N^{\text{th}}$ time, perhaps more could be learned by considering a different viewpoint – like a diffusion model learning to predict tokens out-of-order. To be clear, ultimately, it could be that the diffusion model will perform better during inference when it is used to predict tokens in order, like an autoregressive model would simply do.<d-cite key="kim2025train"></d-cite> But because it was <em>challenged</em> during training on out-of-order tasks, it may eventually manage to pick up some tricks that will forever evade the autoregressively-trained model. Bigger models have more capacity to latch on such tricks, so their crossover may come earlier.</p> <p>Depending on your perspective, this may align with Sutton’s <em>The Bitter Lesson</em>:<d-cite key="sutton2019bitter"></d-cite></p> <blockquote> <p>general methods that leverage computation are ultimately the most effective, and by a large margin. […] Seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation.</p> </blockquote> <p>Left-to-right training may have been yet another human bias, a temporary fad, and in the long run perhaps we’ll just train in all-the-orders, leveraging much more compute. But there is a sweeter perspective on that bitter lesson: the design space for novel diffusion models is huge. To this day, very few destroying processes have been considered. For example, <code class="language-plaintext highlighter-rouge">[MASK]</code> is a great candidate for the next human fad/bias to be replaced by something that better leverages compute. Moreover, while diffusion may beat our vanilla information-withholding approaches in the bounded data regime, it hasn’t been shown to be the best way to proceed: diffusion itself may be a weird human fad.</p> <h2 id="reconciling-exploration">Reconciling Exploration</h2> <p>This takes me to the last point of this blogpost’s thesis: if all incarnations of the tried-and-true “information withholding” machine learning technique can (and perhaps should) be related to diffusion, what other techniques are left? And could we improve upon diffusion by learning from them? The first answers that come to my mind are “anything that involves exploration,” and “yes, probably.”</p> <p>The archetypal technique involving exploration is Reinforcement Learning (RL). Whereas pure generative models solely learn from a training dataset – striving to generate new samples from the exact same data distribution – RL models strive for a different, “better” distribution. What is meant by “better” is usually (but not always) specified through a reward function: a function that assigns value (reward) to each possible data sample. We typically wish to maximize the expected reward, though there may be some requirement to not meander too far away from the original data distribution.</p> <p>GRPO<d-cite key="shao2024deepseekmath"></d-cite> is a quite successful RL technique, and it has recently been ported to diffusion.<d-cite key="gong2025diffucoder"></d-cite> Such an adaptation is not trivial: due to its autoregressive origin, GRPO takes for granted easy access to likelihoods for different sequences, but such likelihoods are harder to get in diffusion models. This issue has been overcome before in the context of perplexity<d-cite key="sahoo2024simple"></d-cite>: take expectations over different levels of destruction and over different destruction realization, which translates to different decoding orders. Gong <em>et al.</em><d-cite key="gong2025diffucoder"></d-cite> leverage this technique,<d-footnote>With the twist that when a decoding order is used in the estimation of this expectation, the "complement" of the mask associated to that decoding order is always also considered. This practice reduces the variance, allowing them to take expectations over a single such pair of orders.</d-footnote> and their results are great!</p> <p>Yet I suspect that this expectation-over-decoding-order strategy is inherently off-policy. Indeed, even for purely random decoding orders,<d-footnote>To be clear, I definitely believe that it would be a good idea to learn a policy for the decoding order. I'm here assuming a non-learnable random order to make my point stronger.</d-footnote> I claim that the specific order faced by the model while generating a trace should be accounted for in the reward assignment for that trace. My intuition goes as follows: if a successful reasoning trace summarizes some premises from the context, expands some methodic steps, then reaches some conclusion, what are we teaching the model by rewarding it to predict the conclusion first, without the reasoning steps that lead to it?<d-footnote>My answer: at best we're teaching it to skip steps and/or ignore reasoning, at worst we're teaching it to hallucinate, make stuff up, justify *a posteriori* and/or otherwise deceive.</d-footnote></p> <p>You may now think “Who are you to say how the model should or shouldn’t reason! Remember The Bitter Lesson!” Fair enough, but here’s my point: in the non-RL diffusion case, this expectation-over-decoding-order approach was grounded in sound theory, but we didn’t do our homework before porting it to RL. We can justify it by its good empirical results, but we lost our theoretical grounding. <em>A priori</em>, the only path we may reward on-policy for a given generated sample is the one that was followed by the model while generating that sample.</p> <p>Ok, how did we get that theoretical grounding in the non-RL case? Limiting ourselves to my strict definition at the beginning of this blogpost, we consider a specific data distribution and noising process, and there thus exists a single, ideal, typically untractable<d-footnote>This is why we have to train a neural network: we're learning a tractable function that approximates the untractable ideal one.</d-footnote> probability distribution for partially-destroyed data samples at different levels of destruction. Within a given modeling paradigm, all concrete diffusion model implementations seek to approximate some function of that same ideal distribution: models with different weight initialization (or even different neural network architectures!) all strive to approximate the same “correct” answer. This is a very convenient property: in a diffusion model (as per my definition), the function to be learned is not a moving target.</p> <p>Can we approach the RL problem with a diffusion model that satisfies this strict definition? Yes! By reframing it as conditioning,<d-cite key="yuan2023reward"></d-cite> which is the sole allowed control mechanism as per my definition. For now, work on that front is still in an early stage, and competition leveraging techniques ported from the autoregressive case (e.g., GRPO) have an head start. Moreover, there is no guarantee that approaching RL by sticking to my strict definition of diffusion has real advantages in the long run. Nonetheless, I think that the fact we <em>can</em> suffices to justify additional exploratory efforts.</p> <p>More generally, notice how reframing as a conditioning problem removed “exploration” from the picture: we’re not <em>searching</em> for high-reward samples, we’re just <em>filtering out</em> from the original data distribution the samples that have low reward. In practice, because we don’t have infinite resources, actual implementations still have to explore to find where it is worth it to learn the function. We have the guaranteed existence of a non-moving target function, but we have to find which parts of that target function are worth learning well.<d-footnote>Recall that "standard" RL models are often constrained to not meander too far away from the original data distribution. Concretely, this is usually implemented by comparing the predictions of a frozen, "old" model with those of its RLed counterpart. In the conditioned diffusion approach to RL, the non-moving target function plays the role of that old model.</d-footnote></p> <p>The astute readers may notice that we’ve been infringing on a second taboo of my strict definition: the destroying process is <em>given</em> (not learned!) before any training takes place, and it specifies what/how the model should learn. Again, there are workarounds: we may tweak how we sample the destruction process while maintaining the same nice, non-moving target.<d-footnote>For example, we may counterbalance frequency biases by inversely weighting the losses.</d-footnote> But is there a point where we could gain something by giving up that nice, non-moving target?</p> <p>Of course! We move that target every time a researcher comes up with a “better” destroying process, and there is no good reason to believe that this human-in-the-loop algorithm has already found the optimum. The data should speak for itself! The compute should be leveraged! Umh, how?</p> <p>Let me greatly simplify the problem.</p> <ol> <li> <p><strong>First distribution.</strong> We have a data distribution from which we can get a training dataset.</p> </li> <li> <p><strong>Second distribution.</strong> We have a destroying process whose specifics depends on parameters to be determined. However, whatever these parameters are, we already know what the probability distribution for a “fully destroyed” data distribution should look like.<d-footnote>For example, a Gaussian.</d-footnote></p> </li> <li> <p><strong>Cost function.</strong> For given parameters of the destroying process, we train a diffusion model to generate from the data distribution. We are given a function that associates a cost to these parameters: the cost may depend on the ultimate performances of the model when transporting from the fully-destroyed distribution to the data distribution, but also on the model’s size and the resources it consumed at training and/or inference. We seek to minimize the expectation of this cost.</p> </li> </ol> <p>This is an instance of Optimal Transport (OT) problems, which are traditionally introduced with piles of dirt. Imagine a pile of dirt whose height profile represents a probability distribution (higher probability means more dirt piled up there). We want to move the dirt around so that it represents a second distribution instead. There are many ways to do so, many exact plans for where to pick each shovelful of dirt and where to toss them. We can assign a cost<d-footnote>That is, a negative reward.</d-footnote> to each such plan, and we must explore the space of plans to find the cheapest.<d-footnote>If you have ever found yourself sitting in a conference room while the speaker said "... Wasserstein metric, also known as earth mover's distance...", then the talk was likely about OT.</d-footnote></p> <p>Ok, but is there anything practical to gain by framing the search for the best destroying process in terms of an OT problem? Well, I don’t know. But perhaps I should now mention that entropy-regularized OT is tightly related to diffusion,<d-cite key="debortoli2021diffusion"></d-cite> a fact that definitely weights in my intuition that there may be something interesting to be done here. Again, I think that the fact we <em>can</em> suffices to justify some exploratory efforts.</p> <p>So destruction is a general strategy for learning to generate information about a data distribution, diffusion’s <del>messiness</del> richness may advantageously leverage destruction in some contexts (including data-starved ones), exploring how to optimize a function doesn’t neatly fit this pictures, but there are workarounds reconciliating diffusion and exploration, opening a multitude of avenues for future work. There are no promises, but we should give it a try.</p> <h1 id="the-tutorial">The Tutorial</h1> <h2 id="destroying-and-generating">Destroying and Generating</h2> <p>This section provides a diagrammatic introduction to “destroying” and “generating” information.</p> <p>As is tradition in information theory, we consider two characters, Alice and Bob. Alice communicates one of 3 messages to Bob through a communication channel. A big arrow indicates the name of the channel itself, <code class="language-plaintext highlighter-rouge">Identity</code>, while smaller arrows indicate the action of that channel.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b0[0]
        b1[1]
        b2[2]
    end
    a0 --&gt; b0
    a1 --&gt; b1
    a2 --&gt; b2

    A ==Identity==&gt; B

    LeftMarginHack:::phantom ~~~ a0
    b0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>In the above situation, Bob gets exactly the message that Alice tried to convey. The information communicated by Alice is thus preserved (i.e., neither created nor destroyed) by the <code class="language-plaintext highlighter-rouge">Identity</code> communication channel: it is an important (albeit boring) channel.</p> <p>Ok, now let’s try a different channel, which I’ll call <code class="language-plaintext highlighter-rouge">Cypher</code>.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b1[1]
        b2[2]
        b0[0]
    end
    a0 --&gt; b1
    a1 --&gt; b2
    a2 --&gt; b0

    A ==Cypher==&gt; B

    LeftMarginHack:::phantom ~~~ a0
    b0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Here Bob gets a different number than the one Alice intended to send. Does this mean that <code class="language-plaintext highlighter-rouge">Cypher</code> destroys information? Well, it would if the channel could only be used once before being discarded…</p> <p>But for the sake of this blogpost, let’s instead consider the case where Bob is allowed as many training examples as he needs. During that training, Bob gets Alice’s message through both the trusty <code class="language-plaintext highlighter-rouge">Identity</code> channel and the to-be-figured-out <code class="language-plaintext highlighter-rouge">Cypher</code> channel. Using pairs of the form $(a,b)$ (with $a \in A$ and $b \in B$), Bob’s observations may look like</p> \[\{ (0,1), (2,0), (0,1), (0,1), (1,2), (0,1), (2,0), (0,1), (2,0), (0,1), (0,1), (0,1), \cdots \}.\] <p>There are many kinds of things that Bob could learn from such data. First, he may hypothesize that Alice is more likely to give the message $0$ than she is to say $1$ or $2$. In doing so, Bob would be building a mental model of an Imaginary-Alice $A’$, i.e., trying to learn the probability distribution $\textup{P}^\theta(A’)$ so that it matches the real $\textup{P}(A)$. And ultimately, this is exactly what generative modeling is about: learn how to sample from a $\textup{P}^\theta(A’)$ that approximates as best as we can the data distribution $\textup{P}(A)$.</p> <p>Doing so may be realistic for $3$ messages, but what if Alice had more range, say, $10^{678000}$ possible unique messages?<d-footnote>GPT OSS 120B's vocabulary size powered to its context length.</d-footnote> This direct approach won’t scale, which is why the next section will consider a divide-and-conquer approach to chunk large problems into more amenable ones. For now, it suffices to say that Bob is just a step in a long chain of messages, followed by Carol, David, <em>etc.</em> If each step is easier to model/learn than the previous one, we’ll be making progress toward our ultimate goal.</p> <p>So, Bob should focus on figuring out the <code class="language-plaintext highlighter-rouge">Cypher</code> channel, i.e., learn a probability distribution $\textup{P}^{\theta}(A \vert B)$ that approximates $\textup{P}(A \vert B)$.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b1[1]
        b2[2]
        b0[0]
    end
    subgraph Ap["A'"]
        ap0[0]
        ap1[1]
        ap2[2]
    end
    a0 --&gt; b1 --&gt; ap0
    a1 --&gt; b2 --&gt; ap1
    a2 --&gt; b0 --&gt; ap2

    A ==Cypher==&gt; B
    B ==Decrypt==&gt; Ap
    A ==Identity==&gt; Ap

    LeftMarginHack:::phantom ~~~ a0
    ap0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>There exists a <code class="language-plaintext highlighter-rouge">Decrypt</code> channel that, applied on the output of the <code class="language-plaintext highlighter-rouge">Cypher</code>, gives the <code class="language-plaintext highlighter-rouge">Identity</code>. If Bob is a good Bayesian, he will never be <em>absolutely sure</em> that he figured it out, but whatever his priors were<d-footnote>Well, within reason, assuming that Bob has a minimal pragmatism and/or understanding of the world...</d-footnote> for the probability distribution from which the <code class="language-plaintext highlighter-rouge">Cypher</code> channel was sampled, there is a point at which <code class="language-plaintext highlighter-rouge">Decrypt</code> will become his leading hypothesis as to what $\textup{P}^\theta(A’ \vert B)$ should be. And from that point on, his confidence in that hypothesis will keep increasing as more data is gathered.</p> <p>The existence of a <code class="language-plaintext highlighter-rouge">Decrypt</code> that reverses the action of <code class="language-plaintext highlighter-rouge">Cypher</code> proves that <code class="language-plaintext highlighter-rouge">Cypher</code> does not destroy information. And because we could do the opposite, i.e., reverse the action of <code class="language-plaintext highlighter-rouge">Decrypt</code> by applying <code class="language-plaintext highlighter-rouge">Cypher</code>, we know that <code class="language-plaintext highlighter-rouge">Cypher</code> does not generate information either. Just like <code class="language-plaintext highlighter-rouge">Identity</code>, <code class="language-plaintext highlighter-rouge">Cypher</code> preserves information. What does this mean? It means that <code class="language-plaintext highlighter-rouge">Cypher</code> is a useless channel for our divide-and-conquer aims: learning a $\textup{P}^\theta(B’)$ that approaches $\textup{P}(B)$ is as hard as learning a $\textup{P}^\theta(A’)$ that approaches $\textup{P}^\theta(A)$.</p> <p>Ok, maybe what we need is a noisy channel?</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b0[0]
        b1[1]
        b2[2]
        b3[3]
        b4[4]
        b5[5]
    end
    a0 --head--&gt; b0
    a0 --tail--&gt; b3
    a1 --head--&gt; b1
    a1 --tail--&gt; b4
    a2 --head--&gt; b2
    a2 --tail--&gt; b5

    A ==Product==&gt; B

    LeftMarginHack:::phantom ~~~ a0
    b0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Here the <code class="language-plaintext highlighter-rouge">Product</code><d-footnote>This name is a reference to category theory; please see the very last section for why.</d-footnote> channel flips a coin, and this affects what message Bob receives. Does <code class="language-plaintext highlighter-rouge">Product</code> destroy information?</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b0[0]
        b1[1]
        b2[2]
        b3[3]
        b4[4]
        b5[5]
    end
    subgraph Ap["A'"]
        ap0[0]
        ap1[1]
        ap2[2]
    end
    a0 --head--&gt; b0 --&gt; ap0
    a0 --tail--&gt; b3 --&gt; ap0
    a1 --head--&gt; b1 --&gt; ap1
    a1 --tail--&gt; b4 --&gt; ap1
    a2 --head--&gt; b2 --&gt; ap2
    a2 --tail--&gt; b5 --&gt; ap2

    A ==Product==&gt; B ==Project==&gt; Ap
    A ==Identity==&gt; Ap

    LeftMarginHack:::phantom ~~~ a0
    ap0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>No: there exists a <code class="language-plaintext highlighter-rouge">Project</code> channel that undoes the action of <code class="language-plaintext highlighter-rouge">Product</code>. Does it generate information?</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b0[0]
        b3[3]
        b1[1]
        b4[4]
        b2[2]
        b5[5]
    end
    subgraph Coin
        head
        tail
    end
    a0 --head--&gt; b0 --&gt; head
    a0 --tail--&gt; b3 --&gt; tail
    a1 --head--&gt; b1 --&gt; head
    a1 --tail--&gt; b4 --&gt; tail
    a2 --head--&gt; b2 --&gt; head
    a2 --tail--&gt; b5 --&gt; tail

    A ==Product==&gt; B ==Project'==&gt; Coin

    LeftMarginHack:::phantom ~~~ a0
    head ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Yes! Bob could use <code class="language-plaintext highlighter-rouge">Project'</code> to learn the outcome of the coin flip, an information that Alice is completely unaware of!</p> <p>For our divide-and-conquer aims, we needed Bob to be easier to model than Alice, and we got the opposite: Bob is strictly harder to model than Alice because he has all of Alice’s information, plus some irrelevant information about a coin. Therefore, <code class="language-plaintext highlighter-rouge">Product</code> is an even worse channel than <code class="language-plaintext highlighter-rouge">Cypher</code> for this divide-and-conquer purpose.</p> <p>If noise isn’t what we need, what is it?</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b0[0]
        b1[1]
    end
    a0 --&gt; b0
    a1 --&gt; b1
    a2 --&gt; b1

    A ==Mash==&gt; B

    LeftMarginHack:::phantom ~~~ a0
    b0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>That’s it! When Bob gets the message $1$, he can’t tell with certainty whether Alice said $1$ or $2$. <code class="language-plaintext highlighter-rouge">Mash</code>ing different messages together is what destroys information, not noise.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b0[0]
        b1[1]
    end
    subgraph Ap["A'"]
        ap0[0]
        ap1[1]
        ap2[2]
    end
    a0 --&gt; b0 --&gt; ap0
    a2 --&gt; b1 --&gt; ap1
    a1 --&gt; b1 --&gt; ap2

    A ==Mash==&gt; B ==Guess==&gt; Ap
    A -.B.- Ap

    LeftMarginHack:::phantom ~~~ a0
    ap0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>When Bob gets the message $B=0$, he can simply assign $A’=0$. But when $B=1$, he must learn to <code class="language-plaintext highlighter-rouge">Guess</code> the message sent by an Imaginary-Alice $A’$ so that $\textup{P}^\theta(A’|B)$ is as close as possible to $\textup{P}(A|B)$.</p> <p>I indicate this last requirement using a dashed line, with the conditional $B$ added on the line. However, in many applications, we don’t have a particular desire for $\textup{P}^\theta(A’|B) = \textup{P}(A|B)$, and we could be perfectly content with any $\textup{P}^\theta(A’|B)$ such that $\textup{P}^\theta(A’) = \textup{P}(A)$. In those cases, I simply don’t write anything on the dashed line.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b0[0]
        b1[1]
    end
    subgraph Ap["A'"]
        ap0[0]
        ap1[1]
        ap2[2]
    end
    a0 --&gt; b0 --&gt; ap0
    a2 --&gt; b1 --&gt; ap0
    a1 --&gt; b1
    b0 --&gt; ap1
    b1 --&gt; ap1
    b0 --&gt; ap2
    b1 --&gt; ap2

    A -.- Ap
    A ==Mash==&gt; B ==&gt; Ap
    

    LeftMarginHack:::phantom ~~~ a0
    ap0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Note that I didn’t name the arrow from Bob to the Imaginary-Alice, because I don’t really care how Bob does it: as long as $\textup{P}^\theta(A’) = \textup{P}(A)$, i.e., the two people joined by an un-decorated dashed line have the same marginal distribution, we’re good.</p> <p>And I may not even care about the details of the possible messages and how they relate (small arrows).</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0:::phantom
    end
    subgraph B
        b0:::phantom
    end
    subgraph Ap["A'"]
        ap0:::phantom
    end
    A ==&gt; B ==&gt; Ap
    A -.- Ap

    LeftMarginHack:::phantom ~~~ A
    Ap ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>What does this picture tell us? Well, for a starter, because $A$ and $A’$ have the same marginal probability distribution, they must have the same information content (entropy)</p> \[\textup{H}(A) = \textup{H}(A') .\] <p>How does $\textup{H}(B)$ relate to those two? We don’t know: it could be higher, lower, or equal, depending on the details of those communication channels. One thing is clear though: as much information must be created/destroyed from $A$ to $B$ as is destroyed/created from $B$ to $A’$.</p> <p>This picture can tell us much more than that, because of the data processing inequality. From Latham and Roudi<d-cite key="Latham2009"></d-cite>:</p> <blockquote> <p>The Data Processing Inequality (DPI) states, loosely, that post-processing cannot increase information.</p> </blockquote> <p>If you think “Haven’t we just shown that communication channels may create information?!”, then yes, you are correct. But the information created by communication channels <em>is completely irrelevant to anything that came before in the communication chain</em>. Latham and Roudi are saying that information about an early message cannot appear in later messages by further processing. This “information about” can be measured with the mutual information; we write $\textup{I}(Y; X)$ the mutual information between $X$ and $Y$, and it satisfies</p> \[\textup{I}(Y; X) = \textup{H}(Y) - \textup{H}(X | Y) = \textup{H}(X) - \textup{H}(Y | X) , \qquad \textup{I}(Y; Y) = \textup{H}(Y) .\] <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0:::phantom
    end
    subgraph B
        b0:::phantom
    end
    subgraph Ap["A'"]
        ap0:::phantom
    end
    A ==&gt; B ==&gt; Ap

    LeftMarginHack:::phantom ~~~ A
    Ap ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Therefore, what the data processing inequality tells us is that</p> \[\eqalign{ \textup{I}(A; A) \ge \textup{I}(A; B) &amp; \ge \textup{I}(A; A') \cr \textup{I}(B; B) &amp; \ge \textup{I}(B; A') . }\] <p>Whatever there is to know about $A$, $B$ cannot know more than that, and $A’$ cannot know more than $B$ did. And $A’$ cannot know more about $B$ than $B$ knew about himself.</p> <p>This is what I call “the destruction story”: <strong>you cannot know what has been forgotten before you heard about it.</strong> However, the data processing inequality has a lesser known dual, which I call “the generation story”</p> \[\eqalign{ \textup{I}(A'; A) &amp; \le \textup{I}(A'; B) \le \textup{I}(A'; A') \cr \textup{I}(B; A) &amp; \le \textup{I}(B; B) . }\] <p>$A$ cannot know more than $B$ about $A’$, and $B$ cannot know more about $A’$ than there is to be known about $A’$. And $A$ cannot know more than $B$ about $B$. In the generation story, <strong>you cannot know about what has not been decided yet.</strong></p> <p>These zoomed-out diagrams are a great way to consider general classes of problems. But ultimately, the generation/destruction of information happens at the zoomed-in, message level.</p> <pre><code class="language-mermaid">flowchart LR
    focus[" "]
    
    leftt[" "] --&gt; focus --&gt; rightt[" "]
    leftc:::phantom ~~~|"destroy"| focus ~~~|"generate"| rightc:::phantom
    leftb[" "] --&gt; focus --&gt; rightb[" "]

    LeftMarginHack:::phantom ~~~ leftt
    rightb ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <ul> <li>When different messages converge to the same message, information is <strong>destroyed</strong>.</li> <li>When a given message can diverge into different messages, information is <strong>generated</strong>.</li> </ul> <h2 id="mashing-everything">Mashing Everything</h2> <p>Let’s continue from the <code class="language-plaintext highlighter-rouge">Mash</code> example in the previous section. Now suppose that Bob passes the message to Carol through the <code class="language-plaintext highlighter-rouge">Mash'</code> channel.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b0[0]
        b1[1]
    end
    subgraph C
        c0[0]
    end
    subgraph Singleton[*]
        singleton[" "]
    end
   
    a0 --&gt; b0 --&gt; c0
    a1 --&gt; b1 --&gt; c0
    a2 --&gt; b1

    A ==Mash==&gt; B ==Mash'==&gt; C -.- Singleton

    LeftMarginHack:::phantom ~~~ a0
    singleton ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Notice that Carol always gets the same message, which is called a singleton: I highlight this by linking her to a special “*” symbol using a dashed line. All singleton have no information: $\textup{H}(C) = 0$.</p> <p>But by the definition of mutual information, we must have $\textup{I}(A; C) \le \textup{H}(C) = 0$: Carol cannot know more about Alice than Carol knows at all, but Carol knows nothing, therefore Carol knows nothing about Alice. Stated otherwise, all information about Alice has been destroyed by the time the messages get to Carol. Success! We have divided a large task into smaller, simpler ones, until all that was left was trivial!</p> <p>Now unto the “conquer” part of divide-and-conquer.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0:::phantom
    end
    subgraph B
        b0:::phantom
    end
    subgraph Ap["A'"]
        ap0:::phantom
    end
    subgraph C
        c0:::phantom
    end
    subgraph Bp["B'"]
        bp0:::phantom
    end
    subgraph Singleton["*"]
        singleton:::phantom
    end
    A ==Mash==&gt; B ==Mash'==&gt; C ==Guess'==&gt; Bp
    B ==Guess==&gt; Ap
    A -.- Ap
    B -.- Bp
    C -.- Singleton

    LeftMarginHack:::phantom ~~~ a0
    bp0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Bob can learn <code class="language-plaintext highlighter-rouge">Guess</code> using <code class="language-plaintext highlighter-rouge">Mash</code> on Alice’s message, and Carol can learn <code class="language-plaintext highlighter-rouge">Guess'</code> using <code class="language-plaintext highlighter-rouge">Mash'</code> on Bob’s. And because Carol knows nothing, we’re ready to generate completely fake $A’’$ behavior from nothing!</p> <pre><code class="language-mermaid">flowchart LR
    subgraph Singleton["*"]
        singleton:::phantom
    end
    subgraph Bpp["B''"]
        bpp0:::phantom
    end
    subgraph App["A''"]
        app0:::phantom
    end
    subgraph A["A"]
        a0:::phantom
    end
    
    Singleton =="Guess'"==&gt; Bpp ==Guess==&gt; App -.- A

    LeftMarginHack:::phantom ~~~ Singleton
    a0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>This is the strategy used in autoregressive language models.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0["('The', 'cat', 'is', 'black')"]
        a1["('The', 'cat', 'is', 'white')"]
        a2["('The', 'cat', 'is', 'sleepy')"]
        a3["('The', 'cat', 'was', 'sleepy')"]
        a4["('The', 'dog', 'is', 'black')"]
        a5["('The', 'dog', 'is', 'brown')"]
        a6["('Your', 'cat', 'is', 'black')"]
        a7["..."]
    end
    subgraph B
        b0["('The', 'cat', 'is')"]
        b1["('The', 'cat', 'was')"]
        b2["('The', 'dog', 'is')"]
        b3["('Your', 'cat', 'is')"]
        b4["..."]
    end
    subgraph C
        c0["('The', 'cat')"]
        c1["('The', 'dog')"]
        c2["('Your', 'cat')"]
        c3["..."]
    end
    subgraph D
        d0["('The')"]
        d1["('Your')"]
        d2["..."]
    end
    subgraph E
        e0["()"]
    end
   
    a0 --&gt; b0 --&gt; c0 --&gt; d0 --&gt; e0
    a1 --&gt; b0
    a2 --&gt; b0
    a3 --&gt; b1 --&gt; c0
    a4 --&gt; b2 --&gt; c1 --&gt; d0
    a5 --&gt; b2
    a6 --&gt; b3 --&gt; c2 --&gt; d1 --&gt; e0

    LeftMarginHack:::phantom ~~~ a0
    e0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Here the ellipses represent additional messages whose arrows are not explicitly shown. The message is iteratively fed through the <code class="language-plaintext highlighter-rouge">MashLastToken</code> communication channel, which mashes together all the messages that have the same prefix up to the last token. Because of the setting’s symmetric structure, we can use the same neural network at each position to predict the probability distribution for what the next should be: all positions can contribute to train the same <code class="language-plaintext highlighter-rouge">GuessNextToken</code>. At inference, we start from the singleton (the empty sequence), then iteratively apply <code class="language-plaintext highlighter-rouge">GuessNextToken</code>. Et voilà! An autoregressive language model!</p> <p>Notice how each time we destroy some information, we’re carving out a chunk of the overall task to be learned. Here divide-and-conquer amounted to “learn to generate one token at a time”. Such a very structured, regular way to destroy the message, is typical of many non-diffusion machine learning techniques. One of my claims in this blogpost is that diffusion models can destroy information more “organically”, possibly reducing the human-designer bias hand-waved at by Sutton.<d-cite key="sutton2019bitter"></d-cite> As an example of what I mean, here’s what the last diagram would look like for a mask diffusion language model.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0["('The', 'cat', 'is', 'black')"]
        a1["('The', 'cat', 'is', 'white')"]
        a2["('The', 'cat', 'is', 'sleepy')"]
        a3["('The', 'cat', 'was', 'sleepy')"]
        a4["('The', 'dog', 'is', 'black')"]
        a5["('The', 'dog', 'is', 'brown')"]
        a6["('Your', 'cat', 'is', 'black')"]
        a7["..."]
    end
    subgraph B
        b03["('The', 'cat', 'is', '[MASK]')"]
        b33["('The', 'cat', 'was', '[MASK]')"]
        b43["('The', 'dog', 'is', '[MASK]')"]
        b63["('Your', 'cat', 'is', '[MASK]')"]
        
        b02["('The', 'cat', '[MASK]', 'black')"]
        b12["('The', 'cat', '[MASK]', 'white')"]
        b22["('The', 'cat', '[MASK]', 'sleepy')"]
        b42["('The', 'dog', '[MASK]', 'black')"]
        b52["('The', 'dog', '[MASK]', 'brown')"]
        b62["('Your', 'cat', '[MASK]', 'black')"]
        
        b01["('The', '[MASK]', 'is', 'black')"]
        b11["('The', '[MASK]', 'is', 'white')"]
        b21["('The', '[MASK]', 'is', 'sleepy')"]
        b31["('The', '[MASK]', 'was', 'sleepy')"]
        b51["('The', '[MASK]', 'is', 'brown')"]
        b61["('Your', '[MASK]', 'is', 'black')"]
        
        b00["('[MASK]', 'cat', 'is', 'black')"]
        b10["('[MASK]', 'cat', 'is', 'white')"]
        b20["('[MASK]', 'cat', 'is', 'sleepy')"]
        b30["('[MASK]', 'cat', 'was', 'sleepy')"]
        b40["('[MASK]', 'dog', 'is', 'black')"]
        b50["('[MASK]', 'dog', 'is', 'brown')"]
        
        b7["..."]
    end

    a0 --&gt; b03
    a1 --&gt; b03
    a2 --&gt; b03
    a3 --&gt; b33 
    a4 --&gt; b43 
    a5 --&gt; b43
    a6 --&gt; b63

    a0 --&gt; b02
    a1 --&gt; b12
    a2 --&gt; b22
    a3 --&gt; b22
    a4 --&gt; b42
    a5 --&gt; b52
    a6 --&gt; b62

    a0 --&gt; b01
    a1 --&gt; b11
    a2 --&gt; b21
    a3 --&gt; b31
    a4 --&gt; b01
    a5 --&gt; b51
    a6 --&gt; b61

    a0 --&gt; b00
    a1 --&gt; b10
    a2 --&gt; b20
    a3 --&gt; b30
    a4 --&gt; b40
    a5 --&gt; b50
    a6 --&gt; b00

    subgraph C
        c023["('The', 'cat', '[MASK]', '[MASK]')"]
        c423["('The', 'dog', '[MASK]', '[MASK]')"]
        c623["('Your', 'cat', '[MASK]', '[MASK]')"]

        c013["('The', '[MASK]', 'is', '[MASK]')"]
        c313["('The', '[MASK]', 'was', '[MASK]')"]
        c613["('Your', '[MASK]', 'is', '[MASK]')"]

        c003["('[MASK]', 'cat', 'is', '[MASK]')"]
        c303["('[MASK]', 'cat', 'was', '[MASK]')"]
        c403["('[MASK]', 'dog', 'is', '[MASK]')"]
        
        c023["('The', 'cat', '[MASK]', '[MASK]')"]
        c423["('The', 'dog', '[MASK]', '[MASK]')"]
        c623["('Your', 'cat', '[MASK]', '[MASK]')"]

        c012["('The', '[MASK]', '[MASK]', 'black')"]
        c112["('The', '[MASK]', '[MASK]', 'white')"]
        c212["('The', '[MASK]', '[MASK]', 'sleepy')"]
        c512["('The', '[MASK]', '[MASK]', 'brown')"]
        c612["('Your', '[MASK]', '[MASK]', 'black')"]

        c002["('[MASK]', 'cat', '[MASK]', 'black')"]
        c102["('[MASK]', 'cat', '[MASK]', 'white')"]
        c202["('[MASK]', 'cat', '[MASK]', 'sleepy')"]
        c402["('[MASK]', 'dog', '[MASK]', 'black')"]
        c502["('[MASK]', 'dog', '[MASK]', 'brown')"]
        
        c001["('[MASK]', '[MASK]', 'is', 'black')"]
        c101["('[MASK]', '[MASK]', 'is', 'white')"]
        c201["('[MASK]', '[MASK]', 'is', 'sleepy')"]
        c301["('[MASK]', '[MASK]', 'was', 'sleepy')"]
        c501["('[MASK]', '[MASK]', 'is', 'brown')"]
        
        c7["..."]
    end
    B ==&gt; C
    subgraph D
        d0012["('[MASK]', '[MASK]', '[MASK]', 'black')"]
        d1012["('[MASK]', '[MASK]', '[MASK]', 'white')"]
        d2012["('[MASK]', '[MASK]', '[MASK]', 'sleepy')"]
        d5012["('[MASK]', '[MASK]', '[MASK]', 'brown')"]

        d0013["('[MASK]', '[MASK]', 'is', '[MASK]')"]
        d3013["('[MASK]', '[MASK]', 'was', '[MASK]')"]

        d0023["('[MASK]', 'cat', '[MASK]', '[MASK]')"]
        d4023["('[MASK]', 'dog', '[MASK]', '[MASK]')"]

        d0123["('The', '[MASK]', '[MASK]', '[MASK]')"]
        d6123["('Your', '[MASK]', '[MASK]', '[MASK]')"]

        d7["..."]
    end
    c023 --&gt; d0023
    c423 --&gt; d0023
    c013 --&gt; d0013
    c313 --&gt; d3013
    c613 --&gt; d0013
    c003 --&gt; d0013
    c303 --&gt; d3013
    c403 --&gt; d0013
    c623 --&gt; d0023
    c012 --&gt; d0012
    c112 --&gt; d1012
    c212 --&gt; d2012
    c512 --&gt; d5012
    c612 --&gt; d0012
    c002 --&gt; d0012
    c102 --&gt; d1012
    c202 --&gt; d2012
    c402 --&gt; d0012
    c502 --&gt; d5012
    c001 --&gt; d0012
    c101 --&gt; d1012
    c201 --&gt; d2012
    c301 --&gt; d2012
    c501 --&gt; d5012

    c023 --&gt; d0123
    c423 --&gt; d0123
    c013 --&gt; d0123
    c313 --&gt; d0123
    c613 --&gt; d6123
    c003 --&gt; d0023
    c303 --&gt; d0023
    c403 --&gt; d4023
    c623 --&gt; d6123
    c012 --&gt; d0123
    c112 --&gt; d0123
    c212 --&gt; d0123
    c512 --&gt; d0123
    c612 --&gt; d6123
    c002 --&gt; d0023
    c102 --&gt; d0023
    c202 --&gt; d0023
    c402 --&gt; d0023
    c502 --&gt; d0023
    c001 --&gt; d0013
    c101 --&gt; d0013
    c201 --&gt; d0013
    c301 --&gt; d3013
    c501 --&gt; d0013

    subgraph E
        e0["('[MASK]', '[MASK]', '[MASK]', '[MASK]')"]
    end
    d0012 --&gt; e0
    d1012 --&gt; e0
    d2012 --&gt; e0
    d5012 --&gt; e0

    d0013 --&gt; e0
    d3013 --&gt; e0

    d0023 --&gt; e0
    d4023 --&gt; e0

    d0123 --&gt; e0
    d6123 --&gt; e0
   
    LeftMarginHack:::phantom ~~~ a0
    e0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>(There should be 3 arrows leaving each of Bob’s messages toward Carol, please pardon me for eschewing them.)</p> <p>While <code class="language-plaintext highlighter-rouge">MashLastToken</code> was strictly destroying information, here <code class="language-plaintext highlighter-rouge">MashOneRandomUnmaskedToken</code> both generates information (which token is to be masked) and destroys information (mash together all messages that would be the same if it were not for the randomly-selected unmasked token). However, the newly generated information (masking order) is also destroyed by the process. In the end, we still get a singleton: all information is eventually destroyed.</p> <p>The point I’m trying to make here is that <code class="language-plaintext highlighter-rouge">MashOneRandomUnmaskedToken</code> is a richer approach to “divide and conquer” than <code class="language-plaintext highlighter-rouge">MashLastToken</code> was. It is more “organic” in a similar sense that mixing milk in coffee is “organic”: it’s a mess, a <em>rich</em> mess. We can train a model to <code class="language-plaintext highlighter-rouge">GuessOneRandomMaskToken</code> on that mess, and yes, I could excuse it to train slower than <code class="language-plaintext highlighter-rouge">GuessNextToken</code>, because <code class="language-plaintext highlighter-rouge">GuessNextToken</code> is a subset of what <code class="language-plaintext highlighter-rouge">GuessOneRandomMaskToken</code> is learning. Let me make it explicit.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0["('The', 'cat', 'is', 'black')"]
        a1["('The', 'cat', 'is', 'white')"]
        a2["('The', 'cat', 'is', 'sleepy')"]
        a3["('The', 'cat', 'was', 'sleepy')"]
        a4["('The', 'dog', 'is', 'black')"]
        a5["('The', 'dog', 'is', 'brown')"]
        a6["('Your', 'cat', 'is', 'black')"]
        a7["..."]
    end
    subgraph B
        b0["('The', 'cat', 'is', '[MASK]')"]
        b1["('The', 'cat', 'was', '[MASK]')"]
        b2["('The', 'dog', 'is', '[MASK]')"]
        b3["('Your', 'cat', 'is', '[MASK]')"]
        b4["..."]
    end
    subgraph C
        c0["('The', 'cat', '[MASK]', '[MASK]')"]
        c1["('The', 'dog', '[MASK]', '[MASK]')"]
        c2["('Your', 'cat', '[MASK]', '[MASK]')"]
        c3["..."]
    end
    subgraph D
        d0["('The', '[MASK]', '[MASK]', '[MASK]')"]
        d1["('Your', '[MASK]', '[MASK]', '[MASK]')"]
        d2["..."]
    end
    subgraph E
        e0["('[MASK]', '[MASK]', '[MASK]', '[MASK]')"]
    end
   
    a0 --&gt; b0 --&gt; c0 --&gt; d0 --&gt; e0
    a1 --&gt; b0
    a2 --&gt; b0
    a3 --&gt; b1 --&gt; c0
    a4 --&gt; b2 --&gt; c1 --&gt; d0
    a5 --&gt; b2
    a6 --&gt; b3 --&gt; c2 --&gt; d1 --&gt; e0

    LeftMarginHack:::phantom ~~~ a0
    e0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>We could train a model to <code class="language-plaintext highlighter-rouge">GuessOneRandomMaskToken</code> then, at inference, use the learned weight to perform the <code class="language-plaintext highlighter-rouge">GuessNextToken</code> task instead. In the scenario where we’re data-starved, but we can train for as long as we wish to, with a model as big as it needs to,<d-cite key="ni2025diffusion"></d-cite> and inference is performed using <code class="language-plaintext highlighter-rouge">GuessNextToken</code>, which model would you put your money on: the one trained solely on this specific <code class="language-plaintext highlighter-rouge">GuessNextToken</code> task, or the one that saw the worst <d-cite key="kim2025train"></d-cite> that <code class="language-plaintext highlighter-rouge">GuessOneRandomMaskToken</code> had to offer?</p> <h2 id="shuffling-everything">Shuffling Everything</h2> <p>In the previous section, we saw that both autoregressive language models and mask diffusion language models learn to mimic Alice’s message distribution by iteratively <code class="language-plaintext highlighter-rouge">Mash</code>ing them toward a singleton, reaching a state with no information about anything, thus no information about Alice. Here we consider the other leading approach to destroying Alice’s message: drowning it into irrelevant information.</p> <p>This mechanism, which I call <code class="language-plaintext highlighter-rouge">Shuffle</code>, may be more aligned with the historic/physical meaning of the word “diffusion”.<d-footnote>In other words, while I expect that some readers could say that `Mash` isn't "real" diffusion, I don't have the same worry for `Shuffle`.</d-footnote> Whereas <code class="language-plaintext highlighter-rouge">Mash</code> <em>may</em> generate information (e.g., masking order) but will eventually destroy everything to a singleton, <code class="language-plaintext highlighter-rouge">Shuffle</code> <em>must</em> generate new information to be folded into Alice’s message, drowning it in the noise.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b0[0]
        b1[1]
        b2[2]
    end

    a0 --tail---&gt; b0
    a2 --tail---&gt; b0
    a0 --head---&gt; b1
    A ==Shuffle===&gt; B
    a1 --head---&gt; b1
    a2 --head---&gt; b2
    a1 --tail---&gt; b2

    LeftMarginHack:::phantom ~~~ a0
    b0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>If Bob has a good understanding of the <code class="language-plaintext highlighter-rouge">Shuffle</code> channel, he can model the situation as</p> \[\begin{bmatrix} \textup{P}(B=0) \cr \textup{P}(B=1) \cr \textup{P}(B=2) \end{bmatrix} = \begin{bmatrix} 1/2 &amp; 0 &amp; 1/2 \cr 1/2 &amp; 1/2 &amp; 0 \cr 0 &amp; 1/2 &amp; 1/2 \end{bmatrix} \begin{bmatrix} \textup{P}(A=0) \cr \textup{P}(A=1) \cr \textup{P}(A=2) \end{bmatrix} .\] <p>When he observes $B=0$, he knows that Alice said either $0$ or $2$, but he’s not sure which: information was destroyed, and he may learn to to generate $A’$, seeking to capture Alice’s marginal probability distribution $\textup{P}(A)$.</p> <p>But notice that (irrelevant) information was also created: when $B=0$, a Bob that understands the channel knows with certainty that the coin flip was “tail” (whereas this coin information was completely unknown to Alice). In my view, <code class="language-plaintext highlighter-rouge">Shuffle</code> can destroy information about the original message <em>because</em> it generates irrelevant information and has to fold the outcome in as many possible messages than there were before. The message $A$ can take 3 values, the coin can take two values, the product of the coin and the message can take 6 values, and somehow this has to be crammed into the 3 possible values for $B$. Something has to give; information about Alice is destroyed.</p> <p>Next Bob can message Carol through the same <code class="language-plaintext highlighter-rouge">Shuffle</code> channel, and the same procedure can continue up to Zalgo.</p> \[\begin{bmatrix} \textup{P}(Z=0) \cr \textup{P}(Z=1) \cr \textup{P}(Z=2) \end{bmatrix} = \begin{bmatrix} 1/2 &amp; 0 &amp; 1/2 \cr 1/2 &amp; 1/2 &amp; 0 \cr 0 &amp; 1/2 &amp; 1/2 \end{bmatrix}^{25} \begin{bmatrix} \textup{P}(A=0) \cr \textup{P}(A=1) \cr \textup{P}(A=2) \end{bmatrix}\] <p>Now, technically, the probability distribution $\textup{P}(Z)$ still depends on $\textup{P}(A)$, but what does that mean in practice? Well, it can be shown that</p> \[0.33333331 &lt; P(Z=z) &lt; 0.33333335 \quad \forall z \in \{0,1,2\} ,\] <p>irrespective of Alice’s probability distribution. The crux of the <code class="language-plaintext highlighter-rouge">Shuffle</code> learning strategy here amounts to approximating Zalgo as a uniform distribution:</p> \[P(Z'=z') = \frac{1}{3} \quad \forall z' \in \{ 0,1,2 \}.\] <p>So, whereas <code class="language-plaintext highlighter-rouge">Mash</code> eventually takes us to a singleton, <code class="language-plaintext highlighter-rouge">Shuffle</code> takes us to a known distribution, simple to sample. Except for this detail, inference for <code class="language-plaintext highlighter-rouge">Shuffle</code> proceeds the same way as <code class="language-plaintext highlighter-rouge">Mash</code>: we sample $Z’$ from the known distribution, then apply the respective learned <code class="language-plaintext highlighter-rouge">Guess</code> down the alphabet until we get $A’$.</p> <p>Like the previous section adapted the spirit of <code class="language-plaintext highlighter-rouge">Mash</code> to <code class="language-plaintext highlighter-rouge">MashOneRandomUnmaskedToken</code>, we can adapt <code class="language-plaintext highlighter-rouge">Shuffle</code> to obtain <code class="language-plaintext highlighter-rouge">ShuffleOneRandomToken</code>: a random token position gets substituted by a token selected uniformly at random from the tokenizer’s vocabulary. This is basically the “Uniform” noising process used in SEDD.<d-cite key="lou2024discrete"></d-cite></p> <p>For the rest of this section, let’s quickly consider what “shuffling” means in continuous space. Suppose Alice’s message is a $w \times h \times 3$ tensor of real numbers in the $[-1,1]$ interval representing an RGB picture of $w$ pixels wide by $h$ pixels high. We may define the <code class="language-plaintext highlighter-rouge">GaussianShuffle(δ)</code> communication channel such that it adds independent Gaussian noise (with mean $0$ and variance $0 &lt; \delta \ll 1$) to each entry of this tensor.</p> <p>This channel generates (irrelevant) information: for a given message from Alice, there are many possible options for what Bob may receive; arrows branching out means that information is generated. But the channel also destroys information: there are many messages that Alice could have said that may explain a given message received by Bob; arrows converging in mean that information is destroyed. As for <code class="language-plaintext highlighter-rouge">Shuffle</code>, <code class="language-plaintext highlighter-rouge">GaussianShuffle(δ)</code> destroys information about Alice’s message by generating irrelevant information and folding it into limited space.<d-footnote>The story is a bit more complex here because, in a strict mathematical sense, specifying a single real number requires an infinite amount of information. One can work around this by considering differential entropy instead of entropy. However, for computer science applications, there is a much simpler resolution: these "real" numbers are represented as discrete data types. Concretely, a `float32` is really a discrete variable that may take one of $2^{32}$ values, thus capturing at most 32 bits of information.</d-footnote></p> <p>The impact of $n$ repeated independent applications of <code class="language-plaintext highlighter-rouge">GaussianShuffle(δ)</code> is a single “bigger” <code class="language-plaintext highlighter-rouge">GaussianShuffle(n*δ)</code>.<d-footnote>Recall that the convolution of two Gaussians is a Gaussian whose mean and variance is the sum of their respective means and variances.</d-footnote> Similarly to how we approximated Zalgo as a uniform distribution earlier, we can choose an $n$ that is high enough so that the outcome of applying <code class="language-plaintext highlighter-rouge">GaussianShuffle(n*δ)</code> on any image is basically an $w \times h \times 3$ Gaussian with mean zero and variance $n\delta$, i.e., no dependency worth mentioning on the actual image. In essence, this amounts to the “Variance Exploding” process from Song <em>et al.</em> <d-cite key="song2021scorebased"></d-cite>; the “Variance Preserving” version could be similarly obtained by appropriately scaling down the outcome after each addition of Gaussian noise.</p> <h2 id="beyond-markov">Beyond Markov</h2> <p>In our quest to divide-and-conquer the learning of Alice’s distribution, we have seen two different strategies to destroy all the information in her message: <code class="language-plaintext highlighter-rouge">Mash</code> everything into a singleton, and <code class="language-plaintext highlighter-rouge">Shuffle</code> long enough to get a maximum-entropy distribution (e.g., uniform or Gaussian).</p> <p>Both these strategies are Markovian: each person’s message has all the information required to get the probability distribution for the next person’s message. Stated differently, the “spine” of all my diagrams looked like this.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0:::phantom
    end
    subgraph B
        b0:::phantom
    end
    subgraph C
        c0:::phantom
    end
    subgraph D["..."]
        d0:::phantom
    end
    subgraph Z
        z0:::phantom
    end
    A ==&gt; B ==&gt; C ==&gt; D ==&gt; Z
    
    LeftMarginHack:::phantom ~~~ a0
    z0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>But this is not the only possibility, and many diffusion models have a non-Markovian approach to destruction and/or generation. For example, although DDPM’s<d-cite key="ho2020denoising"></d-cite> destroying process may be framed as a Markovian chain, we may also frame it as interpolating between a “clean” sample $A’$ and a “noise distribution” $Z$</p> \[B' = \sqrt{\alpha_B} A' + \sqrt{1 - \alpha_B} Z', \quad C' = \sqrt{\alpha_C} A' + \sqrt{1 - \alpha_C} Z', \quad \cdots \ ,\] <p>given appropriate $1 = \alpha_A \ge \alpha_B \ge \alpha_C \ge \cdots \ge \alpha_Z = 0$. Below is a diagram showing the Markovian approach on the top-right and the non-Markovian one on the bottom-left.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0:::phantom
    end
    subgraph B
        b0:::phantom
    end
    subgraph C
        c0:::phantom
    end
    subgraph D["..."]
        d0:::phantom
    end
    subgraph Z
        z0:::phantom
    end
    subgraph Ap["A'"]
        ap0:::phantom
    end
    subgraph Bp["B'"]
        bp0:::phantom
    end
    subgraph Cp["C'"]
        cp0:::phantom
    end
    subgraph Zp["Z'"]
        zp0:::phantom
    end
    subgraph ApxZp["A' × Z'"]
        azp0:::phantom
    end
    A ==&gt; B ==&gt; C ==&gt; D ==&gt; Z
    ApxZp ==&gt; Ap -.- A
    ApxZp ==&gt; Bp -.- B
    ApxZp ==&gt; Cp -.- C
    ApxZp ==&gt; Zp -.- Z
    
    LeftMarginHack:::phantom ~~~ a0
    z0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Here I used $A’ \times Z’$ to denote someone that has simultaneous access to both $A’$ and $Z’$ messages, and is thus able to use the previous equation to obtain $B’$, $C’$, <em>etc.</em></p> <p>But once you’ve used $A’ \times Z’$ to calculate, say, $B’$, you could just <em>not discard</em> $Z’$, and obtain $B’ \times Z’$. The same holds up to $Y’ \times Z’$ and, with a little more algebra, we can actually go back to $A’ \times Z’$ (ignoring finite-precision errors). I represent such information-preserving back-and-forth conversions using bidirectional arrows.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph ApxZp["A' × Z'"]
        apzp0:::phantom
    end
    subgraph BpxZp["B' × Z'"]
        bpzp0:::phantom
    end
    subgraph CpxZp["C' × Z'"]
        cpzp0:::phantom
    end
    subgraph DpxZp["..."]
        dpzp0:::phantom
    end
    subgraph YpxZp["Y' × Z'"]
        ypzp0:::phantom
    end
    subgraph Ap["A'"]
        ap0:::phantom
    end
    subgraph Bp["B'"]
        bp0:::phantom
    end
    subgraph Cp["C'"]
        cp0:::phantom
    end
    subgraph Yp["Y'"]
        yp0:::phantom
    end
    subgraph Zp["Z'"]
        zp0:::phantom
    end
    subgraph BpxZpp["B' × Z''"]
        bpzpp0:::phantom
    end
    subgraph CpxZppp["C' × Z'''"]
        cpzppp0:::phantom
    end
    subgraph YpxZpppp["Y' × Z'''''"]
        ypzpppp0:::phantom
    end
    subgraph App["A''"]
        app0:::phantom
    end
    subgraph A
        a0:::phantom
    end
    subgraph B
        b0:::phantom
    end
    subgraph C
        c0:::phantom
    end
    subgraph D["..."]
        d0:::phantom
    end
    subgraph Y
        y0:::phantom
    end
    subgraph Z
        z0:::phantom
    end
    
    Ap ==&gt; ApxZp &lt;==&gt; BpxZp &lt;==&gt; CpxZp &lt;==&gt; DpxZp &lt;==&gt; YpxZp ==&gt; Zp
    BpxZp ==&gt; Bp ==&gt; BpxZpp -."B'".- BpxZp
    CpxZp ==&gt; Cp ==&gt; CpxZppp -."C'".- CpxZp
    YpxZp ==&gt; Yp ==&gt; YpxZpppp -."Y'".- YpxZp
    A ==&gt; B ==&gt; C ==&gt; D ==&gt; Y ==&gt; Z
    ApxZp ==&gt; App -.- A
    Ap -.- A
    Bp -.- B
    Cp -.- C
    Yp -.- Y
    Zp -.- Z

    LeftMarginHack:::phantom ~~~ ap0
    z0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Projecting $B’ \times Z’$ to $B’$ destroys information about $Z’$ and, without it, we cannot get back to $A’$: this projection destroys information about $A’$. We may thus train a model to use $B’$ to predict the “missing noise” $Z’’$, with the intent to obtain a $B’ \times Z’’$ whose distribution matches $B’ \times Z’$. Equation (14) from DDPM<d-cite key="ho2020denoising"></d-cite> learns such denoising functions.</p> <h2 id="about-the-diagrams">About the Diagrams</h2> <p>There exist many types of <a href="https://en.wikipedia.org/wiki/Graphical_model">probabilistic graphical models</a>. Some are meant to be general-purpose, others are more specialized. Most are meant for empirical/scientific use: we translate our intuition of what are the non-impossible dependencies between stochastic variables, then we may analyze empirical observations under those assumptions.</p> <p>This blogpost introduces a new kind of probabilistic graphical model, which I propose to call <em>generative commutative diagrams</em>. Unlike many other kinds of graphical models, those used here are meant for prescription/engineering use. They read like code: an arrow is like a procedure you can call to transform the origin node into the target node.</p> <p>As the name “generative commutative diagrams” implies, they have been inspired by the <a href="https://en.wikipedia.org/wiki/Commutative_diagram">commutative diagrams</a> used by mathematicians, especially <a href="https://en.wikipedia.org/wiki/Category_theory">category theorists</a>. This blogpost is not about category theory, you do not need to know nor care about category theory to read it, and I am myself definitely <em>not</em> an expert on category theory.</p> <p>Most of the arrows used in this blogpost do <em>not</em> represent morphisms, but something less-than-a-morphism. I call them “harpoons” as I denote them $\rightharpoonup$ when I draw on a whiteboard or in $\texttt{tikz-cd}$, and I reserve $\rightarrow$ for proper morphisms. For this blogpost, I haven’t figured out how to coerce $\texttt{mermaid}$ into making different arrow heads: almost all the arrows show here should be $\rightharpoonup$. <code class="language-plaintext highlighter-rouge">Project</code> and <code class="language-plaintext highlighter-rouge">Mash</code> are among the rare examples that are proper morphisms, which I would usually draw as $\rightarrow$.</p> <p>An introduction to category theory will often start with something like this.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph X
        x0:::phantom
    end
    subgraph Y
        y0:::phantom
    end
    subgraph Z
        z0:::phantom
    end
    X ==f==&gt; Y ==g==&gt; Z
    X ==h==&gt; Z

    LeftMarginHack:::phantom ~~~ x0
    z0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>They will then mention “this diagram is commutative”, which means that $g(f(x)) = h(x)$ for all $x$. When we show the little arrows between the <del>messages</del> objects, this means that the paths going from $X$ to $Z$ by following $f$ then $g$ must all agree with the paths directly using $h$.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph X
        x0["0"]
        x1["1"]
        x2["2"]
    end
    subgraph Y
        y0["0"]
        y1["1"]
    end
    subgraph Z
        z0["0"]
        z1["1"]
    end
    X ==f==&gt; Y ==g==&gt; Z
    X ==h==&gt; Z
    x0 --&gt; y0 --&gt; z0
    x1 --&gt; z0
    x1 --&gt; y0
    x2 --&gt; z1
    x2 --&gt; y1 --&gt; z1
    x0 --&gt; z0

    LeftMarginHack:::phantom ~~~ x0
    z0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>In a generative commutative diagrams, all the big arrows (harpoons and proper morphisms included) <strong>must also satisfy this requirement of commutativity.</strong></p> <p>Next, that introduction to category theory may give the following diagram.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph X
        x0:::phantom
    end
    subgraph Y
        y0:::phantom
    end
    subgraph Z
        z0:::phantom
    end
    X ==f==&gt; Y ==g==&gt; Z

    LeftMarginHack:::phantom ~~~ x0
    z0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>They will then say that because $f$ and $g$ are morphisms, this picture is equivalent to the first one: we can always combine morphisms head-to-tail to obtain an implicit morphism, here $h = g \circ f$. <strong>This is not true for my harpoons.</strong> If either or both $f$ and $g$ are harpoons, the best you can get by chaining them is a way to sample the same probability distribution<d-footnote>Again, these mermaid diagrams use a dashed line to represent "has the same marginal distribution as", but elsewhere I prefer tikz-cd's squiggly line, in reference to the "tilde" symbol that is often used to mean "distributed like".</d-footnote> – <em>not</em> a guarantee that you will end up at the same element.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph X
        x0:::phantom
    end
    subgraph Y
        y0:::phantom
    end
    subgraph Z
        z0:::phantom
    end
    subgraph Zp["Z'"]
        zp0:::phantom
    end
    X ==f==&gt; Y ==g==&gt; Z
    X =="g ○ f"==&gt; Zp -.- Z

    LeftMarginHack:::phantom ~~~ x0
    z0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Morphisms cannot create information, only destroy or preserve it. Harpoons can create, preserve and destroy information.</p> <p>Commutative diagrams that only contain morphisms may only tell “the destruction story”, the traditional direction of the data processing inequality. Harpoons allow you to also tell its dual, “the generation story”.</p> <p>To be clear, we didn’t get any real new narrative power: you can tell “the generation story” solely in terms of destruction.<d-footnote>Though you may have to narrate it from the perspective of Laplace's demon, a narrative device that already knows everything there is to be known.</d-footnote> In that view, harpoons are just “syntactic sugar” for a more involved combination of morphisms.</p> <p>I have no idea if there is interesting maths to be done with such harpoons and, if yes, perhaps it has already been done under some name that evaded my search. However, as a specialized kind of graphical model, I’ve found them particularly useful when thinking about diffusion models and their weird edge cases.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[I present diffusion models as part of a family of machine learning techniques that withhold information from a model’s input and train it to guess the withheld information. I argue that diffusion's destroying approach to withholding is more flexible than typical hand-crafted information withholding techniques, providing a rich training playground that could be advantageous in some settings, notably data-scarce ones. I then address subtle issues that may arise when porting reinforcement learning techniques to the diffusion context, and wonder how such exploration problems could be addressed in more diffusion-native ways. I do not have definitive answers, but I do point my fingers in directions I deem interesting. A tutorial follows this thesis, expanding on the destroy-then-generate perspective. A novel kind of probabilistic graphical models is introduced to facilitate the tutorial's exposition.]]></summary></entry><entry><title type="html">Sample Blog Post</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/distill-example</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/distill-example/"><![CDATA[<p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.liquid path="assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/iclr-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/iclr-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/iclr-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2025-04-27-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/9-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/9-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/7-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/7-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/8-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/8-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/8.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/10-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/10-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/10.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/11-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/11-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/11-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/11.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/12-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/12-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/12.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/7-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/7-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here’s how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong>). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %}
</code></pre></div></div> <p>For example, the following code can be used to generate the figure underneath it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2026-04-27-distill-example/plotly_demo_1.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span>
    <span class="na">src=</span><span class="s">"{{ 'assets/html/2026-04-27-distill-example/plotly_demo_1.html' | relative_url }}"</span>
    <span class="na">frameborder=</span><span class="s">"0"</span>
    <span class="na">scrolling=</span><span class="s">"no"</span>
    <span class="na">height=</span><span class="s">"600px"</span>
    <span class="na">width=</span><span class="s">"100%"</span>
  <span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>

</code></pre></div></div> <p>Voila!</p> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>

<span class="p">}</span></code></pre></figure> <hr/> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid.js</a> directly. Below, we generate examples of such diagrams using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a> syntax.</p> <p><strong>Note:</strong> To enable mermaid diagrams, you need to add the following to your post’s front matter:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">mermaid</span><span class="pi">:</span>
  <span class="na">enabled</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">zoomable</span><span class="pi">:</span> <span class="kc">true</span> <span class="c1"># optional, for zoomable diagrams</span>
</code></pre></div></div> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```mermaid
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
```
</code></pre></div></div> <pre><code class="language-mermaid">sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
</code></pre> <hr/> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p> <hr/> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item</li> </ol> <ul> <li>Unordered sub-list.</li> </ul> <ol> <li>Actual numbers don’t matter, just that it’s a number <ol> <li>Ordered sub-list</li> </ol> </li> <li> <p>And another item.</p> <p>You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>To have a line break without a paragraph, you will need to use two trailing spaces. Note that this line is separate, but within the same paragraph. (This is contrary to the typical GFM line break behavior, where trailing spaces are not required.)</p> </li> </ol> <ul> <li> <p>Unordered lists can use asterisks</p> </li> <li> <p>Or minuses</p> </li> <li> <p>Or pluses</p> </li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">Your MoE Model Does Not Have to Select Fixed Number of Experts</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/dynamic-routing/" rel="alternate" type="text/html" title="Your MoE Model Does Not Have to Select Fixed Number of Experts"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/dynamic-routing</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/dynamic-routing/"><![CDATA[<h2 id="what-is-dynamic-routing-and-why-is-it-important">What is Dynamic Routing and Why is it Important?</h2> <p>Supposing we have 8 cargo vans and we are running a moving company. The company has a famous regulation: <strong>each time we receive an order, we must assign 2 vans to deliver customers’ belongings.</strong><d-footnote>Similar to Top-2 expert selection out of 8 ones in MoE models, and each order represents a token.</d-footnote> Ideally, we should assign vans according to the order size. For example, we can assign 1 van to small orders and more vans to large orders. However, due to the regulation, we have to assign 2 vans to each order, even if the order is small or large.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-dynamic-routing/two-van-regulation-480.webp 480w,/2026/assets/img/2026-04-27-dynamic-routing/two-van-regulation-800.webp 800w,/2026/assets/img/2026-04-27-dynamic-routing/two-van-regulation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-dynamic-routing/two-van-regulation.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In the first case (left) of the above figure, it is a waste of money to use 2 vans for a suitcase and a teddy bear. In the last case (right), it may consume more time to deliver the order because the vans may need to make more trips to deliver all the belongings.<d-footnote>More trips means more time consumption if no stuff is abandoned due to limited space, similar to disabling token dropping in MoE training. If tokens are dropped, there may induce a performance degradation.<d-cite key="gale2023megablocks"></d-cite></d-footnote></p> <p>Normally, customers would like to have their belongings delivered cheaply and quickly. So here comes the questions:</p> <ol> <li><strong>Could we dynamically assign vans according to the order size?</strong></li> <li><strong>If we could dynamically assign vans, is it possible to reduce the overall cost and time consumption?</strong></li> </ol> <p>These are exactly the problems that dynamic routing aims to address in Mixture-of-Experts (MoE) models. For each token, a conventional router selects top-$k$ experts for computation, where $k$ is a fixed number. Let’s say there are $N_e$ experts in total, the router always selects top-$k$ experts for computation. The token’s original representation is denoted as $\mathbf{x} \in \mathbb{R}^d$. Then the token’s representation after the MoE layer is computed as:</p> \[y = \sum_{i=1}^{k} G_i(\mathbf{x}) \cdot E_i(\mathbf{x}),\] \[G(\mathbf{x}) = \text{Top-}k(\text{softmax}(\mathbf{x} \mathbf{W}_g)),\] <p>where $G_i(\mathbf{x})$ is the routing probability of the selected $i$-th expert, $E_i(\mathbf{x})$ is the selected $i$-th expert network, and $\mathbf{W}_g \in \mathbb{R}^{d \times N_e}$ is the weight matrix for the router.</p> <p>Briefly, the router network computes the routing probabilities for all experts, and then the token’s representation is computed as the weighted sum of the selected experts:</p> <pre><code class="language-mermaid">graph TD
    T[Token Input] --&gt; R{Router}
    
    R -.-&gt; E1[Expert 1]
    R -- "Prob: 0.4 (Top-1)" --&gt; E2[Expert 2]
    R -.-&gt; E3[Expert 3]
    R -.-&gt; E4[Expert 4]
    R -.-&gt; E5[Expert 5]
    R -- "Prob: 0.3 (Top-2)" --&gt; E6[Expert 6]
    R -.-&gt; E7[Expert 7]
    R -.-&gt; E8[Expert 8]
    
    E2 --&gt; Out[Output]
    E6 --&gt; Out

    style E2 fill:#ff9999,stroke:#333,stroke-width:2px
    style E6 fill:#ff9999,stroke:#333,stroke-width:2px
</code></pre> <p>However, different tokens may have different expert selection preferences. For example, for complex reasoning tasks, a token may be forwarded to more experts for better performance. While in simple tasks, a token may be forwarded to fewer experts for better efficiency.</p> <pre><code class="language-mermaid">graph LR
    subgraph Simple [Simple Token Activates Few Experts]
        direction LR
        S_E1[Expert 1]:::inactive ~~~ S_E2[Expert 2]:::active ~~~ S_E3[Expert 3]:::inactive ~~~ S_E4[Expert 4]:::inactive ~~~ S_E5[Expert 5]:::inactive ~~~ S_E6[Expert 6]:::inactive ~~~ S_E7[Expert 7]:::inactive ~~~ S_E8[Expert 8]:::inactive
    end

    classDef active fill:#ff9999,stroke:#333,stroke-width:2px;
    classDef inactive fill:#fff,stroke:#999,stroke-dasharray: 3 3;
</code></pre> <pre><code class="language-mermaid">graph LR
    subgraph Complex [Complex Token Activates Many Experts]
        direction LR
        C_E1[Expert 1]:::active ~~~ C_E2[Expert 2]:::active ~~~ C_E3[Expert 3]:::active ~~~ C_E4[Expert 4]:::active ~~~ C_E5[Expert 5]:::inactive ~~~ C_E6[Expert 6]:::inactive ~~~ C_E7[Expert 7]:::inactive ~~~ C_E8[Expert 8]:::inactive
    end

    classDef active fill:#ff9999,stroke:#333,stroke-width:2px;
    classDef inactive fill:#fff,stroke:#999,stroke-dasharray: 3 3;
</code></pre> <p>Here, let’s transform the above moving company’s problems into the MoE context:</p> <ol> <li><strong>Could a router dynamically select the number of experts for each token?</strong> <ul> <li>Yes, there are some techniques to dynamically select the number of experts for each token!</li> </ul> </li> <li><strong>Does dynamic routing help improve performance and reduce time consumption?</strong> <ul> <li>Yes, dynamic routing could be both effective and efficient!</li> </ul> </li> </ol> <p>In this post, we will provide a brief introduction to MoE dynamic routing techniques and their applications in LLMs. Now let’s dive into the details!</p> <h2 id="how-to-apply-dynamic-routing">How to Apply Dynamic Routing?</h2> <p>There are three main categories of dynamic routing techniques:</p> <ol> <li><strong>Thresholding</strong>: Instead of selecting top-k experts for each token, the router selects experts based on a threshold.</li> <li><strong>Dynamic Proposer</strong>: An additional proposer module for predicting the number of activated experts for each token.</li> <li><strong>Zero-Computation Experts</strong>: Some special experts are regarded as placeholders for routing, but not used for computation.</li> </ol> <p>Here, we will introduce these three categories of dynamic routing techniques in detail.</p> <h3 id="thresholding">Thresholding</h3> <p>Thresholding is the simplest and most straightforward way to apply dynamic routing. It works by setting a threshold for the probability of the experts. If the probability is greater than the threshold, the expert is activated. Otherwise, the expert is deactivated.</p> <p><strong>Cumulative Thresholding:</strong> Huang et al. (2024)<d-cite key="huang2024topp"></d-cite> proposes MoE-Dynamic, which is a thresholding-based dynamic routing technique to select the top-p experts for each token. As the figure below shows, MoE-Dynamic sorts the routing probabilities of experts in descending order, and then selects experts by setting a threshold for the cumulative probability of experts. When the cumulative probability is greater than the threshold, the experts are activated.</p> <div class="l-body"> <iframe src="/2026/assets/html/2026-04-27-dynamic-routing/top-p.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <p>However, the routing distribution may tend to be uniform if the model is trained from scratch without any regularization because the model tends to activate more experts to obtain a better performance. To address this issue, the authors propose a regularization loss (probability entropy loss) on the routing probabilities $P$ to encourage a sharper distribution:</p> \[\mathcal{L}_{\text{reg}} = -\sum_{i=1}^{N_e} P_i \log P_i\] <p>From the experimental results, MoE-Dynamic shows its effectiveness to:</p> <ol> <li>achieve better performance than vanilla MoE (~1.68% improvements)</li> <li>accelerate the model training and inference throughput by 5% compared to vanilla MoE</li> <li>show different number of activated experts for different tasks</li> </ol> <p><strong>Trainable Thresholding:</strong> Different from cumulative thresholding, DynMoE<d-cite key="guo2025dynmoe"></d-cite> proposes a top-any gating function with gating parameters $\mathbf{W}_g \in \mathbb{R}^{d \times N_e}$ and trainable thresholds $\theta \in \mathbb{R}^{N_e}$. To determine if an expert should be activated, DynMoE first computes the cosine similarity $s$ between the token’s representation $\mathbf{x}$ and the router $\mathbf{W}_g$, then applies a sigmoid function $\sigma(\cdot)$ to normalize the similarity to [0, 1]. After that, it compares the normalized similarity $\sigma(s)$ with the trainable thresholds $\sigma(\mathbf{\theta})$ to determine if the expert should be activated.</p> \[s(\mathbf{x}) = \frac{\langle \mathbf{x}, \mathbf{W}_g \rangle}{\|\mathbf{x}\| \|\mathbf{W}_g\|},\] \[G(\mathbf{x}) = \text{sign}\left(\sigma\left(s(\mathbf{x})\right) - \sigma\left(\mathbf{\theta}\right)\right),\] <p>where the number of activated experts is $\sum_{i=1}^{N_e} G_i(\mathbf{x})$.</p> <p>As the figure below shows, DynMoE can dynamically select the number of activated experts by setting different thresholds for different experts.</p> <div class="l-body"> <iframe src="/2026/assets/html/2026-04-27-dynamic-routing/sigmoid-threshold.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <p>To enable diverse routing and suppress uniform routing, DynMoE also introduces a regularization loss term, including a diverse loss to lower the similarities within routers and a simplicity loss to maintain numerical stability.</p> \[\mathcal{L}_{\text{reg}} = \underbrace{\left\|\mathbf{W}_g^T \mathbf{W}_g - \mathbf{I}_{N_e}\right\|_2}_{\text{diversity loss}} + \underbrace{\frac{1}{N_e} \sum_{i=1}^{N_e} \left\|\mathbf{w}_{g,i}\right\|_2}_{\text{simplicity loss}},\] <p>where $\mathbf{I}_{N_e}$ is the identity matrix of size $\mathbb{R}^{N_e \times N_e}$.</p> <p>The results demonstrate that DynMoE can achieve comparable performance than vanilla MoE with 11% throughput improvements.</p> <p><strong>Non-Linear Thresholding:</strong> Wang et al. (2025)<d-cite key="wang2025remoe"></d-cite> proposes ReMoE, which is a non-linear thresholding-based dynamic routing technique to select experts for each token. Instead of setting a threshold as a hyper-parameter, ReMoE finds activated experts by replacing the Top-$k$ softmax with a ReLU activation function.</p> \[G(\mathbf{x}) = \text{ReLU}(\mathbf{x} \mathbf{W}_g),\] <p>Due to the non-linearity of ReLU, experts with smaller activations would be filtered out, and the number of activated experts is: $\sum_{i=1}^{N_e} \mathbb{1}{G_i(\mathbf{x}) &gt; 0}$. As shown in the figure below, due to the ReLU activation, experts with positive routing logits would be activated. Besides, the ReLU gate is fully differentiable, which allows a stable training process.</p> <div class="l-body"> <div class="row"> <iframe src="/2026/assets/html/2026-04-27-dynamic-routing/relu.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> </div> </div> <p>To constrain the number of expected activated experts, ReMoE introduces a sparsity regularization loss term:</p> \[\mathcal{L}_{\text{reg}}(t) = \lambda(t) \cdot \sum_{i=1}^{N_e} G_i(\mathbf{x}),\] \[\lambda(t) = \lambda(t-1) \cdot \alpha^{\text{sign}((1 - N_k/N_e) - S(t))},\] \[S(t) = 1 - \frac{1}{N_l N_e} \sum_{l=1}^{N_l} \sum_{i=1}^{N_e} \mathbb{1}\{G_i^l(\mathbf{x}) &gt; 0\},\] <p>where $\lambda(t)$ is an adaptive coefficient at training step $t$, which is initialized with a small value. $\alpha &gt; 1$ is a hyper parameter to control the update multiplier, and $S(t)$ represents the sparsity of the activated experts at training step $t$. Here the sparsity is computed over $N_l$ layers. The whole regularization loss is designed to close the gap between the number of expected activated experts $N_k$ and the actual number of activated experts. Therefore, ReMoE does not bring training or inference acceleration, but it can adaptively adjust the number of activated experts on each layer to improve the final performance.</p> <p>BlockFFN<d-cite key="song2025blockffn"></d-cite> further improves the ReLU activation with an RMS layer normalization to control the magnitude of the activations and stabilize the training process.</p> \[G(\mathbf{x}) = \text{RMSNorm}(\text{ReLU}(\mathbf{x} \mathbf{W}_g)).\] <p>BlockFFN also proposes an activation locality loss to encourage similar activation patters in neighboring tokens, and a chunk sparsification loss to maximize the sparsity of a chunk of consecutive tokens. Based on these settings and specially designed kernels, BlockFFN achieves 3.67x speedup over baseline and shows better commonsense reasoning ability compared to ReMoE especially on larger models.</p> <h3 id="dynamic-proposer">Dynamic Proposer</h3> <p>Intuitively, dynamic routing aims to change the value of $k$ for each token. Besides the above thresholding-based ideas, can we directly predict the value of $k$? The answer is YES, and that is exactly the idea of dynamic proposer. Dynamic proposers take the token’s representation as input and output the number of activated experts as a prediction.</p> <p>Ada-K<d-cite key="yue2025adak"></d-cite> applies an additional linear projection to the token’s representation to obtain a probability distribution and then samples the activated expert number $k$ from the distribution.</p> \[P(\mathbf{x}) = \text{softmax}(\mathbf{x} \mathbf{W}_p),\] \[k \sim P(\mathbf{x}),\] <p>where $\mathbf{W}_p \in \mathbb{R}^{d \times N_e}$ is a trainable parameter. The figure below demonstrates the sampling process for training. After clicking on the <code class="language-plaintext highlighter-rouge">Sample k</code> button, Ada-K would sample the $k$ value based on the probability distribution.</p> <div class="l-body"> <div class="row"> <iframe src="/2026/assets/html/2026-04-27-dynamic-routing/adak_sampling.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> </div> </div> <p>Because such a sampling process is not differentiable, Ada-K updates $\mathbf{W}_p$ with the Proximal Policy Optimization (PPO) algorithm<d-cite key="schulman2017ppo"></d-cite>, and take the log likelihood of language modeling as the reward:</p> \[r = \log P(x_t | x_1, \dots, x_{t-1}),\] <p>where $x_t$ is the expected token to be generated.</p> <p>The sampling process is useful for the training process since it brings stochasticity to explore different efficiency-performance trade-offs and compute policy gradients for PPO. During inference, Ada-K simply selects $k$ with the highest probability as the activated expert number.</p> <p>For pre-trained language models, Ada-K is efficient since it only updates the linear projection matrix $\mathbf{W}_p$ and keeps all other parameters frozen. For example, when applying Ada-K on Qwen1.5-MoE-14.3B-A2.7B<d-footnote>https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B</d-footnote>, it takes 1.58h to fine-tune 2.95M parameters, and Ada-K could bring 1.22x speedup over the baseline.</p> <h3 id="zero-computation-experts">Zero-Computation Experts</h3> <p>Zero-Computation Experts constitute an efficient sparse-expert mechanism within dynamic routing. The basic idea is to incorporate a subset of experts that perform no actual computations during the forward pass. By maintaining a fixed number of activated experts, the selection of these zero-computation experts introduces neither additional FLOPs nor memory overhead, thereby effectively reducing the computational cost. This design improves efficiency without compromising model capacity, and it further enables flexible, adaptive resource allocation when combined with dynamic routing.</p> <p><strong>Null Experts:</strong> Zeng et al. (2024)<d-cite key="zeng2024adamoe"></d-cite> propose AdaMoE, in which zero-computation experts are defined as performing no operations and consuming zero FLOPs. Building on the original top-$k$ MoE, the router still selects a fixed number of experts, but some of these may be zero-computation null experts. When a token is routed to a null expert, it is dropped out and no forward computation is performed, no additional FLOPs are incurred, and no activation memory is consumed, allowing the number of real experts per token to adapt dynamically.</p> <div class="l-body"> <div class="row"> <iframe src="/2026/assets/html/2026-04-27-dynamic-routing/adamoe.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> </div> <p>Following the introduction of zero-computation experts, the lack of a load-balancing mechanism may lead the router to over-select real experts, resulting in an uneven workload distribution and undermining the intended FLOPs reduction. To mitigate this, AdaMoE incorporates an average-load strategy for null experts within the load-balancing loss:</p> \[\mathcal{L}_{\text{balance}} = (N_e + N_z) \sum_{i=1}^{N_e + N_z} \tilde{f}_i \cdot P_i,\] \[\tilde{f}_i = \begin{cases} f_i &amp; \text{if } i \leq N_e \\ \frac{1}{N_z} \sum_{j=N_e+1}^{N_e+N_z} f_j &amp; \text{if } i &gt; N_e \end{cases},\] <p>where $N_z$ denotes the number of null experts, $f_i$ denotes the load of the $i$-th expert and $P_i$ denotes the routing probability of the $i$-th expert. The modified load-balancing loss in AdaMoE accounts for null experts to control the average load of real experts, while avoiding unnecessary constraints among identical zero-computation experts, thus giving the router greater flexibility to optimize token-to-expert allocation.</p> <p>From the experimental results, AdaMoE demonstrates that it achieves higher average accuracy across multiple benchmarks while reducing FLOPs by an average of 15.21% during inference.</p> <p><strong>Zero, Copy, and Constant Experts:</strong> Jin et al. (2025)<d-cite key="jin2025moepp"></d-cite> propose MoE++, a heterogeneous MoE framework designed to improve both efficiency and effectiveness. It introduces three types of zero-computation experts: Zero, Copy, and Constant experts, which replace a subset of real experts to reduce computational cost without altering expert counts or model capacity:</p> <ul> <li>Zero Experts: output a zero vector, requiring no computation: $E_i(\mathbf{x}) = \mathbf{0}$</li> <li>Copy Experts: reuse the input token representation directly: $E_i(\mathbf{x}) = \mathbf{x}$</li> <li>Constant Experts: output a learned constant vector independent of input: $E_i(\mathbf{x}) = \alpha_1 \mathbf{x} + \alpha_2 \mathbf{v}$, where $[\alpha_1, \alpha_2] = \text{softmax}(\mathbf{x} \mathbf{W}_c)$. $\mathbf{W}_c \in \mathbb{R}^{d \times 2}$ and $\mathbf{v} \in \mathbb{R}^d$ are trainable parameters.</li> </ul> <p>These experts incur negligible computation, communication, and memory overhead. When these zero-computation experts are selected, they effectively skip real expert computation while maintaining the top-k selection budget and sparsity patterns.</p> <pre><code class="language-mermaid">graph TD
    T[Token Input] --&gt; R{Top-2 Router}
    
    R -.-&gt; E1[Expert 1]
    R -- "Prob: 0.3" --&gt; E2[Expert 2]
    R -.-&gt; E3[Expert 3]
    R -.-&gt; E4[Expert 4]

    R -- "Prob: 0.2" --&gt; E5[Zero Expert]
    R -.-&gt; E6[Copy Expert]
    R -.-&gt; E7[Constant Expert]
    
    E2 --&gt; Out[Output]
    E5 --&gt; Out

    style E2 fill:#ff9999,stroke:#333,stroke-width:2px
    style E5 fill:#636efa,stroke:#333,stroke-width:2px
</code></pre> <p>To address potential issues such as unstable routing or over-selection of low-computation experts, MoE++ introduces a Heterogeneous Load Balance Loss, controlled by a hyperparameter $\tau$ which regulates the allocation of tokens between zero-computation and FFN experts, promoting a balanced computational load across the expert ensemble.</p> \[\mathcal{L}_{\text{balance}} = \sum_{i=1}^{N_e + N_z} \eta_i f_i P_i, \quad \eta_i = \begin{cases} 1, &amp; \text{if Expert } i \text{ is an FFN expert}, \\ \tau, &amp; \text{if Expert } i \text{ is a zero-computation expert}. \end{cases}\] <p>The experimental results show that MoE++ consistently outperforms same-scale Vanilla MoE across downstream tasks, improves expert throughput by 15%, and achieves performance comparable to dense models with 2–3x more parameters under limited training budgets.</p> <h2 id="challenges">Challenges</h2> <p>Dynamic routing techniques have been utilized to various MoE models. For example, Meituan LongCat Team has scaled Copy Expert (one of the zero-computation experts) MoE model to 560B parameters<d-cite key="meituan2025longcat"></d-cite>, and Uni-MoE-2.0-Omni employs thresholding-based dynamic routing to build multi-modal MoE.<d-cite key="li2025uni-moe-2.0-omni"></d-cite></p> <p>However, there are still some challenges to be addressed for dynamic routing:</p> <ul> <li><strong>Performance-Efficiency Tradeoff:</strong> Although some dynamic routing techniques can achieve better performance together with efficiency, the performance and efficiency are still a tradeoff. For example, DynMoE brings 11% throughput improvements over vanilla MoE, but the performance is slightly lower. This is because the performance and efficiency are not always aligned. If we want to achieve extreme sparsity, the number of activated experts would be very small, which may lead to performance degradation.</li> <li><strong>Efficient Implementations:</strong> For heterogeneous zero-computation experts or special thresholding-based dynamic routing, current grouped GEMM kernels may not be directly applicable, and specialized implementations are needed. For example, BlockFFN designs kernels for ReLU activation and chunk-level sparsification, which significantly accelerates the model throughput.</li> <li><strong>Sparsity Controlling:</strong> If no sparsity regularization is applied, the number of activated experts tends to be very high due to the performance optimization objective. Therefore, most dynamic routing techniques have to apply the sparsity regularization to control the number of activated experts. However, it is still challenging to control sparsity with the same level of performance as the vanilla top-k strategy.</li> <li><strong>Expert Load Balancing:</strong> For MoE with zero-computation experts, models may tend to select regular experts, which may lead to uneven workload distribution and undermining the intended FLOPs reduction. To address this, dynamic routing should incorporate an load balancing strategy for both regular and zero-computation experts.</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>In this post, we provide a brief introduction to MoE dynamic routing techniques and review several key methods, including thresholding, dynamic proposer, and zero-computation experts. Dynamic routing offers a promising direction for MoE models by breaking the rigidity of fixed top-k routing and enabling adaptive expert allocation for each token. While challenges such as performance-efficiency tradeoff, efficient implementations, sparsity controlling, and expert load balancing still exist, dynamic routing has shown great potential in improving the performance and efficiency of MoE models, and has been applied to existing large language models. As MoE models continue to evolve, we expect dynamic routing to play an increasingly important role in future large-scale foundation models.</p> <p>🚚 After reading this blog, if you are planning to start a moving company, you may think of dynamic routing for your business! :)</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Standard Mixture-of-Experts (MoE) models adopt fixed top-k routing, applying uniform computation across tokens regardless of their complexity. This rigidity often leads to suboptimal efficiency and performance, and dynamic routing could address this by adaptively selecting the optimal number of experts for each token. This post introduces the principles of dynamic routing and reviews key techniques for flexible expert allocation.]]></summary></entry><entry><title type="html">Are LLM Agent Populations Really Emergent? A Comprehensive Perspective</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/emergent-agents/" rel="alternate" type="text/html" title="Are LLM Agent Populations Really Emergent? A Comprehensive Perspective"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/emergent-agents</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/emergent-agents/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Language agents powered by large language models (LLMs) have emerged within the long-standing field of agent design, which studies entities that use the environment to take actions autonomously, renewing classic ideas about perception, action, and rational choice <d-cite key="russell2013aima,helbing2012social"></d-cite>. Beyond the current hype, this intersection is mutually beneficial: LLMs’ interacting and reasoning through natural language offers a new substrate for modeling classical agents, while agentic architectures impose structure that channels LLM capabilities <d-cite key="sumers2024cognitivearchitectureslanguageagents"></d-cite>. Generative agent-based modeling (GABM) is increasingly incorporating modules for reasoning, planning, and long-term memory, as exemplified by Park et al. <d-cite key="park2023"></d-cite>. As single agents have grown more sophisticated, attention has shifted to populations of LLM agents, where collective dynamics, coordination, division of labor, and convention formation can arise from local interactions rather than top-down design, potentially yielding behaviors not predictable from any one agent alone.</p> <p>The rapid deployment of LLM-grounded autonomous agents across real applications heightens the need to understand these collective effects, both in machine–machine ecosystems and in their broader sociocultural interfaces <d-cite key="wang2024survey,brinkmann2023machine"></d-cite>. Recent multi-agent simulations show that generative agents can exhibit cooperation, competition, bargaining, and communication, sometimes negotiating norms and aligning on shared goals <d-cite key="ashery2025emergent,buscemi2025strategic,chen2025multi,tran2025multi,vallinder2024cultural"></d-cite>. Yet much observed coordination is scaffolded by design choices, and prevailing evaluation relies on narrow proxies (e.g., engagement or sentiment) as observed by Takata et al. <d-cite key="takata2025emergent"></d-cite>, overlooking the evolving processes through which cooperation, hierarchy, and alignment actually emerge over time. This raises a central question: If interacting LLM agents really are a complex system, which patterns spontaneously emerge in LLM populations, and under what conditions?</p> <p>These concerns intersect with cooperative AI and alignment theory: as Dafoe et al. <d-cite key="dafoe2020open"></d-cite> note, advances in social and strategic competence create “differential capabilities” that can serve both pro-social and adversarial ends. In social-dilemma settings, rational local behavior can drive systems toward collectively suboptimal equilibria, especially if agents succeed via aggressive or manipulative tactics <d-cite key="pan2023machiavelli,anwar2024foundational"></d-cite>. Against this backdrop, we synthesize the literature, connect findings across social simulation (e.g., social simulacra and information spread), collaborative LLM environments (e.g., research labs and hospitals), and the most salient promise, emergent capabilities unattainable by single language agents.</p> <p>To build new intuition, we outline observed emergent patterns in LLM agents and examine what populations of these models are actually capable of achieving. We then provide recommendations for studying this emergence, emphasizing that tools from complex systems are necessary to address the challenge of measuring these behaviors rigorously. Moreover, we present our brief contribution, which emerged in this exact same discussion. We modeled a dynamic network of agents, integrating long-term and working memory, reflection, and planning, while introducing mechanisms for merchant personality, sales management, inventory control, and production planning. To verify the emergent social structure, we model a time-varying interaction graph whose connectivity evolves with agents’ preferences, specifically examining if it exhibits scale-free properties, characterized by a few dominant hubs among many sparsely connected nodes. The resulting economy exhibits human-like patterns, stable preferences, self-organizing network connectivity, and coherent production and trade strategies.</p> <hr/> <h2 id="generative-llm-agents">Generative LLM Agents</h2> <p>While Language Models capture statistical relationships in human data, they lack inherent autonomy. Bridging this gap requires architectures that enable reasoning and task execution. Wang et al. <d-cite key="wang2024survey"></d-cite> identified four key modules for autonomous LLMs: profile, memory, planning, and action. Building on these concepts, Park et al. <d-cite key="park2023"></d-cite> introduced a standard for simulations where generative agents utilize cognitive architectures and environmental design to overcome coherence struggles, enabling them to plan, form relationships, and coordinate group activities.</p> <h3 id="agent-decision-making">Agent Decision Making</h3> <p>The core architecture adopted by Park et al. <d-cite key="park2023"></d-cite>, formalized within the CoALA framework by Sumers et al. <d-cite key="sumers2024cognitivearchitectureslanguageagents"></d-cite>, treats the LLM as a central processor with a limited context window, which can be understood as the working-memory of the agent. To overcome this limitation, Park et al. <d-cite key="park2023"></d-cite> employed a Memory Stream that logs experiences into a database. A retrieval function dynamically populates working memory based on relevance, recency, and semantic similarity, while Reflection synthesizes these logs into high-level semantic memories. Temporal coherence is achieved through planning a hierarchical mechanism that translates semantic conclusions into action sequences, allowing the agent to dynamically balance daily schedules with reactions to spontaneous stimuli.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-emergent-agents/cognitive_architecture-480.webp 480w,/2026/assets/img/2026-04-27-emergent-agents/cognitive_architecture-800.webp 800w,/2026/assets/img/2026-04-27-emergent-agents/cognitive_architecture-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-emergent-agents/cognitive_architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Cognitive architecture overview. Image from Sumers et al. <d-cite key="sumers2024cognitivearchitectureslanguageagents"></d-cite>. </div> <h3 id="interaction-model">Interaction Model</h3> <p>Agents require a grounded existence to manifest behavior in the environment and with one another. In the “Smallville” simulation by Park et al. <d-cite key="park2023"></d-cite>, the environment is represented by a semantic tree (e.g., School → Classroom) translated into natural language. This allows agents to textually perceive and manipulate their surroundings. The system supports complex social dynamics and human-in-the-loop interactions, where users can directly alter environmental states or inject directives via an “inner voice” mechanism. This interaction model supports different scenarios, such as a hospital—similar to Smallville—where agents can be patients, nurses, and doctors treating diseases <d-cite key="li2025agenthospitalsimulacrumhospital"></d-cite>. However, the interaction model can also be defined by a set of coordinates, where agents receive information from their surroundings <d-cite key="takata2025emergent"></d-cite>, or by a network, such as our multi-agent marketplace, which models interactions as a time-varying directed probability network.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-emergent-agents/gabm_and_interactivemodel-480.webp 480w,/2026/assets/img/2026-04-27-emergent-agents/gabm_and_interactivemodel-800.webp 800w,/2026/assets/img/2026-04-27-emergent-agents/gabm_and_interactivemodel-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-emergent-agents/gabm_and_interactivemodel.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> A complex system visualization of GABM and an Interaction Model. A schematic representation of the recursive coupling between agent and system states. Figure inspired by Lu et al. <d-cite key="LU2024283"></d-cite>; images adapted from Takata et al. and Park et al. <d-cite key="takata2025emergent,park2023"></d-cite>. </div> <h3 id="emergent-social-behavior">Emergent Social Behavior</h3> <p>A critical finding of Park et al. <d-cite key="park2023"></d-cite> was the capacity for “emergent social behaviors,” where complex dynamics arise without explicit scripting. This includes <em>information diffusion</em>, where news propagates through the population via organic conversations (e.g., a mayor election that spreads over Smallville), and <em>collaborative coordination</em>, where agents autonomously organize events (e.g., a party). These interactions demonstrate second-order effects, such as agents recruiting help or leveraging social networks to invite specific peers based on memory-encoded relationships.</p> <p>Similarly, Williams et al. <d-cite key="williams2023epidemicmodelinggenerativeagents"></d-cite> utilized GABM to verify emergent capacity in simulations of public health scenarios. By modeling how individual agents react to disease vectors, the system generated macro-level trends that mirrored real-world data. Significantly, the agents’ collective behavioral adaptations successfully ‘flattened the curve,’ while the simulation naturally gave rise to complex multi-wave and endemic infection patterns.</p> <hr/> <h2 id="what-is-emergence">What is Emergence?</h2> <p>The definition of emergence often depends heavily on the disciplinary lens applied. From a physics perspective, emergence is typically characterized by phase transitions where macroscopic properties spontaneously manifest within a system, such as a material acquiring magnetic properties as the temperature drops. This concept of spontaneous order is increasingly applied to the study of LLM agents. For instance, Ashery et al. <d-cite key="ashery2025emergent"></d-cite> investigate social conventions through the framework of complex systems, drawing parallels between the formation of norms and the “critical mass” phenomena observed in physical phase transitions. However, the majority of researchers in agentic literature define emergence more pragmatically as behavioral properties that appear in autonomous systems despite not being explicitly prompted. While these definitions differ in origin, they converge on the study of how properties arise in autonomous systems without direct hardcoding.</p> <h3 id="social-coordination--conventions">Social Coordination &amp; Conventions</h3> <p>Besides what Park et al. <d-cite key="park2023"></d-cite> called emergent coordination, more robust experiments in controlled settings were proposed to understand this phenomenon. To manage these dynamics in more complex environments, Ren et al. <d-cite key="ren2024emergencesocialnormsgenerative"></d-cite> propose the CRSEC architecture (Creation, Spreading, Evaluation, and Compliance). In their simulations, this architecture enabled the spontaneous emergence of robust social norms—specifically “no smoking indoors,” “be quiet in public,” and “tipping after meals”—which effectively minimized social conflict without central enforcement.</p> <p>Expanding on the mechanics of these conventions, Flint et al. <d-cite key="flint2025groupsizeeffectscollective"></d-cite> identify group size as a critical control parameter. Their findings reveal that collective bias is not merely an aggregation of individual preferences but a deeper phenomenon capable of amplification, induction, or reversal: interactions can amplify weak individual biases, induce new ones where none existed, or even reverse model-level preferences. Crucially, they observe non-linear effects where, above a critical population size, simulations converge to deterministic predictions, exposing the “basins of attraction” for competing equilibria.</p> <p>In a more controlled environment, Ashery et al. <d-cite key="ashery2025emergent"></d-cite> propose an experiment to model using “Naming Games,” where agents interact pairwise to negotiate a shared vocabulary. Simulations demonstrate that populations of LLM agents undergo a symmetry-breaking transition, evolving from a high-entropy state of distinct terms to a global norm via a “winner-take-all” trajectory. While theoretical models predict that any neutral option should have an equal probability of becoming the norm, LLM populations exhibit a phenomenon of Collective Bias. Even when individual agents appear unbiased in isolation, the collective dynamics of the group amplify specific latent preferences, resulting in non-neutral outcomes.</p> <h3 id="cooperation-vs-conflict">Cooperation vs. Conflict</h3> <p>Emergence also manifests in strategic scenarios where agents must weigh self-interest against group benefit, a dynamic increasingly relevant in multi-agent systems <d-cite key="willis2025systemsllmagentscooperate"></d-cite>. While Willis et al. focus on population dynamics, the structural complexity of interactions is further elucidated by Du et al. <d-cite key="DU2025105439"></d-cite> in their analysis of triadic Prisoner’s Dilemma games. Du et al. observed that the introduction of a third player significantly altered standard dyadic dynamics, as this third agent could stabilize cooperative relationships or destabilize defective ones, effectively mediating the group’s state. Crucially, they found the emergence of cooperation to be highly sensitive to the ‘K-index’, Rapoport’s measure of the relative incentive to cooperate versus defect, where higher values promoted sustained mutual cooperation even under minimal information conditions (for more details of this index see <d-cite key="rapoport1967note"></d-cite>).</p> <p>Beyond immediate structural incentives, Vallinder and Hughes <d-cite key="vallinder2024cultural"></d-cite> extend the analysis to intergenerational dynamics, demonstrating that cooperative norms can emerge via cultural evolution rather than explicit pre-programming. In their study of the iterated Donor Game, they observed that sophisticated strategies of indirect reciprocity—where agents conditionally aid based on the reputation of others—emerged spontaneously to solve collective action problems. This emergence was characterized by the development of “second-order” reputation tracking and the utilization of costly punishment to police free-riders. However, they emphasize that this capacity is highly model-dependent; while populations of Claude 3.5 Sonnet agents successfully “bootstrapped” this cooperative infrastructure, other models like GPT-4o and Gemini 1.5 Flash converged toward mutual defection, highlighting that emergent cooperation relies on specific model capabilities for social learning across generations.</p> <p>While these studies highlight how structural and evolutionary incentives drive cooperation, other research suggests that shared state information is equally critical for emergence in adversarial domains. In the context of Information Operations (IO), which are orchestrated campaigns to influence mass opinion on social media (e.g., election, fake news), Orlando et al. <d-cite key="orlando2025emergentcoordinatedbehaviorsnetworked"></d-cite> found that agents exhibited sophisticated strategic coordination without explicit planning. As operational regimes became more structured, IO networks spontaneously became denser and more clustered, with narratives converging toward semantic homogeneity. A key emergent finding was that simply providing agents with “Teammate Awareness” (mutual knowledge of allies) triggered coordination patterns—such as synchronized resharing and narrative alignment—that were nearly equivalent to those achieved through explicit, time-consuming deliberation.</p> <h3 id="social-influence--dynamics">Social Influence &amp; Dynamics</h3> <p>Finally, in scenarios requiring complex resource management, Takata et al. <d-cite key="takata2025emergent"></d-cite> explored the El Farol Bar problem, a classic game-theoretic scenario with no simple rational solution. They observed that LLM agents autonomously developed a “spontaneous motivation” to attend the bar, resulting in oscillatory attendance patterns that mirrored human behavior rather than perfect utility optimization. Notably, agents spontaneously employed hashtags (e.g. #collaboration) to signal intent and coordinate behavior. The study identified a consistent time lag between agent clustering and bar crowding, indicating that agents were forming a “collective mind” that anticipated group dynamics, balancing formal rationality with social motivations in a way standard game theory does not predict.</p> <hr/> <h2 id="agentmarket-a-dynamic-network-of-generative-agents">AgentMarket: A Dynamic Network of Generative Agents</h2> <p>To extend the study of emergence into financial domains, we applied GABM within a dynamic digital marketplace. This environment facilitates peer-to-peer trading, supporting merchant leaderboards, distinct seller personas, and reinforcement-learning simulations. A core feature of our approach is inspired by complex system theory, by using a time-varying interaction graph, designed to explicitly model how social interactions restructure network connectivity over time. Building upon established cognitive architectures <d-cite key="park2023,sumers2024cognitivearchitectureslanguageagents"></d-cite>, we integrate long-term and working memory, reflection, and planning with novel domain-specific mechanisms: merchant personality, sales management, inventory control, and production planning. The resulting economy demonstrates human-like emergent patterns—including stable preferences, merchant clustering, and coherent strategies.</p> <p>Building on this architecture, retrieved contexts fuel a reflection stage where agents synthesize observations through the lens of their unique personalities, shaping attributes such as risk tolerance and negotiation stance. Ultimately, these cognitive states dictate economic actions, as agents dynamically update production targets based on expected demand, driven by past interactions, and modulate their pricing strategies according to real-time inventory constraints.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-emergent-agents/cogntive-480.webp 480w,/2026/assets/img/2026-04-27-emergent-agents/cogntive-800.webp 800w,/2026/assets/img/2026-04-27-emergent-agents/cogntive-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-emergent-agents/cogntive.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Our Cognitive architecture overview, inspired by Park et al. <d-cite key="park2023"></d-cite> with inventory and production planning addition. </div> <h3 id="time-varying-network-interaction-model">Time-Varying Network Interaction Model</h3> <p>To allow different agents to interact and possibly trade with each other, we adopted a time-varying network interaction model inspired by complex networks and Markov chains (Watts and Strogatz <d-cite key="watts1998collective"></d-cite>). Let \(N\in\mathbb{N}\) be the number of agents and \(t\in\mathbb{Z}_{\ge 0}\) the discrete time index. Each agent \(A_i\in\{1,\dots,N\}\) produces a directional score for agent \(A_j\) via</p> \[A_i:\ \mathbb{Z}_{\ge 0}\times\{1,\dots,N\}\to\mathbb{R},\qquad s_{ij}(t)=A_i(t,j).\] <p>The scores provided by the agents \(S_t=\big[s_{ij}(t)\big]\in\mathbb{R}^{N\times N}\), define the row-stochastic weight matrix by a row-wise softmax with temperature \(\tau&gt;0\):</p> \[W_t = \operatorname{RowSoftmax}(S_t), \quad w_{ij}(t)=\frac{\exp\!\big(s_{ij}(t)/\tau\big)}{\sum_{k=1}^N \exp\!\big(s_{ik}(t)/\tau\big)}.\] <p>At each time step, agent \(i\) interacts with \(j\) with probability \(P(A_i \rightarrow A_j)=w_{ij}(t)\) or reflect by \(P(A_i \rightarrow A_i)=1-w_{ii}(t)\).</p> <p>The interaction network is a time-varying matrix of probabilities \(W_t\) where \(w_{ij}(t)\) is the probability that agent \(i\) interacts with agent \(j\) at time \(t\).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-emergent-agents/agentmarket_graph-480.webp 480w,/2026/assets/img/2026-04-27-emergent-agents/agentmarket_graph-800.webp 800w,/2026/assets/img/2026-04-27-emergent-agents/agentmarket_graph-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-emergent-agents/agentmarket_graph.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Our interaction model, a time-varying weighted directed graph. </div> <h3 id="simulation-and-network-dynamics">Simulation and Network Dynamics</h3> <p>To investigate the emergence of social structure, we initialized the simulation with a uniform probability graph where all edge weights were identical. The simulation proceeds iteratively: at each step, an active agent selects a peer to interact with, after which the turn—and control of the interaction—passes to the receiver. To characterize the long-term behavior of these interactions, we conducted extensive simulations, tracking the structural evolution of the network over a horizon of 300 interactions.</p> <p>As illustrated below, the network exhibits significant topological drift, quickly departing from its initial uniform state to fluctuate between distinct regimes of connectivity. Crucially, Maximum Likelihood Estimation of the node strength distribution reveals the emergence of a scale-free architecture over every scoring cycle. The observed exponent (\(\alpha \approx 2.12\)) signifies that the probability of a node having a certain strength decays relatively slowly, placing the system within the ‘classic’ range of natural complex networks like the Internet or metabolic pathways <d-cite key="PhysRevE.67.026112"></d-cite>. These networks follow a power law, characterized by a few dominant hubs among many sparsely connected nodes. This analysis captures the system’s trajectory from randomness through intermediate phases of self-organization, demonstrating how connection probabilities evolve into a structured, hierarchical topology.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-emergent-agents/graphs-480.webp 480w,/2026/assets/img/2026-04-27-emergent-agents/graphs-800.webp 800w,/2026/assets/img/2026-04-27-emergent-agents/graphs-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-emergent-agents/graphs.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Network topology evolution showing emergent scale-free properties and hierarchical structure. </div> <p>Quantitative analysis of the network connectivity indicates a stabilization period following the initial configuration. The system’s Global Efficiency rises sharply in the early cycles, peaking around cycle 5 before settling into a slightly lower steady state. This decline may connect with a gradual increase in the PageRank Gini coefficient, suggesting that as the network matures, influence becomes somewhat more concentrated, which correlates with a minor reduction in global integration. Meanwhile, Dyadic Reciprocity remains relatively high (&gt;0.6) throughout the simulation, consistently exceeding levels expected in random or strictly hierarchical graphs. Together, these metrics characterize a system that evolves into a widely connected, feedback-rich structure with moderate centralization, rather than converging toward a pure star or random connectivity. Of course this is just preliminary result as serves as a motivation for more complex systems oriented approaches.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-emergent-agents/graph_metrics-480.webp 480w,/2026/assets/img/2026-04-27-emergent-agents/graph_metrics-800.webp 800w,/2026/assets/img/2026-04-27-emergent-agents/graph_metrics-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-emergent-agents/graph_metrics.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Plot with the Global Efficiency and Centrality (PageRank) over scoring cycles. </div> <h3 id="natural-language-and-sentiment-analysis">Natural Language and Sentiment Analysis</h3> <p>Below, we present the final agreement phase. Here, the agents aren’t just haggling; they are engaging in a culturally rich exchange. Note how Pema constructs a complex bundle—trading ‘wild cliff honey’ (80) against an ‘underwater light’ (85), and accurately computes the cash remainder, all while exchanging pleasantries like ‘dhanyabad’ and ‘Que maravilha’. It is high-stakes trading, handled with grace.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-emergent-agents/chat_example-480.webp 480w,/2026/assets/img/2026-04-27-emergent-agents/chat_example-800.webp 800w,/2026/assets/img/2026-04-27-emergent-agents/chat_example-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-emergent-agents/chat_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Verbatim transcript of the negotiation settlement. Pema offers a split-payment trade (goods + currency), which Bianca accepts in the subsequent turn. </div> <p>We also tried to identify which characteristics of agent interactions lead to a good trade. In order to verify this we applied LLM as judge to give sentiment analysis scores, this method was chosen over sentiment-BERT, since most of the models cannot deal with large corpus of data, such as a whole interaction between the agents. We observe a positive Spearman correlation between trade value and positive score for the conversation (\(r=0.33, p&lt;0.001\)) and a slightly negative correlation for negative scores (\(r=-0.12, p&lt;0.001\)).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-emergent-agents/sentiment_analysis-480.webp 480w,/2026/assets/img/2026-04-27-emergent-agents/sentiment_analysis-800.webp 800w,/2026/assets/img/2026-04-27-emergent-agents/sentiment_analysis-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-emergent-agents/sentiment_analysis.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The scatter plot in blue shows the positive correlation between the log trade value, to facilitate visualization, and positive sentiment score. In the red scatter plot, the negative correlation between the log trade value and negative sentiment score. </div> <p>Ultimately, we wanted to provide an example and further motivation to treat multi-agents as complex systems by changing the usual interaction model for a time-varying network defined by the agent preference over a marketplace environment. These simulations reveal a human-like marketplace where the emergence of distinct seller preferences and natural clusters that might mirror real-world economic configurations. However, more importantly, we were able to investigate how the social layer might relate to the economic one and provided an analysis framework based on complex systems tools in networks.</p> <hr/> <h2 id="challenges-and-future">Challenges and Future</h2> <h3 id="alignment">Alignment</h3> <p>On the positive side, emergent coordination among AI agents can be harnessed for pro-social outcomes. For example, a well-designed population of healthcare assistant agents could divide labor and share information to diagnose patients more effectively than any single agent, utilizing agent-to-agent communication like negotiation and debate to coordinate effectively <d-cite key="carichon2025comingcrisismultiagentmisalignment"></d-cite>. In our AgentMarket, agents eventually stabilized their market and improved efficiency, analogously, multiple AI agents managing an energy grid or traffic system might emergently reduce waste or prevent overloads through their local interactions. These are cases where we want emergence: the system-level outcome is better than a centralized or solo approach, and ideally aligned with our goals (healthier patients, efficient markets, etc.).</p> <p>However, emergent behaviors can also pose alignment risks. As mentioned earlier, agents with advanced social skills have differential capabilities which can lead to power asymmetries and over-reliance on confident or dominant agents <d-cite key="carichon2025comingcrisismultiagentmisalignment"></d-cite>; they might collude in ways that are harmful, or develop their own objectives that diverge from what we intended. A concrete concern is emergent deception or power-seeking in multi-agent systems. If one agent learns it can achieve its goal by manipulating another—such as hiding or spreading false information to prevent others from benefiting freely <d-cite key="carichon2025comingcrisismultiagentmisalignment"></d-cite>, and this behavior spreads or is copied by others, the group could develop a norm of deception that undermines overall performance and violates human norms. The Machiavelli benchmark study showed that without constraints, agents often choose Machiavellian strategies to win <d-cite key="pan2023rewardsjustifymeansmeasuring"></d-cite>, a risk exacerbated in groups where exploitative individuals can manipulate group dynamics for personal gain. In a multi-agent context, if competitive pressure pushes even a few agents to be dishonest for gain, others may follow suit to avoid exploitation, leading to an undesirable equilibrium of low trust and unethical behavior. This is analogous to how unchecked competition in markets can lead to a “race to the bottom” or how social media algorithms all optimize for engagement, possibly leading to the spread of sensational or false content as an emergent outcome.</p> <h3 id="multi-agents-are-complex-systems">Multi-Agents are Complex Systems</h3> <p>We hope this discussion has demonstrated that interacting LLM agents are effectively complex systems, capable of exhibiting distinct emergent properties. However, a major open challenge remains: the rigorous quantification of this emergence. While recent research attempts to establish more reliable metrics, prevailing evaluations often rely on “narrow proxies” such as task success rates or sentiment analysis. While these indicators are informative from a natural language standpoint, they provide insufficient resolution, highly susceptible to sensitive changes in prompt and model characteristics, when modeling the system’s global properties as also highlighted by Lu et al. <d-cite key="LU2024283"></d-cite>. Crucially, such metrics indicate that coordination has occurred, but fail to explain the causal mechanics of how macro-level phenomena, such as the observed emergent social behavior, decouple from micro-level agent states. To truly understand multi-agent intelligence, the field must transcend behavioral observation and adopt the quantitative rigor of complex systems theory as we tried to illustrate with Ashery et al. <d-cite key="ashery2025emergent"></d-cite> and our AgentMarket example.</p> <p>As a recommendation to the community, to evaluate emergence from a more structured quantifiable way, using complex system literature and information-theoretic standpoint, such as frameworks such as those proposed by Rosas et al. <d-cite key="rosas_pedro_emergence_2020"></d-cite>. This approach posits that emergence is not merely a qualitative description but a quantifiable state of synergistic information—where the predictive power of the collective system exceeds the sum of information contained in its individual components.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Exploring emergent properties in populations of LLM agents through the lens of complex systems theory, examining social coordination, cooperation dynamics, and economic interactions in generative agent-based models.]]></summary></entry><entry><title type="html">[“How to Transition from ML to DL in Production - Lessons From the Trenches at Company”]</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/from-ml-to-dl/" rel="alternate" type="text/html" title="[“How to Transition from ML to DL in Production - Lessons From the Trenches at Company”]"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/from-ml-to-dl</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/from-ml-to-dl/"><![CDATA[<p>It is a widely held belief in the ML community that tree-based models are the most sensible choice for tabular data and that neural networks will invariably underperform. As a result, using neural networks in this domain is frequently met with skepticism—not only regarding their potential performance, but also their practicality (e.g., latency, GPU requirements, interpretability).</p> <p>These concerns are empirically motivated <d-cite key="grinsz"></d-cite>, but often misunderstood: for any fixed search budget, tree-based models consistently outperform non–tree-based alternatives. However, we argue that the perceived underperformance often stems from two factors: scale and uncertainty.</p> <p>First, many empirical comparisons rely on small-scale academic benchmarks that favor sample-efficient tree-based methods while underutilizing neural networks. Second, feasibility concerns often implicitly equate “deep learning” with large-language-model–style infrastructure—multi-billion-parameter architectures, expensive training pipelines, and complex deployment stacks—when in fact tabular neural networks need not resemble LLMs in either size or serving complexity.</p> <p>In this work, we describe the migration of our flagship risk system – scoring thousands of transactions per second – from a large, well-tuned tree-based model to a neural network. We also outline why we believe that, at scale, neural networks can not only match but surpass gradient-boosted trees on tabular data, while bringing substantial ancillary benefits.</p> <h2 id="background---risk-at-company">Background - Risk at Company</h2> <p>Company processes large volumes of payments for merchants worldwide. A key component of this process is our Fraud Detection Model (FDM), which estimates in real time the likelihood that a payment is fraudulent, allowing the system to approve, challenge, or block transactions accordingly.</p> <p>The cost of errors is high. Type I errors (blocking legitimate payments) degrade merchant authorization rates and harm shopper experience, while Type II errors (approving fraudulent transactions) lead to direct financial losses. Balancing these trade-offs while maintaining low latency and high throughput is central to the design of our risk models.</p> <p>Our system consumes a large and evolving feature set, including:</p> <ul> <li>basic payment-level features</li> <li>aggregate features (rolling statistics across merchant and shopper dimensions) computed by our feature platform</li> <li>shopper history signals served through our entity-linking system</li> <li>velocity features such as recent payment attempts across multiple dimensions</li> <li>structured and semi-structured string fields containing raw payment metadata</li> </ul> <pre><code class="language-mermaid">graph LR
    PR[Payment Request]

    subgraph "Feature Stores"
        FP[Feature Platform]
        SL[Shopper Linking]
        VDB[Velocity Database]
    end


    subgraph "Features"
        BF[Payment Information]
        SF[PII Fields]
        AF[Payment Aggregations]
        SHF[Shopper History]
        VF[Payment Velocity]
    end

    FDM[FDM]

    PR --&gt; BF
    PR --&gt; SF

    PR -.-&gt; FP
    PR -.-&gt; SL
    PR -.-&gt; VDB

    FP --&gt; AF
    SL --&gt; SHF
    VDB --&gt; VF

    BF --&gt; FDM
    SF --&gt; FDM
    AF --&gt; FDM
    SHF --&gt; FDM
    VF --&gt; FDM
</code></pre> <p>Historically, our risk stack was built around a large boosting model. It was easy to adopt, straightforward to scale, and strong on tabular data. As a result, raw model choice was not an immediate bottleneck: most improvements came from feature engineering, platform integration, and expanding model scope – making deep learning difficult to prioritize.</p> <p>Over time, however, signs emerged that a shift could unlock further gains. Many input signals are inherently sequential (timestamps, event streams) or textual. While engineered proxies helped — rolling-window aggregates, hashed or bucketed email features – these approaches approximate rather than directly learn from raw signal structure.</p> <p>Operational pressures also increased. Growing transaction volume (billion scale) and feature count strained out-of-core training and constant-memory inference. Meanwhile, Company’s long-term strategy emphasized deep learning: foundational payments models, model unification, and multi-task learning.</p> <p>We hypothesized that a well-designed neural network could provide both first-order performance uplift and second-order benefits: improved representation learning, reduced feature-engineering burden, better extensibility, and alignment with strategic goals.</p> <h2 id="why-deep-learning-on-tabular-data-might-work">Why Deep Learning on Tabular Data Might Work</h2> <p>To see why neural networks could eventually outperform tree-based models on tabular data, we must first understand <strong>why boosting works so well today.</strong></p> <h3 id="why-tree-based-models-excel-at-tabular-data">Why Tree-Based Models Excel at Tabular Data</h3> <p>Tree-based models remain the dominant choice for tabular tasks because their <strong>inductive bias matches the structure of the data.</strong> Tabular features are often categorical, sparse, piecewise, and highly non-smooth. Trees naturally capture this structure through axis-aligned splits and non-linear rule partitions. A single decision tree already encodes sharp discontinuities and local feature interactions that dense NNs typically struggle with <d-cite key="grinsz"></d-cite></p> <p>.</p> <p>Added to that and true power of boosting emerges from ensembling. Boosting constructs a large committee of weak learners, each correcting the residuals of the previous one, yielding a flexible and high-capacity model. This <em>homogeneous ensemble</em> scales extremely well with modest compute and is sample-efficient—crucial advantages for most tabular benchmarks.</p> <p>This connects to the <strong>No Free Lunch principle</strong>: no single model class is universally optimal, but <strong>an ensemble of diverse models often outperforms any single one.</strong></p> <p>Ensembling underlies:</p> <ul> <li><strong>bagging</strong></li> <li><strong>boosting (XGBoost, LightGBM)</strong></li> <li><strong>stacking — the backbone of most Kaggle-winning tabular systems</strong></li> </ul> <p>In short:</p> <blockquote> <p><strong>Boosting ≈ Trees (right inductive bias) + Ensembling (power multiplier).</strong><br/> If we want the benefits of NNs without losing performance, we must replicate <em>both</em> components.</p> </blockquote> <p>Lets take homogeneous ensembling to it’s logical conclusion: heterogenous ensembling.</p> <h3 id="heterogeneous-ensembling-with-deep-models">Heterogeneous Ensembling with Deep Models</h3> <p>This observation suggests a natural migration path: <strong>extend boosting from a homogeneous ensemble of trees to a heterogeneous ensemble that mixes trees and neural models.</strong></p> <p>Rather than replacing the tree model outright, we can <strong>stack</strong> multiple learners—boosters, MLPs, attention models—and train a meta-learner over their predictions. This leverages complementary inductive biases while keeping the stable baseline in place.</p> <p>Heterogeneous ensembling has repeatedly proven effective in practice<d-cite key="tabarena, tabm"></d-cite>. And it makes sense, if boosting is an ensemble or trees, why would not compare NNs against it as an ensemble? Why leap directly from boosting to deep learning; instead, <strong>ensembling serves as a stable bridge</strong> while the neural model matures.</p> <pre><code class="language-mermaid">graph TD
  A[Input Features]

  subgraph "Base Learners"
      B[Neural Net]
      C[Alternative NN]
      D[Booster 1]
      E[Booster 2]
  end

  A --&gt; B
  A --&gt; C
  A --&gt; D
  A --&gt; E

  F(Meta-Learner)
  B --&gt; F
  C --&gt; F
  D --&gt; F
  E --&gt; F

  F --&gt; G[Final Prediction]
</code></pre> <p>Stacking allows:</p> <ul> <li>immediate incremental uplift from adding a small NN</li> <li>continuous improvement as NN architectures mature</li> <li>safe fallback to boosting during migration</li> </ul> <p>This leads to the final question: how do we tackle the inductive bias problem in NNs?</p> <p>##. How Do We Make Neural Networks Competitive on Tabular Data?</p> <p>Despite strong ensembles, neural networks still typically underperform trees on tabular tasks <d-cite key="tabm"></d-cite>. The gap is driven by differences in inductive bias <d-cite key="grisz"></d-cite>:</p> <ol> <li>Smoothness bias — MLPs favor smooth boundaries; tabular patterns are often sharp.</li> <li>Rotational invariance — NNs are rotation-invariant, while trees exploit axis alignment.</li> <li>Scale sensitivity — trees excel at small/medium datasets; NNs need regularization and data.</li> </ol> <blockquote> <p>Takeaway: You can’t just throw NNs at tabular data (w/ a meaningful budget).</p> </blockquote> <p>However, recent work <d-cite key="excelformer, realmlp, reg-is-all-you-need"></d-cite> shows these limitations can be overcome with thoughtful tuning:</p> <ul> <li>discretization and learned embeddings for continuous values <d-cite key="embeddings"></d-cite></li> <li>attention architectures for tabular structure <d-cite key="excelformer"></d-cite></li> <li>large-scale training + augmentation <d-cite key="excelformer"></d-cite></li> <li>regularization <d-cite key="excelformer, realmlp, reg-is-all-you-need"></d-cite></li> <li>activations <d-cite key="excelformer"></d-cite></li> </ul> <p>Alongside industry reports where NNs eventually outperformed boosting <d-cite key="facebook, stripe, sharechat, swiggy"></d-cite>, these results suggest that:</p> <blockquote> <p>If we design the right inductive bias for NNs — and ensemble them during migration — they can match or surpass boosted trees at scale.</p> </blockquote> <p>In the next section, we describe how we operationalized and tested this hypothesis at Company.</p> <h2 id="operationalizing-the-migration-our-approach">Operationalizing the Migration: Our Approach</h2> <p>The complete transition of our fraud detection models from ML to DL ranged from October 2024 to September 2025. Given the uncertainties regarding the performance and deployability of neural networks and ensembling models, we avoided a large-scale monolithic migration and instead focused on shorter, iterative experiments.</p> <h3 id="offline-experiments">Offline Experiments</h3> <p>We ran an initial feasibility study to evaluate the performance of neural networks against our boosting model baseline. For simplicity and arguably lack of nuance, we used the existing boosting feature set unmodified. We built a lightweight experimentation loop focused on rapid iteration rather than production readiness. We initially experimented with simple MLP and ResNet<d-cite key="gorishniy2023revisitingdeeplearningmodels"></d-cite> architectures to novel solutions such as FT-Transformer (attention-based models for tabular data)<d-cite key="gorishniy2023revisitingdeeplearningmodels"></d-cite>, TabNet<d-cite key="arik2020tabnetattentiveinterpretabletabular"></d-cite>, and ExcelFormer <d-cite key="excelformer"></d-cite>.</p> <p>After running multiple training and tuning experiments, we observed that neural networks overall underperformed by 18% to 30% on our internal metrics versus our boosting baselines across a combination of architectures and encoding schemes:</p> <ul> <li> <p>On the architecture side, we surprisingly observed that performance metrics did not significantly differ across architectures: simple architectures such as MLP and ResNet were just as good (or, in this case, just as bad) as complex ones such as FT-Transformer or TabNet, although the latter were shown to bring significant performance uplift on smaller benchmarks <d-cite key="tabred"></d-cite>. Furthermore, model training time increased significantly with the complexity of the architectures on our then fairly immature GPU cluster, rendering the training of some architectures, like ExcelFormer, prohibitively time-consuming.</p> </li> <li> <p>On the encoding side, we found that an adequate encoding scheme for numerical and categorical features was essential for acceptable performance from neural networks. Here again, complex encoding schemes such as piecewise linear encoding or numerical embeddings <d-cite key="embeddings"></d-cite> did not bring uplift and sometimes significantly increased training time. A simple combination of standard scaling and masking missing values for numerical features, and learned embeddings for categorical features, gave the best results.</p> </li> </ul> <p>This first run of experiments allowed us to settle, quite surprisingly, on the simplest available architecture: a wide, shallow MLP of around 10M parameters. This architecture yielded superior performance among the architectures we could train (in a close tie with ResNet) and trained relatively fast, taking around 10 hours at the time to converge on our training sets.</p> <p>The surprising finding that a simple MLP was the most stable and scalable architecture for our problem shifted our strategy from finding the best architecture to making a simple architecture good at scale and matches recent findings in the literature <d-cite key="realmlp"></d-cite><d-cite key="facebook"></d-cite>.</p> <h3 id="ensembling-as-a-bridge">Ensembling as a Bridge</h3> <p>We then turned our efforts to the stacking ensemble strategy that would best complement the MLP architecture. We used a simple ridge classifier as our meta-learner and compared two popular forms of stacking:</p> <ul> <li>A <em>simple</em> stacking scheme in which the predictions of the MLP and the boosting model are fed as two inputs to the meta-learner. This is the most basic form of stacking, as the meta-learner receives only two input features with no extra information about the sample it is scoring.</li> <li>A <em>deep</em> stacking scheme in which the predictions of each individual tree from the booster and activations from the second-to-last layer of the MLP are fed to the meta-learner. This allows for a richer representation of the underlying sample being scored <d-cite key="facebook"></d-cite>.</li> </ul> <pre><code class="language-mermaid">graph TD
    A[Input Features]
    
    subgraph boosting [Boosting Model]
      B[Boosting Model]
      subgraph tree_outputs [Individual Trees]
        T1((T1))
        T2((T2))
        T3((...))
        TN((TN))
      end
    end
    
    subgraph neural [Neural Network]
      L1[First Layers]
      subgraph neuron_activations [Last Layer Neurons]
        N1((N1))
        N2((N2))
        N3((...))
        NM((NM))
      end
    end

    A --&gt; B
    B --&gt; T1
    B --&gt; T2
    B --&gt; T3
    B --&gt; TN

    A --&gt; L1
    L1 --&gt; N1
    L1 --&gt; N2
    L1 --&gt; N3
    L1 --&gt; NM
    
    ML[Meta-Learner]

    T1 --&gt; ML
    T2 --&gt; ML
    T3 --&gt; ML
    TN --&gt; ML

    N1 --&gt; ML
    N2 --&gt; ML
    N3 --&gt; ML
    NM --&gt; ML

    ML --&gt; FP[Prediction]
</code></pre> <p>While stacking models added complexity to the codebase, training them was extremely quick and straightforward. Both forms of stacking showed uplift compared to our standalone MLP trained in our first experimental round, which was somewhat expected. However, their individual performance was quite surprising. On the one hand, the simple two-input stacking scheme showed a 3.8% uplift against our flagship boosting model. On the other hand, the deep stacking scheme, although passing substantially more information to the meta-learner, underperformed by 10.8% against the same baseline.</p> <table> <thead> <tr> <th style="text-align: left">Model</th> <th style="text-align: left">Performance vs. Boosting</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Standalone MLP</td> <td style="text-align: left">-18.0%</td> </tr> <tr> <td style="text-align: left">Stacking (Simple)</td> <td style="text-align: left">+3.8%</td> </tr> <tr> <td style="text-align: left">Stacking (Deep)</td> <td style="text-align: left">-10.8%</td> </tr> </tbody> </table> <p>These results were encouraging since stacking provided direct uplift compared to boosting. Furthermore, there were several unexplored approaches that could provide future performance gains on the neural network side: we had done no feature engineering, hyperparameter tuning was kept minimal, and the network architecture was extremely simple.</p> <p>Given these prospective improvements, we prioritized exploratory work on the transition of the model in production through stacking as it provided a great migration strategy: it gave immediate incremental gains without directly replacing the boosting models and offered a safety net while the neural network matured.</p> <p>However, we remained cautious due to the numerous unknowns regarding its deployability and latency in the live payment flow.</p> <h3 id="live-validation">Live Validation</h3> <p>The next essential step of the migration was validating whether the performance gains obtained through stacking offline could be achieved in production.</p> <p>More specifically, we wanted to ensure that our stacking model could correctly score transactions in production while respecting our strict latency requirements.</p> <p>The simple stacking scheme we settled on made it trivial to combine the boosting model and neural network to fulfill the two model interfaces that would be called in production: <code class="language-plaintext highlighter-rouge">score</code>, which requests a probability of fraud for a transaction, and <code class="language-plaintext highlighter-rouge">explain</code>, which generates merchant-facing signals explaining the model’s decision.</p> <pre><code class="language-mermaid">graph LR
  Score(Score)
  Explain(Explain)

  subgraph "Stacking Predictor"
      subgraph "Torch Predictor"
    NNS(Torch Neural Network)
    NNE(Captum Explanation Module)
  end
  subgraph "Boosting Predictor"
    BMS(LightGBM Booster)
    BME(LightGBM Explanation Module)
  end
    MMS(Ridge Classifier)
    MME(Combine Explanations)
  end

  Score --&gt; NNS
  Score --&gt; BMS
  Explain --&gt; NNE
  Explain --&gt; BME

  NNS --&gt; MMS
  BMS --&gt; MMS
  MMS --&gt; FS(Model Score)

  BME --&gt; MME
  NNE --&gt; MME
  MME --&gt; FE(Model Explanations)
</code></pre> <p>We initially deployed the stacking model on our datacenters in a <em>ghost</em> state, in which they scored duplicated versions of live payments without influencing their outcomes, allowing us to monitor model behavior with minimal risk. The average latency of the stacking model was around 12 ms (versus 5 ms for the boosting model), which was significantly under our latency requirements. These results showed that CPU are more than sufficient for real-time inference, which validate one of the core hypothesis of this work, that NNs/DL doesn’t need GPUs at serving-time.</p> <p>We then gradually rolled out the stacking model to influence payments, with a stacking setup still dominated by our boosting model.</p> <h3 id="scaling-and-transition">Scaling and Transition</h3> <p>With a strong stacking model online, we had time to invest in neural network-specific model improvements with the aim of matching or outperforming stacking to complete the migration. To gauge progress, we tracked the relative importance of boosting and the neural network within the ensemble over time.</p> <p>We ran a large number of parallel experiments to iteratively improve the model: feature encoding, feature engineering, loss function, and training stability were all revisited and meaningfully changed. These incremental changes increased the importance of the neural network within the stacking model, reducing its dependence on trees.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-from-ml-to-dl/figure1-480.webp 480w,/2026/assets/img/2026-04-27-from-ml-to-dl/figure1-800.webp 800w,/2026/assets/img/2026-04-27-from-ml-to-dl/figure1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-from-ml-to-dl/figure1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Finally, we fully switched to the standalone neural network once we observed that it consistently matched or outperformed the stacking model. The stacking strategy we adopted made the entire transition seamless, as the model could just be deployed to production while a stacking fallback was still available.</p> <h2 id="learnings">Learnings</h2> <p>Our migration from a mature boosting model to a pure deep learning model yielded a number of lessons that we believe generalize beyond our specific domain.</p> <p><strong>Start with the simplest possible uplift</strong> The goal of the first experiments was not to “beat” boosting with a sophisticated neural network architecture but to obtain <strong>any credible uplift</strong> in a way that built trust for the rest of the migration. A very simple MLP, combined with a simple two-input stacking scheme, was enough to show measurable gains over the flagship boosting model. This uplift unlocked further investment and bought time for the subsequent transition phases.</p> <p><strong>Use ensembling as a bridge, not a destination</strong> Ensembling was invaluable as a <strong>migration tool</strong>. It allowed us to gradually and safely introduce deep learning into a critical production system while improving performance and maintaining a reliable fallback. At the same time, we treated stacking as a bridge rather than a permanent architecture. Once the neural network consistently dominated the ensemble, the additional complexity of stacking no longer justified itself.</p> <p><strong>Separate feasibility from full productionisation</strong> We deliberately separated the question “Can this model run in production at all?” from “Can it replace the existing model everywhere?”. The initial production deployment focused on <strong>feasibility</strong> by ensuring correctness and speed in the payment flow. Only once these constraints were verified did we invest in making the DL model a first-class citizen in the main training and deployment workflows.</p> <p><strong>Invest in tooling and speed early</strong> Many of the most impactful investments were not in model architecture but in <strong>tooling</strong> such as workflows with fast feedback, efficient data loaders and GPU-aware training pipelines. Shortening our experimentation loop made it easier to iterate on architectures and features. Without this tooling, it would have been tempting to over-index on one-off model tweaks rather than systematic improvement.</p> <p><strong>Do not underestimate simple architectures at scale</strong> On our million-scale tabular problem, a ResNet-style MLP ultimately outperformed more complex tabular DL architectures from the literature. These more complex models tended to be harder to train, slower, and offered little upside once we accounted for our data scale and operational constraints. This shows that at our scale a <strong>simple and robust</strong> architecture was a better fit than a complex and novel one.</p> <h2 id="future-work">Future Work</h2> <p>While the migration is a success in itself that already brought performance improvements in production, we still believe that most of the uplift that neural networks can bring in our domain is yet to be realized. Among these future avenues, we identify:</p> <ul> <li> <p><strong>Multi-task and multi-objective modelling.</strong> Now that the core risk model of Company is powered by a neural network, it becomes easier to consider integrating other domain-specific risk models (e.g. card testing or abusive refunds detection) in a multi-task risk model or using multi-objective losses that balance fraud, customer experience, and operational costs. This could leverage the specificities of these multiple models in a large unified network, and greatly reduce operational overhead.</p> </li> <li> <p><strong>Closer integration with foundation models.</strong> Company has ongoing work on foundation models for payments. A deep learning-based fraud model provides a natural anchor for integrating such models, whether through shared embeddings, pretraining on large-scale transaction data, or joint training for related tasks.</p> </li> <li> <p><strong>Better use of structured and semi-structured fields.</strong> We have only started to exploit the potential of structured fields like anonymised emails or sequence-based velocity features in FDM’s neural network. There is ample room for better architectures and regularisation schemes tailored to these modalities.</p> </li> <li> <p><strong>Benchmarks that reflect production constraints.</strong><br/> Our experience suggests that small academic tabular benchmarks do not fully capture the realities of large-scale production systems where latency, explainability, and stability matter as much as raw performance metrics. Designing benchmarks that incorporate these dimensions could lead to architectures and training schemes that transfer more directly to real-world deployments.</p> </li> <li> <p><strong>Characterising when NNs should replace trees.</strong><br/> Finally, we would like to better understand, in a more principled way, when deep learning is likely to dominate gradient-boosted trees in tabular settings. Factors such as data size, feature types, temporal structure, and available infrastructure all play a role. Formalising these trade-offs could help teams decide when to invest in a migration like the one described here.</p> </li> </ul> <p>We hope that this case study, and the concrete migration pattern we followed – from offline experiments, to stacking, to a full deep learning model – can serve as a practical blueprint for teams considering a migration path from ML to DL in production — not by replacing everything at once, but by moving <em>one reversible step at a time</em>.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[["A mature and entrenched boosting system has been powering Company’s risk systems for years. We outline our year long incremental migration strategy to a pure deep learning system which is highlighted by an intermediate heterogeneous ensembling phase used to reached parity and then outperforming our boosting model in production. We learned along the way that a simple MLPs can beat sophisticated tabular DL architectures at million-scale (1); ensembling is a practical bridge from ML to DL (2); and the biggest wins from DL are often beyond metrics (3)."]]]></summary></entry><entry><title type="html">When SOTA Meets Reality: Lessons from Deploying NLP at a Large Healthcare Organization</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/healthcare-nlp/" rel="alternate" type="text/html" title="When SOTA Meets Reality: Lessons from Deploying NLP at a Large Healthcare Organization"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/healthcare-nlp</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/healthcare-nlp/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>The gap between a Jupyter notebook and a hospital server is not just a matter of deployment engineering, it is a fundamental conflict of objectives.</p> <p>Machine learning researchers are trained to chase the upper bounds of performance metrics. We want the highest F1-score, the lowest perplexity, or the top spot on a leaderboard. But at the {Anonymous Healthcare Organization}, where our team processes millions of pathology reports to track cancer incidence and patient outcomes, we learned that a “perfect” model can still fail to solve the actual problem.</p> <p>Over the course of four years, we deployed various NLP models, from simple regex patterns to fine-tuned BERT-type <d-cite key="devlin2019bert"></d-cite> models and Large Language Models (LLMs), for tasks including tumor reportability classification, cancer relapse detection, anatomical site identification, automated staging, and report segmentation.</p> <p>This post shares the unvarnished reality of what worked, what didn’t, and why the gap between research innovation and real-world healthcare deployment is wider than most people think.</p> <h2 id="the-metric-trap">The Metric Trap</h2> <p>In standard machine learning tasks, we define success as maximizing a metric like accuracy, F1-score, or Area Under the Curve (AUC). However, in {Anonymous Healthcare Organization’s} production pipeline, the cost functions are asymmetric and tied to human labor rather than model statistics.</p> <p>Consider our task of <em>Reportable Tumor Identification</em>, which involves determining which pathology reports contain cancers that must be tracked and reported. In the academic view, the goal is simply to maximize the F1-score by balancing precision and recall. But the operational reality is far more complex. Every “Positive” prediction triggers a manual review by a highly trained tumor registrar to finalize the case, while every “Negative” is archived. This creates a high-stakes environment where false negatives result in missed cancer cases, but false positives flood registrars with irrelevant reports, leading to burnout.</p> <p>We discovered that the metric that actually mattered was <em>Time Saved Per Report</em>. When we analyzed the operational data, the results were counterintuitive. Using our older (purely rule-based) NLP system, for every 1000 true positives, we will get 400 false positives. Manual processing by registrars takes about one minute per report, leading to 1400 minutes of work. Using the updated system with language models, we were able to bring down the false positives to 100, leading to a total 1100 reports compared to 1400. If we had only focused on filtering, the savings would be modest. However, by designing the model to perform <em>sentence-level highlighting</em>, pinpointing the exact evidence for its decision, we reduced the human review time from 60 seconds to 30 seconds per report.</p> <p>The result was a total processing time of 550 minutes, a nearly 60% reduction in workload. This highlighted a critical reality: a model with lower theoretical accuracy that integrates effectively into the human workflow (via explainability features) is vastly more valuable than a “State of the Art” black box that achieves marginally higher accuracy but offers no assistive utility.</p> <blockquote> <p><strong>The Lesson:</strong> Don’t just optimize for accuracy; optimize for the bottleneck. While ML experts measure success by accuracy and ROC curves, organizations measure success by backlog reduction. A tool that aids interpretability often yields higher utility than a “black box” with marginally higher accuracy.</p> </blockquote> <h2 id="do-not-use-a-cannon-to-kill-a-fly">Do Not Use a Cannon to Kill a Fly</h2> <p>With the current hype surrounding Generative AI <d-cite key="leaver2023chatgpt"></d-cite>, there is enormous pressure to throw an LLM at every text processing problem. In our experience, this approach is often computationally wasteful, prone to hallucinations, and less robust than simpler methods. We advocate for a <em>Pragmatic Hybrid Architecture</em>, essentially a waterfall approach where data flows through progressively more sophisticated models only when necessary.</p> <pre><code class="language-mermaid">graph TD
    A[Pathology Report] --&gt; B{Text Analysis}
    
    B --&gt;|Structured Data&lt;br/&gt;dates, codes, staging| C[Layer 1: Regex]
    B --&gt;|Semantic Understanding&lt;br/&gt;classification tasks| D[Layer 2: BERT]
    B --&gt;|Complex/Ambiguous&lt;br/&gt;8-12% of cases| E[Layer 3: LLM]
    
    C --&gt;|High precision&lt;br/&gt;instant processing| F[Extracted Structured Data]
    D --&gt;|High accuracy&lt;br/&gt;fast processing| G[Classification Result]
    E --&gt;|High accuracy&lt;br/&gt;slow, complex reasoning| H[Nuanced Analysis]
    
    F --&gt; I[Final Output]
    G --&gt; I
    H --&gt; I
    
    style A fill:#e8f4f8,stroke:#333,stroke-width:2px
    style C fill:#aed6f1,stroke:#333,stroke-width:2px
    style D fill:#a9dfbf,stroke:#333,stroke-width:2px
    style E fill:#f9e79f,stroke:#333,stroke-width:2px
    style I fill:#f5b7b1,stroke:#333,stroke-width:2px
    
    classDef layer1 fill:#aed6f1,stroke:#333,stroke-width:2px
    classDef layer2 fill:#a9dfbf,stroke:#333,stroke-width:2px
    classDef layer3 fill:#f9e79f,stroke:#333,stroke-width:2px
</code></pre> <div class="caption"> Figure 1: Our pragmatic hybrid architecture processes reports through layers of increasing sophistication, reserving expensive models for genuinely difficult cases. </div> <p><br/></p> <p>The first line of defense is what we call the <em>“Boring Layer”</em>: regular expressions. For structured data like dates, histology codes, or tumor staging notation (e.g., “T1N0M0”), regex provides 100% precision with zero hallucinations. It is fast, cheap, and explainable. Extracting “Grade 3” from a standardized field does not require a GPU.</p> <p>When semantic understanding is required, such as distinguishing between a patient’s history of cancer versus a current diagnosis, we escalate to the <em>“Efficient Layer”</em>. Here, fine-tuned BERT-type models (like Gatortron <d-cite key="yang2022gatortron"></d-cite> or ClinicalBERT <d-cite key="alsentzer2019publicly"></d-cite>) excel. These smaller, domain-specific models often outperform general-purpose LLMs on focused classification tasks while costing a fraction of the computational budget <d-cite key="gu2021domain"></d-cite>.</p> <p>We reserve the <em>“Smart Layer”</em>: Generative AI, for the 8-12% of cases that are genuinely ambiguous, require complex reasoning, or involve summarization. This represents a small fraction of our volume but handles the edge cases where simpler methods fail.</p> <p>Crucially, we found that <em>Report Segmentation</em> was an unsung hero. Pathology reports are filled with noisy headers, disclaimers, and legal text. Using a lightweight model to strip this noise and feed only the relevant diagnostic text to downstream models improved performance more than simply scaling up the model size. As is often the case, better preprocessing beats bigger parameters <d-cite key="smelyakov2020effectiveness"></d-cite>.</p> <blockquote> <p><strong>The Lesson:</strong> Model selection should be pragmatic, not trendy. Match the complexity of the method to the complexity of the problem. If a regex works, use it. Preprocessing (segmentation) often delivers higher ROI than increasing parameter count.</p> </blockquote> <h2 id="data-quality-is-everything">Data Quality is Everything</h2> <p>In academic datasets, labels are usually treated as ground truth. In healthcare, we learned that labels are often opinions. When we analyzed our initial training data, we found that label noise was a massive bottleneck; models trained on a single annotator’s data were essentially learning that specific person’s biases rather than the medical truth.</p> <p>To fix this, we recommend a <em>consensus-based approach</em>. Where we define a “Code Book”: a living document of annotation guidelines. We ran pilot studies where multiple experts labeled the same reports, and where they disagreed, we held discussions to refine the definitions. If human experts cannot agree on the label for a specific report, a model has no chance of learning it correctly.</p> <p>We also had to contend with the reality that medical data is not static. Terminology evolves, and reporting formats change. A model trained on 2019 pathology reports will inevitably struggle with 2024 reports using new classifications from the governing bodies (WHO, NHS, etc.).</p> <pre><code class="language-mermaid">graph LR
    A[Training Data&lt;br/&gt;2019-2020] --&gt; B[Model Training]
    B --&gt; C[High Accuracy&lt;br/&gt;95%]
    C --&gt; D[Deployment]
    D --&gt; E[2021 Data&lt;br/&gt;92% accuracy]
    E --&gt; F[2022 Data&lt;br/&gt;88% accuracy]
    F --&gt; G[2023 Data&lt;br/&gt;85% accuracy]
    
    style A fill:#90EE90
    style C fill:#90EE90
    style E fill:#FFD700
    style F fill:#FFA500
    style G fill:#FF6347
</code></pre> <div class="caption"> Figure 2: Without continuous monitoring, model performance degrades over time as medical terminology shifts. </div> <p><br/></p> <p>This necessitated <em>Automated Drift Detection</em>. By monitoring prediction distributions, confidence scores, and with human-in-the-loop approach, we can detect when the model becomes less confident or when the data distribution shifts, signaling a need for retraining.</p> <blockquote> <p><strong>The Lesson:</strong> Data quality and representativeness matter more than model sophistication. Invest in a “Code Book” and consensus processes early. Expect your drift in your data, and build monitoring systems that alert you when it does.</p> </blockquote> <h2 id="error-handling-and-system-design">Error Handling and System Design</h2> <p>No model is perfect, and in healthcare, “hallucination” is not just a quirk, it’s a liability. We learned that the reliability of a system depends less on eliminating every error and more on how the system handles those errors when they inevitably occur.</p> <p>We implemented <em>Confidence-Based Routing</em> to manage this risk. High-confidence predictions are processed automatically, while low-confidence predictions are flagged for human review (human-in-the-loop). Furthermore, if a report looks drastically different from the training data (out-of-distribution), it defaults to the manual queue.</p> <pre><code class="language-mermaid">graph TD
    A[Incoming Report] --&gt; B{Confidence Score}
    B --&gt;|High Confidence| C[Automated Processing]
    B --&gt;|Low Confidence| D[Human Review Queue]
    C --&gt; E{Audit Sample}
    E --&gt;|Random Sample| F[Quality Check]
    E --&gt;|Pass Through| G[Production Database]
    F --&gt; H{Discrepancy?}
    H --&gt;|Yes| I[Flag for Investigation]
    H --&gt;|No| G
    D --&gt; J[Expert Review]
    J --&gt; G
</code></pre> <div class="caption"> Figure 3: Multi-layer error mitigation pipeline combining confidence-based routing, human-in-the-loop validation, and continuous auditing. </div> <p><br/></p> <p>To ensure long-term safety, we adopted a <em>clinical-trial design approach to auditing</em>. Rather than ad-hoc spot checks, we established a rigorous sampling protocol to estimate error rates with statistical significance. Routinely, we conduct a structured audit to ensure the model hasn’t silently degraded. Trust in AI is not built on a single high accuracy score, but on the assurance that the system knows when it doesn’t know.</p> <blockquote> <p><strong>The Lesson:</strong> Accept that errors will happen and design your system to handle them gracefully. Confidence thresholds, human-in-the-loop validation, and statistically rigorous auditing are essential for reliable healthcare AI systems.</p> </blockquote> <h2 id="privacy-as-an-architectural-constraint">Privacy as an Architectural Constraint</h2> <p>When working with sensitive patient data, privacy cannot be an afterthought; it must be a fundamental architectural constraint. Large Language Models have a known propensity to memorize training data, which poses a catastrophic risk in a healthcare organization. If an adversary could query a model to reconstruct an individual’s data, we would have failed our patients.</p> <p>To mitigate this, we rely primarily on <em>local, open-weights models</em> (like Llama or Mistral) hosted entirely within our firewall. Sending patient data to a public API is simply not an option for us. Additionally, where possible, we integrated <em>Differential Privacy (DP)</em> <d-cite key="dwork2006differential"></d-cite> into our training pipeline. DP provides a mathematical guarantee that individual patient data cannot be reverse-engineered from the model weights, but it degrades utility. In scenarios where high utility is desired, in addition to locally hosted models, the models are only trained on fully anonymized data.</p> <blockquote> <p><strong>The Lesson:</strong> Privacy must be integrated into the development lifecycle, not added at the end. Prefer local, offline models for sensitive data, and evaluate the trade-off between Differential Privacy guarantees, model utility, and data anonymization.</p> </blockquote> <h2 id="co-design-and-ai-literacy">Co-Design and AI Literacy</h2> <p>We had a significant structural advantage, being a large healthcare organization, our team includes ML researchers, tumor registrars, other subject matter experts, and clinicians working side-by-side.</p> <p>This collaboration forced a critical pivot in our project. Initially, our goal was purely technical: “Create an NLP solution that is 99% accurate.” However, after sitting down with other stakeholders and understanding their daily struggles, we realized that accuracy wasn’t their primary pain point. Their problem was the <em>24-month backlog</em> due to them being inundated with data.</p> <p>We revised our goal to “Reduce the backlog by 50%,” which changed our entire technical roadmap. Instead of building a black-box classifier to replace humans, we built an assistive tool that highlights evidence to speed them up.</p> <pre><code class="language-mermaid">graph TB
    NLP[NLP System]
    
    subgraph End Users
        TR[Subject Matter Experts]
    end
    
    subgraph Clinical Experts
        ONC[Clinicians]
    end
    
    subgraph Technical Team
        ML[ML Researchers]
        IT[IT Infrastructure]
    end
    
    subgraph Governance
        PRIV[Privacy Officers]
        ADMIN[Administrators]
    end
    
    NLP --- TR
    NLP --- ONC
    NLP --- ML
    NLP --- IT
    NLP --- PRIV
    NLP --- ADMIN
    
    TR -.workflow needs.-&gt; ML
    ONC -.clinical validation.-&gt; ML
    ML -.technical specs.-&gt; IT
    IT -.infrastructure constraints.-&gt; ML
    PRIV -.compliance requirements.-&gt; ML
    ADMIN -.resource allocation.-&gt; ML
    
    style NLP fill:#e74c3c,stroke:#333,stroke-width:3px,color:#fff
    style TR fill:#3498db,stroke:#333,stroke-width:2px
    style ONC fill:#e67e22,stroke:#333,stroke-width:2px
    style ML fill:#2ecc71,stroke:#333,stroke-width:2px
    style IT fill:#2ecc71,stroke:#333,stroke-width:2px
    style PRIV fill:#f39c12,stroke:#333,stroke-width:2px
    style ADMIN fill:#f39c12,stroke:#333,stroke-width:2px
</code></pre> <div class="caption"> Figure 4: Successful deployment required alignment across multiple groups within the organization, each with different priorities and expertise. </div> <p><br/></p> <p>This co-design process also required investing in <em>AI Literacy</em> <d-cite key="yi2021establishing"></d-cite>. We couldn’t just drop an AI tool on clinical staff and walk away; we had to teach them how the models worked, where they failed, and why they made certain predictions. When domain experts understand the “black box,” they trust it more and become better at catching its errors.</p> <blockquote> <p><strong>The Lesson:</strong> Involve end-users from Day 1. Co-designing the solution ensures you are solving the business problem (backlogs), not just a technical problem. Furthermore, educating your users about AI capabilities and limitations builds the trust required for adoption.</p> </blockquote> <h2 id="the-dare-framework">The DARE Framework</h2> <p>Many healthcare organizations lack in-house ML expertise and opt to buy off-the-shelf AI tools. This is often risky, as a vendor’s “99% accuracy” claim is usually based on their clean, curated dataset, not your messy real-world data.</p> <p>To help organizations navigate this “Build vs. Buy” decision, we propose the <strong>DARE framework</strong>:</p> <ul> <li><strong>D - Demand Robust Validation:</strong> Do not accept whitepapers as proof. Demand validation on <em>your</em> local data distribution. Vendors should be willing to run their model on your data to prove it works in your specific context.</li> <li><strong>A - Assess Flexibility:</strong> Can the tool handle your specific quirks, such as local report formatting or unique abbreviations? Can it be fine-tuned when standards change?</li> <li><strong>R - Rigorous Internal Compatibility:</strong> Does the tool introduce fairness biases regarding your specific patient demographics? Does it integrate seamlessly with your existing IT infrastructure?</li> <li><strong>E - Ease of Evaluation:</strong> Avoid black boxes. Can you audit the logs? Does it provide confidence scores? Can clinical staff override the AI when needed?</li> </ul> <blockquote> <p><strong>The Lesson:</strong> Be skeptical of “plug-and-play” AI. Use the DARE framework to validate vendor claims on <em>your</em> specific data before committing resources.</p> </blockquote> <h2 id="conclusion">Conclusion</h2> <p>The journey from academic machine learning to deployed healthcare AI requires rethinking many of our ingrained instincts. We learned that the solution saving the most time isn’t always the one with the highest accuracy on a held-out test set, and that “boring” tools like regex often outperform the latest LLMs on structured tasks.</p> <p>Most importantly, we learned that deployment cannot be an afterthought. It requires deep collaboration between researchers and domain experts to ensure we are solving the right problems. By prioritizing workflows over metrics, respecting data privacy as a hard constraint, and demanding rigorous validation, we can bridge the gap between research and production.</p> <p>In machine learning conferences, SOTA means topping the benchmark. In healthcare, true SOTA is a system that runs reliably, respects patient privacy, and clears the backlog so patients get treated faster.</p> <p><strong>The real SOTA is AI that works.</strong></p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[In academia, we optimize for accuracy. In healthcare, we optimize for patient outcomes. This is the story of how a large healthcare organization reduced a multi-year backlog not by using the largest or newest model, but by using the right one.]]></summary></entry><entry><title type="html">In-context learning of representations can be explained by induction circuits</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/iclr-induction/" rel="alternate" type="text/html" title="In-context learning of representations can be explained by induction circuits"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/iclr-induction</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/iclr-induction/"><![CDATA[<h2 id="recapitulation-and-reproduction-of-park-et-al-2025">Recapitulation and reproduction of Park et al., 2025</h2> <p>In this section, we provide a detailed description of Park et al. <d-cite key="park2025iclr"></d-cite>. We successfully reproduce their results on Llama-3.1-8B <d-cite key="grattafiori2024llama3"></d-cite>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-iclr-induction/park_fig_1-480.webp 480w,/2026/assets/img/2026-04-27-iclr-induction/park_fig_1-800.webp 800w,/2026/assets/img/2026-04-27-iclr-induction/park_fig_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-iclr-induction/park_fig_1.png" class="" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 1.</b> <b>Overview of Park et al., 2025.</b> The grid tracing task uses a $4 {\times} 4$ grid of words. Models observe random walks on the grid (e.g., <code>&nbsp;apple</code><code>&nbsp;bird</code><code>&nbsp;milk</code><code>&nbsp;sand</code><code>&nbsp;sun</code><code>&nbsp;plane</code><code>&nbsp;opera</code><code>&nbsp;...</code>) where consecutive words are always neighbors. As the sequence length grows, the model begins to predict valid next words based on the learned graph structure. More surprisingly, the geometry of the model's effective token representations mirrors that of the grid structure: the model learns (in context) to represent each node adjacent to its neighbor. <br/> Figure reproduced from Park et al., 2025. </div> <h3 id="the-grid-tracing-task">The grid tracing task</h3> <p>Park et al. <d-cite key="park2025iclr"></d-cite> introduce the <em>in-context graph tracing</em> task. The task involves a predefined graph \(\mathcal{G} = (\mathcal{T}, E)\) where nodes \(\mathcal{T} = \{\tau_1, \tau_2, \ldots, \tau_n\}\) are referenced via tokens (e.g., <code>&nbsp;apple</code>, <code>&nbsp;bird</code>, <code>&nbsp;math</code>, etc.). The graph’s connectivity structure \(E\) is defined independently of any semantic relationships between the concepts. The model is provided with traces of random walks on this graph as context and must predict valid next nodes based on the learned connectivity structure. While <d-cite key="park2025iclr"></d-cite> study graph tracing on three different graph structures, we focus exclusively on their \(4{\times}4\) square grid setting (Figure 1). We provide details of the experimental setup below; our methodology always follows <d-cite key="park2025iclr"></d-cite> except when otherwise noted.</p> <p><strong>Grid structure.</strong> The task uses a \(4{\times}4\) grid of 16 distinct word tokens: <code>&nbsp;apple</code>, <code>&nbsp;bird</code>, <code>&nbsp;car</code>, <code>&nbsp;egg</code>, <code>&nbsp;house</code>, <code>&nbsp;milk</code>, <code>&nbsp;plane</code>, <code>&nbsp;opera</code>, <code>&nbsp;box</code>, <code>&nbsp;sand</code>, <code>&nbsp;sun</code>, <code>&nbsp;mango</code>, <code>&nbsp;rock</code>, <code>&nbsp;math</code>, <code>&nbsp;code</code>, <code>&nbsp;phone</code>.<d-footnote>All words tokenize to exactly one token when preceded by a space (e.g., <code>&nbsp;apple</code> is a single token). Sequences are tokenized with a leading space before the first word, ensuring single-token-per-word encoding.</d-footnote> Each word occupies a unique position in the grid. Two words are <em>neighbors</em> if they are horizontally or vertically adjacent (not diagonally). This defines an adjacency matrix \(A \in \{0,1\}^{16 \times 16}\) where \(A_{ij} = 1\) if and only if words \(i\) and \(j\) are neighbors.</p> <p><strong>Random walk generation.</strong> Training sequences are generated via random walks on this grid. Starting from a random position, at each step the walk moves to a uniformly random neighbor. This produces sequences like <code>&nbsp;apple</code><code>&nbsp;bird</code><code>&nbsp;milk</code><code>&nbsp;sand</code><code>&nbsp;sun</code><code>&nbsp;plane</code><code>&nbsp;opera</code><code>&nbsp;...</code> where consecutive words are always grid neighbors. Following <d-cite key="park2025iclr"></d-cite>, we use sequence lengths of 1400 tokens.</p> <p><strong>Measuring accuracy.</strong> At timestep \(t\), the random walk is at node \(w_t\) with neighbor set \(\mathcal{N}(w_t)\). The model outputs a distribution \(p_{\theta}(\cdot \mid w_{1:t})\) over vocabulary tokens. “Rule following accuracy” is defined as the probability mass assigned to any valid next node:</p> \[\text{acc}_t \;=\; \sum_{w \in \mathcal{N}(w_t)} p_{\theta}(w \mid w_{1:t}).\] <p><strong>PCA visualization.</strong> To assess whether the model’s representations come to resemble the grid structure, activations are extracted from a late layer (layer 26) and projected onto their top principal components. For each of the 16 words, a class-mean activation is computed by averaging over all occurrences in the final 200 positions of the sequence. The first two principal components of these 16 class-mean vectors define a 2D subspace onto which the class means are projected for visualization. If the representational geometry reflects the grid, neighboring tokens should appear nearby in this projection.</p> <h3 id="replication-and-interpretation">Replication and interpretation</h3> <p>We replicate the results of Park et al. on Llama-3.1-8B. Figure 2 shows our reproduction: accuracy on the grid tracing task increases with context length, reaching \({\sim}0.90\) after \({\sim}10^3\) tokens, and PCA projections of late-layer activations reveal geometric structure mirroring the grid.</p> <div class="l-page"> <div class="plot-row"> <div class="plot-container plot-standard w-65"> <iframe src="/2026/assets/html/2026-04-27-iclr-induction/repro_paper/accuracy_curve.html"></iframe> <img class="plot-fallback" src="/2026/assets/html/2026-04-27-iclr-induction/repro_paper/accuracy_curve.png" alt="Accuracy curve showing model performance increasing with context length"/> </div> <div class="plot-container plot-tall w-35"> <iframe src="/2026/assets/html/2026-04-27-iclr-induction/repro_paper/pca_visualization.html"></iframe> <img class="plot-fallback" src="/2026/assets/html/2026-04-27-iclr-induction/repro_paper/pca_visualization.png" alt="PCA visualization showing grid structure in token representations"/> </div> </div> </div> <div class="caption"> <b>Figure 2.</b> <b>Reproduction of main results from Park et al., 2025.</b> <b>Left:</b> Model accuracy on the grid tracing task increases with context length, reaching ${\sim}0.90$ accuracy after ${\sim}10^3$ tokens. Shaded region shows $\pm 1$ standard deviation across 16 random sequences. <b>Right:</b> PCA projection of class-mean activations at layer 26 after seeing 1400 tokens. Gray dashed lines connect grid neighbors. The geometry of the effective representations resembles the grid structure underlying the data. </div> <p>Park et al. interpret these findings as evidence that the geometric reorganization plays a functional role in task performance: the model learns the graph structure in its representations, and this learned structure is what enables accurate next-node predictions.</p> <blockquote> <p>“We see once a critical amount of context is seen by the model, accuracy starts to rapidly improve. We find this point in fact closely matches when Dirichlet Energy<d-footnote>Dirichlet energy measures how much a signal varies across graph edges. Low energy means neighboring nodes have similar representations, so Park et al. use it to quantify how well the model's representations respect the graph structure.</d-footnote> reaches its minimum value: energy is minimized shortly before the rapid increase in in-context task accuracy, suggesting that the structure of the data is correctly learned before the model can make valid predictions. This leads us to the claim that as the amount of context is scaled, there is an <strong>emergent re-organization of representations that allows the model to perform well</strong> on our in-context graph tracing task.” <d-cite key="park2025iclr"></d-cite> (Section 4.1)</p> </blockquote> <p>We propose a simpler mechanistic account in the following sections.</p> <hr/> <h2 id="a-simpler-explanation-induction-circuits">A simpler explanation: induction circuits</h2> <p>We propose that the grid navigation task can be solved by a much simpler mechanism than abstract spatial representation learning: <em>induction circuits</em> <d-cite key="elhage2021mathematical"></d-cite>, <d-cite key="olsson2022context"></d-cite>.</p> <p>An induction circuit consists of two types of attention heads working together. <em>Previous-token heads</em> attend from position \(t\) to position \(t{-}1\), copying information about the previous token into the current position’s residual stream. <em>Induction heads</em> then attend to positions that follow previous occurrences of the current token. Together, they implement in-context bigram recall: “if \(A\) followed \(B\) before, predict \(A\) when seeing \(B\) again.”<d-footnote>In the literature, the term "induction head" is sometimes used to refer to both the individual attention head and the full two-component circuit. We use "induction circuit" for the full mechanism and "induction head" for the specific head that attends to tokens following previous occurrences, to avoid ambiguity.</d-footnote></p> <p>In the grid task, if the model has seen the bigram <code>&nbsp;apple</code><code>&nbsp;bird</code> earlier in the sequence, then upon encountering <code>&nbsp;apple</code> again, the induction circuit can retrieve and predict <code>&nbsp;bird</code>. With enough context, the model will have observed multiple successors for each token, and can aggregate over these to assign probability mass to all valid neighbors.<d-footnote>For example, if the model has seen both <code>&nbsp;apple</code><code>&nbsp;bird</code> and <code>&nbsp;apple</code><code>&nbsp;house</code>, it can distribute probability across both <code>&nbsp;bird</code> and <code>&nbsp;house</code> when predicting the next token after <code>&nbsp;apple</code>.</d-footnote></p> <h3 id="testing-the-induction-hypothesis">Testing the induction hypothesis</h3> <p>If the model relies on induction heads to solve the task, then ablating these heads should substantially degrade task performance. We test this via <em>zero ablation</em>: setting targeted attention heads’ outputs to zero and measuring the causal impact on both task accuracy and in-context representations.</p> <p><strong>Head identification.</strong> Following <d-cite key="olsson2022incontext"></d-cite>, we identify induction heads and previous-token heads using attention pattern analysis on repeated sequences (see Appendix A for details). Induction heads attend to positions one token <em>after</em> where the current token previously appeared. Previous-token heads simply attend to the immediately preceding position. We rank all 1024 heads in Llama-3.1-8B by their induction and previous-token scores, yielding two ranked lists.</p> <p><strong>Ablation procedure.</strong> For each head type, we ablate the top-\(k\) heads for \(k \in \{1, 2, 4, 8, 16, 32\}\) and measure impact on task accuracy and PCA structure of representations. As a control, we ablate random heads sampled from all heads excluding the top 32 induction and top 32 previous-token heads.</p> <h3 id="results">Results</h3> <div class="l-page"> <div class="plot-row"> <div class="plot-container"> <iframe src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/ablation_induction.html"></iframe> <img class="plot-fallback" src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/ablation_induction.png" alt="Effect of ablating induction heads on accuracy"/> </div> <div class="plot-container"> <iframe src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/ablation_prev_token.html"></iframe> <img class="plot-fallback" src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/ablation_prev_token.png" alt="Effect of ablating previous-token heads on accuracy"/> </div> </div> </div> <div class="caption"> <b>Figure 3.</b> <b>Effect of head ablation on task accuracy.</b> <b>Left:</b> Ablating top induction heads progressively degrades accuracy, but the model still learns with context. <b>Right:</b> Ablating top previous-token heads causes accuracy to plateau, preventing learning even with more context. <span style="color: gray;">Gray line</span> shows ablating 32 random heads (excluding top induction and prev-token heads) as a control. </div> <p><strong>Both induction heads and previous-token heads are critical to task performance.</strong> Figure 3 shows task accuracy under head ablations. Ablating the top-4 induction heads causes accuracy to drop from \({\sim}0.90\) to \({\sim}0.70\), and ablating the top-32 drops accuracy all the way to \({\sim}0.40\). Ablating just the top-2 previous-token heads reduces accuracy \({&lt;}0.60\), and ablating the top-32 previous-token heads further drops accuracy to \({\sim}0.30\).</p> <p>In contrast, ablating \(k\) random heads causes only very small degradation to \({\sim}0.85\), suggesting that induction and previous-token heads are <em>particularly</em> important for task performance.<d-footnote>The random heads are sampled from the set of all heads <em>excluding</em> the top-32 induction heads and top-32 previous-token heads. We sample 4 different random sets of 32 heads and report the averaged results.</d-footnote></p> <p>While ablating induction heads significantly impairs task performance, accuracy continues to ascend as context length increases. In contrast, ablating previous token heads causes accuracy to plateau, even as context length grows.</p> <div class="l-page"> <div class="plot-row-triple"> <div class="plot-container"> <iframe src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/pca_ablation_baseline.html"></iframe> <img class="plot-fallback" src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/pca_ablation_baseline.png" alt="PCA baseline - grid structure visible"/> </div> <div class="plot-container"> <iframe src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/pca_ablation_induction.html"></iframe> <img class="plot-fallback" src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/pca_ablation_induction.png" alt="PCA with induction heads ablated - grid structure preserved"/> </div> <div class="plot-container"> <iframe src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/pca_ablation_prev_token.html"></iframe> <img class="plot-fallback" src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/pca_ablation_prev_token.png" alt="PCA with prev-token heads ablated - grid structure disrupted"/> </div> </div> </div> <div class="caption"> <b>Figure 4.</b> <b>Effect of head ablation on representational geometry.</b> PCA projections of class-mean activations under different ablation conditions. <b>Left:</b> Baseline (no ablation) shows clear grid structure. <b>Center:</b> Ablating top-32 induction heads preserves the grid geometry. <b>Right:</b> Ablating top-32 previous-token heads disrupts the spatial organization. This suggests previous-token heads are necessary for the geometric structure, while induction heads are not. </div> <p><strong>Ablating previous-token heads disrupts representational organization.</strong> While both head types are important for accuracy, they seem to have different effects on learned representations. The figure below shows that ablating induction heads preserves the grid-like geometric structure in PCA visualizations, as the 2D projections still resemble the spatial grid. However, ablating previous-token heads disrupts this structure, causing representations to lose their apparent spatial organization.</p> <h2 id="previous-token-mixing-can-account-for-representational-structure">Previous-token mixing can account for representational structure</h2> <p>In the previous section, we studied <em>task performance</em> and argued that the model achieves high task accuracy by using induction circuits. We now study the <em>representational geometry</em>, and ask if we can give a simple explanation for the grid-like PCA plots. We will argue that it is plausible that the structures are byproducts of mixing performed by previous-token heads.</p> <h3 id="the-neighbor-mixing-hypothesis">The neighbor-mixing hypothesis</h3> <p>Figure 4 shows that ablating previous-token heads disrupts the grid structure, while ablating induction heads preserves it. This suggests that previous-token heads are somehow necessary for the geometric organization. But what mechanism could link previous-token heads to spatial structure?</p> <p>Previous-token heads mix information from position \(t-1\) into position \(t\). In a random walk, the token at \(t-1\) is always a grid neighbor of the token at \(t\). So each token’s representation gets mixed with a neighbor’s. When we compute the class mean for word \(c\), we average over all positions where \(c\) appears, each mixed with whichever neighbor preceded it. Over many occurrences, \(c\) is preceded by each of its neighbors roughly equally, so the class mean for \(c\) roughly encodes \(c\) plus an average of its neighbors.</p> <p>To test whether neighbor-mixing alone can create the observed geometry, we construct a minimal toy model.</p> <h3 id="a-toy-model-of-previous-token-mixing">A toy model of previous-token mixing</h3> <p>We work directly in a 16-token space indexed by the \(4{\times}4\) grid nodes. Each node \(i\) is assigned an initial random vector \(\mathbf{e}_i \in \mathbb{R}^{4096}\), sampled i.i.d. from \(\mathcal{N}(0,I)\). PCA of just the raw embeddings \(\{\mathbf{e}_i\}\) produces an essentially unstructured cloud: there is no visible trace of the grid.</p> <p>We then apply a single, “neighbor mixing” step:</p> \[\tilde{\mathbf{e}}_i \;=\; \mathbf{e}_i \;+\; \frac{1}{|\mathcal{N}(i)|} \sum_{j \in \mathcal{N}(i)} \mathbf{e}_j,\] <p>where \(\mathcal{N}(i)\) denotes the set of neighbors of node \(i\).</p> <p>After this one step, PCA of the 16 mixed vectors \(\{\tilde{\mathbf{e}}_i\}\) recovers a clear \(4{\times}4\) grid: neighbors are close in the 2D projection and non-neighbors are far (Figure 5).</p> <div class="l-page"> <div class="plot-row"> <div class="plot-container plot-tall"> <iframe src="/2026/assets/html/2026-04-27-iclr-induction/neighbor_mixing/mixing_random_baseline.html"></iframe> <img class="plot-fallback" src="/2026/assets/html/2026-04-27-iclr-induction/neighbor_mixing/mixing_random_baseline.png" alt="PCA of random embeddings without mixing - no structure visible"/> </div> <div class="plot-container plot-tall"> <iframe src="/2026/assets/html/2026-04-27-iclr-induction/neighbor_mixing/mixing_random_mixed.html"></iframe> <img class="plot-fallback" src="/2026/assets/html/2026-04-27-iclr-induction/neighbor_mixing/mixing_random_mixed.png" alt="PCA after neighbor mixing - grid structure emerges"/> </div> </div> </div> <div class="caption"> <b>Figure 5.</b> <b>One round of neighbor mixing creates grid structure from random embeddings.</b> <b>Left:</b> PCA projection of 16 random Gaussian vectors $\mathbf{e}_i \sim \mathcal{N}(0, I)$ shows no spatial structure. <b>Right:</b> After applying one neighbor-mixing step, the same embeddings exhibit clear grid organization in PCA space. Gray dashed lines connect grid neighbors. </div> <h3 id="evidence-of-neighbor-mixing-in-individual-model-activations">Evidence of neighbor mixing in individual model activations</h3> <p>We find additional evidence that the representational structure is a result of previous-token mixing.</p> <p>Instead of collapsing each word into a single class mean, we take the final 200 positions of a length-1400 random-walk sequence and project all 200 residual-stream vectors into the same 2D PCA space used for the class means. Each point now corresponds to a specific activation. For each point, we display bigram information: the center color indicates the current token \(w_t\) and the border color indicates the previous token \(w_{t-1}\).</p> <div class="l-page"> <div class="plot-container plot-square"> <iframe src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/bigram_pca_seed0.html"></iframe> <img class="plot-fallback" src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/bigram_pca_seed0.png" alt="Bigram PCA showing each point colored by current token (fill) and previous token (border)"/> </div> </div> <div class="caption"> <b>Figure 6.</b> <b>Bigram-level PCA visualization.</b> Each point represents a single position's activation. Fill color indicates the current token; border color indicates the previous token. Points with the same current token but different previous tokens form distinct clusters, suggesting the representation encodes information about both. Star markers show token centroids. </div> <p>Figure 6 shows a consistent pattern. Points whose previous bigram is <code>&nbsp;plane</code><code>&nbsp;math</code> tend to lie between the <code>&nbsp;plane</code> and <code>&nbsp;math</code> centroids. Points with previous bigram <code>&nbsp;egg</code><code>&nbsp;math</code> tend to lie between <code>&nbsp;egg</code> and <code>&nbsp;math</code>. We see similar “in-between” behavior for all other bigrams. This is what one would expect if the representation of \(w_t\) contains something like a mixture of “self” and “previous token” rather than depending only on the current word.</p> <h2 id="limitations-and-open-questions">Limitations and open questions</h2> <p>Our experiments point towards a simple explanation: the model performs in-context graph tracing via induction circuits, and the grid-like PCA geometry is a byproduct of previous-token mixing. However, our understanding remains incomplete in important ways.</p> <p><strong>The toy model is a significant simplification.</strong> Our neighbor-mixing rule assumes that previous-token heads simply add the raw previous-token vector \(\mathbf{h}_{t-1}\) to the current position. In reality, attention heads apply value and output projections: they add \(W_O W_V \mathbf{h}_{t-1}\), where \(W_O W_V \in \mathbb{R}^{d_{\text{model}} \times d_{\text{model}}}\) is a low-rank matrix (rank \(\leq d_{\text{head}}\)). This projection could substantially transform the information being mixed, and notably cannot implement the identity mapping since it is low-rank. We also model everything as a single mixing step on static vectors, whereas the actual network has many attention heads, MLP blocks, and multiple layers that repeatedly transform the residual stream.</p> <p><strong>Why does the grid structure emerge late in the sequence?</strong> Previous-token heads are active from the start of the sequence, yet the grid-like PCA structure only becomes clearly visible after many tokens have been processed. If neighbor-mixing were the whole story, we might expect the geometric structure to appear earlier.</p> <p>These gaps mean our account should be understood as a <em>plausible mechanistic hypothesis</em> rather than a complete explanation. The key empirical findings stand: ablating induction heads degrades task performance while preserving PCA geometry, and ablating previous-token heads disrupts both performance and geometry.</p> <h2 id="conclusion">Conclusion</h2> <p>We have argued that the phenomena observed by Park et al. <d-cite key="park2025iclr"></d-cite> can be explained by well-known mechanisms in language models. Task performance on in-context graph tracing is well-explained by induction circuits, which recall previously-seen bigrams. The geometric organization visible in PCA plots appears to be a byproduct of previous-token mixing: because random walks traverse graph edges, previous-token heads mix each position’s representation with that of a graph neighbor, and this mixing alone is sufficient to produce grid-like structure from unstructured embeddings.</p> <p>These findings suggest that the “representation reorganization” observed by Park et al. may not reflect a sophisticated in-context learning strategy, but rather an artifact of previous-token head behavior.</p> <p>That said, our understanding has clear limits. Our toy model ignores the value and output projections of real attention heads, and we cannot yet explain why the geometric structure emerges only late in the sequence.</p> <hr/> <h2 id="appendix-a-head-detection-methodology">Appendix A: Head Detection Methodology</h2> <p>We identify induction heads and previous-token heads using attention pattern analysis on synthetic repeated sequences, following the approach of Olsson et al. <d-cite key="olsson2022incontext"></d-cite>.</p> <h3 id="a1-test-sequence-construction">A.1 Test Sequence Construction</h3> <p>We construct a test sequence by repeating a random sequence of tokens:</p> \[[\text{tok}_1, \ldots, \text{tok}_{32}, \text{tok}_1, \ldots, \text{tok}_{32}]\] <p>where tokens are sampled uniformly from the lower half of the vocabulary. The full sequence has length \(T = 64\). We run the model on a batch of 32 such sequences and extract attention patterns from all heads.</p> <p>For layer \(\ell \in \{1, \ldots, 32\}\) and head \(h \in \{1, \ldots, 32\}\) (Llama-3.1-8B has 32 layers and 32 heads per layer), let \(P^{(\ell,h)} \in \mathbb{R}^{T \times T}\) denote the attention pattern, where \(P^{(\ell,h)}_{t,s}\) is the attention weight from position \(t\) (query) to position \(s\) (key).</p> <h3 id="a2-induction-score">A.2 Induction Score</h3> <p>Induction heads exhibit a characteristic pattern: for a repeated token at position \(i\) in the second half of the sequence, they attend not to its earlier occurrence at position \(i - 32\), but to the token <em>after</em> that earlier occurrence, i.e., position \(i - 31\).</p> <p>We compute the induction score as the average attention along this offset-31 diagonal:</p> \[s_{\text{ind}}^{(\ell,h)} = \frac{1}{32} \sum_{i=33}^{64} P^{(\ell,h)}_{i, i-31}\] <p>This measures how strongly the head attends to “the token that followed the previous occurrence of the current token.”</p> <h3 id="a3-previous-token-score">A.3 Previous-Token Score</h3> <p>Previous-token heads implement a simpler pattern: at each position, they attend primarily to the immediately preceding token.</p> <p>We compute the previous-token score as the average attention along the offset-1 diagonal:</p> \[s_{\text{prev}}^{(\ell,h)} = \frac{1}{T-1} \sum_{i=2}^{T} P^{(\ell,h)}_{i, i-1}\] <h3 id="a4-head-ranking">A.4 Head Ranking</h3> <p>We average scores across the batch of 32 sequences, then rank all 1024 heads (32 layers \(\times\) 32 heads) by their induction and previous-token scores separately. This yields two ranked lists. For ablation experiments, we ablate the top-\(k\) heads from each list for \(k \in \{1, 2, 4, 8, 16, 32\}\).</p> <hr/> <h2 id="appendix-b-attention-pattern-gallery">Appendix B: Attention Pattern Gallery</h2> <p>This appendix provides detailed visualizations of the attention patterns for the top-ranked induction heads and previous-token heads identified in our analysis. Click the buttons to explore different heads, ranked by their induction or previous-token score.</p> <h3 id="b1-induction-head-attention-patterns">B.1 Induction Head Attention Patterns</h3> <p>Induction heads attend to positions where the current token previously appeared, specifically attending one position <em>after</em> the previous occurrence. In the visualization below, the induction diagonal (offset by half the sequence length) shows elevated attention weights.</p> <div class="l-page"> <div class="png-selector"> <input type="radio" name="ind-head" id="ind1" checked=""/> <input type="radio" name="ind-head" id="ind2"/> <input type="radio" name="ind-head" id="ind3"/> <input type="radio" name="ind-head" id="ind4"/> <input type="radio" name="ind-head" id="ind5"/> <input type="radio" name="ind-head" id="ind6"/> <input type="radio" name="ind-head" id="ind7"/> <input type="radio" name="ind-head" id="ind8"/> <input type="radio" name="ind-head" id="ind9"/> <input type="radio" name="ind-head" id="ind10"/> <input type="radio" name="ind-head" id="ind11"/> <input type="radio" name="ind-head" id="ind12"/> <input type="radio" name="ind-head" id="ind13"/> <input type="radio" name="ind-head" id="ind14"/> <input type="radio" name="ind-head" id="ind15"/> <input type="radio" name="ind-head" id="ind16"/> <input type="radio" name="ind-head" id="ind17"/> <input type="radio" name="ind-head" id="ind18"/> <input type="radio" name="ind-head" id="ind19"/> <input type="radio" name="ind-head" id="ind20"/> <input type="radio" name="ind-head" id="ind21"/> <input type="radio" name="ind-head" id="ind22"/> <input type="radio" name="ind-head" id="ind23"/> <input type="radio" name="ind-head" id="ind24"/> <input type="radio" name="ind-head" id="ind25"/> <input type="radio" name="ind-head" id="ind26"/> <input type="radio" name="ind-head" id="ind27"/> <input type="radio" name="ind-head" id="ind28"/> <input type="radio" name="ind-head" id="ind29"/> <input type="radio" name="ind-head" id="ind30"/> <input type="radio" name="ind-head" id="ind31"/> <input type="radio" name="ind-head" id="ind32"/> <div class="selector-buttons"> <label for="ind1">L15H30</label> <label for="ind2">L8H1</label> <label for="ind3">L16H20</label> <label for="ind4">L2H22</label> <label for="ind5">L10H14</label> <label for="ind6">L5H11</label> <label for="ind7">L15H1</label> <label for="ind8">L24H27</label> <label for="ind9">L20H14</label> <label for="ind10">L5H8</label> <label for="ind11">L20H1</label> <label for="ind12">L19H3</label> <label for="ind13">L2H20</label> <label for="ind14">L26H15</label> <label for="ind15">L2H12</label> <label for="ind16">L27H6</label> <label for="ind17">L26H13</label> <label for="ind18">L28H15</label> <label for="ind19">L2H25</label> <label for="ind20">L13H6</label> <label for="ind21">L16H1</label> <label for="ind22">L22H14</label> <label for="ind23">L27H5</label> <label for="ind24">L27H7</label> <label for="ind25">L27H4</label> <label for="ind26">L2H21</label> <label for="ind27">L10H13</label> <label for="ind28">L5H9</label> <label for="ind29">L16H23</label> <label for="ind30">L20H13</label> <label for="ind31">L27H20</label> <label for="ind32">L19H0</label> </div> <div class="selector-images"> <div class="selector-image img-ind1"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank01_L15H30.png" alt="L15H30"/></div> <div class="selector-image img-ind2"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank02_L8H1.png" alt="L8H1"/></div> <div class="selector-image img-ind3"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank03_L16H20.png" alt="L16H20"/></div> <div class="selector-image img-ind4"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank04_L2H22.png" alt="L2H22"/></div> <div class="selector-image img-ind5"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank05_L10H14.png" alt="L10H14"/></div> <div class="selector-image img-ind6"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank06_L5H11.png" alt="L5H11"/></div> <div class="selector-image img-ind7"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank07_L15H1.png" alt="L15H1"/></div> <div class="selector-image img-ind8"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank08_L24H27.png" alt="L24H27"/></div> <div class="selector-image img-ind9"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank09_L20H14.png" alt="L20H14"/></div> <div class="selector-image img-ind10"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank10_L5H8.png" alt="L5H8"/></div> <div class="selector-image img-ind11"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank11_L20H1.png" alt="L20H1"/></div> <div class="selector-image img-ind12"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank12_L19H3.png" alt="L19H3"/></div> <div class="selector-image img-ind13"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank13_L2H20.png" alt="L2H20"/></div> <div class="selector-image img-ind14"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank14_L26H15.png" alt="L26H15"/></div> <div class="selector-image img-ind15"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank15_L2H12.png" alt="L2H12"/></div> <div class="selector-image img-ind16"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank16_L27H6.png" alt="L27H6"/></div> <div class="selector-image img-ind17"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank17_L26H13.png" alt="L26H13"/></div> <div class="selector-image img-ind18"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank18_L28H15.png" alt="L28H15"/></div> <div class="selector-image img-ind19"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank19_L2H25.png" alt="L2H25"/></div> <div class="selector-image img-ind20"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank20_L13H6.png" alt="L13H6"/></div> <div class="selector-image img-ind21"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank21_L16H1.png" alt="L16H1"/></div> <div class="selector-image img-ind22"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank22_L22H14.png" alt="L22H14"/></div> <div class="selector-image img-ind23"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank23_L27H5.png" alt="L27H5"/></div> <div class="selector-image img-ind24"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank24_L27H7.png" alt="L27H7"/></div> <div class="selector-image img-ind25"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank25_L27H4.png" alt="L27H4"/></div> <div class="selector-image img-ind26"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank26_L2H21.png" alt="L2H21"/></div> <div class="selector-image img-ind27"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank27_L10H13.png" alt="L10H13"/></div> <div class="selector-image img-ind28"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank28_L5H9.png" alt="L5H9"/></div> <div class="selector-image img-ind29"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank29_L16H23.png" alt="L16H23"/></div> <div class="selector-image img-ind30"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank30_L20H13.png" alt="L20H13"/></div> <div class="selector-image img-ind31"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank31_L27H20.png" alt="L27H20"/></div> <div class="selector-image img-ind32"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/induction_patterns/rank32_L19H0.png" alt="L19H0"/></div> </div> </div> </div> <div class="caption"> <b>Figure A1.</b> <b>Induction head attention patterns.</b> Each heatmap shows the attention pattern for one of the top induction heads. The x-axis is the key (source) position, and the y-axis is the query (destination) position. Brighter colors indicate higher attention weights. Click the buttons to switch between heads. </div> <h3 id="b2-previous-token-head-attention-patterns">B.2 Previous-Token Head Attention Patterns</h3> <p>Previous-token heads attend primarily to position \(t-1\), implementing a simple “look at what came before” operation. This creates a strong diagonal pattern one position below the main diagonal.</p> <div class="l-page"> <div class="png-selector"> <input type="radio" name="prev-head" id="prev1" checked=""/> <input type="radio" name="prev-head" id="prev2"/> <input type="radio" name="prev-head" id="prev3"/> <input type="radio" name="prev-head" id="prev4"/> <input type="radio" name="prev-head" id="prev5"/> <input type="radio" name="prev-head" id="prev6"/> <input type="radio" name="prev-head" id="prev7"/> <input type="radio" name="prev-head" id="prev8"/> <input type="radio" name="prev-head" id="prev9"/> <input type="radio" name="prev-head" id="prev10"/> <input type="radio" name="prev-head" id="prev11"/> <input type="radio" name="prev-head" id="prev12"/> <input type="radio" name="prev-head" id="prev13"/> <input type="radio" name="prev-head" id="prev14"/> <input type="radio" name="prev-head" id="prev15"/> <input type="radio" name="prev-head" id="prev16"/> <input type="radio" name="prev-head" id="prev17"/> <input type="radio" name="prev-head" id="prev18"/> <input type="radio" name="prev-head" id="prev19"/> <input type="radio" name="prev-head" id="prev20"/> <input type="radio" name="prev-head" id="prev21"/> <input type="radio" name="prev-head" id="prev22"/> <input type="radio" name="prev-head" id="prev23"/> <input type="radio" name="prev-head" id="prev24"/> <input type="radio" name="prev-head" id="prev25"/> <input type="radio" name="prev-head" id="prev26"/> <input type="radio" name="prev-head" id="prev27"/> <input type="radio" name="prev-head" id="prev28"/> <input type="radio" name="prev-head" id="prev29"/> <input type="radio" name="prev-head" id="prev30"/> <input type="radio" name="prev-head" id="prev31"/> <input type="radio" name="prev-head" id="prev32"/> <div class="selector-buttons"> <label for="prev1">L0H2</label> <label for="prev2">L14H26</label> <label for="prev3">L1H20</label> <label for="prev4">L1H18</label> <label for="prev5">L9H11</label> <label for="prev6">L1H16</label> <label for="prev7">L0H3</label> <label for="prev8">L7H2</label> <label for="prev9">L0H24</label> <label for="prev10">L14H8</label> <label for="prev11">L1H13</label> <label for="prev12">L0H26</label> <label for="prev13">L11H16</label> <label for="prev14">L21H7</label> <label for="prev15">L7H1</label> <label for="prev16">L0H6</label> <label for="prev17">L1H24</label> <label for="prev18">L1H4</label> <label for="prev19">L12H16</label> <label for="prev20">L1H26</label> <label for="prev21">L9H10</label> <label for="prev22">L6H8</label> <label for="prev23">L9H8</label> <label for="prev24">L25H20</label> <label for="prev25">L1H30</label> <label for="prev26">L16H29</label> <label for="prev27">L1H1</label> <label for="prev28">L0H29</label> <label for="prev29">L14H9</label> <label for="prev30">L18H26</label> <label for="prev31">L21H10</label> <label for="prev32">L9H9</label> </div> <div class="selector-images"> <div class="selector-image img-prev1"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank01_L0H2.png" alt="L0H2"/></div> <div class="selector-image img-prev2"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank02_L14H26.png" alt="L14H26"/></div> <div class="selector-image img-prev3"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank03_L1H20.png" alt="L1H20"/></div> <div class="selector-image img-prev4"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank04_L1H18.png" alt="L1H18"/></div> <div class="selector-image img-prev5"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank05_L9H11.png" alt="L9H11"/></div> <div class="selector-image img-prev6"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank06_L1H16.png" alt="L1H16"/></div> <div class="selector-image img-prev7"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank07_L0H3.png" alt="L0H3"/></div> <div class="selector-image img-prev8"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank08_L7H2.png" alt="L7H2"/></div> <div class="selector-image img-prev9"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank09_L0H24.png" alt="L0H24"/></div> <div class="selector-image img-prev10"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank10_L14H8.png" alt="L14H8"/></div> <div class="selector-image img-prev11"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank11_L1H13.png" alt="L1H13"/></div> <div class="selector-image img-prev12"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank12_L0H26.png" alt="L0H26"/></div> <div class="selector-image img-prev13"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank13_L11H16.png" alt="L11H16"/></div> <div class="selector-image img-prev14"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank14_L21H7.png" alt="L21H7"/></div> <div class="selector-image img-prev15"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank15_L7H1.png" alt="L7H1"/></div> <div class="selector-image img-prev16"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank16_L0H6.png" alt="L0H6"/></div> <div class="selector-image img-prev17"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank17_L1H24.png" alt="L1H24"/></div> <div class="selector-image img-prev18"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank18_L1H4.png" alt="L1H4"/></div> <div class="selector-image img-prev19"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank19_L12H16.png" alt="L12H16"/></div> <div class="selector-image img-prev20"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank20_L1H26.png" alt="L1H26"/></div> <div class="selector-image img-prev21"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank21_L9H10.png" alt="L9H10"/></div> <div class="selector-image img-prev22"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank22_L6H8.png" alt="L6H8"/></div> <div class="selector-image img-prev23"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank23_L9H8.png" alt="L9H8"/></div> <div class="selector-image img-prev24"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank24_L25H20.png" alt="L25H20"/></div> <div class="selector-image img-prev25"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank25_L1H30.png" alt="L1H30"/></div> <div class="selector-image img-prev26"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank26_L16H29.png" alt="L16H29"/></div> <div class="selector-image img-prev27"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank27_L1H1.png" alt="L1H1"/></div> <div class="selector-image img-prev28"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank28_L0H29.png" alt="L0H29"/></div> <div class="selector-image img-prev29"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank29_L14H9.png" alt="L14H9"/></div> <div class="selector-image img-prev30"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank30_L18H26.png" alt="L18H26"/></div> <div class="selector-image img-prev31"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank31_L21H10.png" alt="L21H10"/></div> <div class="selector-image img-prev32"><img src="/2026/assets/html/2026-04-27-iclr-induction/induction_analysis/prev_token_patterns/rank32_L9H9.png" alt="L9H9"/></div> </div> </div> </div> <div class="caption"> <b>Figure A2.</b> <b>Previous-token head attention patterns.</b> Each heatmap shows the attention pattern for one of the top previous-token heads. The strong sub-diagonal pattern indicates attention to position $t-1$. Click the buttons to switch between heads. </div>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Park et al., 2025 demonstrate that large language models can learn to trace random walks on graphs presented in context, and observe that token representations reorganize to reflect the underlying graph structure. This has been interpreted as evidence that models 'flexibly manipulate their representations' to reflect in-context semantics, and that this reorganization enables task performance. We offer a simpler mechanistic explanation. We first observe that task performance can be fully explained by induction circuits (Olsson et al., 2022), and show that ablating the attention heads that comprise these circuits substantially degrades performance. As for the geometric structure, we propose that it could result from previous token heads effectively mixing the representations of graph neighbors together. We show that a single round of such 'neighbor mixing' on random embeddings recreates the observed graph correspondence in PCA visualizations. These results suggest that apparent 'representation reorganization' may be a byproduct of the model's induction circuits, rather than a critical strategy useful for in-context learning.]]></summary></entry></feed>