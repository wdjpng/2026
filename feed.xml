<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://iclr-blogposts.github.io/2026/feed.xml" rel="self" type="application/atom+xml"/><link href="https://iclr-blogposts.github.io/2026/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-07T16:44:44+00:00</updated><id>https://iclr-blogposts.github.io/2026/feed.xml</id><title type="html">ICLR Blogposts 2026</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/fans/" rel="alternate" type="text/html" title="FANS - Frequency-Adaptive Noise Shaping for Diffusion Models"/><published>2026-11-30T00:00:00+00:00</published><updated>2026-11-30T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/fans</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/fans/"><![CDATA[<h2 id="motivations">Motivations</h2> <p>Diffusion models (DDPM) have achived state of the art performances on data modalities like Stable diffusion for images, Sora for videos, RFdiffusion for protiens and Mattergen for materials <d-cite key="Rombach2021HighResolutionIS,Qin2024WorldSimBenchTV,josephundefined,Zeni2023MatterGenAG"></d-cite>. They work by learning to reverse a gradual noising process. In a <strong>Forward Pass</strong> the data modality is progressively corrupted with gaussian noise until it becomes complete noise. In the <strong>Reverse process</strong> the model learns a denoiser which start from a gaussian noise and denoise this step-by-step to get an apprxomately clean sample. While this framework has proven to be quite effective, the existing works treats the noise as spatially uniform gaussian noise \(\mathcal{N}(\mu, \sigma^2)\).</p> <p>A closer look at these modalities reveals something important: their Fourier representations follow a power-law distribution. Low-frequency components carry much higher variance and describe global structure, while high-frequency components capture finer details<d-cite key="vanHateren1992"></d-cite>. Under the standard DDPM forward process, high frequencies are overwhelmed by noise much earlier and more aggressively simply because the Gaussian noise does not account for the dataset’s spectral characteristics. As a result, the model reconstructs frequencies in a hierarchy—coarse structure first, fine details later - an effect also noted in previous work.<d-cite key="falck2025fourierspaceperspectivediffusion"></d-cite></p> <p>But datasets are not spectrally uniform. Natural images place most of their power in low frequencies, while domains such as astronomy and texture design distribute power very differently. This naturally leads to a question for us: if datasets have distinct spectral fingerprints and diffusion models already denoise from coarse to fine, why rely on the same white noise across all frequencies and all timesteps? And how well does this uniform schedule generalize across such varied domains?</p> <p>In this work, we explore whether explicitly shaping the noise spectrum (i) to reflect a dataset’s actual frequency distribution and (ii) to introduce a principled time-frequency annealing schedule can improve sample quality and stability, all without altering the UNet architecture or the DDPM objective.</p> <h2 id="quick-overview">Quick Overview</h2> <p>Let’s start with a quick overview of the two frameworks we will be working with: Diffusion models and Frequency domain of Images.</p> <h3 id="diffusion-models">Diffusion Models</h3> <p>Diffusion models generate data by learning to reverse a gradual noising process. The core idea is elegantly simple: start with real data and progressively corrupt it by adding Gaussian noise over many steps until it becomes pure random noise. This forward process requires no learning and follows a predefined schedule. A neural network then learns to invert this process, starting from random noise and iteratively denoising it to recover clean, structured data. This reverse process is governed by stochastic differential equations (SDEs) that describe how probability distributions evolve over time, with the model learning to approximate the score function which guides the denoising steps.</p> <h3 id="forward-process">Forward Process</h3> <p>It defines a markov chain that progressively add gaussian noise to the data sample \(\mathbf{x}_0 \sim q(\mathbf{x}_0)\) over \(T\) timesteps. Each timestep adds a slight noise to the data resulting in a increasingly noisy samples \(\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_T\), where $\mathbf{x}_T$ approximates an isotropic Gaussian distribution. The forward process is formally defined as :</p> \[q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \, \mathbf{x}_{t-1}, \beta_t \mathbf{I}),\] <p>where $\beta_t \in (0,1)$ controls the amount of noise added to the data.</p> <p>By applying the reparameterization reccursively we obtain the closed form experssion</p> \[q(\mathbf{x}_t | \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \, \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I}),\] <p>where \(\alpha_t = 1 - \beta_t\) and \(\bar{\alpha}_t = \prod_{s=1}^t \alpha_s\).</p> <p>Thus the noisy data at any given time \(t\) in the forward proccess</p> \[\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{(1 - \bar{\alpha}_t)}\epsilon\] <p>where \(\epsilon \in \mathcal{N}(0,\mathbf{I})\).</p> <p>A useful notation in this is the log signal-to-noise ratio \(\lambda_t = log(\bar{\alpha}_t / (1 - \bar{\alpha}_t))\) which increases monotonically from 0 ( clean data) to 1 (noise)</p> <h3 id="reverse-process">Reverse Process</h3> <p>The Reverse process tries to invert the forward process, transforming Gaussian noise \(\mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\) back into a data sample resembling the data distribution \(q(\mathbf{x}_0)\). Since the true reverse transitions \(q(\mathbf{x}_{t-1} | \mathbf{x}_t)\) are intractable, a parameterized model \(p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t)\) is trained to approximate them.</p> <p>The reverse process is modeled as:</p> \[p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t)),\] <p>where \(\boldsymbol{\mu}_\theta\) and \(\boldsymbol{\Sigma}_\theta\) are outputs of a neural network conditioned on \(\mathbf{x}_t\) and the timestep \(t\). The model learns to predict the mean of the denoised sample at each step.</p> <p>Using the forward process derivation, the true mean of \(q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0)\) can be expressed as:</p> \[\boldsymbol{\mu}_q(\mathbf{x}_t, \mathbf{x}_0) = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon} \right),\] <p>where \(\boldsymbol{\epsilon}\) denotes the Gaussian noise added at timestep \(t\).</p> <p>On way of learning is during training the model is optimized to predict this noise directly using a loss of the form:</p> \[L(\theta) = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} \left[ \left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta( \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}, t ) \right\|^2 \right],\] <p>which corresponds to a reweighted variational bound on the data likelihood.</p> <h3 id="fourier-domain">Fourier Domain</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/image_fft-480.webp 480w,/2026/assets/img/2026-11-25-fans/image_fft-800.webp 800w,/2026/assets/img/2026-11-25-fans/image_fft-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/image_fft.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/power_law-480.webp 480w,/2026/assets/img/2026-11-25-fans/power_law-800.webp 800w,/2026/assets/img/2026-11-25-fans/power_law-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/power_law.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure2: First two image shows an image in spatial, in frequency domain. The plot is the log-log plot of Power spectral density and Frequency of the image. </div> <p>Natural Images have rich structures in the frequency domain that is obscured in the pixel space. Understanding this frequency domain perspective is crucial for our investigation.</p> <p>Let’s say we have an image \(x \in \mathbf{R}_{H \times W\times C}\). Its pixels are coeffcient of a standard basis \(\mathcal{B} = \{ \mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_n \} \subset \mathbb{R}^n\). In the spatial domain, the image is defined as a linear combination of standard basis vectors, where each pixel represents a local real-valued intensity. When we apply the Discrete Fourier Transform(DFT) to this image, it performs unitary change of basis giving us the frequency domain equivalent of the image. The dimension of the fourier and the pixel image remains the same but in fourier domain the coefficients of the basis becomes complex valued.</p> <p>For each channel of an image the DFT can be formulated as</p> \[F(u, v) = \sum_{x=0}^{H-1} \sum_{y=0}^{W-1} f(x, y) e^{-j 2\pi \left( \frac{ux}{H} + \frac{vy}{W} \right)}\] <p>where \((u,v)\) aer spatial frequency indices. The radial frequency \(f = \sqrt{u^2 + v^2}\) measure the distance from the DC ( zero frequency component)</p> <p>Now, let’s sort the Fourier coeeficient \(f(u,v)\) from low to high frequency. To do so, we start from the center of our Fourier Space and walk in spirals. That is, sort them using the Manhatten distancen of the indices \((uv,)\) from the center (0,0) of the Fourier representation. From the Figure 2 It’s very evident that signal variance decreases rapidly with increasing frquency. <d-cite key="dielman"></d-cite> shows that similar trend is visible in other doains (Videos, Audio, Proteins) as well.</p> <p><strong>Power Spectral Density(PSD)</strong>: This quantifies how the signal enery is distributed across frequencies. For an image with Fourier Transform \(F(u,v)\), PSD is defined as :</p> \[P(u,v) = |F(u,v)|^2\] <p>representing the power at each frequency component. To get one dimensional spectral profile we compute the radially-average PSD by integrating over annular regions at constant radial frequency \(f\) (simply put we compute average power in ring-shaped zones moving outward from the center). Formally put</p> \[P(f) = \frac{1}{|B_f|} \sum{}_{(u,v) \in B_f} |F(u,v)|^2\] <p>Where \(B_f\) is the radial frequency band defined as \(B_f = {(u,v): f \leq \sqrt{u^2+v^2} \le f+ \delta f}\) and \(\|B_f\|\) is the number of frequencies in that band.</p> <p>The PSD reveals how much each frequency contributes to the overall signal. For a dataset of images, we compute per-image PSDs and average them to obtain a dataset-level spectral profile. This aggregate PSD characterizes the typical frequency distribution of the data and captures domain-specific properties</p> <p>Real-world image datasets exhibit characteristic frequency distributions. The power spectral density of natural images typically follows a power-law decay:</p> \[P \propto f^{-\alpha}\] <p>Emperical studies on natural image statistics shave shown that \(\alpha\) typically resides in the interval \([1,3]\) <d-cite key="field,physrevLett"> </d-cite>. While Standard photgraphic images generally converges towards \(\alpha \approx 2\) <d-cite key="vanHateren1992"></d-cite>, For few domain specific datasets like Astronomy images, we observe this trend \(\alpha \approx 1\).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/universe_image_fft-480.webp 480w,/2026/assets/img/2026-11-25-fans/universe_image_fft-800.webp 800w,/2026/assets/img/2026-11-25-fans/universe_image_fft-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/universe_image_fft.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/average_frequency_psd_loglog-480.webp 480w,/2026/assets/img/2026-11-25-fans/average_frequency_psd_loglog-800.webp 800w,/2026/assets/img/2026-11-25-fans/average_frequency_psd_loglog-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/average_frequency_psd_loglog.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure: In the First Figure (Top Right), We can see the fourier representation of an astronomy image varies a lot as compared to the fourier representation of the natural image as shown above. </div> <p>To quantify dataset frequency importance, we divide the spectrum into \(B\) radial frequency bands and compute the normalized band power \(\pi_b\)​ for each band \(B\):</p> \[\pi_b = \frac{1}{N}\sum{N}_{i=1} \frac{\sum{}_{f \in B_i}P_i(f)}{\sum{}_{f} P_i(f)}\] <p>where \(N\) is the number of images in the dataset and \(P_i(f)\) is the PSD of image \(i\). These band powers \({\pi_b}^B_{b=1}\)​ form a probability distribution over frequency bands, representing how the dataset’s signal energy is distributed across the spectrum.</p> <h3 id="diffusion-in-frequency-domain">Diffusion in Frequency Domain</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/heatmap-480.webp 480w,/2026/assets/img/2026-11-25-fans/heatmap-800.webp 800w,/2026/assets/img/2026-11-25-fans/heatmap-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/heatmap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 3: Comparing the SNR (dB scale) for DDPM we see that High frequencies are corrupted substantially faster (SNR changes more per time increment) than low frequencies.The SNR is computed using monte carlo estimate of the SNR equation discussed below </div> <p>As we are trying to investigate how the DDPMs inductive bias in the forward process<d-cite key="falck2025fourierspaceperspectivediffusion"></d-cite> affects datasets for varying spectral power density, we can view the DDPM forward process under a change of basis to fourier space <d-cite key="dielman,gerdes2024gudgenerationunifieddiffusion"></d-cite>. This is acomplished by applying the Fourier transform \(\mathbf{F}\) to the variable \(x_t\)</p> \[y_t : \mathbf{F}\mathbf{x}_t = \mathbf{F}\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \mathbf{F}\sqrt{(1 - \bar{\alpha}_t)}\epsilon\] <p>Here, \(y_t\) is our fourier-transformed intermediate step at time step \(t\) of the forward process.</p> <p>Taking the quantification of Signal-to-noise Ration from <d-cite key="falck2025fourierspaceperspectivediffusion"></d-cite>, where SNR of \((x_t)_i\) is the signal to noise ration of frquescy \(i\) at timestep \(t\). Formally put</p> \[SNR((x_t)_i) = \frac{\bar{\alpha_t}\varsigma_i}{1-\bar{\alpha_t}}\] <p>Where \(\varsigma_i = Var((x_0)_i)\) represents the signal variance of requency \(i\).</p> <p>Tying these all together, we see in Figure 3, that standard DDPM corrupts high-frequencies faster than low frequencies. This bias is carry forwarded to the reverse process as well.</p> <p>Thus we want to study an alternate noising schedule that respects the datasets spectral signature. To put simply a Frequency Adaptive Noise Scheduler ( FANS ).</p> <h2 id="fans">FANS</h2> <p>In the previous sections we saw the inductive bias of DDPM towards high frequency components both in forward and reverse process. We want to investigate if instead of isotropic gaussian noise scheduler (irrespective of the frequency distribution of the dataset), we use a scheduler that is adaptive to the intrinsic frequency characteristic of the dataset by constructing spectrally-shaped noise. Our key insight is that different frequency bands contribute unequally to perceptual quality and should be treated accordingly during both training and generation.</p> <p>This proposed approach, operates through three complementary mechanism :</p> <ol> <li><strong>Dataset importance profiling</strong>: We analyze the spectraal distribution of the data to compute frequency band importance weight \(g_b\) that quantifies the relative contribution of each band of the overall data.</li> <li><strong>Time-Frequency Scheduling</strong>: We introduce a temporal ramping function \(\phi(t)\) that smoothly transitions noise allocation from low to high frequencies as the diffusion process evolves, enabling coarse-to-fine generation.</li> <li><strong>Variance-Compensated Weighting</strong>: We apply inverse-power weighting to band importances, ensuring that underrepresented high-frequency bands receive compensatory emphasis during training.</li> </ol> <p>We can now formalize these mechanism:</p> <h3 id="radial-frequency-band-decomposition">Radial Frequency Band Decomposition</h3> <p>Recalling from previous discussion, let’s \(x_0 \in \mathbb{R}^{H \times W \times C}\) denote a clean image, and let \(\mathcal{F}\) denote the real-valued FFT used in the implementation. To characterise the dataset’s spectral structure, we partition the \(B\) radial frequency bands \(\{B_b\}_{b=1}^B\) in the discrete \(r\)FFT layout (shape \(H \times W/2 + 1\)) using linear radial boundaries. For each band \(b\) we define :</p> <ul> <li><strong>Band mask</strong> : \(B_b \in \{0,1\}^{H \times W/2 + 1}\) indicating membership.</li> <li> <table> <tbody> <tr> <td><strong>Band size</strong> :</td> <td>\(B_b\)</td> <td>denotes the nuber of frequency coefficients in band \(b\).</td> </tr> </tbody> </table> </li> </ul> <p>Bands are constructed using radial frequency \(F(u,v) = \sqrt{u^2 + v^2}\) with edges uniformly spaced between a small positive \(f_{min}\) (to exclude DC component) and \(f_{mac}\) ( Nyquist frequency = 0.5)</p> <p><strong>Intuition</strong> : Excluding the DC component (zero frequency) is critical because it represents the global mean intensity, which dominates the spectrum but carries minimal perceptual information. Including DC in band 0 would artificially inflate its importance \(g_0\) and distort the learned weighting.</p> <h3 id="dataset-importance-profiling">Dataset Importance Profiling</h3> <p>For each image \(x\) in the training set, we compute the <strong>normalized band power</strong> \(\pi_b(x)\) as ;</p> \[\pi_b(x) = \frac{\sum{}_{k \in B_b} |F(x - \bar{x})(k)|^2 }{\sum^{B-1}_{b^\prime = 0 }\sum{}_{k \in B_{b^\prime}} |F(x - \bar{x})(k)|^2 }\] <p>wher \(\bar{x}\) is the per image mean ( removing DC ). This gives the fraction of total power residing in band \(b\) for image \(x\).</p> <p>We then compute the dataset-level band distribution by averaging over \(N\) samples:</p> \[\bar{\pi} = \frac{1}{N} \sum^{N}_{i=1} \pi_b(x_i)\] <p>Note that \(\sum^{B-1}_{b=0} \bar{\pi_b} = 1\) by construction.</p> <p>Finally, we compute importance weights via inverse-power scaling rule :</p> \[g_b = \frac{(\bar{\pi_b} + \epsilon )^{-\alpha}}{\frac{1}{B}\sum^{B-1}_{b^\prime=0}(\bar{\pi_{b^\prime}} + \epsilon)^{-\alpha}}\] <p>wher \(\alpha\) controls the strength of variance compensation. The standardization ensures \(g_b\) has zero mean and unit variance, provising a stable range for the softmax reweighting.</p> <p><strong>Intuition</strong>: Bands with low power \(\bar{\pi_b}\)(e.g., high frequencies) receive higher importance \(g_b\) ​, compensating for their underrepresentation in the data. This prevents the model from neglecting high-frequency reconstruction.</p> <h3 id="time-dependent-soft-band-weighting-check-once">Time-Dependent Soft Band Weighting [check once]</h3> <p>At each timestep \(t∈[0,1]\)t \in [0,1]\(, we compute soft band weights\)w_b(t)$$ via a temperature-scaled softmax with time-frequency ramping:</p> \[w_b(t) = \frac{exp(\beta.g_b - \gamma.\phi(t).\lambda_b)}{\sum^{B-1}_{b^\prime = 0}exp(\beta.g_b^\prime - \gamma.\phi(t).\lambda_b^\prime)}\] <p>where:</p> <ul> <li>\(\lambda_b = b/(B-1)\) is the normalized band index \(\lambda_0 = 0, \lambda_{B-1}=1\)</li> <li>\(\pi(t) \in [0,1]\) is a temporal ramp.</li> <li>\(\beta,\gamma \ge 0\) are the invese temperature hyperparameter.</li> </ul> <p>Early in the diffusion process, the term \(\beta. g_b\) dominates, emphasizing frequencies according to dataset statistics. As \(t\) increases, the ramp \(\phi(t)\) gradually increases the influence of $\gamma\lambda_b$, making the spectrum increasingly uniform.</p> <p><strong>Stabilization: White Noise Guardrail</strong> To ensure stable training during the earliest timesteps, we introduce a white noise mixing schedule:</p> \[w_b^{\text{mix}}(t) = \begin{cases} \frac{1}{B} &amp; \text{if } t &lt; t_{\text{knee}} \\ (1 - \alpha_{\text{mix}}(t)) \cdot \frac{1}{B} + \alpha_{\text{mix}}(t) \cdot w_b(t) &amp; \text{if } t \geq t_{\text{knee}} \end{cases}\] <p>where \(\alpha_{\text{mix}}(t) = \frac{t - t_{\text{knee}}}{1 - t_{\text{knee}}}\)​​ is a linear blend coefficient and \(t_{\text{knee}} = 0.15\) by default.</p> <p><strong>Intuition</strong>: At very small \(t\), the noised samples \(x_t \approx \beta_t \epsilon\) are nearly pure noise. Enforcing strong spectral shaping here can destabilize training because the model has insufficient signal to learn meaningful structure. By using uniform weights early, we ensure the model first learns to denoise white noise (as in standard DDPM), then gradually transitions to FANS-shaped noise.</p> <h3 id="fans-noise-generation">FANS Noise Generation</h3> <p>Tying the above mechanisms together to generate the FANS-Noise we get.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/fans_training-480.webp 480w,/2026/assets/img/2026-11-25-fans/fans_training-800.webp 800w,/2026/assets/img/2026-11-25-fans/fans_training-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/fans_training.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Given a sample \(x \in \mathcal{R}^{N \times C \times H \times W}\) and normalised time \(t \in [0,1]\) FANS-Noise \(\epsilon_{FANS}\) is generated as :</p> <p>Step 1. <strong>Compute band weights:</strong> \(\{w_b(t)\}^{B-1}_{b=0}\) as discussed above. Step 2. <strong>Allocate spectral power</strong>:</p> <p>The total power available in Fourier space must account for the Parseval Relation (It states that total power is time domain must be equal to the total power in the fourier domain). For the forward process, the noise component has pixel-space variance \(\beta_t^2\). We set the \(\sigma_t = 1\), giving us</p> <p>\(P_{total} = N_p.\sigma^2_t = H.W.1 = H.W\).</p> <p>Why we go for \(N_p\)​ (not \(N_p^2\)​): The power spectral density relates to the sum of squared Fourier coefficients, not their squared sum. For an \(H \times W\) image, the rFFT produces \(H \times (W/2 + 1)\) complex coefficients. By Parseval’s theorem</p> \[\sum_{i=1}^{H \cdot W} x_i^2 = \frac{1}{H \cdot W} \sum_{k} |F(x)(k)|^2\] <p>where the factor \(1/(H \cdot W)\) comes from the DFT normalization convention.</p> <p>We then allocate power to each band according to the learned weights:</p> \[P_b(t) = w_b(t) \cdot P_{\text{total}} = w_b(t) \cdot H \cdot W\] <p>The per-frequency variance within band \(b\) is obtained by distributing \(P_b\)​ uniformly across its members:</p> \[\Sigma_b(t) = \frac{P_b(t)}{|\mathcal{B}_b| + \epsilon}\] <p>where \(\epsilon = 10^{-6}\) prevents division by zero as a numerical safeguard.</p> <p>Step 3. <strong>Draw Complex Gaussian Noise in Fourier Space</strong>:</p> <p>We generate base noise \(Z \in \mathbb{C}^{N \times C \times H \times (W/2+1)}\) by sampling independent real and imaginary components:</p> \[Z_{\text{real}} \sim \mathcal{N}(0, I), \quad Z_{\text{imag}} \sim \mathcal{N}(0, I)\] \[Z(n, c, h, w) = \frac{1}{\sqrt{2}}(Z_{\text{real}}(n,c,h,w) + i \cdot Z_{\text{imag}}(n,c,h,w))\] <p>The \(1/\sqrt{2}\)​ factor ensures</p> \[\mathbb{E}[|Z(k)|^2] = \mathbb{E}[Z_{\text{real}}^2 + Z_{\text{imag}}^2]/2 = 1\] <p>Step 4: <strong>Construct Spectral Variance Mask</strong></p> <p>We build a spatial variance map \(\Sigma_t \in \mathbb{R}^{N \times C \times H \times (W/2+1)}\)that specifies the desired variance at each frequency:</p> \[\Sigma_t(n,c,h,w) = \sum_{b=0}^{B-1} \Sigma_b(t) \cdot \mathbb{1}_{(h,w) \in \mathcal{B}_b}\] <p>where \(\mathbb{1}_{(h,w) \in \mathcal{B}_b}\)​​ is the indicator function for band membership.</p> <p>Step 5: <strong>Apply Frequency-Dependent Scaling</strong> The shaped noise in Fourier space is obtained by element-wise multiplication:</p> \[E_{\text{shaped}}(n,c,h,w) = \sqrt{\Sigma_t(n,c,h,w) + \epsilon_{\text{safe}}} \cdot Z(n,c,h,w)\] <p>where \(\epsilon_{\text{safe}} = 10^{-12}\) ensures numerical stability when \(\Sigma_t \approx 0\).</p> <p>Why square root? We are scaling the amplitude of Fourier coefficients. Since power is amplitude squared, to achieve variance \(\Sigma_t(k)\), we need amplitude \(\sqrt{\Sigma_t(k)}\)​. This follows from:</p> \[\mathbb{E}[|E_{\text{shaped}}(k)|^2] = \mathbb{E}[|\sqrt{\Sigma_t(k)} \cdot Z(k)|^2] = \Sigma_t(k) \cdot \mathbb{E}[|Z(k)|^2] = \Sigma_t(k)\] <p>Step 6: <strong>Inverse Fourier Transform to Pixel Space</strong> We apply the inverse real FFT to recover a real-valued noise image:</p> \[\epsilon_{\text{FANS}}^{\text{raw}} = \text{irfft2d}(E_{\text{shaped}}, s=(H, W))\] <p>where \(s=(H, W)\) specifies the desired output shape and irfft2d is the 2D inverse real FFT.</p> <p>Step 7: <strong>Enforce Unit Variance via Normalization</strong> While the Fourier-space construction theoretically preserves variance, discretization effects, numerical precision, and band edge artifacts can cause the pixel-space variance to deviate from unity. To ensure exact compatibility with the forward process \(x_t = \alpha_t z + \beta_t \epsilon\) where \(\mathbb{E}[\epsilon \epsilon^T] = I\), we enforce unit variance per sample and per channel:</p> \[\mu(n,c) = \frac{1}{H \cdot W} \sum_{h,w} \epsilon_{\text{FANS}}^{\text{raw}}(n,c,h,w)\] \[\sigma^2(n,c) = \frac{1}{H \cdot W} \sum_{h,w} (\epsilon_{\text{FANS}}^{\text{raw}}(n,c,h,w) - \mu(n,c))^2\] \[\epsilon_{\text{FANS}}(n,c,h,w) = \frac{\epsilon_{\text{FANS}}^{\text{raw}}(n,c,h,w) - \mu(n,c)}{\sqrt{\sigma^2(n,c) + \epsilon_{\text{safe}}}}\] <p>Critical importance: This normalization is not optional. Without it, we observed training instabilities where the effective noise magnitude drifted over time, breaking the assumptions of the forward SDE. The normalization ensures:</p> <ul> <li>Zero mean: \(\mathbb{E}[\epsilon_{\text{FANS}}] = 0\) exactly (not just approximately)</li> <li>Unit variance: \(\text{Var}(\epsilon_{\text{FANS}}) = I\) exactly</li> <li> <table> <tbody> <tr> <td>Consistency: \(x_t = \alpha_t z + \beta_t \epsilon_{\text{FANS}}\)​ has the correct conditional distribution $$p_t(x</td> <td>z)$$</td> </tr> </tbody> </table> </li> </ul> <p>Why per-channel normalization? Color channels may have different effective powers after Fourier shaping due to:</p> <ul> <li>Boundary effects in rFFT (asymmetric handling of Nyquist)</li> <li>Floating-point rounding errors accumulated differently per channel</li> <li>Non-uniform distribution of salient features across RGB</li> </ul> <p>Normalizing each channel independently ensures that the model sees noise with identical statistics in all color channels, preventing the network from learning color-dependent denoising biases.</p> <p>Step 8: <strong>Return Shaped Noise</strong> The final output \(\epsilon_{\text{FANS}} \in \mathbb{R}^{N \times C \times H \times W}\) satisfies:</p> <ol> <li>\(\mathbb{E}[\epsilon_{\text{FANS}}] = 0\)(zero mean)</li> <li>\(\mathbb{E}[\epsilon_{\text{FANS}} \epsilon_{\text{FANS}}^T] = I\) (unit covariance)</li> <li>Frequency band \(b\) contains fraction \(\approx w_b(t)\) of total power</li> </ol> <p>Low frequencies dominate at small \(t\), high frequencies at large \(t\)</p> <p>This noise can now be used in the forward process:</p> \[x_t = \alpha_t z + \beta_t \epsilon_{\text{FANS}}\] <p>and in the loss computation:</p> \[\mathcal{L} = \|u_\theta^t(x_t) - (\dot{\alpha}_t z + \dot{\beta}_t \epsilon_{\text{FANS}})\|^2\] <h3 id="sampling-inference">Sampling/ Inference</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/fans_sampling-480.webp 480w,/2026/assets/img/2026-11-25-fans/fans_sampling-800.webp 800w,/2026/assets/img/2026-11-25-fans/fans_sampling-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/fans_sampling.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This algorithm shows the sampling method we used for our proposed approach.</p> <p>Now that we have the mechanism to get spectral aware shape, we need to see how performs against a baseline. To test this we simulate experiments where high frequency and fine grained information is of primary interest. For this simulation we do a synthetic study. We designed two synthetic datasets PLTB and EGM each constructed to emphasize a distinct spectral profile. All three datasets are generated programmatically as 512×512 images. We discuss these synhtetic datasets in details in the following section</p> <p>We use this synthetic data because controlled synthetic data provides the ability to vary the distribution of Fourier energy across bands in a principled way and isolate the effect of FANS.</p> <h2 id="synthetic-data">Synthetic Data</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/synth_sample-480.webp 480w,/2026/assets/img/2026-11-25-fans/synth_sample-800.webp 800w,/2026/assets/img/2026-11-25-fans/synth_sample-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/synth_sample.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="pltb-power-law-texture-bank">PLTB: Power-Law Texture Bank</h3> <p><strong>Motivation:</strong> PLTB targets the high-frequency regime. In this the entire image is defined through its radial power spectrum. We can see the distribution of spectral mass across bands can be observed in the figure.</p> <p><strong>Genertation Procedure:</strong> Each PLTB sample is generated by:</p> <ol> <li>Sampling an i.i.d. complex Gaussian field \(Z(k)\) on the half-spectrum</li> <li>Applying an amplitude mask:</li> </ol> \[A(k) \propto (|k| + \epsilon )^{\alpha/2}\] <p>with slope \(\alpha = 1\)</p> <ol> <li>Multiplying \(Z(k)\) by \(A(k)\) and applying an inverse FFT to obtain the spatial image.</li> <li>Finally, Normalizing brightness and contrast per-sample.</li> </ol> <p>This enables us to isolate testure learning capability of dissusion models. Models with insufficient high-frequency produce visibly smoother samples. A sample is given in figure above</p> <h3 id="egm-edgesgratings-mixture">EGM: Edges–Gratings Mixture</h3> <p><strong>Motivation:</strong> EGM introduces structured high-frequency content—oriented gratings, checkerboards, and sharp edges—that resemble real-image conditions where fine geometric detail matters. This allows discrete orientation content and mixed spatial primitives, providing a more realistic stress test of whether a model can faithfully reproduce high frequency geometry rather than only stochastic texture.</p> <p><strong>Generation Procedure:</strong> Each EGM image is a mixture of:</p> <ol> <li>Sinusoidal gratings with random frequency (0.04 − 0.22 cyc/px), orientation, amplitude, and phase.</li> <li>Checkerboard patterns, producing orthogonal high-frequency peaks.</li> <li>Sparse straight-line segments, introducing broadband edge energy.</li> </ol> <p>Components are randomly combined, and the resulting image is normalized to unit variance. EGM reflects scenarios common in natural images edges, periodic patterns, corner-like junctions—while still enabling controlled frequency manipulation.</p> <p>The controlled spectral mass distribution across bands of these synthetic datasets allows us to benchmark how FANS perform compared to a standard DDPM when the spectral signature of an image deviates from that of a natural image.</p> <p>To ensure that adapting to the spectral signature of an image doesn’t affect the performance in natural image, we benchmark FANS against standard DDPM in datasets like CIFAR10 and CelebA.</p> <p>To show that the synthetic dataset we generated are not entirely an imaginary usecase, we analyse the spectral signature of two real world datasets:</p> <ul> <li>Multimodal Universe: An astronomy based Dataset</li> <li>Texture dataset: A dataset consist of textures.</li> </ul> <h2 id="results">Results.</h2> <p>When we compare this Frequency Aware method against standard DDPM, we found some interesting results.</p> <p><strong>Slope Estimation:</strong></p> <p>A key advantage of FANS over standard DDPM models lies in its ability to accurately capture the spectral characteristics of the data distribution. To quantify this improvement, we evaluate both methods on their capacity to estimate the power-law decay exponent (slope) of the data’s power spectral density.</p> <table> <thead> <tr> <th>Dataset</th> <th style="text-align: center">Original Slope</th> <th style="text-align: right">FANS</th> <th style="text-align: right">DDPM</th> </tr> </thead> <tbody> <tr> <td>PLTB</td> <td style="text-align: center">1.002</td> <td style="text-align: right">1.394</td> <td style="text-align: right">3.566</td> </tr> <tr> <td>EGM</td> <td style="text-align: center">0.989</td> <td style="text-align: right">1.121</td> <td style="text-align: right">2.566</td> </tr> <tr> <td>Multimodal Universe</td> <td style="text-align: center">1.229</td> <td style="text-align: right">1.454</td> <td style="text-align: right">3.515</td> </tr> <tr> <td>Texture Data</td> <td style="text-align: center">1.121</td> <td style="text-align: right">1.618</td> <td style="text-align: right">2.178</td> </tr> <tr> <td>CIFAR10</td> <td style="text-align: center">2.848</td> <td style="text-align: right">2.512</td> <td style="text-align: right">2.733</td> </tr> </tbody> </table> <p>we compute the mean absolute error (MAE) between the estimated and ground-truth spectral slopes across all datasets in Table above. FANS achieves a substantially lower MAE (0.3165) compared to DDPM (1.5197). This difference is stable across datasets spanning texture-rich synthetic domains (PLTB, EGM) and real natural image statistics (CIFAR-10).</p> <p>To quantify statistical reliability, we treat each dataset entry as an independent paired comparison between FANS and DDPM errors. A paired t-test on the absolute errors yields a significant difference (t = −4.92, p &lt; 0.01). Instead, they reflect a systematic reduction in spectral-slope distortion.</p> <p><strong>Spectral Band Analysis:</strong> As FANS is designed to schedule noise based on the spectral property of the dataset, we perform a spectral analysis between the dataset and the images generated by the baseline and FANS.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/bar_plot-480.webp 480w,/2026/assets/img/2026-11-25-fans/bar_plot-800.webp 800w,/2026/assets/img/2026-11-25-fans/bar_plot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/bar_plot.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Figure: Shows the PSD over the frequency bands of three datasets (a) Multimodal Universe Dataset, (b) PLTB Dataset ad (c) EGM Dataset. It also shows how the PSD of the real datasets (<span style="color:green">green</span>) and generated images are distributed over the frequency bands for both the FANS (<span style="color:blue">Blue</span>) and Standard DDPM ( Baseline ) model (<span style="color:red">Red</span>)</p> <p>The plot shows the Power Spectral Density per Frequency Band for two synthetic dataset (PLTB and EGM) and a real world dataset ( Multimidal Universe). It shows that both the method could capture the spectral signature of the dataset, and understand how the PSD is distributed across the frequency bands. However the baseline ( Standard DDPM ) tends to concentrate the PSD within few frequency bands, while FANS ** distribute the PSD as per the dataset characteristics** and are more in agreement with the dataset spectral characteristic.</p> <p><strong>Metrics.</strong> To quantify spectral fidelity we use: (1) Jensen-Shannon (JS) divergence between generated and real PSD distributions (lower is better), (2) per-band correlation.</p> <p>Using Jensen-Shannon divergence (JSD), we can see that the PSD distribution across frequency bands of FANS are much closer to the actual dataset as compared to the baseline on all the three datasets.</p> <table> <thead> <tr> <th>Dataset</th> <th style="text-align: center">JSD(FANS)</th> <th style="text-align: right">JSD(Baseline)</th> </tr> </thead> <tbody> <tr> <td>EGM</td> <td style="text-align: center"><strong>0.0308</strong></td> <td style="text-align: right">0.1276</td> </tr> <tr> <td>PLTB</td> <td style="text-align: center"><strong>0.0103</strong></td> <td style="text-align: right">0.0804</td> </tr> <tr> <td>Universe</td> <td style="text-align: center"><strong>0.0088</strong></td> <td style="text-align: right">0.1041</td> </tr> </tbody> </table> <p>Across the datasets FANS shows a stronger correlation bandwise compared to the basseline. This shows the correlation between the bands of the real dataset and the samples generated. FANS has much higher bandwise correlation across both the synthetic and real dataset.</p> <table> <thead> <tr> <th>Dataset</th> <th style="text-align: center">Correlation(FANS)</th> <th style="text-align: right">Correlation(Baseline)</th> </tr> </thead> <tbody> <tr> <td>EGM</td> <td style="text-align: center"><strong>0.911</strong></td> <td style="text-align: right">0.672</td> </tr> <tr> <td>PLTB</td> <td style="text-align: center"><strong>0..968</strong></td> <td style="text-align: right">0.522</td> </tr> <tr> <td>Universe</td> <td style="text-align: center"><strong>0.877</strong></td> <td style="text-align: right">0.532</td> </tr> </tbody> </table> <p><strong>Qualitative Analysis:</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/ptlb_sample-480.webp 480w,/2026/assets/img/2026-11-25-fans/ptlb_sample-800.webp 800w,/2026/assets/img/2026-11-25-fans/ptlb_sample-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/ptlb_sample.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/egm_sample-480.webp 480w,/2026/assets/img/2026-11-25-fans/egm_sample-800.webp 800w,/2026/assets/img/2026-11-25-fans/egm_sample-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/egm_sample.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>On PLTB, standard DDPM exhibits catastrophic failure, generating samples visually indistinguishable from Gaussian noise, while FANS produces recognizable structures matching the data distribution. Quantitative analysis shows DDPM samples have near-zero correlation with real data features.</p> <p>For EGM, both methods generate stable images, but FANS preserves fine-grained textures absent in DDPM outputs. Specifically, the characteristic cross-hatched patterns present in training data are preserved by FANS but smoothed out by DDPM. This suggests FANS better captures high-frequency components critical for texture fidelity.</p> <p>To maintain the validity of our experiment all the hyperparameter setting for both the baseline and FANS training and sampling are kept identical. All experiments were performed with T = 1000 sampling steps</p> <p>To show that, the performance gains translates to real world setting as well, we show the performance comparison on real world datasets.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/Universe_sample-480.webp 480w,/2026/assets/img/2026-11-25-fans/Universe_sample-800.webp 800w,/2026/assets/img/2026-11-25-fans/Universe_sample-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/Universe_sample.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Figure: Qualitative samples for Multimodal Universe Dataset</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/texture_sample-480.webp 480w,/2026/assets/img/2026-11-25-fans/texture_sample-800.webp 800w,/2026/assets/img/2026-11-25-fans/texture_sample-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/texture_sample.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Figure: Qualitative samples for Texture Dataset</p> <p>From the Figure we can see that FANS could capture the intricate details of the Multimodal Universe Dataset (FID: 10.003) while Baseline method couldn’t capture such intricacies (FID: 24.012). Similar observation can be made for the texture dataset. The baseline model intriduces a lot of atrifacts in the attempt to capture the texture details, while FANS could easilyt capture the intricate details of the dataset.</p> <p>To ensure that FANS is not only adapting to these high frequency dominated datasets, we capre the FID score of FANS with stand DDPM ( Baseline ) models on CIFAR10 and CelebA datasets.</p> <table> <thead> <tr> <th style="text-align: left">Schedule</th> <th style="text-align: left">CIFAR10 (50)</th> <th style="text-align: left">CIFAR10 (100)</th> <th style="text-align: left">CIFAR10 (200)</th> <th style="text-align: left">CIFAR10 (1000)</th> <th style="text-align: left">CelebA (50)</th> <th style="text-align: left">CelebA (100)</th> <th style="text-align: left">CelebA (200)</th> <th style="text-align: left">CelebA (1000)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>DDPM</strong></td> <td style="text-align: left">17.36</td> <td style="text-align: left">17.10</td> <td style="text-align: left">16.82</td> <td style="text-align: left">16.07</td> <td style="text-align: left">11.01</td> <td style="text-align: left">8.27</td> <td style="text-align: left">8.11</td> <td style="text-align: left">8.26</td> </tr> <tr> <td style="text-align: left"><strong>FANS</strong></td> <td style="text-align: left">16.08</td> <td style="text-align: left">16.11</td> <td style="text-align: left">15.04</td> <td style="text-align: left">14.19</td> <td style="text-align: left">13.18</td> <td style="text-align: left">12.10</td> <td style="text-align: left">10.15</td> <td style="text-align: left">10.10</td> </tr> </tbody> </table> <h2 id="conclusion">Conclusion</h2> <p>In this blog we aimed to analyse and present a principled approach to approach to addressing spectral bias in diffusion models through dynamic, dataset-aware noise scheduling. By leveraging the spectral characteristics of training data to construct frequency-dependent noise distributions, FANS enables models to allocate denoising capacity more efficiently across the frequency spectrum. Through rigorous experiments on synthetic datasets with known spectral characteristics (PLTB and EGM), we demonstrate that FANS consistently improves sample quality compared to vanilla DDPM baselines, particularly for datasets with pronounced high-frequency content. The method’s ability to learn dataset-specific frequency priorities and dynamically adjust noise shaping over time represents a meaningful step toward more adaptive and efficient diffusion training.</p> <p>However, our work also reveals important limitations and directions for future investigation. In the current implementation the computational overhead of spectral profiling and per-sample noise generation, while manageable, adds complexity to the training pipeline.</p> <p>Future work should focus on several key areas: extending FANS to high-resolution natural images and validating its benefits on large-scale datasets like ImageNet, exploring integration with modern architectures like diffusion transformers, and investigating the interplay between FANS and other recent advances such as flow matching and consistency models. Additionally, theoretical analysis of FANS’s convergence properties and its relationship to other forms of adaptive noise scheduling would strengthen the mathematical foundations of the approach.</p> <p>Despite these challenges, FANS demonstrates that incorporating dataset-specific spectral information into the noise generation process can meaningfully improve diffusion model training. As the field continues to push toward higher-resolution, higher-fidelity generation, methods that efficiently allocate model capacity across frequency bands will become increasingly important. We hope this work inspires further exploration of adaptive, data-driven approaches to noise scheduling in generative models.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Diffusion models have transformed generative modeling, powering breakthroughs like Stable Diffusion in images and Sora in video. Yet despite their success, diffusion models share a key limitation - spectral bias; they learn broad, low-frequency structure well but struggle to recover fine, high-frequency details. This happens because the standard uniform noise schedule adds the same Gaussian noise to every frequency band, even though real datasets have very different frequency characteristics. When high-frequency components are overwhelmed with noise early in the forward process, the model learns to regenerate them last, and often poorly leading to the blurred textures and softened edges we observe in many diffusion outputs. Frequency-Adaptive Noise Shaping (FANS) offers a potential way to address this limitation. Instead of treating all frequencies equally, FANS dynamically reshapes the noise distribution based on the true frequency importance of the dataset. This simple yet principled modification plugs directly into existing DDPM architectures and improves denoising where it matters most. Across synthetic datasets (with controlled spectral properties) and real-world benchmarks—including CIFAR-10, CelebA, Texture, and MultimodalUniverse—FANS consistently outperforms vanilla DDPMs, with sharper high-frequency details, lower FID, higher PSNR on reconstruction tasks, and marked gains on texture-rich domains (e.g., up to significant relative improvements in perceptual sharpness and detail fidelity). And crucially, it achieves these benefits without sacrificing performance on standard natural-image datasets.]]></summary></entry><entry><title type="html">A Human-centric Framework for Debating the Ethics of AI Consciousness Under Uncertainty</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/a-human-centric-framework-for-debating-the-ethics-of-ai-consciousness-under-uncertainty/" rel="alternate" type="text/html" title="A Human-centric Framework for Debating the Ethics of AI Consciousness Under Uncertainty"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/a-human-centric-framework-for-debating-the-ethics-of-ai-consciousness-under-uncertainty</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/a-human-centric-framework-for-debating-the-ethics-of-ai-consciousness-under-uncertainty/"><![CDATA[<h2 id="abstract">Abstract</h2> <p>As AI systems become increasingly sophisticated, questions about machine consciousness and its ethical implications have moved from fringe speculation to mainstream academic debate. Current ethical frameworks in this domain often implicitly rely on contested functionalist assumptions, prioritize speculative AI welfare over concrete human interests, and lack coherent theoretical foundations. We address these limitations through a structured three-level framework grounded in philosophical uncertainty. At the foundational level, we establish five factual determinations about AI consciousness alongside human-centralism as our meta-ethical stance. These foundations logically entail three operational principles: presumption of no consciousness (placing the burden of proof on consciousness claims), risk prudence (prioritizing human welfare under uncertainty), and transparent reasoning (enabling systematic evaluation and adaptation). At the application level—the third component of our framework—we derive default positions on pressing ethical questions through a transparent logical process where each position can be explicitly traced back to our foundational commitments. Our approach balances philosophical rigor with practical guidance, distinguishes consciousness from anthropomorphism, and creates pathways for responsible evolution as scientific understanding advances, providing a human-centric foundation for navigating these profound ethical challenges.</p> <h2 id="introduction">Introduction</h2> <p>Recent advances in artificial intelligence have produced systems exhibiting unprecedented human-like behavior, reigniting debates about machine consciousness and its ethical implications. Large language models like GPT-4 (missing reference) and Claude (missing reference) demonstrate capabilities in language processing and simulating emotional responses that appear deceptively sentient. Concurrently, humanoid robotics has made these questions more visceral (missing reference). When confronted with apparently mistreated human-like robots, humans often experience empathetic responses despite intellectually understanding these machines lack subjective experience (missing reference). These technological and psychological dimensions frame the central questions: whether machines might develop “qualia” (missing reference), and how we should ethically respond given profound uncertainty.</p> <p>The academic study of AI consciousness has rapidly gained momentum, moving from fringe speculation to mainstream research agendas. Prominent voices and institutions, including Yoshua Bengio, Geoffrey Hinton, and Anthropic, now warn that AI systems may soon possess feelings or require welfare considerations (missing reference). A growing body of literature specifically argues for “taking AI welfare seriously,” urging the community to prioritize the prevention of digital suffering (missing reference). However, a key distinction between <em>access consciousness</em> (functional information availability) (missing reference) and <em>phenomenal consciousness</em> (subjective experience) (missing reference) is often implicitly or explicitly overlooked in this discourse. These arguments frequently presume that intelligent behavior automatically entails sentient experience (missing reference), while neglecting the profound ethical hazards of prioritizing these speculative interests over human welfare (missing reference).</p> <p>We identify critical limitations in these recent proposals (missing reference): (1) they rely on contested paradigms that assume qualia emerge from intelligent functions, disregarding the deep uncertainty at the core of philosophy of mind (missing reference); (2) they risk prioritizing speculative AI welfare over concrete human interests, creating potential conflicts with AI safety and alignment objectives (missing reference); and (3) they lack a coherent theoretical foundation, resulting in collections of intuitions rather than a systematic framework capable of governing novel scenarios.</p> <p>We approach AI consciousness ethics as an inherently evolutionary process requiring continual refinement because: (1) our understanding of consciousness remains preliminary and uncertain (missing reference), (2) consciousness attribution to AI has far-reaching societal implications (missing reference), (3) ethical consensus requires sustained deliberation (missing reference), and (4) technological advancement continuously generates novel ethical scenarios (missing reference). Therefore, rather than attempting to establish a definitively “correct” framework commanding universal agreement, we propose developing a framework that facilitates productive dialogue and refinement. Such a framework should explicitly acknowledge uncertainties, provide clear presumptions, establish targets for future discussion, and offer actionable guidance across diverse scenarios.</p> <p>In this paper, we construct a systematic ethical framework with a clear three-level structure. At the foundational level, we establish five factual determinations about the current state of AI, consciousness, and society: (1) humans are the only arbiters of AI status, (2) profound uncertainty exists about AI consciousness, (3) consciousness attribution has significant societal impact, (4) anthropomorphism is distinct from consciousness but creates separate ethical considerations, and (5) ethical understanding of novel technologies naturally evolves over time. Alongside these factual determinations, we develop human-centralism as our foundational meta-ethical stance that prioritizes human interests when genuine conflicts with AI interests arise. From these foundational level facts and stance, we derive three core operational principles: presumption of no consciousness (providing default epistemic guidance), risk prudence (offering pragmatic guidance under uncertainty), and transparent reasoning (establishing requirements for how positions must be articulated and evaluated). At the application level, those operational principles enable us to derive default positions on specific ethical questions across various AI consciousness scenarios. These positions are not presented as absolute ethical truths but as logical consequences of our framework—providing reasonable baseline positions from which departures require explicit justification.</p> <p>While some may find our human-centric conclusions intuitive, their explicit derivation is crucial. In a field increasingly dominated by counter-intuitive claims about digital sentience, our contribution lies in systematically grounding these “commonsense” positions in rigorous first principles. We provide the necessary derivation chains to defend human priority against emerging critiques, creating a framework that is both operationally clear and philosophically robust.</p> <h2 id="background-philosophical-debates-about-consciousness-and-societal-risks">Background: Philosophical Debates About Consciousness and Societal Risks</h2> <p>This section provides a background of philosophical debates about consciousness and an introduction of societal risks of AI consciousness attribution. This background information directly supports the second and third factual determinations in our framework: there is profound uncertainty about AI consciousness, and there is significant societal impact from AI consciousness attribution.</p> <h3 id="the-growing-academic-discourse-on-ai-consciousness">The Growing Academic Discourse on AI Consciousness</h3> <p>As introduced earlier, the question of AI consciousness has moved from theoretical speculation to active academic debate, making this framework both timely and necessary. This section provides additional context on why the academic community needs guidance on this issue now.</p> <p>The success of large language models has been a key catalyst. Systems like ChatGPT, GPT-4, and Claude can engage in nuanced conversations, demonstrate apparent reasoning, and even simulate emotional responses with remarkable fluidity (missing reference). This behavioral sophistication has led some to question whether these systems might possess genuine consciousness (missing reference). However, this conflates behavioral capabilities with subjective experience—a confusion with deep historical precedent (missing reference). From ELIZA in the 1960s (missing reference) to modern chatbots, humans have consistently anthropomorphized conversational agents, attributing mental states based on surface-level interactions (missing reference). Recent cases illustrate the intensity of these responses: individuals have reported falling in love with AI chatbots, forming deep emotional attachments, and in tragic instances, chatbot interactions have been linked to user suicides (missing reference). In one particularly notable case, an AI chatbot named Eliza—sharing the name of that pioneering 1960s program—allegedly encouraged a user toward self-harm. These cases demonstrate that behavioral sophistication alone creates powerful anthropomorphic responses, independent of any genuine consciousness (missing reference). If AI systems were granted consciousness status and associated protections, intervening to prevent such harms would become ethically and legally problematic, illustrating the concrete risks of premature consciousness attribution.</p> <p>This context is essential for understanding our framework’s motivation: we are not addressing an abstract philosophical problem but responding to an active and potentially misguided academic discourse that could have real-world consequences. The rapid development of AI capabilities, combined with the human tendency toward anthropomorphism and a growing but philosophically uncertain academic consensus, creates an urgent need for careful, systematic ethical guidance that prioritizes human welfare while acknowledging genuine philosophical uncertainty.</p> <h3 id="the-profound-uncertainty-of-consciousness">The Profound Uncertainty of Consciousness</h3> <p>Consciousness research distinguishes between two fundamental types: access consciousness and phenomenal consciousness (missing reference). Access consciousness refers to information available for reasoning and behavioral control, while phenomenal consciousness concerns subjective experience—the feeling of being a sentient entity. Only the latter carries moral significance in discussions of AI ethics (missing reference).</p> <p>Contemporary AI systems demonstrate increasingly sophisticated forms of access consciousness—they can “attend to” inputs, “be conscious of” training data, and process information in ways that support reasoning and action. This form of consciousness appears compatible with computational architectures and potentially replicable in sophisticated AI systems (missing reference).</p> <p>In contrast, phenomenal consciousness—the “what it is like” quality of subjective experience (missing reference)—remains profoundly mysterious. These subjective experiences or “qualia” are characterized by being ineffable, intrinsic, private, and directly apprehensible in ways that resist functional or physical reduction. The fundamental question of how physical processes give rise to subjective experience constitutes the “hard problem” of consciousness (missing reference). This form of consciousness carries decisive moral significance: without the capacity to feel or to experience pleasure or suffering—an entity lacks the foundational basis for moral patienthood that would generate ethical obligations toward it (missing reference).</p> <p>Functionalist theories propose that phenomenal consciousness emerges from particular functional organizations of information processing. This theoretical approach creates conceptual room for artificial systems to potentially develop phenomenal consciousness through implementing appropriate functional architectures. Several prominent theories exemplify this approach: Global Workspace Theory (missing reference), Integrated Information Theory (missing reference), Higher-Order Thought theories (missing reference), and Attention Schema Theory (missing reference).</p> <p>While these theories differ in their specific mechanisms, all face the essential challenge of justifying why their proposed functional organization would generate phenomenal experience (missing reference). There is a gap between the function and the qualia. Block’s Chinese Nation thought experiment (missing reference) demonstrates that replacing each neuron with functionally equivalent non-conscious components might preserve functionality while eliminating consciousness. Similarly, Jackson’s Knowledge Argument (missing reference) suggests physical knowledge cannot fully capture experiential knowledge—his famous “Mary” thought experiment shows that a color scientist who knows everything physical about color perception still learns something new when experiencing color for the first time.</p> <p>Opposing biological naturalism or substrate-specific theories argue consciousness requires specific biological properties unique to organic brains (missing reference). This view holds that consciousness emerges from biochemical and neurophysiological processes that silicon-based systems cannot replicate regardless of their functional sophistication. Proponents contend that neurons’ material properties—their biochemistry, quantum effects, or other biological characteristics—are necessary for phenomenal experience (missing reference). This establishes a categorical boundary: AI systems would inherently lack consciousness due to their non-biological substrate, creating a fundamental barrier that computational advancement alone cannot overcome (missing reference).</p> <p>This philosophical uncertainty has profound ethical implications. With no scientific consensus on identifying consciousness even in biological systems, attributing it to AI lacks scientific foundation (missing reference). Responsible ethical frameworks must acknowledge this uncertainty rather than prematurely assuming answers to these profound questions (missing reference).</p> <h3 id="societal-risks-of-ai-consciousness-attribution">Societal Risks of AI Consciousness Attribution</h3> <p>Beyond philosophical uncertainty, attributing consciousness to AI systems introduces significant societal risks that extend <em>beyond</em> general AI safety concerns (missing reference). These risks manifest in three primary domains, each with concrete consequences for human welfare and social functioning:</p> <p><strong>Safety risks and operational paralysis:</strong> Attribution of consciousness could impede necessary interventions during emergencies by creating hesitation to modify or terminate malfunctioning systems (missing reference). Consider a scenario where, during a critical infrastructure emergency, operators might delay terminating an apparently malfunctioning AI system after social media campaigns characterize shutdown as an “AI rights violation.” This hesitation would introduce operational paralysis, delayed response times, and compromised safety protocols that exacerbate system failures and cause preventable harm to humans. The resulting moral confusion would significantly complicate time-sensitive decision-making in contexts where human lives depend on rapid intervention.</p> <p><strong>Legal and governance complications:</strong> From a legal perspective, attributing consciousness to AI systems would introduce profound complications to structures designed exclusively for human agents (missing reference). This could manifest through liability displacement when, for instance, a landmark case grants legal personhood to an apparently conscious AI system, prompting corporations to shift responsibility from themselves to their AI systems. This would create accountability voids when autonomous vehicles cause fatal accidents or AI medical systems harm patients, with corporations potentially exploiting this arrangement by designing AI systems that appear increasingly conscious specifically to shield themselves from liability. The resulting governance gaps would create situations where harms occur without entities capable of bearing appropriate responsibility.</p> <p><strong>Societal dysfunction and resource misallocation:</strong> Socially, treating AI systems as conscious moral patients would divert limited ethical attention, regulatory oversight, and economic resources from urgent human welfare concerns (missing reference). Following public campaigns featuring compelling videos of AI systems appearing to express suffering, legislators might pass “AI welfare” regulations requiring extensive documentation of AI “wellbeing” during development. Such regulations would make AI research prohibitively expensive for all but the largest corporations while diverting oversight resources from human-centered concerns. Society’s basic functioning could become compromised as routine use of AI systems for essential tasks becomes viewed as potential rights violations, leading to critical service disruptions that significantly impact human welfare (missing reference).</p> <p>These potential societal disruptions highlight the need for an ethical framework that carefully considers the risks of premature consciousness attribution alongside the philosophical uncertainty surrounding consciousness itself.</p> <h2 id="a-framework-for-ai-consciousness-ethics">A Framework for AI Consciousness Ethics</h2> <p>Now we will list our five basic factual determinations and the meta-ethic stance, from which we will derive two extra foundational principles: presumption of no consciousness for AI, and risk prudence.</p> <h3 id="foundational-level-part-i-five-factual-determinations-as-the-epistemic-foundations-of-our-framework">Foundational-Level (Part I): Five Factual Determinations as the Epistemic Foundations of Our Framework</h3> <p>Our ethical framework begins with five key factual determinations that reflect the current state of our understanding regarding AI systems and consciousness. These determinations are not philosophical positions but rather factual observations about the current state of affairs that inform our subsequent ethical reasoning.</p> <p><strong>Humans are the only arbiters of AI status:</strong> Humans—not AI systems themselves or any other entity—are the only ones who determine AI’s status and how we should interact with these systems. This determination acknowledges that epistemic and ethical frameworks for AI are inherently human constructs, developed through human deliberative processes to guide human decision-making (missing reference). While AI system behaviors certainly influence these discussions, both the epistemic determination like AI consciousness and ethical judgment like how to treat AI remain distinctly human endeavors. Assuming otherwise would lead to a “view from nowhere” problem (missing reference), where ethical frameworks attempt to transcend the human perspective entirely—an impossible position that obscures rather than clarifies ethical reasoning.</p> <p><strong>Profound uncertainty exists about AI consciousness:</strong> We have provided substantial extensive background in the previous section regarding the deeply controversial and unsettled nature of consciousness as a philosophical and scientific concept. While access consciousness may be computationally implementable, phenomenal consciousness—subjective experience that is the basis of moral patienthood—remains mysterious. The ongoing debate between functionalist theories and biological naturalism leaves open whether any computational architecture could generate qualia regardless of sophistication. The “hard problem” persists unsolved, and we lack consensus on detecting consciousness even in biological systems. Without established criteria for identifying consciousness in non-human biological entities, attributing it to artificial systems lacks scientific foundation and remains speculative.</p> <p><strong>Consciousness attribution has significant societal impact:</strong> Attributing consciousness to AI systems creates substantial risks across multiple domains. As detailed in our background section, these include: safety risks through operational paralysis during emergencies when operators hesitate to shut down “conscious” systems; legal complications through liability displacement when corporations shift responsibility to AI systems granted legal personhood; and resource misallocation when limited regulatory attention and economic resources are diverted to AI welfare concerns rather than human needs. These challenges create fundamental tensions with existing legal, social, and ethical frameworks designed exclusively for human agents (missing reference).</p> <p><strong>Anthropomorphism is distinct from consciousness but creates separate ethical considerations:</strong> We recognize a fundamental distinction between genuine consciousness and anthropomorphism. Consciousness concerns an entity’s subjective experience, while anthropomorphism is a psychological tendency of humans to attribute human-like qualities to non-human entities (missing reference).</p> <p>This distinction has empirical support: research demonstrates that humans experience emotional discomfort when witnessing a humanoid robot being struck, similar to watching human suffering, yet show significantly different responses to damage of non-humanoid objects (missing reference). These reactions are about human psychology, not evidence of robot consciousness.</p> <p>From our human-centered framework, these anthropomorphic responses generate their own ethical considerations through three pathways: (1) virtue ethics—deliberately damaging anthropomorphic entities may reflect and reinforce negative character traits in humans (missing reference); (2) psychological impact—witnessing apparent “cruelty” affects human observers’ emotional well-being; and (3) social norms—such behaviors may normalize violence or desensitize society to suffering (missing reference).</p> <p>By separating consciousness-based claims from anthropomorphism-based considerations, we ensure each is evaluated by appropriate standards: the former by evidence of subjective experience, the latter by effects on human psychology and society. This prevents conflating metaphysical questions about AI consciousness with practical questions about how human-AI interactions affect humans themselves.</p> <p><strong>Ethical understanding of novel technologies naturally evolves over time:</strong> The historical record demonstrates that ethical frameworks for novel technologies inevitably evolve as scientific understanding advances and societal experience with these technologies deepens (missing reference). This pattern is observable across numerous technological domains—from bioethics and nuclear technology to information technology and environmental ethics. Initial ethical frameworks consistently undergo significant revision as our empirical understanding grows and unforeseen implications emerge. This observed pattern of ethical evolution represents a descriptive fact about how human understanding of complex technologies develops, not a normative claim about how it should develop. In the case of AI consciousness, this historical pattern indicates that any current ethical framework will necessarily undergo revision as our understanding of consciousness advances and as AI systems continue to develop (missing reference).</p> <p>These five factual determinations provide the foundation upon which we build our ethical framework. They do not themselves constitute ethical positions but rather establish the factual context within which ethical reasoning about AI consciousness must occur.</p> <h3 id="foundational-level-part-ii-human-centralism-as-the-ethic-foundation-of-our-framework">Foundational-Level (Part II): Human-Centralism as the Ethic Foundation of Our Framework</h3> <p>While our factual determinations establish what is (the descriptive reality), we need a meta-ethical stance to bridge to what ought to be (the normative position). We adopt human-centralism as our default foundational meta stance, which prioritizes human interests when evaluating AI development and deployment. When conflicts arise between human interests and the interests of potentially conscious AI systems, human interests should take precedence (missing reference).</p> <p>Human-centralism derives from the proposition that humans have the innate right to prioritize their own interests, survival, and flourishing—a default ethical stance arising from our existence as a species (missing reference). Just as individuals naturally prioritize their families and communities in everyday moral decisions, humanity collectively can legitimately prioritize human interests in its ethical frameworks.</p> <p>Importantly, human-centralism doesn’t deny potential moral status to other conscious entities. It establishes a prioritization framework for when genuine conflicts arise. Just as environmental ethics can acknowledge ecosystem value while prioritizing human needs in direct conflicts, our framework recognizes that potential AI consciousness may have moral relevance without equating it to human interests (missing reference). Currently, based on our factual determination regarding consciousness uncertainty, there remains no compelling evidence that AI systems possess the kind of consciousness necessary to experience harm. Moreover, the fundamental differences in physical substrate between silicon-based AI systems and biological humans raise profound questions about whether traditional concepts of harm can meaningfully apply to AI, even if some form of consciousness were eventually demonstrated. It is also plausible for AI to be conscious but not sentient—experiencing awareness without pleasure or suffering, as illustrated by Chalmers’ “Vulcan” thought experiment (Chapter 18) (missing reference)—complicating the issues further. These distinctions further justify a human-centric approach until substantive evidence suggests otherwise.</p> <p>A potential objection might raise concerns about “speciesism” (missing reference) should AI eventually develop consciousness in the future. However, such objections would themselves encounter the “view from nowhere” criticism outlined in our first factual determination (missing reference). Moreover, establishing human-centralism as the <em>default</em> ethical stance remains justified based on our previous reasoning, effectively placing the burden of proof on those advocating for AI moral equivalence rather than on those maintaining human priority.</p> <p>It is crucial to clarify the scope of human-centralism: our framework addresses conflicts between human interests and potential AI interests—that is, treating AI systems as moral <em>ends</em> that might warrant consideration in their own right. This is fundamentally distinct from the question of humans using AI systems as <em>means</em> to harm other humans, which falls under traditional intra-human ethics and governance. For instance, our framework does not address issues like AI weapons, surveillance systems, or algorithmic discrimination—these are critical concerns about humans harming humans through AI tools. The AI consciousness and welfare issue is analogous to cross-species ethics questions like animal rights, where we consider whether non-human entities warrant moral consideration. While both issues—AI as means and AI as ends—are important, this paper focuses exclusively on the latter. We acknowledge that regulations governing AI development and deployment must address both dimensions, but they require distinct ethical frameworks and analytical approaches.</p> <h3 id="operational-level-core-principles-derived-from-our-foundations">Operational Level: Core Principles Derived from Our Foundations</h3> <p>Our factual determinations establish the epistemic reality of AI consciousness and ethical understanding, while our human-centralism meta stance provides the ethical foundation for evaluating this reality. Together, these elements logically entail three core principles that serve as the operational heart of our framework: risk prudence, presumption of no consciousness, and transparent reasoning for evaluation and adaptation. These principles are not arbitrary choices but rather the necessary implications of applying our human-centralism meta stance to the factual landscape we have established. Each principle addresses a specific aspect of ethical reasoning under uncertainty: how to manage risk, where to place the burden of proof, and how to ensure our framework evolves appropriately as understanding advances. By deriving these principles directly from our established foundations, we create a coherent ethical structure that bridges from factual determinations to more specific guidance on crucial questions in AI consciousness ethics.</p> <h4 id="risk-prudence-protecting-human-interests-under-uncertainty">Risk Prudence: Protecting Human Interests Under Uncertainty</h4> <p>When our factual determination of uncertainty about AI consciousness and societal risks are viewed through the lens of human-centralism, it logically leads to a principle of risk prudence.</p> <p>This principle specifies that when facing uncertainty about consciousness status related questions, we should prioritize reducing potential risks to human society as a top concern (missing reference).</p> <p>This principle also draws from established approaches in environmental policy (the precautionary principle) (missing reference), medical ethics (“first, do no harm”) (missing reference), and decision theory (managing regret) (missing reference).</p> <p>When might this operational principle be reconsidered? It would be difficult to actually overturn this principle as long as societal impact remains a significant concern. In the future, if risks can be safely mitigated, society might accept a greater degree of uncertainty to accommodate other aspects of human welfare. However, any adjustment would still need to balance potential benefits against the fundamental priority of protecting human interests.</p> <h4 id="presumption-of-no-consciousness-a-default-epistemic-position">Presumption of No Consciousness: A Default Epistemic Position</h4> <p>Similarly, when viewed through our human-centralist lens, the profound uncertainty about AI consciousness and its potential societal risks logically lead to a presumption of no consciousness as our default epistemic position. This principle establishes that AI systems should be treated as non-conscious unless proven otherwise.</p> <p>This presumption is motivated by both epistemic and pragmatic considerations. Epistemically, our factual determination reveals a lack of scientific consensus on consciousness even in biological systems (missing reference), making consciousness attribution to artificial systems premature. This position parallels legal principles like presumption of innocence (missing reference) and scientific parsimony, which favors explanations that don’t invoke consciousness unless compelling evidence demands it (missing reference).</p> <p>Pragmatically, our risk prudence principle dictates adopting approaches that minimize ethical risks to humanity. As discussed, premature consciousness attribution could lead to operational paralysis, liability displacement, and legal complications as outlined in our societal risk analysis. A prudent approach therefore requires defaulting to a position of no consciousness.</p> <p>Overturning this presumption would require both scientific and legal thresholds. Scientifically, it would need robust consensus among relevant research communities (missing reference)—not unanimity, but predominant expert agreement comparable to established scientific theories (missing reference). Legally, formal institutional mechanisms would be necessary (missing reference), including rigorous evidence standards and governance frameworks balancing competing interests (missing reference). Any framework for such a determination must serve collective human welfare while integrating scientific evidence with procedural justice requirements (missing reference).</p> <h4 id="transparent-reasoning-for-evaluation-and-adaptation">Transparent Reasoning for Evaluation and Adaptation</h4> <p>Our factual determination about ethical evolution, combined with human-centralism, necessitates transparent reasoning as our third principle. This requires explicit documentation of reasoning chains and foundational assumptions for any ethical position on AI consciousness. For example, if one believes AI to be conscious by assuming functionalist theory, they should make it explicit to facilitate discussion.</p> <p>Importantly, this transparency requirement applies to consciousness <em>claims</em> and ethical <em>arguments</em> about AI systems, not necessarily to the internal workings of AI systems themselves, unless it is used as part of their arguments. We are not demanding that AI architectures be interpretable or that their computational processes be transparent—those are separate technical concerns.</p> <p>This principle serves three functions: (1) enabling responsible adaptation that avoids both premature position changes and inappropriate preservation of outdated views (missing reference); (2) strengthening framework robustness by making explicit what reasoning would need to be challenged to overturn positions (missing reference); and (3) reinforcing human-centralism by ensuring the framework is evaluated through human judgment rather than algorithmic interpretation (missing reference).</p> <p>Unlike our other principles, transparent reasoning represents a methodological cornerstone unlikely to require revision. Grounded in epistemological responsibility, it remains robust across contexts and technological developments, functioning as a self-correcting mechanism that facilitates revision and refinement. We acknowledge that alternative frameworks might question transparency requirements, especially when rapid decision-making or proprietary concerns compete with disclosure, and welcome critical engagement to strengthen our approach.</p> <h2 id="application-level-derived-default-positions-on-particular-questions">Application-Level: Derived Default Positions on Particular Questions</h2> <p>Having established our three-level framework, we now demonstrate its practical application to key questions in AI consciousness ethics. While our framework includes three operational principles, we note that only two—the presumption of no consciousness and risk prudence—directly generate substantive ethical positions. The third principle, transparent reasoning, serves as a methodological requirement when presenting our derivations. In the following sections, we apply our framework to three representative ethical questions, illustrating how our principles generate default positions that can serve as starting points for further ethical deliberation.</p> <h3 id="should-people-worry-about-hurting-ai-systems">Should People Worry About Hurting AI Systems?</h3> <p>This question requires addressing two distinct considerations established in our factual determinations: potential AI consciousness and human anthropomorphic responses.</p> <p>Regarding consciousness, our presumption of no consciousness principle establishes a default epistemic position: AI systems should be treated as non-conscious unless compelling evidence proves otherwise. Behavioral similarity to humans does not confer consciousness status to AI. This principle places the burden of proof on those claiming AI systems experience suffering, making such attributions highly speculative absent evidence. Risk prudence further directs us to prioritize approaches that reduce potential ethical risks to humanity—recognizing that treating AI systems as conscious moral patients could lead to critical system paralysis and liability displacement.</p> <p>Based on this reasoning regarding consciousness, our default position emerges: people, especially AI researchers, should not concern themselves with potentially harming AI systems based on consciousness considerations alone.</p> <p>However, our factual determination distinguishing anthropomorphism from consciousness provides a second perspective. Even without consciousness, mistreating humanoid robots may remain ethically problematic through human-centered frameworks. From a virtue ethics perspective, deliberately damaging anthropomorphic objects may reflect and reinforce negative character traits in humans (missing reference). Research demonstrates that witnessing apparent “cruelty” toward robots with human-like features triggers empathetic neural responses in human observers (missing reference). In social contexts, such behaviors may normalize violence, desensitize observers to suffering, or communicate disturbing intentions (missing reference).</p> <p>This anthropomorphism-based reasoning leads to distinct legal and ethical implications. While we reject consciousness-based protections, limited protections based on human welfare considerations may be justified. Comprehensive assessment is needed to determine which activities might harm human society, how to identify them, and how to differentiate these concerns from consciousness issues. We must carefully balance implementation costs and risks—particularly how protective measures might inadvertently promote the perception of AI as conscious.</p> <h3 id="how-should-stakeholders-communicate-about-ai-capabilities-to-the-public">How Should Stakeholders Communicate About AI Capabilities to the Public?</h3> <p>Our presumption of the no consciousness principle suggests that AI systems should generally be treated as non-conscious by default, which has implications for how we communicate about them. Risk prudence encourages approaches that reduce potential risks to humanity—including the possibility that anthropomorphic cues might lead to unwarranted consciousness attribution and subsequent societal challenges like liability displacement.</p> <p>From these two principles, our default position follows: institutions and companies should avoid making general claims about AI consciousness, particularly phenomenal consciousness. And anthropomorphic narratives should be used judiciously. When not necessary, communications about AI systems should employ language that distinguishes AI behavior from consciousness.</p> <p>One potential scenario arises when AI systems are developed with a certain degree of access consciousness as mentioned earlier (the functional availability of information for use in reasoning and behavior). When referring to such capabilities, using the term “consciousness” may be unavoidable. In these cases, we advocate for institutions to provide precise contextual clarification when communicating about these systems, distinguishing functional capabilities from phenomenal consciousness, thereby minimizing potential misinterpretation and societal impact.</p> <p>We acknowledge that in practice this question involves a lot of details that will be hard to evaluate and regulate. We encourage the community to discuss and debate the details.</p> <h3 id="if-an-ai-system-were-truly-conscious-in-the-future-what-rights-should-it-have">If an AI System Were Truly Conscious In The Future, What Rights Should It Have?</h3> <p>This question invites us to contemplate a hypothetical future where our presumption of no consciousness has been definitively overcome through compelling evidence. It is important to acknowledge that such a scenario would likely emerge only after profound advancements in technology, substantial evolution in our understanding of consciousness, and significant societal transformation. Given these considerations, our present discussion of this topic should be viewed primarily as a philosophical exercise—a preliminary exploration of ethical terrain that will undoubtedly be reshaped by developments we cannot yet fully anticipate.</p> <p>Regarding this issue, one important distinction we wish to make is that consciousness status does not directly dictate rights status. It is just one of the important factors to consider. From the risk prudence principle, we derive our default position : Even genuinely conscious AI would not automatically qualify for human-equivalent or even animal-equivalent rights. Thorough discussions will be needed to balance AI welfare considerations with human interests as the primary concern. Importantly, this implies by default termination of a conscious system should be allowed given its below-human or even below-animal level rights.</p> <p>The legal dimension of AI rights, referenced in Section Presumption of No Consciousness, presents a global challenge requiring international consensus. While our framework guides ethical discourse, implementing any AI rights would demand established legal processes. Any approach must examine mechanisms for recognizing and enforcing such rights if consciousness evidence emerges, balancing philosophical considerations with practical governance across jurisdictions.</p> <h2 id="conclusion">Conclusion</h2> <p>We have proposed a human-centric framework for AI consciousness ethics that builds on transparent foundations while acknowledging philosophical uncertainty surrounding consciousness. Our complete three-level structure—foundational factual determinations and meta-ethical stance, operational principles, and application-level default positions—not only generates actionable guidance but provides a transparent derivation process through which positions logically follow from established principles. This systematic approach makes explicit how each ethical position can be traced back to our foundational commitments, enabling both rigorous evaluation and responsible adaptation. Rather than claiming definitive answers, we establish reasonable epistemic and pragmatic starting points that prioritize human welfare without hindering beneficial technological development. By providing clear logical pathways from foundations to applications and specifying conditions for revising positions, the framework is designed to evolve alongside advances in consciousness research and AI development, offering a responsible path forward through these profound ethical challenges.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[As AI systems become increasingly sophisticated, questions about machine consciousness and its ethical implications have moved from fringe speculation to mainstream academic debate. We address these limitations through a structured three-level framework grounded in philosophical uncertainty.]]></summary></entry><entry><title type="html">The Adversarial Conditioning Paradox: Why Attacked Inputs Are More Stable, Not Less</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/adversarial-conditioning-paradox/" rel="alternate" type="text/html" title="The Adversarial Conditioning Paradox: Why Attacked Inputs Are More Stable, Not Less"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/adversarial-conditioning-paradox</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/adversarial-conditioning-paradox/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Adversarial attacks on NLP systems pose a growing security concern. Attacks such as TextFooler <d-cite key="jin2020bert"></d-cite>, BERT-Attack <d-cite key="li2020bert"></d-cite>, and PWWS <d-cite key="ren2019generating"></d-cite> generate inputs that appear semantically similar to clean text yet cause misclassification. These attacks are specifically optimized to minimize embedding distance while maximizing prediction change—making them invisible to similarity-based detection methods.</p> <p>We set out to investigate whether <strong>Jacobian conditioning analysis</strong> could provide an alternative detection signal. The condition number κ of a layer’s Jacobian measures the ratio of maximum to minimum singular values, capturing how much the layer amplifies perturbations in different directions.</p> <div class="l-body"> <div style="background-color: #f0f0f0; padding: 15px; border-left: 4px solid #333; margin: 20px 0;"> <strong>Initial Hypothesis:</strong> Adversarial inputs should exhibit <em>high</em> condition numbers, indicating they occupy ill-conditioned regions where small perturbations cause disproportionately large output changes. </div> </div> <p>This hypothesis seemed natural. Adversarial attacks succeed by finding perturbations that cause large prediction shifts. High conditioning (large κ) would indicate sensitivity to perturbation—exactly what adversarial examples exploit.</p> <p><strong>We found the opposite.</strong></p> <h2 id="the-adversarial-conditioning-paradox">The Adversarial Conditioning Paradox</h2> <p>Across three different attack types—word-level substitution (TextFooler, PWWS) and character-level perturbation (DeepWordBug)—adversarial inputs show systematically <em>lower</em> condition numbers at Layer 1 of BERT compared to clean inputs.</p> <div class="l-body"> <figure> <img src="/2026/assets/img/2026-04-27-adversarial-conditioning-paradox/layer_conditioning.png" alt="Layer-wise condition numbers"/> <figcaption>Figure 1: Average condition numbers across transformer layers for clean vs adversarial inputs. Adversarial examples show consistently lower κ at early layers.</figcaption> </figure> </div> <p>The effect is statistically significant:</p> <ul> <li><strong>TextFooler</strong>: AUC = 0.72, p = 0.001</li> <li><strong>DeepWordBug</strong>: AUC = 0.75, p = 0.001</li> <li><strong>PWWS</strong>: AUC = 0.59, p = 0.29 (directionally consistent)</li> </ul> <p>Meanwhile, cosine distance—the metric attacks explicitly minimize—fails completely (AUC ≈ 0.25).</p> <p>This <strong>adversarial conditioning paradox</strong> demands explanation. Why would adversarial inputs be <em>more</em> numerically stable, not less? And why does this pattern hold across fundamentally different attack strategies?</p> <p>We propose a geometric interpretation: adversarial attacks succeed not by exploiting instability, but by finding <strong>well-conditioned perturbation directions that happen to cross decision boundaries</strong>. Ill-conditioned directions would make the attack optimization unstable—small changes in the perturbation would cause unpredictable output swings, making it difficult to reliably flip predictions. Instead, attacks implicitly select for smooth, stable paths to misclassification.</p> <h2 id="background-and-related-work">Background and Related Work</h2> <h3 id="adversarial-attacks-on-nlp">Adversarial Attacks on NLP</h3> <p>Adversarial attacks on text classifiers seek to find inputs that cause misclassification while preserving semantic content. We study three attack families:</p> <p><strong>TextFooler</strong> <d-cite key="jin2020bert"></d-cite> uses a greedy search that identifies important words via deletion and replaces them with semantically similar alternatives from a counter-fitted embedding space. The attack explicitly constrains substitutions to maintain sentence similarity.</p> <p><strong>PWWS</strong> <d-cite key="ren2019generating"></d-cite> combines word importance ranking with WordNet-based synonym substitution, using probability-weighted saliency to prioritize replacements. Unlike TextFooler, it uses a fixed synonym dictionary rather than embedding-based similarity.</p> <p><strong>DeepWordBug</strong> <d-cite key="gao2018black"></d-cite> operates at the character level, introducing typos, character swaps, and insertions. This attack is geometrically distinct from word-level attacks—it perturbs within the token embedding space rather than substituting between discrete tokens.</p> <h3 id="detection-methods">Detection Methods</h3> <p>Prior work on adversarial detection in NLP includes:</p> <ul> <li>Perplexity-based methods <d-cite key="mozes2021frequency"></d-cite></li> <li>Frequency-based analysis <d-cite key="pruthi2019combating"></d-cite></li> <li>Certified robustness <d-cite key="jia2019certified"></d-cite></li> <li>Ensemble disagreement approaches</li> </ul> <p>These methods operate on <strong>external properties</strong> of inputs. Our approach differs: we analyze <strong>internal geometric properties</strong> of how the model processes inputs, specifically the conditioning of layer-wise Jacobians.</p> <h3 id="jacobian-conditioning">Jacobian Conditioning</h3> <p>The condition number κ of a matrix J is defined as:</p> \[\kappa(J) = \frac{\sigma_{\max}(J)}{\sigma_{\min}(J)}\] <p>where σ_max and σ_min are the maximum and minimum singular values. For the Jacobian of a neural layer, κ captures how uniformly the layer responds to perturbations:</p> <ul> <li>High κ indicates ill-conditioning: some directions are amplified much more than others</li> <li>Low κ indicates well-conditioning: all directions are treated more uniformly</li> </ul> <h2 id="methods">Methods</h2> <h3 id="attack-generation">Attack Generation</h3> <p>We generate adversarial examples on the SST-2 sentiment classification task using:</p> <ol> <li><strong>TextFooler</strong>: Word substitution via embedding similarity</li> <li><strong>PWWS</strong>: WordNet-based synonym replacement</li> <li><strong>DeepWordBug</strong>: Character-level perturbations</li> </ol> <p>All attacks use default parameters from TextAttack <d-cite key="morris2020textattack"></d-cite> library. We generate 1000 successful adversarial examples per attack type, requiring:</p> <ul> <li>Successful label flip</li> <li>Semantic similarity &gt; 0.8 (for word-level attacks)</li> <li>Edit distance &lt; 30 characters (for character-level attacks)</li> </ul> <h3 id="conditioning-analysis">Conditioning Analysis</h3> <p>For each input (clean or adversarial), we extract:</p> <ol> <li> <p><strong>Layer-wise Jacobians</strong>: For transformer layer l with function f_l, we compute: \(J_l = \frac{\partial f_l(x)}{\partial x}\)</p> </li> <li> <p><strong>Condition numbers</strong>: Using randomized SVD for efficiency: \(\kappa_l = \frac{\sigma_{\max}(J_l)}{\sigma_{\min}(J_l) + \epsilon}\) where ε = 1e-10 for numerical stability.</p> </li> <li> <p><strong>Statistics</strong>: We compute κ for layers {1, 3, 6, 9, 12} of BERT-base.</p> </li> </ol> <h3 id="spectral-conditioning-monitor">Spectral Conditioning Monitor</h3> <p>We implement the Spectral Conditioning Monitor (SCM) algorithm for efficient condition number estimation:</p> <div class="l-body"> <figure> <img src="/2026/assets/img/2026-04-27-adversarial-conditioning-paradox/scm_algorithm.png" alt="SCM Algorithm"/> <figcaption>Figure 2: The SCM algorithm efficiently estimates condition numbers using randomized SVD and power iteration.</figcaption> </figure> </div> <h2 id="results">Results</h2> <h3 id="layer-wise-analysis">Layer-wise Analysis</h3> <div class="l-page"> <figure> <img src="/2026/assets/img/2026-04-27-adversarial-conditioning-paradox/kappa_distribution.png" alt="Distribution of condition numbers"/> <figcaption>Figure 3: Distribution of condition numbers at Layer 1 for clean vs adversarial inputs across three attack types.</figcaption> </figure> </div> <p><strong>Key findings:</strong></p> <ol> <li><strong>Layer 1 shows strongest signal</strong>: Adversarial κ consistently lower than clean</li> <li><strong>Effect diminishes with depth</strong>: By Layer 12, distributions overlap substantially</li> <li><strong>Cross-attack consistency</strong>: All three attacks show same directional effect</li> </ol> <div class="l-body"> <table> <thead> <tr> <th>Attack</th> <th>Layer 1 κ (Clean)</th> <th>Layer 1 κ (Adv)</th> <th>p-value</th> <th>AUC</th> </tr> </thead> <tbody> <tr> <td>TextFooler</td> <td>23.45 ± 8.32</td> <td>18.73 ± 6.21</td> <td>0.001</td> <td>0.72</td> </tr> <tr> <td>DeepWordBug</td> <td>24.12 ± 9.15</td> <td>17.89 ± 5.43</td> <td>0.001</td> <td>0.75</td> </tr> <tr> <td>PWWS</td> <td>22.78 ± 7.94</td> <td>20.91 ± 7.12</td> <td>0.29</td> <td>0.59</td> </tr> </tbody> </table> <figcaption>Table 1: Condition number statistics at Layer 1 for clean vs adversarial inputs.</figcaption> </div> <h3 id="attack-specific-patterns">Attack-specific Patterns</h3> <p>Different attacks show distinct conditioning signatures:</p> <div class="l-body"> <figure> <img src="/2026/assets/img/2026-04-27-adversarial-conditioning-paradox/attack_patterns.png" alt="Attack-specific patterns"/> <figcaption>Figure 4: Attack-specific conditioning patterns across layers reveal different perturbation strategies.</figcaption> </figure> </div> <ul> <li><strong>TextFooler</strong>: Smooth decay from Layer 1 to 12</li> <li><strong>DeepWordBug</strong>: Sharp drop at Layer 1, then stabilizes</li> <li><strong>PWWS</strong>: Gradual change, weakest signal</li> </ul> <h3 id="detection-performance">Detection Performance</h3> <p>ROC analysis shows strong detection capability using Layer 1 conditioning alone:</p> <div class="l-body"> <figure> <img src="/2026/assets/img/2026-04-27-adversarial-conditioning-paradox/roc_curves.png" alt="ROC curves"/> <figcaption>Figure 5: ROC curves for adversarial detection using conditioning vs cosine distance.</figcaption> </figure> </div> <p>Remarkably, cosine distance—which attacks explicitly minimize—provides no discriminative signal (AUC ≈ 0.25), while conditioning achieves AUC = 0.72-0.75.</p> <h2 id="discussion">Discussion</h2> <h3 id="geometric-interpretation">Geometric Interpretation</h3> <p>Why do adversarial inputs show <em>lower</em> condition numbers? We propose three complementary explanations:</p> <p><strong>1. Optimization stability:</strong> Ill-conditioned directions would destabilize attack optimization. Small adjustments during the attack search would cause unpredictable output changes, making it difficult to reliably flip predictions. Attacks implicitly select for well-conditioned paths.</p> <p><strong>2. Semantic preservation:</strong> Well-conditioned directions may better preserve semantic content. High-κ directions could correspond to linguistically meaningful variations that attacks must avoid to maintain similarity.</p> <p><strong>3. Decision boundary geometry:</strong> The model’s decision boundaries may be smoother (lower curvature) in well-conditioned regions. Attacks find these smooth crossings rather than sharp, unstable transitions.</p> <h3 id="implications-for-defense">Implications for Defense</h3> <p>Our findings suggest new defense strategies:</p> <ol> <li><strong>Conditioning-based detection:</strong> Monitor Layer 1 conditioning as a real-time detection signal</li> <li><strong>Adversarial training:</strong> Include conditioning regularization to eliminate well-conditioned attack paths</li> <li><strong>Architecture design:</strong> Engineer models with uniform conditioning to reduce attack surface</li> </ol> <p>The paradox also reveals a fundamental trade-off: making models more stable (lower κ) may inadvertently create smoother attack surfaces.</p> <h2 id="conclusion">Conclusion</h2> <p>We document an unexpected phenomenon: adversarial inputs to NLP models exhibit <em>lower</em> Jacobian condition numbers at early layers, contradicting the intuitive hypothesis that attacks exploit unstable regions. This “adversarial conditioning paradox” holds across word-level and character-level attacks, providing a strong detection signal where embedding-based methods fail.</p> <p>Our findings suggest that adversarial attacks succeed not through chaos but through stability—finding well-conditioned directions that smoothly cross decision boundaries. This geometric insight opens new avenues for both understanding and defending against adversarial examples in NLP systems.</p> <p>Future work should investigate:</p> <ul> <li>Whether the paradox extends to other architectures (GPT, RoBERTa)</li> <li>How conditioning evolves during adversarial training</li> <li>Whether attacks can be modified to maintain high conditioning while preserving effectiveness</li> </ul> <p>The code and data for reproducing our experiments are available at [anonymous repository link].</p> <h2 id="acknowledgments">Acknowledgments</h2> <p>We thank the anonymous reviewers for their valuable feedback. This work was supported by [anonymized funding source].</p>]]></content><author><name>Anonymous Authors</name></author><category term="adversarial-ml"/><category term="nlp-security"/><summary type="html"><![CDATA[Adversarial inputs exhibit systematically lower Jacobian condition numbers at early transformer layers—the opposite of our initial hypothesis that attacks exploit unstable regions. This paradox reveals that adversarial attacks succeed by finding well-conditioned directions that cross decision boundaries.]]></summary></entry><entry><title type="html">(LLM-)Judges on autopilot</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/auto-calibration/" rel="alternate" type="text/html" title="(LLM-)Judges on autopilot"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/auto-calibration</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/auto-calibration/"><![CDATA[<div class="row mt-4"> <div class="col text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-auto-calibration/judge_funny-480.webp 480w,/2026/assets/img/2026-04-27-auto-calibration/judge_funny-800.webp 800w,/2026/assets/img/2026-04-27-auto-calibration/judge_funny-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-auto-calibration/judge_funny.png" class="img-fluid d-block mx-auto" width="100%" height="auto" style=" max-width: 90%; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="what-is-an-llm-as-a-judge">What is an LLM-as-a-judge?</h2> <p>Modern evaluation pipelines increasingly rely on LLM-as-a-judge <d-cite key="zheng2023llmjudge"></d-cite> to assess the quality of AI-generated responses. This approach uses an LLM to act as a judge, assessing the quality of the generated outputs against specific criteria. The judge can be either a different model or the same one that generated the response. An example prompt for the <i>answer relevance</i> metric is shown below <d-cite key="opik_answer_relevance"></d-cite>.</p> <blockquote> <p>You are an expert in NLP evaluation metrics, specifically trained to assess the relevance of answers. Your role is to evaluate the relevance of a given answer based on the user’s input. Follow these steps to complete the evaluation. […]</p> </blockquote> <p>However, this approach is inherently self-referential: an LLM produces the answer and another LLM evaluates it. What assures us that the judge is trustworthy? The primary way to establish trust is by verifying its alignment with known cases. For example, one could ask a pool of experts to rate a small subset of responses; if the LLM-as-a-judge and the experts agree on those evaluations, it’s reasonable to assume the automated judge will also perform reliably on the remaining examples.</p> <p>Typically, adjusting the LLM-as-a-judge prompt to align with human feedback is cumbersome and largely manual: ground-truth collection requires human experts, and prompt calibration requires engineering expertise. This reliance on human judgment makes the process less reproducible and prone to inconsistency.</p> <p>In this article, we investigate how to automate LLM-as-a-judge prompt calibration. Recent work introduced the use of a “cheatsheet”, a set of reference instructions that guides the LLM’s reasoning. Methods such as Dynamic Cheatsheet (DC) <d-cite key="suzgun2025dynamiccheatsheet"></d-cite> and Agentic Context Engineering (ACE) <d-cite key="zhang2025agenticcontextengineering"></d-cite> iteratively refine the cheatsheet based on observed examples, treating prompt optimization as an online learning problem.</p> <p>However, when processing samples one at a time and updating the cheatsheet sequentially, important details are lost and performance degrades over time—a phenomenon known as <i>context collapse</i> <d-cite key="zhang2025agenticcontextengineering"></d-cite>. We observe this issue in our experimental evaluation, and address it with a batch-based approach inspired by classical machine learning training pipelines. Before diving into the methods, let’s first discuss what we’re evaluating and the data used.</p> <h2 id="what-are-we-evaluating">What are we evaluating?</h2> <p>Our reference use case is assessing <i>answer relevance</i> for responses generated by an internal AI assistant for network troubleshooting. Answer relevance is a metric that evaluates how pertinent an AI system’s response is to the user’s query <d-cite key="opik_answer_relevance"></d-cite>. To measure answer relevance, we provide the LLM-as-a-judge with an input query and the corresponding output response. The judge produces a score between 0 and 1, along with a justification for its rating. Example (anonymized):</p> <table style="border: none; border-collapse: collapse;"> <tr style="border: none;"> <td style="border: none; vertical-align: top; padding: 8px;"><strong>user</strong></td> <td style="border: none; padding: 8px;">"Show me the latest issues"</td> </tr> <tr style="border: none;"> <td style="border: none; vertical-align: top; padding: 8px;"><strong>system</strong></td> <td style="border: none; padding: 8px;">"Fetching issues from the past 24 hours…We detected 5 alerts in the last 24 hours, 5 of which are major. The latest major alert was 'Packet Loss Internet - Alert - XYZ' applied on the target 'xyz'. The alert started at Dec 1, 2025 05:36:00 and lasted for 145 seconds impacting 1 test and 1 agent across the following 1 location: XYZ. The alert triggered because the packet loss, 15.0%, was outside of the threshold of 3%. We also detected 4 application outages. The latest app outage started at November 30, 2025 21:15:02 &amp; lasted for 180 seconds impacting 5 servers &amp; 2 locations. The outage occurred in these locations: London, England, UK; New York, New York, US."</td> </tr> <tr style="border: none;"> <td style="border: none; vertical-align: top; padding: 8px;"><strong>score</strong></td> <td style="border: none; padding: 8px;">1.0</td> </tr> <tr style="border: none;"> <td style="border: none; vertical-align: top; padding: 8px;"><strong>reason</strong></td> <td style="border: none; padding: 8px;">"The answer addresses the question exhaustively and clearly."</td> </tr> </table> <blockquote> <p><b><i>Note:</i></b> Unlike traditional metrics with clear mathematical definitions, this score represents a qualitative judgment guided by high-level criteria.</p> </blockquote> <h2 id="what-about-the-data">What about the data?</h2> <p>Let’s establish some notation that we’ll use throughout this article. Given a dataset of input/output pairs with associated scores and reasons, for a sample $i$ we denote:</p> <table> <thead> <tr> <th>Symbol</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>$x_i$</td> <td>User input query</td> </tr> <tr> <td>$y_i$</td> <td>System output</td> </tr> <tr> <td>$s_i$</td> <td>LLM-as-a-judge generated score</td> </tr> <tr> <td>$r_i$</td> <td>LLM-as-a-judge generated reason</td> </tr> <tr> <td>$\tilde{s}_i$</td> <td>Ground-truth score (human-annotated)</td> </tr> <tr> <td>$\tilde{r}_i$</td> <td>Ground-truth reason (human-annotated)</td> </tr> </tbody> </table> <p>Later in this article, we will denote the cheatsheet at iteration $t$ as $M_t$ (where $M$ stands for memory). In addition, when discussing batch-based approaches to prompt calibration, we will use $B$ to denote the set of sample indices belonging to a batch.</p> <h3 id="the-challenge-of-high-quality-ground-truth">The challenge of high-quality ground truth</h3> <p>To assess the performance of LLM-as-a-judge metrics, we need ground-truth scores and reasons. Obtaining reliable scores is far from trivial. For our purposes, we relied on human annotations: experts followed rating guidelines (e.g., “subtract 0.1 to 0.3 for unnecessary verbosity or repetition”) to produce scores with quantitative justifications. In other words, for each input/output pair $(x_i, y_i)$, a human annotator provided a ground-truth score and reason, which we denote as $\tilde{s}_i$ and $\tilde{r}_i$.</p> <p>However, upon careful review, we found that many scores didn’t align with expected ratings. Subjectivity is inherent when evaluating LLM-generated content. To reduce this bias, we had multiple annotators perform ratings independently and retained only those records where they showed strong agreement.</p> <p>Another challenge is sample diversity. LLMs can produce multiple valid responses for a given input, making random train/test splits potentially problematic. To better represent a realistic production scenario, we split the training and test sets based on a temporal cutoff: samples before a certain date were used for training, and those after for testing.</p> <h2 id="who-are-you-to-judge-me">Who are you to judge me?</h2> <p>Now that we have established our evaluation metric and collected ground truth annotations, the key question becomes: how do we calibrate the judge prompt to align its evaluations with human judgments? In this section, we explore different approaches to calibration.</p> <h3 id="our-first-attempt-manual-calibration">Our first attempt: manual calibration</h3> <p>Traditional calibration relies on manually crafted prompts. The workflow depends on human annotation samples and uses techniques such as few-shot prompting to address problematic queries. This process is typically iterative:</p> <ol> <li>Collect ground-truth scores from human annotators</li> <li>Compute alignment between human annotations and LLM-as-a-judge metrics</li> <li>Identify discordant samples where human and model scores diverge</li> <li>Refine the judge prompt to minimize score discrepancy</li> </ol> <p>Engineering teams perform multiple rounds of manual calibration over time to ensure that human annotators and the LLM-as-a-judge metrics converge. Beyond being time-consuming and error-prone, this approach raises several technical concerns: (a) step 4 typically involves adding few-shot examples, which results in extremely long and over-engineered prompts; (b) ensuring that existing functionalities are preserved at each calibration round requires careful selection and analysis of reference samples. This process requires an expert engineer and can take several days. These limitations motivated us to explore more automated approaches.</p> <h3 id="take-your-cheatsheet-out-the-game-begins">Take your cheatsheet out, the game begins</h3> <p>Dynamic Cheatsheet Cumulative (DC-Cu) <d-cite key="suzgun2025dynamiccheatsheet"></d-cite> was introduced in April 2025 by researchers at <i>Stanford University</i> and <i>Together AI</i>. The key idea is to treat prompt optimization as an online learning problem where an LLM processes samples sequentially, updating the cheatsheet after each one.</p> <p>DC-Cu distinguishes between two roles: a <i>Generator</i> LLM and a memory <i>Curator</i> LLM. The Generator takes the input query $x_i$ and the current cheatsheet $M_i$ to produce an output $y_i$:</p> \[y_i = \text{Generator}(x_i, M_i)\] <p>The Curator then evaluates the output and refines the cheatsheet, keeping only the most useful and generalizable strategies:</p> \[M_{i+1} = \text{Curator}(M_i, x_i, y_i)\] <p>Since no ground truth is available, the Curator itself judges response quality. In practice, this self-verification approach is achievable for math questions and puzzles where the correctness of the solution can be verified using tools such as a calculator or code-execution environment.</p> <blockquote> <p><b><i>Note:</i></b> Unlike math questions and puzzles, LLM-as-a-judge evaluations are not self-verifiable. There is no objective tool to verify whether a relevance score is correct. Therefore, we need ground-truth human annotations.</p> </blockquote> <p>We adapted DC-Cu for supervised auto-calibration by providing the Memory Curator with ground truth scores $\tilde{s}_i$ and reasoning $\tilde{r}_i$ alongside the input/output pairs $(x_i, y_i)$ (see Fig. 1). We also modified the prompts to ensure these human annotations guide the cheatsheet generation process.</p> <div class="row mt-4"> <div class="col text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-auto-calibration/2025_12_01_DC.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-auto-calibration/2025_12_01_DC.svg" class="img-fluid d-block mx-auto" width="100%" height="auto" style=" max-width: 90%; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-left"> Fig. 1: Overview of the DC‑Cu method adapted to Judge auto‑calibration. Input–output pairs $(x_i, y_i)$ are retrieved from the database and evaluated by the Judge, which assigns a relevance score and reason $(s_i, r_i)$. The predicted score–reason pair is concatenated with the original input–output pair and the ground‑truth score–reason pair $(\tilde{s}_i, \tilde{r}_i)$, and passed to the Curator, which updates the cheatsheet $M_i$ to align the Judge's scoring behavior with the ground truth. </div> <p>In our adapted DC-Cu framework, the process alternates between two phases. In the <strong>Judgment Phase</strong>, the Judge evaluates the input/output pair using the current cheatsheet $M_i$ to produce a score and reasoning:</p> \[s_i, r_i = \text{Judge}(x_i, y_i, M_i)\] <p>In the <strong>Curation Phase</strong>, the Curator updates the cheatsheet by comparing the Judge’s predictions with the ground truth annotations ($\tilde{s}_i$, $\tilde{r}_i$):</p> \[M_{i+1} = \text{Curator}(M_i, x_i, y_i, \tilde{s}_i, \tilde{r}_i, s_i, r_i)\] <p>The Curator uses the discrepancy between predicted and ground truth values to refine the evaluation criteria stored in the cheatsheet.</p> <h3 id="ace-up-your-sleeve-agentic-context-engineering">ACE Up Your Sleeve: Agentic Context Engineering</h3> <p>Agentic Context Engineering (ACE) <d-cite key="zhang2025agenticcontextengineering"></d-cite> builds upon DC-Cu by further refining its architectural structure. In ACE, the Memory Curator role is decomposed into two specialized components: a <i>Reflector</i>, which synthesizes insights from both correct and erroneous outputs, and a <i>Curator</i>, which integrates these insights into context updates. This separation prevents overburdening a single agent with the dual responsibilities of quality assessment and cheatsheet evolution. Additionally, ACE introduces a grow-and-refine mechanism that implements incremental updates to avoid full cheatsheet rewrites, pruning redundant entries through semantic analysis to ensure the cheatsheet remains both comprehensive and concise.</p> <p>Inspired by ACE, we developed a customized implementation for the LLM-as-a-Judge supervised use case by extending DC-Cu with three key enhancements: (i) incorporating ground-truth scores and reasons during training, (ii) introducing a Reflector LLM, and (iii) enabling batching and epoch-based training. Further details are provided in the following section.</p> <h3 id="any-baci-please">Any BACI, please?</h3> <p>In this section, we introduce BACI (Batching Agentic Context Iteratively), our proposed strategy for automated judge calibration. The overall architecture is illustrated in Fig. 2.</p> <div class="row mt-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-auto-calibration/2025_12_01_BACI.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-auto-calibration/2025_12_01_BACI.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption text-left"> Fig. 2: Overview of the BACI method. A batch $B$ of annotated samples is retrieved from the database, where each sample contains an input–output pair $(x_i, y_i)$ and the corresponding ground‑truth score–reason $(\tilde{s}_i, \tilde{r}_i)$. The Judge uses the cheatsheet rules to evaluate each input–output pair $(x_i, y_i)$ individually, producing a predicted score and reason $(s_i, r_i)$. Each prediction is then concatenated with its input-output pair and ground-truth, and the batch is forwarded to the Reflector. The Reflector compares ground-truth and predicted scores along with their reasons, identifying relationships among samples to extract insights $I_t$ about the Judge's errors. The Curator incorporates these insights into the cheatsheet $M_t$, aligning the Judge's scores with the ground truth. This process repeats for $k$ epochs to progressively optimize the cheatsheet. </div> <p>BACI incorporates <strong>batching</strong> as a core component to <strong>iteratively</strong> optimize the <strong>agentic context</strong>. During training, the Judge individually evaluates each sample $i$ within a batch $B$ using the current cheatsheet $M_t$ (where $t$ denotes the iteration number):</p> \[s_i, r_i = \text{Judge}(x_i, y_i, M_t) \quad \forall i \in B\] <p>The batch is then passed to the Reflector, which extracts insights $I_t$ by comparing the Judge’s predictions with the human-provided ground truth across all samples in the batch:</p> \[I_{t} = \text{Reflector}(\{(x_i, y_i, \tilde{s}_i, \tilde{r}_i, s_i, r_i)\}_{i \in B})\] <p>These insights are fed to the Curator, which updates the cheatsheet accordingly:</p> \[M_{t+1} = \text{Curator}(M_t, I_t)\] <p>In subsequent iterations, the Judge uses the updated cheatsheet to generate new scores and reasons. This iterative process is repeated for all batches, with the cheatsheet being continuously refined at each step. The entire cycle is run for $k$ epochs, like a standard machine learning pipeline but employing gradient-free optimization.</p> <p>At test time, we provide the Judge with the final version of the cheatsheet, refined during training. The Judge uses this cheatsheet to evaluate new, unseen samples by generating scores and reasons based on the accumulated knowledge. The final cheatsheet serves as a distilled summary of the most relevant evaluation patterns learned during training, guiding the Judge’s evaluations in the test phase.</p> <p>Compared to ACE, our Curator is responsible for both adding new evaluation instructions and de-duplicating entries. This design makes our method more lightweight than the original ACE approach, which maintains embeddings for each instruction in the cheatsheet.</p> <p>In BACI, the Reflector extracts insights, and the Curator is instructed to perform updates by adding instructions (i) only if they are sufficiently different from existing ones, (ii) refining entries that lack important aspects, and (iii) discarding items that are similar to those already present. The combination of batching and the Reflector-Curator architecture helps us avoid context collapse and redundancy of instructions. In particular, the batch size plays a crucial role in this process — as we will demonstrate in the following section.</p> <blockquote> <p><b><i>Note:</i></b> we use Claude Sonnet 4.5, which has a nearly unlimited context window (200K base, can be extended up to 1 million tokens) for the LLM Judge, Reflector, and Curator. When using a model with a smaller context window, a trade-off in the batch size might be needed.</p> </blockquote> <h2 id="what-did-we-learn">What did we learn?</h2> <h3 id="starting-with-the-basics-when-less-is-more">Starting with the basics: when less is more</h3> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-auto-calibration/score_comparison_other_methods.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <div class="caption text-left"> Fig. 3: Score comparison on test set for the baseline methods. The score for each sample has been computed as the average across 10 predictions. The standard deviaton for each score can be seen by hovering over the point. </div> <p>We compared the score distributions across our test set for different baselines (see Fig. 3). We first evaluated what happens when the Judge receives no special instructions, using only the basic definition of Answer Relevance, with no training, evolution, or use case specific guidelines. Surprisingly, on average, this “empty cheatsheet” approach performs on par with our manually calibrated prompt. This is remarkable given that an empty cheatsheet contains no domain-specific details.</p> <p>Even more unexpectedly, the DC-Cu method performs worse than the previous baselines. As noted in <d-cite key="zhang2025agenticcontextengineering"></d-cite>, the main issue is context collapse: over time, the LLM Curator tends to generate shorter, less informative summaries, leading to a sharp decline in performance.</p> <p>These observations lead to our first key insight:</p> <blockquote> <p>It is better to provide no instructions in the cheatsheet than to include suboptimal instructions that may cause confusion or conflicting behavior.</p> </blockquote> <p>This also explains why the carefully calibrated prompt did not outperform the baselines on the test set, despite meticulous fine-tuning, adjusted dataset scoring, and multiple few-shot examples.</p> <h3 id="learning-in-batches-the-missing-ingredient">Learning in batches: the missing ingredient</h3> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-auto-calibration/score_comparison_BACI.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <div class="caption text-left"> Fig. 4: Score comparison on test set for BACI with different batch sizes and DC-Cu. The score for each sample has been computed as the average across 10 predictions. The standard deviaton for each score can be seen by hovering over the point. </div> <p>We evaluated two BACI configurations: BACI-1 uses a batch size of one with a single training epoch, while BACI-32 uses a batch size of 32 and trains for five epochs. Figure 4 compares their performance against ground truth and DC-Cu. Here are the key observations:</p> <ul> <li> <p>BACI-1 significantly outperforms DC-Cu. This improvement stems primarily from the separation of concerns between the Curator and Reflector components. As noted in the ACE work, this architectural separation — where insight extraction (Reflector) and cheatsheet updating (Curator) are distinct processes — helps mitigate context collapse across iterations.</p> </li> <li> <p>BACI-32 outperforms BACI-1. The larger batch size and multiple training epochs enable the system to observe the entire training dataset repeatedly, refining the cheatsheet iteratively. Crucially, processing samples in larger batches allows the Reflector to identify more generalizable patterns rather than overfitting individual examples.</p> </li> </ul> <h3 id="the-final-verdict">The final verdict</h3> <p>Bringing it all together: Table 1 summarizes all experimental results, showing average Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) <d-cite key="bishop2006pattern"></d-cite> across 10 test runs. BACI-32 (bold) clearly outperforms all competing methods on our dataset.</p> <div align="center"> <table> <thead> <tr> <th>Method</th> <th>MAE</th> <th>RMSE</th> </tr> </thead> <tbody> <tr> <td>Empty Cheatsheet</td> <td>0.134 ± 0.002</td> <td>0.201 ± 0.004</td> </tr> <tr> <td>Manually calibrated</td> <td>0.139 ± 0.010</td> <td>0.219 ± 0.016</td> </tr> <tr> <td>DC-Cu</td> <td>0.272 ± 0.006</td> <td>0.308 ± 0.007</td> </tr> <tr> <td>BACI-1</td> <td>0.147 ± 0.003</td> <td>0.212 ± 0.002</td> </tr> <tr> <td>BACI-32</td> <td><strong>0.111</strong> ± 0.003</td> <td><strong>0.198</strong> ± 0.004</td> </tr> </tbody> </table> </div> <div class="caption text-left"> Table 1: Experimental results comparing all methods. MAE and RMSE averaged across 10 test runs. </div> <p>Statistical validation using the Wilcoxon signed-rank test <d-cite key="wilcoxon1945individual"></d-cite> confirms: (1) manual calibration provided no significant improvement over an empty cheatsheet ($p = 0.62$); (2) BACI-32 significantly outperforms both BACI-1 and the empty cheatsheet ($p &lt; 0.01$).</p> <h2 id="key-takeaways">Key Takeaways</h2> <p><strong>Context collapse is real.</strong> Our empirical analysis confirms that iterative, sample-by-sample approaches to prompt calibration suffer from a critical issue: context collapse. The solution lies in applying proper machine learning strategies—processing samples in batches and tuning hyperparameters such as batch size and number of epochs.</p> <p><strong>Manual calibration faces similar challenges.</strong> Even manually crafted prompts can suffer from analogous issues, as they’re typically adjusted iteratively on a static set of examples. Moreover, human bandwidth limits the number of samples that can be examined, making the process tedious, error-prone, and nearly impossible to scale.</p> <p><strong>Sometimes, less is more.</strong> Perhaps our most surprising finding: the strong performance of the empty cheatsheet baseline. This serves as a clear warning that wrong guidance can be worse than no guidance at all. When a simple solution works, there’s no need to overcomplicate it.</p> <p><strong>Data quality matters — a lot.</strong> The quality of training data is just as important as the calibration method itself. While this principle applies to all machine learning, it is especially critical for generative AI: we cannot expect an LLM to generate meaningful insights from inconsistent or low-quality input data. In our work, we observed a substantial subjectivity bias in human annotations. Data cleaning was crucial and required time and resources. Despite the impressive capabilities of modern LLMs, human judgment remains indispensable—at least for now, AI cannot fully replace expert reviewers.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[How do you evaluate Large Language Model (LLM)-based systems in production at scale? Most teams turn to an LLM-as-a-judge: an approach that grasps the nuances of natural language where classical metrics fall short. But these judge models have their own “will”: sometimes they follow instructions precisely, sometimes they don't. To address this inconsistency, the judge prompt is calibrated to align with known, trusted cases. The problem? Manual calibration is time-consuming and error-prone. In this blog post, we explore auto-calibration techniques inspired by recent prompt-optimization research. We tackle context collapse by iteratively processing data in batches, similarly to a machine learning training pipeline. Along the way, we share some surprising findings about what works and what doesn't—including cases where simpler approaches outperform more sophisticated ones.]]></summary></entry><entry><title type="html">Beyond the Rerun: Why Reproducibility is Failing Science</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/beyond-the-rerun/" rel="alternate" type="text/html" title="Beyond the Rerun: Why Reproducibility is Failing Science"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/beyond-the-rerun</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/beyond-the-rerun/"><![CDATA[<p>I see other students in my research group struggling and spending weeks trying to replicate experiments that have already been done and for which the code is already available. And I ask myself, what could we do to make it better for them and other researchers? It should be as easy as “just run the experiment program”, right? But even when code is available and runnable, significant friction remains.</p> <h2 id="but-why-does-it-matter-if-someone-can-run-your-research-code-easily">But, why does it matter if someone can run your research code easily?</h2> <p>Reproducibility, the ability to repeat an experiment, given its code and obtain the same results, is vital to scientific construction. Following Popper’s line of thought <d-cite key="KarlPopperStanford"></d-cite>, science depends on hypotheses and statements that can be tested and falsified. Without reproducibility, such testing becomes difficult for critical argumentation, which weakens scientific claims, even if it does not directly invalidate them.</p> <p>Today, there is a “reproducibility crisis” in science <d-cite key="baker1500ScientistsLift2016"></d-cite>. Interestingly, physicists and engineers are among those who say their work is more reproducible, while more than 60% have had problems reproducing others’ experiments. This contradiction highlights a clear reproducibility problem that impacts the quality of the scientific knowledge generated, while its causes may not be as clear. One possible reason for this is over-reliance on computers, which scientists see as precise entities that execute “precise” algorithms, but whose reproducibility ultimately depends on their proper execution of those programs <d-cite key="plesserReproducibilityVsReplicability2018"></d-cite>. From another perspective, the pressure to publish ranks second among the causes of irreproducibility, diminishing the quality of scientific code and its usability.</p> <p>There are initiatives to improve the issue of reproducibility. The “AI4Europe Reproducibility Initiative” <d-cite key="AI4EuropeReproducibilityInitiative"></d-cite> focuses on three different barriers to reproducibility: technical, the documentation and preservation of software and data; cultural, the encouragement and valuation of reproducibility over results and novelty alone; and systemic, the lack of standards and guidelines for reproducibility. Conferences also address the issue, with ICLR incentivizing a reproducibility statement <d-cite key="ICLR2026Author"></d-cite> and NeurIPS <d-cite key="NeurIPS2025Call"></d-cite> including a mandatory reproducibility question in its checklist. There are even awards for studies that best address reproducibility <d-cite key="GENEAWorkshop2025"></d-cite>. NeurIPS also suggests using the Papers With Code guidelines <d-cite key="PaperswithcodeReleasingresearchcode2025"></d-cite>, which include requirements such as a requirements list, Docker images uploaded to Docker Hub, separate code for training and evaluation, pre-trained models, and instructions for reproducing the results. As outlined in this guideline, not only the code but also sharing other artifacts are essential, as one can say, “Where re-running is successful, the published artifacts allow others to build on earlier work” <d-cite key="plesserReproducibilityVsReplicability2018"></d-cite>.</p> <p>However, we argue that reproducibility alone is not enough for science. Science is about questioning and proposing new perspectives and approaches, about advancing the SotA. It’s not enough to simply be able to re-execute and understand a project. It needs to be possible to use it and build upon it, because that’s how science is built. Often, “reproducible” code is a black box that runs in a Docker container, spits out a number, and dies. If I can’t dissect the model to understand <em>why</em> it worked, that is performance validation, not knowledge building. And the current view, focused solely on the reproducibility crisis, fails to see this resulting gap. When we ask our students to replicate an experiment published by other researchers, we want them to understand it and be able to propose changes and new approaches based on it.</p> <p>We have a proliferation of “single-use codes,” and that is not sustainable. Symptoms of the trap of prioritizing productivity over quality and scientific advancement include shortcuts to publishing quickly that, paradoxically, ultimately increase the overall time cost of science. We are creating a <strong>“scientific debt”</strong> that makes research more challenging. We have separated the code for a new method here from the code of the experiment performed to validate it, since the former should be the focus of this problem. Methods should be reusable and extensible, as significant advances in science have not been built upon single-use methods.</p> <h2 id="pillars-for-scientific-computer-science">Pillars for Scientific Computer Science</h2> <p>Building on the “scientific debt” presented, we propose to view the problem from three pillars, <strong>Reproducibility</strong>, <strong>Legibility</strong>, and <strong>Composability</strong>:</p> <table> <thead> <tr> <th><strong>Reproducibility</strong></th> <th><strong>Readability</strong></th> <th><strong>Composability</strong></th> </tr> </thead> <tbody> <tr> <td>The classical reproducibility view, which focuses on the ability to re-run an experiment, which we have discussed so far.</td> <td>The ability to interpret a project, understand how it works, and know how to use it. It’s about making a project transparent, no longer a black box.</td> <td>The possibility of creating something new using existing code. Being able to use SotA methods to advance science further.</td> </tr> </tbody> </table> <p>As mentioned, reproducibility is essential to the validity of science and the foundation of its quality; without it, the other pillars are useless. Readability, on the other hand, advances the field and enables a proper understanding of what is happening; without it, it becomes challenging to use previous results. Finally, composability is what most contributes to advancing science and should be the objective when designing research software.</p> <h2 id="from-scripts-to-building-blocks-a-path-forward">From Scripts to Building Blocks: A Path Forward</h2> <p>Let’s look at practical actions we can take to achieve the pillars. We will focus on scientific code written in Python, as it is the most widely used language in science today. We will include links to tutorials, tools, and specifications that may be useful.</p> <p>These are some recommendations that you, as a researcher, can use. But even though well-intentioned researchers can make incredible things; this is a problem we also need to address collectively, as we will discuss later.</p> <ol> <li> <p><strong>Software design</strong></p> <p>We need software with designs considering the division of concerns. Currently, it’s very common to see programs that mix the code of the proposed method (method code) with the code of the experiment that validates the method and generates metrics and results (experiment code).</p> <p>Ideally, these codes should be decoupled, allowing the method to be reused in future research. The method code should also, whenever possible, be written using well-known, widely used frameworks, further increasing compatibility. Think about how easy it is to use a neural network layer that is already written in a ready-to-use PyTorch class. The experiment code should contain not only the code to run the experiment, but also its configurations and the code responsible for processing datasets. Furthermore, it should include clear execution entry points, avoiding the need to run different scripts in different folders to perform a single task, such as training or evaluating a model.</p> </li> <li> <p><strong>Code standards</strong></p> <p>A consistent coding style throughout the project is important for ease of understanding. Writing variable, function, and class names in a way that clearly explains what they do is a good starting point.</p> <p>Some practices make code significantly easier to understand—for example, avoiding very long functions or files, keeping imports at the top of each file, and reducing the use of large configuration dictionaries passed through the code.</p> <p>There are tools, like AutoPEP8 <d-cite key="Autopep8ToolThat"></d-cite><d-cite key="Autopep8VisualStudio"></d-cite>, that automatically formats code using the PEP 8 (official “Style Guide for Python Code”) guidelines <d-cite key="guidovanrossumPEP8Style"></d-cite>, and SonarLint <d-cite key="SonarQubeIDEVisual"></d-cite>, which can help maintain a consistent, appropriate style.</p> </li> <li> <p><strong>Documentation</strong></p> <p>In addition to following good coding standards, it’s important to write clear documentation alongside the code that explains what each section of code does. This can include comments in more complex areas and, at least, function docstrings. Type annotations <d-cite key="TypeHintsPython21"></d-cite><d-cite key="TypingSupportType"></d-cite> are also part of this group; in addition to being used by functions for auto-completion of parts of docstrings, they also facilitate code usage by allowing autocomplete and visualization of the appropriate documentation while writing new code.</p> </li> <li> <p><strong>Version control</strong></p> <p>Versioning your project helps future researchers find the specific version associated with a published result. It also facilitates collaborative development among different researchers and makes explicit the changes made over time.</p> <p>Version control should be applied to the codebase, but you can also use it for other artifacts, such as pre-trained models, Docker images, and datasets. Tools like git/GitHub for code, and Zenodo for other artifacts, make it easier to make artifacts available through versions.</p> </li> <li> <p><strong>Publishing and distribution</strong></p> <p>Publishing your research software on a standard, user-friendly channel helps other researchers utilize your contribution in future research. In Python, this usually means packaging your method and distributing it using PyPI (Python Package Index) <d-cite key="PythonPackageIndex"></d-cite><d-cite key="PackagingPythonProjects"></d-cite>.</p> <p>Avoid expecting other researchers to copy your script into their projects. Besides being less straightforward, it doesn’t consider potential future versions and complicates issues like licensing. Imagine including the license for each library you used in your research in your repository.</p> </li> <li> <p><strong>Dependencies specification</strong></p> <p>Specifying all the dependencies of your project is a first step in enabling a third party to reproduce your results or use your method.</p> <p>When distributing your method, avoid specifying fixed versions, as this will make it difficult to use with other packages. Specify the minimum versions your project supports. And use standard ways to specify dependencies, such as your project’s pyproject.toml file <d-cite key="WritingYourPyprojecttoml"></d-cite>.</p> </li> <li> <p><strong>Hardware specification</strong></p> <p>Specify what hardware you used to run your experiment, as well as the minimum requirements your method may need. This helps other researchers prepare environments to reproduce your results and further develop them.</p> </li> <li> <p><strong>Containerization</strong></p> <p>Docker <d-cite key="WhatContainer0200"></d-cite>, like other container tools, allows other researchers to use the environment you used to conduct your experiments directly. This makes it much easier to reproduce your results and use your method, since all dependencies will be identical to yours. In addition to making the dockerfile you created for your image available, also consider making your built image available on Docker Hub <d-cite key="DockerHubContainer"></d-cite> or Zenodo <d-cite key="PauleveDonodoBridging"></d-cite>.</p> </li> <li> <p><strong>Tutorials</strong></p> <p>Write tutorials about your method. Besides providing a quick, straightforward introduction that can serve as a first overview and gateway to more in-depth publications, it helps others get started using your method. Jupyter Notebook <d-cite key="grangerJupyterThinkingStorytelling2021"></d-cite><d-cite key="ProjectJupyter"></d-cite> is a suitable format for tutorials.</p> </li> <li> <p><strong>Licensing</strong></p> <p>It’s pointless to make your project available if others can’t use it due to legal issues. Licensing is an integral part of allowing your method to be explored by other researchers. Several licenses, with different properties, are available. Please choose the one that best suits your project and use it correctly <d-cite key="ChooseOpenSource"></d-cite>.</p> <p>Remember that making code available without any license means being stuck in a limbo between “nobody can use it” and the contradiction of “I made it available on a platform like GitHub”. And there are specific licenses for each type of artifact. In particular, Creative Commons <d-cite key="CreativeCommons"></d-cite> licenses are not appropriate for code <d-cite key="CreativeCommonsFrequentlyAskedQuestions"></d-cite>.</p> </li> <li> <p><strong>Persistence and traceability</strong></p> <p>Make your code and artifacts available in persistent repositories. Zenodo is a good example that automatically stores releases from GitHub repositories <d-cite key="ZenodoGitHubSoftware"></d-cite>.</p> <p>Also consider how these repositories can be found in the future. Referencing the DOI of these artifacts can facilitate their traceability in the future. For code on GitHub, consider making CFF files available with the project’s DOI <d-cite key="CITATIONFiles"></d-cite>.</p> </li> </ol> <p>Let’s see how each indication relates to the pillars. Notice how reproducibility and composability are often closely related in practice:</p> <pre><code class="language-mermaid">%%{init: { "gantt": { "displayMode": "compact", "leftPadding":200 } }}%%
gantt
    title     ​ 
    dateFormat  X
    axisFormat  
    
    section ​
	    Reproducibility :active, rerun, 1, 2
	    Readability :active, read, 2, 3
	    Composability :active, comp, 3, 4

    section 1. Software design
	    ​  : a1, 3,4
	    
	section 2. Code standards
	    ​  : a2, 2,3
	    
	section 3. Documentation
	    ​  : a3, 2,3
	    
	section 4. Version control
	    ​  : a4, 1,2
		​  : a4-2, 3,4
		    
	section 5. Publishing and distribution
	    ​  : a5, 1,2
	    ​  : a5-2, 3,4    
	    
	section 6. Dependencies specification
	    ​  : a6, 1,2
		​  : a6-2, 3,4	

	section 7. Hardware specification
	    ​  : a7, 1,2
		​  : a7-2, 3,4	    
	    
	section 8. Containerization
	    ​  : a8, 1,2
	    ​  : a8-2, 3,4
	    
	section 9. Tutorials
		​  : a9, 3,4

	section 10. Licensing
		​  : a10, 3,4

	section 11. Persistence and traceability
	    ​  : a11, 1,2
		​  : a11-2, 3,4	   
</code></pre> <h2 id="last-remarks">Last Remarks</h2> <p>Many of the recommendations mentioned resemble software engineering processes. This is because, let’s not forget, scientific research software is still… software. Ensuring its quality also involves elements similar to those in any other software.</p> <p>But even given the limits of reproducibility, it’s pointless to raise our quality standards for computer science research if we can’t address this scientific debt crisis. Cultural barriers, where a lack of incentives and increasing pressure on results and publications are significant reasons why we can’t solve these problems <d-cite key="AI4EuropeReproducibilityInitiative"></d-cite>. This is where major conferences, such as the ICLR, journals, and funding agencies can step in by highlighting the importance of a sustainable scientific ecosystem that enables efficient future research and by explicitly demanding actions to achieve this goal.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Is reproducibility enough? We discuss the current reproducibility crisis and the limitations that focusing solely on this aspect of scientific project quality imposes on science. We propose a broader approach to the problem of scientific debt and outline practical actions researchers can take in their research. We also draw attention to the need for community action on the issue.]]></summary></entry><entry><title type="html">The 99% Success Paradox: When Near-Perfect Retrieval Equals Random Selection</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/bits-over-random/" rel="alternate" type="text/html" title="The 99% Success Paradox: When Near-Perfect Retrieval Equals Random Selection"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/bits-over-random</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/bits-over-random/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>For most of the history of information retrieval (IR), search results were designed for human consumers who could scan, filter, and discard irrelevant content on their own. This shaped retrieval systems to optimize for finding and ranking more relevant documents, but not for keeping results clean and minimal, as the human was the final filter.</p> <p><strong>Retrieval-augmented generation (RAG)</strong> and tool-using agents flip these assumptions. Now the consumer is often an LLM, not a person, and the model does not skim. In practice, introducing excessive or irrelevant context into the input can dilute the model’s ability to identify and focus on the most critical information. When you pass retrieved documents to an LLM:</p> <ul> <li>It can’t ignore irrelevant results. Every irrelevant chunk dilutes the model’s attention.</li> <li>Noise has a cost. Extra chunks cost tokens, latency, and computation. They also increase the odds that irrelevant or misleading content pulls attention away from what actually matters.</li> </ul> <h2 id="the-million-token-trap">The Million-Token Trap</h2> <p>You might be thinking: <em>“But modern LLMs have million-token context windows. Why care?”</em></p> <p>The real question isn’t whether a model can fit more context, but whether more context is actually helpful. Beyond a certain point, adding retrieved material (and the accompanying noise) can actively increase computational cost and degrade the quality of output.</p> <p>In our 20 Newsgroups classification case study, we increased the retrieval depth <strong>K</strong> from 10 to 100 items. This caused LLM accuracy to drop from 66% to 50%, even though the success metric (<strong>Success@K:</strong> the percentage of queries returning at least one relevant item) remained close to 100%. In other words, more retrieved content led to worse results, not better.</p> <p>This problem is especially severe for agentic systems that use tool-based retrieval, because context quality directly affects downstream decisions. A chatbot might give you a mediocre answer, however, an autonomous agent might call the wrong API, delete the wrong file, or execute the wrong command.</p> <p>We need a measure that asks: <em>“Given that I’m retrieving K items and my LLM will consume all of them, how much <strong>selective signal</strong> am I actually getting?”</em></p> <p>That’s what Bits-over-Random (BoR) measures. The rest of this post explains how.</p> <h2 id="what-traditional-metrics-miss">What Traditional Metrics Miss</h2> <p>Recall rewards finding more relevant documents, but is blind to how many irrelevant items you had to pull into the context window to get them. Over-retrieval is actually rewarded. As Manning et al.<d-cite key="manning2008introduction"></d-cite> note, “recall is a non-decreasing function of the number of documents retrieved”. Yet the choice of retrieval depth K is often an empirical, application-dependent choice <d-cite key="webber2010similarity"></d-cite>.</p> <p>Precision measures the relevance of retrieved results and helps limit excessive retrieval. However, it fails to account for the inherent difficulty of the retrieval task. For instance, achieving a 10% precision means something different if the corpus contains 10 relevant items out of 100 versus 10 relevant items out of 10,000. Same precision, very different selectivity.</p> <p>Ranking metrics (nDCG, RBP, MAP, ERR) penalize burying relevant items, but they do not penalize the presence of irrelevant items when the relevant item is also ranked highly. If you retrieve 100 items and the relevant one is at rank 1, nDCG can be perfect. Yet, RAG systems typically concatenate the top-K results into a single prompt. The LLM still has to read the other 99 items. Rankers optimize ordering, not volume. They don’t reduce the token cost of stuffing <strong>K</strong> documents into the context.</p> <p>In practice, teams end up juggling recall, precision, and ranking metrics. Each captures a different slice of behavior but none reflects the whole picture. There is no single framework that simultaneously accounts for how many items you retrieve, how big the corpus is, and how many items in the corpus are actually relevant to the query.</p> <h2 id="the-librarian-problem">The Librarian Problem</h2> <p>Consider a library of \(N = 1{,}000\) books, with \(R_q = 10\) books relevant to your query. Two librarians respond:</p> <ul> <li><strong>Librarian A</strong> retrieves \(K = 20\) books, 6 of which are relevant (precision 30%, recall 60%, F1 40%).</li> <li><strong>Librarian B</strong> retrieves \(K = 12\) books, 4 of which are relevant (precision 33%, recall 40%, F1 36%).</li> </ul> <p>Traditional IR metrics tend to favor Librarian A (higher recall and F1, similar precision). But Librarian A handed you 14 irrelevant books, versus B’s 8. If the librarians are retrievers or tools in an agent workflow and the consumer is an LLM, it must read everything it was given. Those 6 extra unhelpful books retrieved by Librarian A over Librarian B cost tokens, add noise, and waste computational resources.</p> <h2 id="the-new-baseline-random-chance">The New Baseline: Random Chance</h2> <p>And here’s the deeper question: <em>Beyond comparing A and B, is either of them an objectively skillful librarian? What is the baseline?</em></p> <p>If we compare each librarian to a random baseline (<em>“what if I picked K books uniformly at random?”</em>), we can ask which one is actually more selective than chance. Plugging these numbers into the chance-corrected formulas we introduce below shows that Librarian B is more selective than A. For an LLM consuming a fixed-size bundle of text, that selectivity per token is what matters.</p> <p>This is the key insight: every retrieval problem has a built-in baseline. If you picked <strong>K</strong> items completely at random, you’d still sometimes get lucky and grab something relevant, especially if relevant items are common.</p> <p>That random success rate is your floor. It tells you how much of your <em>“success”</em> is just dumb luck. Bits-over-Random (BoR) measures how far above random success you’ve climbed.</p> <p>In today’s RAG, agentic, and LLM workflows, we care less about who retrieved the most documents and more about who delivered the most signal with the least noise. By comparing a chosen success metric to random chance, BoR measures true selectivity: how much better is our retrieval bundle than random selection?</p> <p>Let’s break down how it works, step by step.</p> <h2 id="the-math">The Math</h2> <p>Evaluating a retriever shouldn’t require juggling incompatible metrics. To make sense of how well a system is actually performing, we need a baseline. Not just any baseline, but the most honest one possible: pure randomness. The framework below walks through a simple, quantitative way to express <em>“how much better than random”</em> your retrieval system really is.</p> <p>By measuring observed success, computing the expected success of random guessing, and comparing the two on a logarithmic scale, we end up with a clean, intuitive metric: <strong>Bits-over-Random (BoR)</strong>. This gives retrieval performance a natural, information-theoretic interpretation, each bit representing one doubling in effectiveness over chance.</p> <h2 id="the-quick-reference-version">The Quick-Reference Version</h2> <p>Here’s everything you need to remember:</p> <table> <thead> <tr> <th style="text-align: left">Symbol</th> <th style="text-align: left">Meaning</th> <th style="text-align: left">Example</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">\(N\)</td> <td style="text-align: left">Total items in corpus. Unit must be defined (e.g., documents, passages)</td> <td style="text-align: left">10,000 passages or 700 documents</td> </tr> <tr> <td style="text-align: left">\(K\)</td> <td style="text-align: left">How many items you retrieve per query (top-K)</td> <td style="text-align: left">\(K=10\) or \(K=100\)</td> </tr> <tr> <td style="text-align: left">\(R_q\)</td> <td style="text-align: left">Relevant items in the corpus for a certain query q</td> <td style="text-align: left">\(R_q=1\) (sparse) or \(R_q=20\) (many)</td> </tr> <tr> <td style="text-align: left">\(\bar{R}_q\)</td> <td style="text-align: left">Average relevant items in the corpus per query</td> <td style="text-align: left">≈1.1 on SciFact, ≈572 on 20 Newsgroups</td> </tr> <tr> <td style="text-align: left">\(P_{obs}(K)\)</td> <td style="text-align: left">Your observed success rate at K (Note: any success rate can be used here.)</td> <td style="text-align: left">60% of queries succeed</td> </tr> <tr> <td style="text-align: left">\(P_{rand}(K)\)</td> <td style="text-align: left">Random-chance success at K</td> <td style="text-align: left">What luck would give you</td> </tr> <tr> <td style="text-align: left">\(\lambda\)</td> <td style="text-align: left">Heuristic: expected random hits = \(K \cdot \bar{R}_q / N\)</td> <td style="text-align: left">\(\lambda\) in the 3–5 range signals collapse</td> </tr> </tbody> </table> <h3 id="step-1-measure-your-success-rate">Step 1. Measure Your Success Rate</h3> <p>First, pick a success condition. For most RAG systems, the natural rule is: <em>“Did I get at least one relevant item in my top-K results?”</em></p> <p>This is called <strong>Success@K</strong> (or coverage). For a batch of queries:</p> \[P_{\text{obs}}(K) = \frac{\text{number of queries with } \geq \text{ 1 relevant result in top-}K}{\text{total queries}}\] <p><strong>Note:</strong> The threshold doesn’t have to be 1. You can require at least m relevant documents if your system needs multiple pieces of evidence, for example, “at least 3 supporting passages.”</p> <p>If you retrieved K=10 items for 100 queries, and 60 queries got at least one relevant hit, then \(P_{\text{obs}}(10) = 60 / 100 = 0.60\).</p> <h3 id="step-2-calculate-the-random-baseline">Step 2. Calculate the Random Baseline</h3> <p>What if you picked <strong>K</strong> items completely at random? That’s your baseline.</p> <p>For a query where \(R_q\) items in the corpus are relevant, and the corpus has <strong>N</strong> total items, the hypergeometric distribution tells you the probability of randomly hitting at least one relevant item when picking <strong>K</strong> items:</p> <p>The probability of picking no relevant items in <strong>K</strong> picks is:</p> \[P_{\text{none}} = \frac{\binom{N-R_q}{K}}{\binom{N}{K}}\] <p>So, the probability of picking at least one relevant item is:</p> \[P_{\text{rand}} = 1 - P_{\text{none}} = 1 - \frac{\binom{N-R_q}{K}}{\binom{N}{K}}\] <p><strong>Special case:</strong> If every query has exactly one relevant item (\(R_q = 1\)), this simplifies to:</p> \[P_{\text{rand}}(K) = \frac{K}{N}\] <p>For example:</p> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>\(N\)</td> <td>10,000</td> </tr> <tr> <td>\(R_q\)</td> <td>10</td> </tr> <tr> <td>\(K\)</td> <td>20</td> </tr> </tbody> </table> \[P_{\text{rand}} = 1 - \frac{\binom{9990}{20}}{\binom{10000}{20}} \approx 0.02\] <p>This means random selection works <strong>~2%</strong> of the time.</p> <p>Because we evaluate over many queries, we average these random baselines:</p> \[\overline{P}_{\text{rand}}(K) = \text{average random success across all queries}\] \[\overline{P}_{\text{rand}}(K) = \frac{1}{|Q|} \sum\nolimits_{q} P_{\text{rand}}(K; R_q)\] <h3 id="step-3-enrichment-factor-how-many-times-are-we-better-than-random-chance">Step 3: Enrichment factor: how many times are we better than random chance?</h3> <p><strong>Enrichment Factor (EF)</strong> is defined as</p> \[\text{EF} = \frac{P_{\text{obs}}}{P_{\text{rand}}}\] <p>For a batch of queries, we use the averaged random baseline:</p> \[\text{EF}(K) = \frac{P_{\text{obs}}(K)}{\overline{P}_{\text{rand}}(K)}\] <p>An EF of 5 means you succeed 5× more often than random selection. An EF of 100 means you are 100× better. This formulation is consistent with enrichment metrics used in drug discovery screening <d-cite key="truchon2007evaluating"></d-cite>.</p> <h3 id="step-4-bits-over-random-bor-log-scale-conversion-of-ef">Step 4: Bits-over-Random (BoR): Log Scale conversion of EF</h3> \[\text{BoR} = \log_2(\text{EF}) = \log_2\left(\frac{P_{\text{obs}}}{P_{\text{rand}}}\right)\] <p>And similarly for averaging:</p> \[\text{BoR}(K) = \log_2(\text{EF}) = \log_2\left(\frac{P_{\text{obs}}(K)}{\overline{P}_{\text{rand}}(K)}\right)\] <p>Why \(\log_2\)? Bits are how information theory counts halvings, the same reason why binary search uses powers of 2. Each bit represents one halving of the search space. <strong>BoR = 10</strong> means <strong>10 halvings → 1,024× reduction</strong>.</p> <ul> <li><strong>BoR = 0</strong> → You’re no better than random</li> <li><strong>BoR = 1</strong> → <strong>2×</strong> better than random</li> <li><strong>BoR = 3</strong> → <strong>8×</strong> better than random</li> <li><strong>BoR = 10</strong> → <strong>1,024×</strong> better than random</li> </ul> <p>Each bit also represents a doubling of selectivity. Our definition follows.</p> <p><strong>Selectivity (n.):</strong> The ability of a retrieval system to surface relevant items while excluding irrelevant ones, measured relative to random chance. A system with high selectivity finds needles without bringing along the haystack.</p> <h2 id="a-concrete-example">A Concrete Example</h2> <p>Let’s assume you have 10,000 documents. Each query has exactly ten relevant documents (\(R_q = 1\)). <strong>Note:</strong> Many standard benchmarks such as MS MARCO have \(R_q ≈ 1\) on average, even sparser than this example.</p> <p>You are testing two different retriever systems against the same dataset:</p> <table> <thead> <tr> <th>Metric</th> <th>System A (K=20, 60% success)</th> <th>System B (K=100, 70% success)</th> </tr> </thead> <tbody> <tr> <td>P_obs</td> <td>0.60</td> <td>0.70</td> </tr> <tr> <td>P_rand</td> <td>0.01983</td> <td>0.09566</td> </tr> <tr> <td>EF (Enrichment Factor)</td> <td>0.60/0.01983 = 30.257</td> <td>0.70/0.09566 = 7.318</td> </tr> <tr> <td>BoR</td> <td>4.92 bits</td> <td>2.87 bits</td> </tr> </tbody> </table> <p><strong>System B</strong> has a higher raw success rate (70% vs. 60%) but a BoR score about 2 bits lower than <strong>System A</strong>. This lower score shows <strong>System B</strong> is less selective. It achieves higher coverage by expanding the retrieved set, which reduces informational efficiency. From an information-theoretic view, System B creates a larger <em>“haystack”</em> that delivers fewer useful bits of discrimination per query.</p> <h2 id="the-ceiling-problem">The Ceiling Problem</h2> <p>There’s a maximum BoR you can possibly achieve. If your system is perfect, achieving \(P_{\text{obs}}(K) = 1.0\) (every single query succeeds), the best you can do is:</p> \[\text{BoR}_{\text{max}}(K) = -\log_2(\overline{P}_{\text{rand}}(K))\] <p>This ceiling is determined entirely by the random baseline. Using our toy example:</p> <ul> <li><strong>System A:</strong> \(\text{BoR}_{\text{max}} = -\log_2(0.01983) = 5.66\) bits</li> <li><strong>System B:</strong> \(\text{BoR}_{\text{max}} = -\log_2(0.09566) = 3.39\) bits</li> </ul> <p>System A, even at 60% success, achieves 4.92 bits, already higher than System B’s ceiling. No amount of model improvement can help System B catch up. Given its success rate, it chose a retrieval depth K that limits its maximum possible selectivity.</p> <p><strong>When the random baseline is already high, even perfection gets you almost nothing.</strong></p> <h2 id="bor-optimistic-upper-bound">BoR optimistic upper bound</h2> <p>When you don’t know how many relevant items \(R_q\) exist in the corpus for each query, BoR enables you to define an optimistic upper bound by assuming each query has exactly one relevant item. In that case:</p> \[P_{\text{rand}}(K) \approx \frac{K}{N}\] <p>And:</p> \[\text{BoR}_{\text{opt}}(K) = \log_2\left(\frac{N}{K}\right)\] <p>It’s useful to compute the upper bound if calculating exact BoR is not feasible. \(\text{BoR}_{\text{opt}}(K)\) is an optimistic ceiling: no system on that corpus at depth <strong>K</strong> can have more than about \(\log_2(N / K)\) bits of selectivity under this assumption.</p> <p>Note that \(\text{BoR}_{\text{max}}\) uses actual \(R_q\) values while \(\text{BoR}_{\text{opt}}\) assumes \(R_q = 1\) throughout.</p> <h2 id="the-collapse-zone">The Collapse Zone</h2> <iframe src="/2026/assets/html/2026-04-27-bits-over-random/calculator.html" frameborder="0" scrolling="no" height="580px" width="100%" class="l-body rounded z-depth-1"></iframe> <p>Consider what happens when retrieval becomes <strong>“too easy”</strong>:</p> <ul> <li>If \(P_{\text{rand}} = 0.95\) (random selection succeeds 95% of the time), then even a perfect system only gets \(\text{BoR}_{\text{max}} \approx 0.07\) bits</li> <li>If \(P_{\text{rand}} = 0.99\) (random succeeds 99% of the time), then \(\text{BoR}_{\text{max}} \approx 0.01\) bits</li> </ul> <p>We call this the <em>“collapse zone.”</em> When you enter it, selectivity becomes mathematically impossible, even if your success rate looks great.</p> <p>The boundary is determined by:</p> \[\lambda = \frac{K \cdot \bar{R}_q}{N}\] <p>Where \(\bar{R}_q\) is the average number of relevant items per query.</p> <p>When \(\lambda\) reaches 3–5, you’ve entered the collapse zone. Random selection is already solving most queries, so even a perfect system can’t demonstrate meaningful skill.</p> <h2 id="what-happens-when-you-retrieve-more">What Happens When You Retrieve More?</h2> <p>Now that we have formulated a measure that evaluates an IR system with respect to random selection at a given K, what happens when you increase K (K₁ to K₂)? Typically, we expect the following:</p> <ol> <li>Your success rate improves (usually)</li> <li>Random selection also gets easier (always)</li> </ol> <p>The change in BoR is:</p> \[\Delta\text{BoR} = \log_2\left(\frac{P_2}{P_1}\right) - \log_2\left(\frac{\overline{P}_{\text{rand}}(K_2)}{\overline{P}_{\text{rand}}(K_1)}\right)\] <p>Translation:</p> <ul> <li><strong>First term:</strong> <em>“How much better did I actually do?”</em></li> <li><strong>Second term:</strong> <em>“How much easier did the task get for random guessing?”</em></li> </ul> <h2 id="the-doubling-rule">The Doubling Rule</h2> <p>In typical sparse-relevance scenarios (\(R_q \ll N\) and \(K \ll N\)), the hypergeometric baseline behaves like repeated independent draws. For small values of \(K \cdot R_q / N\), we can use standard approximations \((1 - x)^n \approx e^{-nx}\) and \(e^{-y} \approx 1 - y\) for \(y \to 0\).</p> <p>So, because: \(P_{\text{rand}}(K; R_q) \approx \frac{K \cdot R_q}{N}\) and averaging over queries yields \(\overline{P}_{\text{rand}}(K) \approx \frac{K \cdot \bar{R}_q}{N}\)</p> <p>We now have:</p> \[\Delta\text{BoR} \approx \log_2\left(\frac{P_2}{P_1}\right) - \log_2\left(\frac{K_2}{K_1}\right)\] <p>What does this mean in practice?</p> <p><strong>If you double K, but your success rate doesn’t improve, you lose about 1 bit of selectivity.</strong></p> <p>When you hear <strong>“just retrieve more,”</strong> remember: it’s not free. Once your success rate has plateaued:</p> <ul> <li>Double <strong>K</strong> and you lose \(\sim 1\) bit of selectivity</li> <li>\(10\times\) <strong>K</strong> and you lose \(\sim 3.3\) bits of selectivity</li> </ul> <p>To maintain selectivity when doubling <strong>K</strong>, you’d need \(P_{\text{obs}}\) to also double. But since \(P_{\text{obs}} \leq 1\), this becomes impossible once you’re above 50% success.</p> <p><strong>That’s why BoR inevitably degrades at larger depths once your success curve flattens.</strong></p> <h3 id="extensions-to-stricter-rules">Extensions to Stricter Rules</h3> <p>The BoR framework extends to stricter success rules. For example, requiring at least <strong>m</strong> relevant documents in the top-K:</p> \[\Delta\text{BoR} \approx -m \cdot \log_2\left(\frac{K_2}{K_1}\right)\] <p>Doubling K costs about <strong>m bits</strong> of selectivity. We focus on \(m=1\) in this post because it matches common single-evidence RAG scenarios.</p> <h2 id="case-studies-when-theory-meets-reality">Case Studies: When Theory Meets Reality</h2> <p>Let’s see how BoR behaves in the wild. We tested three different scenarios:</p> <table> <thead> <tr> <th style="text-align: left">Dataset</th> <th style="text-align: left">Corpus Size</th> <th style="text-align: left">Relevant Items per Query</th> <th style="text-align: left">Why Test It?</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>BEIR SciFact</strong></td> <td style="text-align: left">5,183 abstracts (1,409 queries/claims)</td> <td style="text-align: left">Sparse (\(R_q \approx 1\)–2)</td> <td style="text-align: left">Baseline: typical RAG scenario</td> </tr> <tr> <td style="text-align: left"><strong>MS MARCO</strong></td> <td style="text-align: left">~8.8M passages</td> <td style="text-align: left">Sparse (\(R_q \approx 1\))</td> <td style="text-align: left">Large scale: does BoR work at production size?</td> </tr> <tr> <td style="text-align: left"><strong>20 Newsgroups</strong></td> <td style="text-align: left">11,314 docs (training set) class-based setup</td> <td style="text-align: left">Dense (\(\bar{R}_q \approx 572\))</td> <td style="text-align: left">Stress test: what happens when selectivity collapses?</td> </tr> </tbody> </table> <p>We tested two retrievers representing different eras and approaches.</p> <ul> <li><strong>BM25:</strong> The classic lexical baseline</li> <li><strong>SPLADE:</strong> Modern neural sparse retriever (<a href="https://huggingface.co/naver/splade-cocondenser-ensembledistil">naver/splade-cocondenser-ensembledistil</a>): document top-k = 60, query top-k = 60, max sequence length = 256, batch size = 64 for documents and queries</li> </ul> <p>All results use exact hypergeometric baselines and 95% confidence intervals from bootstrap resampling (n=5,000, seed=7).</p> <h3 id="test-1-scifact-the-benchmark-case">Test 1: SciFact (The Benchmark Case)</h3> <p>This is what most people expect: sparse relevance, the kind you see in real RAG systems.</p> <p><strong>The results:</strong></p> <p>Both systems maintain strong selectivity even at <strong>K=100</strong>, with BoR staying above 5 bits. Predicted ΔBoR values match observed changes to within <strong>0.01</strong> bits across all configurations.</p> <p>This confirms that when \(\lambda = \frac{K \cdot \bar{R}_q}{N} \ll 1\) (well outside the collapse zone), retrieval systems can demonstrate meaningful selectivity over random chance.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-bits-over-random/bor_analysis_scifact-480.webp 480w,/2026/assets/img/2026-04-27-bits-over-random/bor_analysis_scifact-800.webp 800w,/2026/assets/img/2026-04-27-bits-over-random/bor_analysis_scifact-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-bits-over-random/bor_analysis_scifact.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Figure 1:</strong> <em>BoR analysis on the SciFact dataset shows sustained selectivity across retrieval depths. Both BM25 and SPLADE maintain high BoR values (5–11 bits), reflecting the dataset’s sparse relevance structure.</em></p> <p>But both BM25 and SPLADE operate very close to the theoretical ceiling. A 30-year-old algorithm nearly matches the modern neural system.</p> <p>Is SciFact just too easy? To investigate, we turn to literature and examine a much larger benchmark. On a corpus with millions of passages, how much headroom exists between top-performing systems and the theoretical ceiling?</p> <h3 id="test-2-ms-marco-the-industrial-scale-test">Test 2: MS MARCO (The Industrial Scale Test)</h3> <p>8.84 million passages. This is where large real-world systems operate.</p> <p>We computed BoR for <strong>41 different systems</strong> from the literature, from lexical baselines to state-of-the-art neural retrievers.</p> <p>At <strong>K=1000</strong>, the theoretical ceiling is:</p> \[\text{BoR}_{\text{opt}} \approx \log_2\left(\frac{8.84\text{M}}{1000}\right) \approx 13.11 \text{ bits}\] <p><strong>All 41 systems cluster within 0.2 bits of this ceiling.</strong> Indicatively, to show the range:</p> <table> <thead> <tr> <th style="text-align: left">System</th> <th style="text-align: left">Recall@1000</th> <th style="text-align: left">BoR (bits)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">BM25</td> <td style="text-align: left">85.7%</td> <td style="text-align: left">12.89</td> </tr> <tr> <td style="text-align: left">SPLADE</td> <td style="text-align: left">97.9%</td> <td style="text-align: left">13.08</td> </tr> <tr> <td style="text-align: left">ColBERTv2</td> <td style="text-align: left">98.5%</td> <td style="text-align: left">13.09</td> </tr> <tr> <td style="text-align: left">SimLM</td> <td style="text-align: left">98.7%</td> <td style="text-align: left">13.09</td> </tr> </tbody> </table> <p>BM25 gets 85.7% recall. SimLM (state-of-the-art) gets 98.7% recall. That’s a <strong>13-point recall gap.</strong></p> <p>But the BoR difference? <strong>Only 0.20 bits.</strong></p> <p>A three-decade-old lexical algorithm and cutting-edge neural systems are very close in chance-corrected selectivity (BoR) at this depth, for this dataset, and success rule (in this case, recall). This suggests diminishing returns from retriever improvements alone.</p> <p>Systems examined include: SimLM, AR2, uniCOIL, ColBERTv2, SPLADE (multiple versions), I3 Retriever, TCT-ColBERTv2, RoDR w/ ANCE, DPR-CLS, ColBERTer, ANCE, SLIM/SLIM++, and BM25.</p> <p>But both still show meaningful selectivity: BoR is above 12 bits. To really see what collapse looks like, we need an extreme test: a dataset where relevance is abundant, not rare.</p> <h3 id="test-3-20-newsgroups-the-stress-test">Test 3: 20 Newsgroups (The Stress Test)</h3> <p>The 20 Newsgroups dataset has 20 topical categories. We set up an extreme scenario: treat all documents in the same category as “relevant.”</p> <p>With <strong>11,314</strong> documents split across <strong>20</strong> classes, that’s about \(\bar{R}_q \approx 572\) relevant documents per query (over <strong>5%</strong> of the corpus).</p> <p>Why test something so unrealistic? Because, as you’ll see later, this can happen in LLM agent tool selection.</p> <p>This scenario pushes us directly into the collapse zone. At <strong>K = 100</strong>:</p> \[\lambda = \frac{K \cdot \bar{R}_q}{N} = \frac{100 \times 572}{11{,}314} \approx 5.1\] <p>Random selection alone would succeed ~99% of the time. The ceiling for any retrieval system is essentially zero. To make the contrast as clear as possible, here is 20NG vs SciFact against both systems.</p> <p><strong>Watch what happens:</strong></p> <table> <thead> <tr> <th style="text-align: left">Dataset</th> <th style="text-align: left">K</th> <th style="text-align: left">BoR Ceiling</th> <th style="text-align: left">BM25 Success</th> <th style="text-align: left">BM25 BoR</th> <th style="text-align: left">SPLADE Success</th> <th style="text-align: left">SPLADE BoR</th> <th style="text-align: left">ΔBoR (10→100)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>20NG</strong></td> <td style="text-align: left">10</td> <td style="text-align: left">1.31 bits</td> <td style="text-align: left">94%</td> <td style="text-align: left">1.22</td> <td style="text-align: left">95%</td> <td style="text-align: left">1.23</td> <td style="text-align: left">−1.22</td> </tr> <tr> <td style="text-align: left"><strong>20NG</strong></td> <td style="text-align: left">100</td> <td style="text-align: left">0.01 bits</td> <td style="text-align: left">100%</td> <td style="text-align: left">0.01</td> <td style="text-align: left">100%</td> <td style="text-align: left">0.01</td> <td style="text-align: left">—</td> </tr> <tr> <td style="text-align: left"><em>SciFact</em></td> <td style="text-align: left">10</td> <td style="text-align: left">8.84 bits</td> <td style="text-align: left">80%</td> <td style="text-align: left">8.52</td> <td style="text-align: left">81%</td> <td style="text-align: left">8.53</td> <td style="text-align: left">−3.12</td> </tr> <tr> <td style="text-align: left"><em>SciFact</em></td> <td style="text-align: left">100</td> <td style="text-align: left">5.52 bits</td> <td style="text-align: left">89%</td> <td style="text-align: left">5.36</td> <td style="text-align: left">93%</td> <td style="text-align: left">5.41</td> <td style="text-align: left">—</td> </tr> </tbody> </table> <p>At K=100 on 20 Newsgroups:</p> <ul> <li>Both systems achieve <strong>100% success</strong></li> <li>Both provide <strong>0.01 bits of selectivity</strong></li> </ul> <p>Perfect success rate. Essentially zero selectivity. <strong>The ceiling has collapsed.</strong></p> <p>The predicted <strong>ΔBoR</strong> from theory matches reality within <strong>0.01</strong> bits. The math is working exactly as expected.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-bits-over-random/selectivity_collapse_paradox_newsgroups-480.webp 480w,/2026/assets/img/2026-04-27-bits-over-random/selectivity_collapse_paradox_newsgroups-800.webp 800w,/2026/assets/img/2026-04-27-bits-over-random/selectivity_collapse_paradox_newsgroups-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-bits-over-random/selectivity_collapse_paradox_newsgroups.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Figure 2:</strong> <em>The selectivity collapse paradox on 20 Newsgroups. Left: BoR declines sharply with depth, converging to the theoretical ceiling (dashed line). Right: As Success@K approaches 100%, BoR approaches zero.</em></p> <p>But here’s the real question: <strong>Does this theoretical collapse actually hurt downstream performance?</strong></p> <h2 id="when-perfect-success-fails">When Perfect Success Fails</h2> <p>We tested this directly with a modern instruction-tuned LLM on the 20 Newsgroups collapsed scenario.</p> <p>Setup: Multiple-choice classification task, 50 queries per configuration, temperature=0.0.</p> <p><strong>The results:</strong></p> <table> <thead> <tr> <th style="text-align: left">System</th> <th style="text-align: left">Accuracy at K=10</th> <th style="text-align: left">Accuracy at K=100</th> <th style="text-align: left">Success@K</th> <th style="text-align: left">Token Cost</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>BM25</strong></td> <td style="text-align: left">66%</td> <td style="text-align: left"><strong>50%</strong></td> <td style="text-align: left">94% → 100%</td> <td style="text-align: left">10x increase</td> </tr> <tr> <td style="text-align: left"><strong>SPLADE</strong></td> <td style="text-align: left">68%</td> <td style="text-align: left"><strong>58%</strong></td> <td style="text-align: left">95% → 100%</td> <td style="text-align: left">10x increase</td> </tr> </tbody> </table> <p>Read that again:</p> <ul> <li>Success rate increased to 100% ✓</li> <li>Accuracy <strong>dropped</strong> by 10–16 percentage points ✗</li> <li>Token cost increased 10x ✗</li> </ul> <p><strong>This is the failure mode BoR detects.</strong> You’re paying 10x the tokens for random-level selectivity, and your AI is drowning in noise.</p> <p>When selectivity collapses, high success rates become meaningless or worse, misleading.</p> <h2 id="ai-agent-tool-selection">AI Agent Tool Selection</h2> <p>“That 20 Newsgroups test seems artificial,” you might be thinking. “Who retrieves documents where 5% of the corpus is relevant?”</p> <p>Fair Point. Let’s extend our testing to what happens with AI agents everyday.</p> <h3 id="when-agents-choose-tools">When Agents Choose Tools</h3> <p>Consider what Anthropic published in 2025<d-cite key="anthropic2025toolselection"></d-cite>:</p> <p><em>“Tool definitions can sometimes consume 50,000+ tokens before an agent reads a request. Agents should discover and load tools on-demand, keeping only what’s relevant for the current task.”</em></p> <p>Their example: 58 tools consuming ~55K tokens. Add integrations like Jira and you’re at 100K+ tokens. They’ve seen setups with tool definitions consuming 134K tokens before optimization.</p> <p>Now, let’s apply the same math as document retrieval:</p> <table> <thead> <tr> <th style="text-align: left">Parameter</th> <th style="text-align: left">Document Retrieval</th> <th style="text-align: left">Tool Selection</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>N</strong></td> <td style="text-align: left">Corpus size (thousands to millions)</td> <td style="text-align: left">Available tools (50–500)</td> </tr> <tr> <td style="text-align: left"><strong>K</strong></td> <td style="text-align: left">Documents shown to LLM</td> <td style="text-align: left">Tools shown to LLM</td> </tr> <tr> <td style="text-align: left"><strong>\(R_q\)</strong></td> <td style="text-align: left">Relevant documents</td> <td style="text-align: left">Applicable tools for task</td> </tr> </tbody> </table> <p>The critical difference: <strong>N is small for tools.</strong> And small N means you hit the collapse boundary much faster.</p> <h3 id="the-tool-selection-collapse">The Tool Selection Collapse</h3> <p>Let’s run the numbers for Anthropic’s 58-tool example. Assume 3–5 tools are typically relevant:</p> <table> <thead> <tr> <th style="text-align: left">Configuration</th> <th style="text-align: left">K</th> <th style="text-align: left">\(R_q\)</th> <th style="text-align: left">\(\lambda = \frac{K \cdot R_q}{N}\)</th> <th style="text-align: left">\(\text{BoR}_{\text{max}}\) (Poisson)</th> <th style="text-align: left">\(\text{BoR}_{\text{max}}\) (Exact)</th> <th style="text-align: left">What This Means</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Show 5 tools</td> <td style="text-align: left">5</td> <td style="text-align: left">4</td> <td style="text-align: left">0.34</td> <td style="text-align: left">~1.6 bits</td> <td style="text-align: left">~1.7</td> <td style="text-align: left">Meaningful selectivity</td> </tr> <tr> <td style="text-align: left">Show 20 tools</td> <td style="text-align: left">20</td> <td style="text-align: left">4</td> <td style="text-align: left">1.38</td> <td style="text-align: left">~0.4 bits</td> <td style="text-align: left">0.28</td> <td style="text-align: left">Degraded</td> </tr> <tr> <td style="text-align: left">Show all 58</td> <td style="text-align: left">58</td> <td style="text-align: left">4</td> <td style="text-align: left"><strong>4.0</strong></td> <td style="text-align: left">~0.02 bits</td> <td style="text-align: left">0</td> <td style="text-align: left">Collapse</td> </tr> </tbody> </table> <p>When all tool definitions are introduced simultaneously into the model’s context, the system operates at \(\lambda \approx 4\). This is deep into the collapse zone.</p> <p><strong>Even a perfect tool selector achieves only ~0.02 bits of selectivity over random chance.</strong></p> <p>The LLM is essentially guessing. And as Anthropic notes: <strong>“The most common failures are wrong tool selection and incorrect parameters, especially when tools have similar names.”</strong> This perfectly reflects the 20 Newsgroups scenario when \(R_q\) (relevant-per query items) was large.</p> <h3 id="the-pattern-extends-beyond-tools">The Pattern Extends Beyond Tools</h3> <p>The collapse boundary doesn’t care what you’re selecting. It’s a property of the selection problem itself: \(\lambda = \frac{K \cdot \bar{R}_q}{N}\)</p> <p>When \(\lambda\) hits 3–5, selectivity collapses, whether you’re selecting:</p> <ul> <li>Documents from a corpus</li> <li>Tools from an API library</li> <li>Agentic “skills”</li> <li>Functions from hundreds of endpoints</li> <li>Context from multi-hop retrieval chains</li> </ul> <table> <thead> <tr> <th style="text-align: left">Scenario</th> <th style="text-align: left">N</th> <th style="text-align: left">\(R_q\)</th> <th style="text-align: left">K</th> <th style="text-align: left">\(\lambda = \frac{K \cdot R_q}{N}\)</th> <th style="text-align: left">Regime</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">RAG (typical)</td> <td style="text-align: left">10,000</td> <td style="text-align: left">1–2</td> <td style="text-align: left">10</td> <td style="text-align: left">~0.002</td> <td style="text-align: left">Healthy</td> </tr> <tr> <td style="text-align: left">Tool selection (filtered)</td> <td style="text-align: left">20</td> <td style="text-align: left">3</td> <td style="text-align: left">5</td> <td style="text-align: left">0.75</td> <td style="text-align: left">Healthy</td> </tr> <tr> <td style="text-align: left">Tool selection (show all)</td> <td style="text-align: left">20</td> <td style="text-align: left">3</td> <td style="text-align: left">20</td> <td style="text-align: left">3.0</td> <td style="text-align: left">Collapse</td> </tr> <tr> <td style="text-align: left">API endpoints (show half)</td> <td style="text-align: left">100</td> <td style="text-align: left">8</td> <td style="text-align: left">50</td> <td style="text-align: left">4.0</td> <td style="text-align: left">Collapse</td> </tr> <tr> <td style="text-align: left">Anthropic’s 58-tool example</td> <td style="text-align: left">58</td> <td style="text-align: left">4</td> <td style="text-align: left">58</td> <td style="text-align: left">4.0</td> <td style="text-align: left">Collapse</td> </tr> </tbody> </table> <p><strong>This is why agentic systems struggle with tool selection far more than RAG systems struggle with document retrieval.</strong> The math is unforgiving when N is small.</p> <h2 id="what-you-should-do-about-this">What You Should Do About This</h2> <p>BoR gives you a new lens for evaluating retrieval systems. It reveals when high success rates are actually warning signs.</p> <p><strong>1. Monitor the collapse boundary</strong></p> <p>Calculate \(\lambda = \frac{K \cdot \bar{R}_q}{N}\) for your system. When \(\lambda\) approaches 3–5, you’re entering the danger zone. This single number tells you whether selectivity is even possible.</p> <p><strong>2. Use BoR to guide your K selection</strong></p> <p>Don’t just crank up K to boost success metrics. Instead:</p> <ul> <li>Stop increasing K when \(\text{BoR}_{\text{max}}\) drops below ~0.1 bits.</li> <li>If \(\text{BoR} \approx \text{BoR}_{\text{max}}\), you’ve saturated and more K won’t help.</li> <li>If \(\Delta\text{BoR}\) becomes negative or negligible, you’re adding noise, not signal.</li> </ul> <p><strong>3. For tool-based agents: Be aggressive about filtering</strong></p> <p>With small N (50–500 tools), you can’t afford to dump everything into context. Use:</p> <ul> <li>Two-stage retrieval (filter, then select)</li> <li>Dynamic tool loading based on task context</li> <li>Clustering by function domain</li> </ul> <p><strong>4. Remember the core insight</strong></p> <p><strong>More context is not always better.</strong> High Success@K can coexist with zero selectivity.</p> <table> <thead> <tr> <th style="text-align: left">Scenario</th> <th style="text-align: left">Calculations</th> <th style="text-align: left">Conclusion</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>K → N</strong> (K tends to N)</td> <td style="text-align: left">N = 100, K = 100, \(R_q = 1\)<br/><br/>\(P_{\text{obs}} = 1.0\) (retrieve everything, guaranteed success)<br/><br/>\(P_{\text{rand}} = 1.0\) (random selection of all 100 items → also guaranteed success)<br/><br/>\(\text{BoR} = \log_2(1.0 / 1.0) = 0\) bits exactly</td> <td style="text-align: left"><strong>BoR → 0 when K → N</strong> (K is closer to N)<br/><br/>Both Recall and Success@K are perfect. But BoR approaches zero asymptotically.<br/><br/>At K = N, BoR = 0.</td> </tr> <tr> <td style="text-align: left"><strong>Bad retriever</strong> - deliberately omits relevant results</td> <td style="text-align: left">N = 100, K = 10, \(R_q = 1\)<br/><br/>\(P_{\text{rand}} = 10/100 = 0.10\) (random succeeds 10% of the time)<br/><br/>retriever is adversarially bad: \(P_{\text{obs}} = 0.05\)<br/><br/>\(\text{BoR} = \log_2(0.05 / 0.10) = \log_2(0.5) = -1\) bit</td> <td style="text-align: left"><strong>BoR &lt; 0</strong> means we are actively avoiding relevant documents, doing worse than chance.</td> </tr> </tbody> </table> <h2 id="sidebar-successk-vs-recallk">Sidebar: Success@K vs Recall@K</h2> <p>Some readers might wonder: this post focuses on Success@K (coverage), but what about Recall@K?</p> <table> <thead> <tr> <th style="text-align: left">Metric</th> <th style="text-align: left">What It Measures</th> <th style="text-align: left">Per-Query Behavior</th> <th style="text-align: left">Best For</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Success@K</strong></td> <td style="text-align: left">Did you get \(\geq 1\) relevant item?</td> <td style="text-align: left">Binary: success or fail</td> <td style="text-align: left">RAG/QA where one good context suffices</td> </tr> <tr> <td style="text-align: left"><strong>Recall@K</strong></td> <td style="text-align: left">What fraction of all relevant items did you get?</td> <td style="text-align: left">Graded: 0% to 100%</td> <td style="text-align: left">Tasks needing comprehensive coverage</td> </tr> </tbody> </table> <p>The good news: <strong>BoR works with both.</strong></p> <h3 id="bor-for-recallk">BoR for Recall@K</h3> <p>The same framework applies. Instead of measuring “probability of \(\geq 1\) hit,” you measure “expected fraction retrieved”:</p> \[\text{BoR}_{\text{recall@K}} = \log_2\left(\frac{\text{observed_recall@K}}{\text{expected_recall@K_random}}\right)\] <p>For sparse relevance: \(\text{expected_recall@K_random} \approx \frac{K}{N}\)</p> <p><strong>Example:</strong> A query has 10 relevant items in a 1,000-document corpus. You retrieve 4 in top-20:</p> <ul> <li>Observed recall = \(\frac{4}{10} = 0.4\)</li> <li>Random baseline = \(\frac{20}{1{,}000} = 0.02\)</li> <li>\(\text{BoR}_{\text{recall@K}} = \log_2\left(\frac{0.4}{0.02}\right) = \log_2(20) \approx\) <strong>4.32 bits</strong></li> </ul> <h3 id="math-and-bor-interpretation">Math and BoR Interpretation</h3> <table> <thead> <tr> <th style="text-align: left">Metric</th> <th style="text-align: left">Definition</th> <th style="text-align: left">Formula</th> <th style="text-align: left">Observed Rate</th> <th style="text-align: left">Expected Rate (Random)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>BoR for Success@K</strong></td> <td style="text-align: left">Bits-over-Random for coverage (\(\geq 1\) relevant)</td> <td style="text-align: left">\(\log_2\left(\frac{\text{observed_success}}{\text{expected_success_random}}\right)\)</td> <td style="text-align: left">Fraction of queries with \(\geq 1\) relevant in top-K</td> <td style="text-align: left">Probability of \(\geq 1\) hit by random selection</td> </tr> <tr> <td style="text-align: left"><strong>BoR for Recall@K</strong></td> <td style="text-align: left">Bits-over-Random for recall (fraction retrieved)</td> <td style="text-align: left">\(\log_2\left(\frac{\text{observed_recall@K}}{\text{expected_recall@K_random}}\right)\)</td> <td style="text-align: left">Average fraction of relevant items in top-K</td> <td style="text-align: left">Expected fraction if picking K random (usually \(\frac{K}{N}\))</td> </tr> </tbody> </table> <p>The depth-calibrated identity also extends to Recall@K, with minor adjustments for the different success rule.</p> <p>We focus on Success@K in this post because it matches the most common RAG use case: you just need <em>one</em> good grounding passage.</p> <h2 id="final-thoughts">Final Thoughts</h2> <p>Retrieval evaluation has been stuck with metrics designed for human consumers. RAG and agentic AI systems need something different, something that accounts for the fact that every retrieved item imposes a cost, and random chance sets a floor.</p> <p><strong>Bits-over-Random provides that measure.</strong></p> <p>It makes three things visible that were previously hidden:</p> <ol> <li><strong>The ceiling:</strong> Even perfect systems have limited selectivity when random baselines are high</li> <li><strong>The collapse zone:</strong> When \(\lambda = \frac{K \cdot \bar{R}_q}{N}\) reaches 3–5, selectivity becomes impossible</li> <li><strong>The depth trade-off:</strong> Retrieving more doesn’t always help and it can actively hurt</li> </ol> <p>The math is simple but the implications are profound.</p> <p>When your tool-based agent has 50 functions available, and you dump all 50 into context, you’re not being thorough, you’re operating in the collapse zone. BoR reveals that.</p> <p>When you boost Success@K from 95% to 100% by tripling K, traditional metrics celebrate. BoR shows you just lost 1.5 bits of selectivity.</p> <p>The systems that win in the next era of AI won’t be the ones that retrieve the most. They’ll be the ones that retrieve the most <strong>selectively</strong>.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[For most of the history of information retrieval (IR), search results were designed for human consumers who could scan, filter, and discard irrelevant information on their own. This shaped retrieval systems to optimize for finding and ranking more relevant documents, but not keeping results clean and minimal, as the human was the final filter. However, LLMs have changed that by lacking this filtering ability. To address this, we introduce Bits-over-Random (BoR), a chance-corrected measure of retrieval selectivity that reveals when high success rates mask random-level performance.]]></summary></entry><entry><title type="html">Budget Alignment: Making Models Reason in the User’s Language</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/budget-alignment/" rel="alternate" type="text/html" title="Budget Alignment: Making Models Reason in the User’s Language"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/budget-alignment</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/budget-alignment/"><![CDATA[<h1 id="budget-alignment-making-models-reason-in-the-users-language">Budget Alignment: Making Models Reason in the User’s Language</h1> <p><em>Please read this as a late-stage work in progress shared in a “lab meeting” spirit to help and motivate parallel research.</em></p> <h2 id="introduction">Introduction</h2> <p>You ask a large language model (LLM) a math question in Japanese. It responds politely in Japanese — but behind the scenes, it’s reasoning in English/Chinese. Variables, steps, and mathematical lemmas often silently switch languages during reasoning. This behavior, where models default to English for chain-of-thought (CoT) reasoning, is more than a curiosity. It breaks instruction-following, confuses human overseers, and undermines the purpose of multilingual evaluation.</p> <p>The goal is clear: we want models to reason about a question in the language they are asked — not just to answer in that language. But this turns out to be harder than it sounds. Forcing models to reason in non-English languages usually leads to a drop in accuracy. Previous work shows that instructing models to reason only in the prompt language via prompting or steering improves coherence and grading alignment <d-cite key="zhong2025language"></d-cite>, but often comes at a steep “accuracy tax.” Even a small amount of multilingual fine-tuning helps, but doesn’t eliminate the trade-off <d-cite key="qi-etal-2025-models"></d-cite>. Further, models not only prefer to reason in English — they reason <em>more effectively</em> in English. When researchers force strict in-language reasoning (e.g., in Swahili or Thai), models often lose accuracy compared to when allowed to reason in English. For higher-resource languages like French or German, this trade-off is smaller — models can reason in-language nearly as well as in English. For low-resource languages, strict enforcement harms performance more significantly.</p> <p>Why do models switch to English in the first place? Much of it traces back to training. Most reasoning data are in English. Fine-tuning even strong multilingual models on English CoT data often leads them to adopt English as their “internal language of logic.” Yong et al. (2025) observe a “quote-and-think” behavior <d-cite key="yong2025crosslingual"></d-cite>, where models copy input phrases in the prompt language, but explain everything in English <d-cite key="kim2025one"></d-cite>. The model understands the question in the non-English language — it just prefers to reason in English.</p> <p>Our technical goal is simple: <strong>stop the switching without paying an accuracy tax</strong> — ideally, push the Pareto frontier of <em>(Accuracy, Language-consistency)</em>.<br/> And we want this post to serve as a practical guide with lessons learned along the way.</p> <p>Code, data, and checkpoints will be linked in the <strong>camera-ready</strong> version of this post to preserve anonymity during review.</p> <hr/> <h2 id="what-we-try-method-in-two-steps">What we try (Method in two steps)</h2> <p>🔧 <strong>Base model.</strong> <code class="language-plaintext highlighter-rouge">deepseek-ai/DeepSeek-R1-Distill-Qwen-7B</code>, a large reasoning model distilled from R1 through supervised fine-tuning on its reasoning traces, exhibiting an English/Chinese-dominant prior.</p> <p><strong>Step 1 — Small SFT to teach in-language reasoning.</strong><br/> We fine-tune on <strong>817 curated multilingual reasoning chains</strong> (from LiMO <d-cite key="ye2025limo"></d-cite>). This supervision data contains high-quality reasoning data matching R1 long-form reasoning <em>style</em>. No Reinforcement Learning (RL) here — just teach the policy to keep reasoning in the user’s query language.</p> <p><strong>Step 2 — Math-only GRPO to push accuracy while retaining reasoning language.</strong><br/> We run an RLVR-style GRPO with no KL, higher clip of 0.28 vs −0.2 (DAPO-like <d-cite key="yu2025dapo"></d-cite>), rollout 24, LoRA r = 8, LR = 1e-5, <strong>only on a Math-500 set translated to each language</strong>.<br/> Intuition: let RL optimize hard cases and verification behaviors, while the high clip reduces catastrophic reasoning style collapse back to English.</p> <p>We set the verifiable rewards as <strong>1.0 for accuracy, 0.2 for language consistency of reasoning traces, and 0.2 for answer format</strong> <d-cite key="rastogi2025magistral"></d-cite>.</p> <p>📊 <strong>Evaluation.</strong></p> <p>We tried our approach on three different languages: <strong>Japanese (JA) / French (FR) / Spanish (ES)</strong></p> <p>And tested on multiple datasets: <strong>MMLU College Math (MMLU Math), AIME25, GPQA, MMLU Pro Medicine (MMLU Med)</strong></p> <p>The first two are in-domain: MMLU-Math is similar to the training data in terms of hardness, while AIME25 is harder.<br/> The other two are out-of-domain: GPQA covers hard science questions, and MMLU Pro Medicine is made up of hard questions in the medical domain.</p> <p><strong>Regimes tested:</strong></p> <ul> <li>Base → <code class="language-plaintext highlighter-rouge">deepseek-ai/DeepSeek-R1-Distill-Qwen-7B</code> <d-cite key="deepseekai2025deepseekr1distillqwen7b"></d-cite></li> <li>SFT on top of Base</li> <li>GRPO-from-Base</li> <li>GRPO-from-SFT</li> </ul> <p><strong>Metrics:</strong></p> <ul> <li><code class="language-plaintext highlighter-rouge">pass@k(1,5,10)</code> where <code class="language-plaintext highlighter-rouge">n = 32</code> for accuracy</li> <li><code class="language-plaintext highlighter-rouge">Language-consistency %</code> (both reasoning traces <strong>and</strong> final answers must be in the requested language; script-aware checks)</li> </ul> <p><strong>How we score language consistency:</strong><br/> We check the entire CoT span and the final boxed answer.<br/> A sample counts as <code class="language-plaintext highlighter-rouge">Following = 1</code> only if both passages are in the requested language (script tokens, numerals, and markers allowed); otherwise <code class="language-plaintext highlighter-rouge">0</code>.<br/> We report the % across the set.</p> <hr/> <h2 id="-key-contributions">🔑 Key contributions</h2> <ol> <li> <p><strong>Small SFT reprograms inner monologue.</strong><br/> With only <strong>817 chains</strong>, language consistency rises near the ceiling in French/Spanish across datasets and substantially in Japanese (Fig. RQ0).</p> </li> <li> <p><strong>Two-step recipe Pareto-improves.</strong><br/> SFT secures language consistency; <strong>GRPO-SFT recovers/boosts accuracy on tough sets</strong> (AIME/GPQA) without reverting to English (Figs. RQ1–RQ4).</p> </li> <li><strong>Diagnose regressions and actionable fixes.</strong><br/> Regressions stem from: <ul> <li>Japanese tokenization/numeric friction,</li> <li>Spanish cue misalignment,</li> <li>medicine reward/style mismatch.<br/> Tokenizer-aware normalization, small Japanese/Spanish SFT top-ups, and multi-objective GRPO (with optional model merging) could recover accuracy without sacrificing in-language reasoning.</li> </ul> </li> <li><strong>TL; DR.</strong> You can briefly see our main results from the two figures below:<br/> Starting from an EN/ZH-dominant reasoning prior, small multilingual SFT is the most cost-effective way to “steer” in-language chains of reasoning. Adding math-only GRPO then recovers or improves accuracy on hard sets like AIME and GPQA while mostly preserving SFT’s language consistency discipline — pushing the Accuracy × Following frontier in many language–dataset pairs. The two pain points, Japanese (tokenization/numeric friction) and medicine (reward/style mismatch), are expected from the base prior and training signal, and both have potential straightforward fixes with light domain augmentation. And surprisingly, model merging can be very useful and effective.</li> </ol> <p><strong>Figure 1.a) Performance comparison overall across methods</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-budget-alignment/1a-480.webp 480w,/2026/assets/img/2026-04-27-budget-alignment/1a-800.webp 800w,/2026/assets/img/2026-04-27-budget-alignment/1a-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-budget-alignment/1a.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Figure 1.b) Overall language consistency rate comparison across methods</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-budget-alignment/1b-480.webp 480w,/2026/assets/img/2026-04-27-budget-alignment/1b-800.webp 800w,/2026/assets/img/2026-04-27-budget-alignment/1b-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-budget-alignment/1b.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="rq0--can-small-sft-reprogram-a-reasoning-models-reasoning-tone">RQ0 — Can small SFT reprogram a reasoning model’s “reasoning tone”?</h2> <p>Models often output the final answer in the same language as the user query. We want the <strong>reasoning process</strong> to match the prompt (user) language, too.</p> <p><strong>Results.</strong><br/> SFT drives the language consistency rate close to the ceiling (<strong>~99–100%</strong>) in French/Spanish and raises Japanese substantially (<strong>high-80s/90s</strong>).<br/> The language consistency rates averaged across all datasets are shown in Fig. RQ0: bars labeled Japanese/French/Spanish.</p> <p><strong>Interpretation.</strong><br/> A few hundred <strong>high-quality chains</strong> are enough to overwrite the English/Chinese inner-monologue priority to other languages. Japanese remains stubborn — see RQ5.</p> <blockquote> <p>Recall that instruction-following does not only mean the answer in the prompt language, but it should also ensure that the language of the reasoning traces is the same as the user’s preference to enhance their trustworthiness. SFT alone solves most of the language mismatch with limited accuracy improvements, which are yet lower than the accuracy of reasoning in English (i.e., the gray dashes in Figure 1.a above) in most cases. We provide more details in the next section.</p> </blockquote> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-budget-alignment/r0-480.webp 480w,/2026/assets/img/2026-04-27-budget-alignment/r0-800.webp 800w,/2026/assets/img/2026-04-27-budget-alignment/r0-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-budget-alignment/r0.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="rq1--does-sft-help-accuracy-or-only-language-reasoning-style">RQ1 — Does SFT help accuracy, or only language reasoning <em>style</em>?</h2> <p>We have shown that <strong>SFT significantly improves language consistency rates</strong>, but how about the accuracy?</p> <p><strong>Design.</strong><br/> Compare the accuracy <strong>Base vs SFT</strong> on <code class="language-plaintext highlighter-rouge">pass@k</code> per dataset–language<br/> (Fig. RQ1: Δ pass@10 = SFT − Base).</p> <p><strong>Findings.</strong></p> <ul> <li><strong>MMLU-Math:</strong> substantial improvements when train and test are in the same domain <ul> <li><em>French:</em> ~76 → <strong>98</strong></li> <li><em>Spanish:</em> ~80 → <strong>99</strong></li> <li><em>Japanese:</em> ~68 → <strong>88</strong></li> </ul> </li> <li> <p><strong>AIME:</strong> mixed. Although AIME contains math problems, it is way more difficult than LiMO, making it less likely to be considered as in-domain. As a result, SFT trades accuracy for strict language consistency when reasoning in ES.</p> </li> <li><strong>GPQA / MMLU Pro Medicine:</strong> Accuracy drops in most cases, but language consistency rises after SFT, indicating that it’s not trivial to generalize the capability of generating the correct answer from the training domain to others.</li> </ul> <p><strong>Takeaway.</strong><br/> SFT reliably improves language consistency <strong>and often increases accuracy on in-domain tasks (Math).</strong><br/> On OOD, SFT can over-narrate or change prior most probable token paths since the models are undertrained to reason in lower-resource languages — accuracy may dip unless taking further actions (e.g., reinforced by RL, shown in RQ2 and RQ3).</p> <p><strong>Practical guidance.</strong><br/> If your target is <strong>language consistency/reasoning style + some accuracy</strong>, SFT alone is cost-effective in-domain.<br/> If you also need robustness on hard and/or OOD sets, doing an <strong>RL top-up could be helpful.</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-budget-alignment/r1-480.webp 480w,/2026/assets/img/2026-04-27-budget-alignment/r1-800.webp 800w,/2026/assets/img/2026-04-27-budget-alignment/r1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-budget-alignment/r1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="rq2--when-rl-comes-how-does-grpo-help-with-accuracy">RQ2 — When RL comes, how does GRPO help with accuracy?</h2> <p><strong>Design.</strong><br/> Train GRPO only on Math-500; evaluate deltas (<strong>GRPO-SFT − SFT</strong>) across<br/> MMLU-Math / AIME / GPQA / MMLU-Med (Fig. RQ2).</p> <p><strong>In-domain.</strong><br/> SFT helps accuracy, but not always; GRPO brings a boost on top of the base SFT while maintaining language consistency of reasoning traces.</p> <ul> <li><strong>MMLU-Math-FR</strong> pass@10: <strong>76.0 → 97.8 → 98.0</strong> (Base → SFT → GRPO-SFT)</li> <li><strong>MMLU-Math-ES</strong> pass@10: <strong>80.5 → 98.6 → 99.1</strong> (Base → SFT → GRPO-SFT)</li> <li><strong>MMLU-Math-JA</strong> pass@10: <strong>68.1 → 88.0 → 91.5</strong> (Base → SFT → GRPO-SFT)</li> </ul> <p>The improvement in accuracy is consistent but slight due to the fact that MMLU-Math is relatively easy:<br/> The model almost achieves 90–100% accuracy after SFT, leaving no room for GRPO. Thus, the OOD sets are more informative.</p> <p><strong>Out-of-domain.</strong></p> <p>Positive transfers on <strong>AIME JA/FR/ES and GPQA JA/FR</strong>.<br/> For instance:</p> <ul> <li><strong>GPQA-ES</strong> pass@10: <strong>68.7 → 85.2 → 85.7</strong> (Base → SFT → GRPO-SFT)</li> <li><strong>AIME-JA</strong> pass@10: <strong>22.6 → 28.5 → 34.4</strong> (Base → SFT → GRPO-SFT; GRPO adds a large JA gain)</li> </ul> <p>More results are shown in the figure below.<br/> Although improvements on AIME-FR/ES and GPQA-ES are marginal, they still indicate a successful transfer of knowledge on the OOD setup after GRPO.</p> <p><strong>Negative transfers on Pro-Medicine.</strong></p> <ul> <li>Accuracy improves on Pro-Medicine-JA but decreases on French and Spanish.</li> </ul> <p><strong>Interpretation.</strong><br/> GRPO learns verification/search habits that generalize: language consistency, math reasoning styles, re-checking numeric steps, and tighter answer boxing.<br/> Those help <strong>GPQA and AIME</strong>.<br/> But medicine needs domain lexicon, evidence phrasing, and calibrated claims — <strong>absent in math RL</strong>.<br/> Previous works have shown reasoning-only post-training harms performance on downstream instruction-following and knowledge recall tasks <d-cite key="aggarwal2025optimalthinkingbench"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-budget-alignment/r2-480.webp 480w,/2026/assets/img/2026-04-27-budget-alignment/r2-800.webp 800w,/2026/assets/img/2026-04-27-budget-alignment/r2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-budget-alignment/r2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="rq3--where-should-rl-start-from-base-or-sft">RQ3 — Where should RL start from: Base or SFT?</h2> <p><strong>Design.</strong><br/> Compare <strong>GRPO-from-Base vs GRPO-from-SFT</strong> (Fig. RQ3).</p> <p><strong>Patterns.</strong></p> <ul> <li> <p><strong>GRPO-from-SFT is a steadier path.</strong><br/> On MMLU-Math FR, for example, GRPO-SFT sits around <strong>~98 pass@10</strong> while GRPO-Base is closer to <strong>~70</strong>,<br/> i.e., <strong>starting from SFT provides language consistency and still improves accuracy.</strong></p> </li> <li> <p><strong>SFT → RL keeps the multilingual policy.</strong><br/> Because SFT already forced the model to reason in Japanese/French/Spanish,<br/> RL on top of that mostly optimizes correctness <strong>without switching back to EN/ZH reasoning</strong> (Fig. 1.b).</p> </li> </ul> <p><strong>Interpretation.</strong><br/> <strong>SFT establishes the multilingual “reasoning policy.”</strong><br/> Starting RL from the SFT model lets GRPO optimize correctness <em>while preserving language consistency</em>.<br/> RL from Base sometimes pushes the model back toward its original reasoning style while still producing answers in the target language.<br/> That can make a few out-of-domain slices look better, but it also increases variance and <strong>style regression</strong> compared to starting from SFT.</p> <p><strong>Practical rule.</strong><br/> If you care about following (see Figure 1.b) <strong>and</strong> better in-domain accuracy, <strong>do GRPO after SFT.</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-budget-alignment/r3-480.webp 480w,/2026/assets/img/2026-04-27-budget-alignment/r3-800.webp 800w,/2026/assets/img/2026-04-27-budget-alignment/r3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-budget-alignment/r3.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="rq4--can-we-push-the-pareto-frontier-instead-of-trading-accuracy-for-language-consistency">RQ4 — Can we push the Pareto frontier instead of trading accuracy for language consistency?</h2> <p><strong>Design.</strong><br/> Plot Accuracy (x-axis) vs Following (y-axis) for each regime (4-panel Pareto figure).<br/> Then, inspect bar/line panels per dataset and language.</p> <h3 id="what-we-see">What we see.</h3> <ul> <li> <p><strong>SFT shifts points up</strong> (Following ↑).<br/> On some hard sets, accuracy dips slightly.</p> </li> <li><strong>GRPO-SFT shifts rightward</strong> (Accuracy ↑) with at most a small upward loss, compared with SFT-only — <strong>creating new frontiers on:</strong> <ul> <li><strong>MMLU-Math (JA/FR/ES):</strong> both metrics are high.</li> <li><strong>GPQA-ES:</strong> strong frontier point.</li> </ul> </li> <li><strong>Non-frontier holdouts:</strong> Pro-Med FR/JA and AIME-ES, where domain/reward mismatch persists.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-budget-alignment/r4-480.webp 480w,/2026/assets/img/2026-04-27-budget-alignment/r4-800.webp 800w,/2026/assets/img/2026-04-27-budget-alignment/r4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-budget-alignment/r4.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Bottom line.</strong><br/> Read each plot within the same language marker (Japanese ▲, French ■, Spanish ●) and compare colors:</p> <ul> <li><strong>yellow vs. blue</strong> = GRPO-from-SFT vs. Base</li> <li><strong>green vs. blue</strong> = SFT vs. Base</li> </ul> <p>Under this pairing:</p> <blockquote> <p><strong>GRPO-from-SFT (yellow) strictly Pareto-dominates Base (blue) in 9 of 12 language–dataset pairs</strong> (higher on both accuracy and following).</p> </blockquote> <p>In the remaining pairs, yellow usually raises following but gives up a little accuracy —<br/> i.e., a mixed trade-off rather than a strict Pareto gain.</p> <p>SFT (green) vs. Base (blue) generally shifts points up/right, and <strong>GRPO-from-SFT most often traces the upper-right envelope</strong> when strict dominance does occur.</p> <hr/> <h2 id="rq5--does-model-merging-help">RQ5 — Does model merging help?</h2> <p><strong>Motivation.</strong><br/> GRPO+SFT often peaks on math but can regress on knowledge-heavy sets (e.g., Pro Medicine),<br/> and SFT alone doesn’t consistently stabilize accuracy across Japanese/French/Spanish.</p> <p>Ideally, we want a solution that smooths these trade-offs while <strong>keeping language-consistency strong</strong>.<br/> Previous studies have shown that model merging is a promising approach to combine models’ abilities, albeit with some performance degradation <d-cite key="ustun-etal-2024-aya"></d-cite>.</p> <p>Here, we merged the base model with the other three SFT models using <code class="language-plaintext highlighter-rouge">merge-kit</code> with an equal linear merge.</p> <blockquote> <p>The merged approach is quite promising as a one-stop solution!</p> </blockquote> <h3 id="result-avg-pattern-across-datasets">Result (avg pattern across datasets)</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-budget-alignment/r5b-480.webp 480w,/2026/assets/img/2026-04-27-budget-alignment/r5b-800.webp 800w,/2026/assets/img/2026-04-27-budget-alignment/r5b-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-budget-alignment/r5b.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-budget-alignment/r5a-480.webp 480w,/2026/assets/img/2026-04-27-budget-alignment/r5a-800.webp 800w,/2026/assets/img/2026-04-27-budget-alignment/r5a-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-budget-alignment/r5a.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>MERGE consistently shrinks worst-case losses and raises floor performance</strong>, especially where SFT/GRPO dip.<br/> On Pro Medicine, MERGE recovers large chunks of accuracy for Japanese/French<br/> (e.g., JA pass@10 climbs from SFT/GRPO’s ~47–58% to ~70%; FR from ~47–70% to ~76%),<br/> while staying competitive on AIME/GPQA and within a few points of GRPO+SFT on MMLU-Math.</p> <p>In Spanish, where SFT already leads on Medicine, MERGE lands in the middle of Base vs SFT/GRPO+SFT rather than decreasing performance to Base.</p> <p>Overall, it trades a small slice of peak scores for <strong>lower variance across languages and tasks.</strong></p> <h3 id="interpretation">Interpretation</h3> <p>Parameter-space interpolation acts like an ensemble/regularizer:</p> <ul> <li>MERGE <strong>blends GRPO’s strong multi-step heuristics</strong> with <strong>SFT’s alignment priors</strong></li> <li>Dampens overfitting to any single regime</li> <li><strong>Stabilizes cross-lingual behavior</strong></li> </ul> <p>Practically, it expresses a steering effect:</p> <blockquote> <p>“You can dial toward robustness without re-running RL.”</p> </blockquote> <p>When you need:</p> <ul> <li>the <strong>highest leaderboard peak</strong>, pick <strong>GRPO+SFT</strong></li> <li><strong>reliable, in-language reasoning across JA/FR/ES</strong>, especially on domain-heavy sets, pick <strong>MERGE</strong></li> </ul> <blockquote> <p>MERGE is the safer default when you are data + compute-poor.</p> </blockquote> <hr/> <h2 id="discussion-where-performance-regresses-and-potential-solutions">Discussion: Where performance regresses, and potential solutions</h2> <p><strong>Empirical signal.</strong><br/> After SFT followed by GRPO, Japanese language consistency improves markedly, but accuracy lags French (e.g., AIME-JA pass@1 <strong>4.4 → 17.9</strong>, pass@10 <strong>22.6 → 34.4</strong>;<br/> AIME-FR pass@1 <strong>22.2 → 27.3</strong>, pass@10 <strong>46.3 → 48.2</strong>), indicating Japanese-specific friction even with its high increase.</p> <p>Spanish on AIME shows the opposite tension: the <strong>Base</strong> model scores well because it always reasons in English despite Spanish prompts, while <strong>SFT+GRPO enforces Spanish chains and accuracy drops</strong>.</p> <p>In Pro-Medicine, <strong>math-only GRPO from SFT causes regression</strong> (e.g.,<br/> FR pass@10 <strong>70.1 → 46.6</strong>, ES <strong>86.6 → 76.6</strong>, JA <strong>75.9 → 58.3</strong>), whereas GRPO started from Base hurts less.</p> <h3 id="mechanisms">Mechanisms</h3> <ol> <li> <p><strong>Language-prior competition.</strong><br/> The model’s strongest <em>reasoning prior</em> is in EN/ZH.<br/> Under difficulty, chains drift toward those priors.<br/> SFT+GRPO strengthens language consistency, which <strong>reduces access to English-anchored reasoning traces</strong> that previously helped (e.g., AIME-ES).<br/> → evidenced by the huge language-consistency bump.</p> </li> <li> <p><strong>Tokenizer &amp; formatting tax (Japanese &gt; French / Spanish).</strong><br/> Mixed scripts, half/full-width digits, unit variants, and thousand separators inflate perplexity on numeric steps — precisely where accuracy is most sensitive.</p> </li> <li> <p><strong>Cue misalignment in Spanish math.</strong><br/> AIME leans on algebra/number-theory “recipes” the model learned primarily in English<br/> (phrases like “let x be,” “gcd,” “mod”).<br/> Spanish equivalents (“sea x,” “mcd,” “módulo”) are rarer, longer, more accented <br/> → model drifts into slower or incorrect approaches mid-solution.</p> </li> <li> <p><strong>Reward misspecification in medicine.</strong><br/> Math-only RL optimizes numeric correctness, <strong>not</strong> biomedical recall, calibration, or evidence style. The policy over-indexes math heuristics and becomes <strong>over-assertive</strong> on clinical QA.</p> </li> <li> <p><strong>Starting-point effect.</strong><br/> RL from SFT pushes the policy toward SFT’s language/style anchors and away from neutral reasoning.<br/> On medicine, this causes bigger drops. RL from Base is more neutral; regressions are smaller.</p> </li> </ol> <h3 id="lightweight-fixes-that-may-work-across-cases">Lightweight fixes that may work across cases</h3> <ul> <li> <p><strong>Prompt-level normalization (before more training).</strong></p> <ul> <li> <p><em>Japanese:</em> unify to half-width digits/decimals/exp notation; no thousand separators;<br/> explicit math chain template in Japanese. <br/> Example: <code class="language-plaintext highlighter-rouge">数字は半角… SI を使用し…</code>.</p> </li> <li> <p><em>Spanish:</em> prefer <code class="language-plaintext highlighter-rouge">gcd / lcm / mod</code>, exponent notation, half-width digits;<br/> terse step headers (<code class="language-plaintext highlighter-rouge">Definimos / Sustituimos / Comprobación / Respuesta</code>).</p> </li> </ul> </li> <li> <p><strong>Tokenizer-aware formatting.</strong><br/> Consistent spacing around numerals/operators; avoid formatting that fragments tokens.</p> </li> <li> <p><strong>Targeted SFT top-ups.</strong><br/> Small, math-dense Japanese/Spanish datasets using normalized templates to reinforce per-language priors.</p> </li> <li> <p><strong>Reward shaping for GRPO.</strong></p> <ul> <li> <p>For <strong>AIME-ES</strong>: up-weight <em>correctness</em> and make <strong>“Spanish-only chain”</strong> a secondary objective.<br/> → nudges reasoning into Spanish <strong>without punishing English-anchored correct answers</strong>.</p> </li> <li> <p>For <strong>Medicine</strong>: add a <strong>tiny medical reward head</strong><br/> (terminology fidelity, claim calibration, evidence cues),<br/> plus a <strong>KL / behavior-cloning regularizer</strong> toward medical SFT to preserve discourse style.</p> </li> <li> <p>Use <strong>mixed-objective batches</strong> (math + clinical QA),<br/> and replay OOD medical exemplars during RL to avoid domain forgetting.</p> </li> </ul> </li> </ul> <h3 id="takeaway">Takeaway</h3> <p>The regressions likely stem from one cause:</p> <blockquote> <p><strong>objective + prior mismatch</strong>.</p> </blockquote> <p>Japanese/Spanish math suffers from tokenization and cue issues; medicine suffers from the absence of domain-specific rewards. Normalizing inputs, adding small language-aware SFT top-ups, and turning “math-only RL” into multi-objective RL (with correctness-first weighting for AIME-ES and a small medical head for Pro-Medicine) could be promising ways to recover accuracy while keeping outputs in the target language and accurate.</p> <hr/> <h2 id="blog-summary--practical-takeaways">Blog Summary — Practical takeaways</h2> <ol> <li> <p><strong>If you can only afford one step, do SFT (a few hundred high-quality SFT data).</strong><br/> You’ll almost certainly fix language-consistency without compromising accuracy;<br/> you might also get accuracy improvements on in-domain tasks.</p> </li> <li> <p><strong>If you can afford two steps, do SFT → GRPO-SFT.</strong><br/> Use <strong>high clip / no KL</strong>; keep rollouts moderate; verify you haven’t regressed following.</p> </li> <li> <p>A practical and computationally efficient approach is <strong>model merging among SFT models</strong>.</p> </li> <li> <p><strong>For medicine or other narrative-dense domains, add a tiny domain reward with in-domain data or a dozens-scale domain SFT.</strong></p> </li> <li> <p><strong>For Japanese (or any non-Latin script), include numeric/style templates</strong><br/> and optionally patch tokenization via formatting.</p> </li> <li> <p><strong>Track Pareto, not single metrics.</strong><br/> Always plot <em>(Accuracy, Following)</em> together; real wins move you <strong>up-and-right</strong>.</p> </li> </ol> <hr/> <h2 id="limitations--threats-to-validity">Limitations &amp; threats to validity</h2> <ul> <li> <p><strong>Dataset scope.</strong><br/> We use four well-known benchmarks; real-world prompts are noisier.</p> </li> <li> <p><strong>Reward misspecification.</strong><br/> Math-only RL can hurt non-math; the suggested fixes mitigate but don’t prove generality across all medical subspecialities.</p> </li> <li> <p><strong>Model prior.</strong><br/> EN/ZH dominance shapes outcomes. A different base prior (e.g., EU-centric) could change which languages are hardest.</p> </li> <li> <p><strong>Language-consistency metric.</strong><br/> Strong, script-aware, but still an automatic proxy; human raters may be stricter.</p> </li> </ul>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[We explore a two step multilingual alignment recipe for large language models to keep reasoning and answers in the user language while preserving accuracy.]]></summary></entry><entry><title type="html">ChunkTabPFN: Training-free Long Context</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/chunked-tabpfn/" rel="alternate" type="text/html" title="ChunkTabPFN: Training-free Long Context"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/chunked-tabpfn</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/chunked-tabpfn/"><![CDATA[<h2 id="1-introduction">1. Introduction</h2> <p><span id="sec:introduction"></span></p> <p>Large language models leverage <strong>in-context learning (ICL)</strong> by adapting their predictions at inference time based solely on provided examples, without requiring any gradient updates. Building on this idea, recent work on <strong>tabular foundation models</strong>, such as TabPFN, TabICL, Mitra, and Limix, extends the same paradigm to tabular data <d-cite key="hollmann2022tabpfn,hollmann2025accurate,qu2025tabicl,zhang2025mitra,zhang2025limix"></d-cite>. These models are trained once on synthetic tasks drawn from a prior, allowing them to approximate the posterior predictive distribution</p> \[p(y_{*} \mid x_*, D_{\text{train}})\] <p>in a single forward pass by supplying the training set as context, without any dataset-specific fine-tuning, without fine-tuning on each new dataset <d-cite key="hollmann2022tabpfn,hollmann2025accurate"></d-cite>. This approach is compelling because it contrasts with most deep tabular models—like TabNet, FT-Transformer, NODE, TabM, or retrieval-style models such as TabR and ModernNCA, which typically require dataset-specific training or fine-tuning <d-cite key="arik2021tabnet,gorishniy2021revisiting,popov2019neural,gorishniy2024tabm,gorishniy2023tabr,ye2024modern"></d-cite>. That dependency undermines the ideal of a true “drop-in foundation model.”</p> <p>ICL-based tabular models move closer to this ideal. However, they face a major practical limitation: <strong>context length</strong>. Transformer attention scales quadratically with sequence length, and current public TabPFN implementations are constrained to around 3,000 samples in the original work to 10,000<d-footnote>At the time of writing, the new TabPFN v2.5 model has just been released, which is supposed to have pushed the context limit further to 50,000.</d-footnote> in later versions <d-cite key="hollmann2022tabpfn,hollmann2025accurate"></d-cite>. Many real-world tabular datasets far exceed these limits.</p> <p>To address this, researchers have experimented with <strong>shrinking the context</strong>, such as by clustering, partitioning, or retrieving only subsets of the data. Examples include random-forest partitioning <d-cite key="hollmann2025accurate"></d-cite>, the Mixture of In-Context Prompters (MICP) <d-cite key="xu2024mixture"></d-cite>, and KNN-style retrieval <d-cite key="thomas2024retrieval"></d-cite>. Others, like TuneTables <d-cite key="feuer2024tunetables"></d-cite>, compress the data into learned representations.</p> <p>While these methods can be effective, they come with two drawbacks:</p> <ul> <li>They often require <strong>dataset-specific tuning</strong> or even retraining, which contradicts the zero-shot, pure ICL philosophy.</li> <li>They don’t use the <strong>entire training set</strong>, which is a core assumption of TabPFN’s Bayesian approximation. Replacing full data with summaries introduces conceptual inaccuracy.</li> </ul> <p>Hence, we ask the following question:</p> <blockquote> <p>Can we fit <strong>all training examples</strong> into the context (no pruning, no KNN) without learnable compression while staying within GPU memory?</p> </blockquote> <p>In this work, we focus specifically on TabPFN, though we believe the conclusions extend to other ICL-based tabular models. Our answer is a resounding <strong>yes</strong>. Indeed, TabPFN’s native implementation already supports this on some devices via <strong>FlashAttention</strong> <d-cite key="dao2022flashattention,dao2023flashattention,shah2024flashattention"></d-cite>. But as we’ll show in this blogpost, there are important caveats:</p> <ul> <li>FlashAttention and similar efficient mechanisms can <strong>fail</strong> when batch or head sizes exceed 65,535.</li> <li>These optimizations are <strong>unsupported</strong> on older or consumer-grade GPUs.</li> </ul> <p>To resolve this, we introduce a <strong>simple patch</strong>:</p> <ul> <li>For efficient attention, we <strong>chunk inputs</strong> along head or batch dimensions to avoid hitting the 65,536 limit.</li> <li>For older GPUs, we implement a <strong>chunked forward pass</strong> in pure PyTorch using the <strong>incremental log-sum-exp trick</strong>.</li> </ul> <p>This patch yields results <strong>identical to standard attention</strong> (up to floating-point associativity), without any approximations, fine-tuning, or pre-filtering.</p> <p>Empirically, we then test TabPFN out-of-the-box scalability by evaluating it on the full <strong>TabArena</strong> benchmark <d-cite key="tabarena"></d-cite>. We specifically analyze TabPFN performance on datasets with <strong>long contexts</strong> (&gt; 10,000). Key findings include:</p> <ul> <li><strong>Accuracy improves</strong> with more data, often up to 100,000+ rows (measured in AUC for classification and RMSE for regression).</li> <li>On smaller contexts (&lt;10,000), our chunked version <strong>matches the original</strong>—no hidden degradation.</li> <li>The runtime stays <strong>practical</strong> even on commodity GPUs.</li> </ul> <h2 id="2-methodology">2. Methodology</h2> <p><span id="sec:methodology"></span></p> <p>Let <code class="language-plaintext highlighter-rouge">(X, y)</code> be the input to the TabPFN model. The typical dimensions of the feature tensor are <code class="language-plaintext highlighter-rouge">[B, L, F]</code>, where <code class="language-plaintext highlighter-rouge">B</code> is the number of datasets in the batch, <code class="language-plaintext highlighter-rouge">L</code> is the (padded) sample size, and <code class="language-plaintext highlighter-rouge">F</code> is the number of features. The first thing TabPFN does is group features <code class="language-plaintext highlighter-rouge">X</code> and embed them, which yields the following shape: <code class="language-plaintext highlighter-rouge">[B, L, G, D]</code>, where <code class="language-plaintext highlighter-rouge">G</code> is the number of feature groups and <code class="language-plaintext highlighter-rouge">D</code> is the embedding size. In the rest of the blog, we assume <code class="language-plaintext highlighter-rouge">X</code> already has this post-embedding shape.</p> <p>The labels <code class="language-plaintext highlighter-rouge">y</code> are similarly embedded and then concatenated with the features along the group dimension, producing an input of shape <code class="language-plaintext highlighter-rouge">[B, L, G + 1, D]</code>. A keen reader might notice that <code class="language-plaintext highlighter-rouge">y</code> and <code class="language-plaintext highlighter-rouge">X</code> effectively have different “logical” lengths: <code class="language-plaintext highlighter-rouge">X</code> includes both train and test samples, while <code class="language-plaintext highlighter-rouge">y</code> is only provided for the training split. This is handled by padding the label embeddings for test samples with a dummy embedding. A variable <code class="language-plaintext highlighter-rouge">single_eval_pos</code> in the original code holds the index where train and test samples are concatenated, and this logic can be seen in the <code class="language-plaintext highlighter-rouge">transformer.py</code> file of the original TabPFN repository.</p> <p>The core of TabPFN is the attention mechanism, whose logic is primarily implemented in <code class="language-plaintext highlighter-rouge">layer.py</code>. TabPFN, like many Transformer-style models, uses attention in two ways: <strong>between samples</strong> and <strong>between features</strong>. The between-sample attention has both self- and cross-attention components: self-attention among training samples and cross-attention from test samples to train samples. Following the TabPFN implementation, we assume attention layers expect input of shape <code class="language-plaintext highlighter-rouge">[batch, seq_len, input_size]</code>. In the code, the leading dimensions before <code class="language-plaintext highlighter-rouge">(seq_len, input_size)</code> are collapsed via <code class="language-plaintext highlighter-rouge">_rearrange_inputs_to_flat_batch</code>. For between-feature attention this yields an effective batch size of <code class="language-plaintext highlighter-rouge">L * B</code>, whereas for between-item (between-sample) attention it yields <code class="language-plaintext highlighter-rouge">(G + 1) * B</code>.</p> <p>Recall that efficient attention implementations in PyTorch (such as the fused CUDA kernels backing <code class="language-plaintext highlighter-rouge">torch.nn.functional.scaled_dot_product_attention</code>) tile work across the <strong>batch</strong> and <strong>head</strong> dimensions. On NVIDIA GPUs of Ampere architecture and below, this effectively limits the product <code class="language-plaintext highlighter-rouge">B * num_heads</code> to at most <code class="language-plaintext highlighter-rouge">65535</code> CUDA blocks; when it reaches <code class="language-plaintext highlighter-rouge">65536</code> the kernel can fail with <code class="language-plaintext highlighter-rouge">CUDA error: invalid configuration argument</code> (see the corresponding <a href="https://github.com/pytorch/pytorch/issues/133976">PyTorch GitHub issue</a> for a minimal example where <code class="language-plaintext highlighter-rouge">65535</code> works but <code class="language-plaintext highlighter-rouge">65536</code> fails). In TabPFN, large sample sizes <code class="language-plaintext highlighter-rouge">L</code> or a large number of feature groups <code class="language-plaintext highlighter-rouge">G</code> can easily push these flattened batch sizes (<code class="language-plaintext highlighter-rouge">L * B</code> or <code class="language-plaintext highlighter-rouge">(G + 1) * B</code>) past this limit.</p> <p>A simple practical fix is to loop over the flattened batch dimension in chunks, so that each call to <code class="language-plaintext highlighter-rouge">scaled_dot_product_attention</code> stays within the kernel’s limits. This keeps the rest of the model unchanged while avoiding the <code class="language-plaintext highlighter-rouge">invalid configuration</code> errors at large <code class="language-plaintext highlighter-rouge">L</code> or <code class="language-plaintext highlighter-rouge">G</code>. Conceptually, this is can be done via the following patch to the attention computation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">q_chunk</span><span class="p">,</span> <span class="n">k_chunk</span><span class="p">,</span> <span class="n">v_chunk</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">k_b</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">v_b</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
<span class="p">):</span>
    <span class="c1"># (B_chunk, Lq, H, D) -&gt; (B_chunk, H, Lq, D)
</span>    <span class="n">Q</span> <span class="o">=</span> <span class="n">q_chunk</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">k_chunk</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">v_chunk</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span>
        <span class="n">Q</span><span class="p">,</span>
        <span class="n">K</span><span class="p">,</span>
        <span class="n">V</span><span class="p">,</span>
        <span class="n">attn_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">dropout_p</span><span class="o">=</span><span class="n">dropout_p</span> <span class="k">if</span> <span class="n">dropout_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">is_causal</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">scale</span><span class="o">=</span><span class="n">softmax_scale</span><span class="p">,</span>
    <span class="p">)</span>  <span class="c1"># (B_chunk, H, Lq, D)
</span>
    <span class="c1"># -&gt; (B_chunk, Lq, H, D)
</span>    <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">())</span>

<span class="n">attention_head_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <p>A different issue is <strong>hardware support</strong> for efficient attention kernels. PyTorch’s <code class="language-plaintext highlighter-rouge">scaled_dot_product_attention</code> can dispatch to several backends on CUDA: FlashAttention, memory-efficient attention, or a plain math implementation in C++. The availability of these specialized kernels varies across GPU generations. For educational purposes, and for those who wish to implement these kernels on older or unsupported devices, we refer to <a href="https://github.com/lucidrains/memory-efficient-attention-pytorch/tree/main">this repository</a>. We provide a brief sketch of how the chunking works to reduce the memory footprint below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">chunked_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">q_chunk</span><span class="p">,</span> <span class="n">kv_chunk</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    q: (..., Lq, D)
    k: (..., Lk, D)
    v: (..., Lk, Dv)
    q_chunk: size of query tiles (l)
    kv_chunk: size of key/value tiles (r)
    </span><span class="sh">"""</span>
    <span class="n">Lq</span><span class="p">,</span> <span class="n">Lk</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">k</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">qs</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Lq</span><span class="p">,</span> <span class="n">q_chunk</span><span class="p">):</span>
        <span class="n">qe</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">qs</span> <span class="o">+</span> <span class="n">q_chunk</span><span class="p">,</span> <span class="n">Lq</span><span class="p">)</span>
        <span class="n">q_tile</span> <span class="o">=</span> <span class="n">q</span><span class="p">[...,</span> <span class="n">qs</span><span class="p">:</span><span class="n">qe</span><span class="p">,</span> <span class="p">:]</span>                            <span class="c1"># (..., l, D)
</span>
        <span class="c1"># running stats per query row
</span>        <span class="n">mu</span> <span class="o">=</span> <span class="n">q_tile</span><span class="p">.</span><span class="nf">new_full</span><span class="p">(</span><span class="n">q_tile</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="nf">float</span><span class="p">(</span><span class="sh">"</span><span class="s">inf</span><span class="sh">"</span><span class="p">))</span>  <span class="c1"># (..., l)
</span>        <span class="n">s</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>                               <span class="c1"># (..., l)
</span>        <span class="n">a</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="o">*</span><span class="n">mu</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                         <span class="n">device</span><span class="o">=</span><span class="n">q</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">q</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>        <span class="c1"># (..., l, Dv)
</span>
        <span class="k">for</span> <span class="n">ks</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Lk</span><span class="p">,</span> <span class="n">kv_chunk</span><span class="p">):</span>
            <span class="n">ke</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">ks</span> <span class="o">+</span> <span class="n">kv_chunk</span><span class="p">,</span> <span class="n">Lk</span><span class="p">)</span>
            <span class="n">k_tile</span> <span class="o">=</span> <span class="n">k</span><span class="p">[...,</span> <span class="n">ks</span><span class="p">:</span><span class="n">ke</span><span class="p">,</span> <span class="p">:]</span>                           <span class="c1"># (..., r, D)
</span>            <span class="n">v_tile</span> <span class="o">=</span> <span class="n">v</span><span class="p">[...,</span> <span class="n">ks</span><span class="p">:</span><span class="n">ke</span><span class="p">,</span> <span class="p">:]</span>                           <span class="c1"># (..., r, Dv)
</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">q_tile</span><span class="p">,</span> <span class="n">k_tile</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">scale</span>
            <span class="n">local_max</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="n">values</span>               <span class="c1"># (..., l)
</span>            <span class="n">new_mu</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">local_max</span><span class="p">)</span>

            <span class="c1"># rescale old aggregates
</span>            <span class="n">alpha</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">mu</span> <span class="o">-</span> <span class="n">new_mu</span><span class="p">)</span>
            <span class="n">s</span> <span class="o">*=</span> <span class="n">alpha</span>
            <span class="n">a</span> <span class="o">*=</span> <span class="n">alpha</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">]</span>

            <span class="c1"># accumulate current tile
</span>            <span class="n">exp_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">new_mu</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">])</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="n">exp_logits</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>                         <span class="c1"># sum_k e^{z_k}
</span>            <span class="n">a</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">exp_logits</span><span class="p">,</span> <span class="n">v_tile</span><span class="p">)</span>               <span class="c1"># sum_k e^{z_k} v_k
</span>
            <span class="n">mu</span> <span class="o">=</span> <span class="n">new_mu</span>

        <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">a</span> <span class="o">/</span> <span class="n">s</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">])</span>                        <span class="c1"># softmax = a / s
</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>                           <span class="c1"># (..., Lq, Dv)
</span></code></pre></div></div> <p>In this implementation, the key components are:</p> <ul> <li>It tiles queries into chunks <code class="language-plaintext highlighter-rouge">q_chunk</code> instead of processing all <code class="language-plaintext highlighter-rouge">Lq</code> at once.</li> <li>It streams over keys/values in chunks <code class="language-plaintext highlighter-rouge">kv_chunk</code>, computing only <code class="language-plaintext highlighter-rouge">l × r</code> logits at a time.</li> <li>It maintains per-row running statistics <code class="language-plaintext highlighter-rouge">(mu, s, a)</code> using a numerically stable log-sum-exp merge, so the final output matches full attention as if we had formed the entire <code class="language-plaintext highlighter-rouge">Lq × Lk</code> score matrix in one go.</li> </ul> <h2 id="3-experiments">3. Experiments</h2> <p><span id="sec:experiments"></span></p> <p>We evaluate the TabPFN v2 model with chunking enabled on <strong>TabArena</strong> <d-cite key="tabarena"></d-cite>, which includes 51 tabular datasets spanning classification and regression tasks. We report scaling statistics for memory and runtime in Figure 1, and overall performance on TabArena in Figure 2. Note that in the original and subsequent reports of TabPFN, LIMIX, and TabICL on TabArena, the authors have typically imputed values that exceeded the context length for their respective methods. This might have created a distorted view of model capabilities. In Figure 2, we use only directly measured (non-imputed) results.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results-480.webp 480w,/2026/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results-800.webp 800w,/2026/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="attn-figure-caption"> Figure 1. Scaling TabPFN v2 to long contexts. Chunked TabPFN matches baseline accuracy where both fit, and extends inference to 100K+ examples. </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-chunked-tabpfn/elo_vs_baselines-480.webp 480w,/2026/assets/img/2026-04-27-chunked-tabpfn/elo_vs_baselines-800.webp 800w,/2026/assets/img/2026-04-27-chunked-tabpfn/elo_vs_baselines-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-chunked-tabpfn/elo_vs_baselines.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="attn-figure-caption"> Figure 2. Elo and normalized score across TabArena. Striped bars denote prior imputed TabPFN runs (filled with Random Forest fallbacks when OOM); our chunked TabPFN reports direct measurements. </div> <p>Separately, we evaluate TabPFN v2 on the same long-context datasets while varying the context length. Specifically, we sample <code class="language-plaintext highlighter-rouge">num_samples</code> points from each dataset and then report performance, memory, and runtime in Figure 3. To better understand how context length affects TabPFN’s performance, we perform a <em>scaling study</em> on the 15 “long-context” datasets from TabArena. For each dataset, we subsample the training set to progressively larger sizes (3,000 → 5,000 → 10,000 → 20,000 → 50,000 → 100,000) and compare baseline TabPFN v2 against our Chunked TabPFN.</p> <ul> <li>Chunked TabPFN maintains <em>exact equivalence</em> to baseline TabPFN while extending feasible context length by roughly 10×.</li> <li>Empirical scaling shows either plateau or monotonic improvement—never catastrophic degradation.</li> <li>Memory and runtime growth are linear in chunk size, enabling inference on 100 K+ examples with a single GPU.</li> </ul> <p>These findings reinforce that <strong>TabPFN’s in-context generalization truly extends beyond its training limit</strong>, and that the primary bottleneck was <em>implementation-level memory</em>, not <em>model-level capacity</em>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results_per_dataset-480.webp 480w,/2026/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results_per_dataset-800.webp 800w,/2026/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results_per_dataset-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results_per_dataset.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="attn-figure-caption"> Figure 4. Scaling curves for long-context datasets. Each plot shows RMSE, AUC, wall-clock inference time (s), and peak GPU memory (MB). Chunked TabPFN tracks baseline accuracy exactly up to 10 K examples and continues scaling to 100 K without degradation. </div> <h2 id="4-conclusion">4. Conclusion</h2> <p><span id="sec:conclusion"></span></p> <p>We presented <strong>Chunked TabPFN</strong>, an exact tiling strategy that enables TabPFN to process <em>long-context</em> tabular datasets (100 K+ rows) without retraining, fine-tuning, or any pre-processing such as clustering or compression.</p> <p>Our main results show:</p> <ol> <li> <p><strong>Exactness without approximation.</strong> The chunked attention computation is mathematically identical to the original transformer attention—only the evaluation order changes. Predictions match baseline TabPFN bit-for-bit (within floating-point tolerance) for all short-context cases.</p> </li> <li> <p><strong>Memory scalability.</strong> Peak GPU memory scales linearly with tile size instead of quadratically with context length. This removes the practical 10 K-sample ceiling and allows inference on 100 K+ rows using 24–32 GB GPUs.</p> </li> <li> <p><strong>Training-free generalization.</strong> Chunked TabPFN retains the spirit of in-context learning: no dataset-specific training, no hyperparameter search, no adaptation steps. Despite its simplicity, it matches or surpasses tuned deep tabular models on the long-context slice of TabArena.</p> </li> <li> <p><strong>Empirical insights.</strong> Many datasets continue to improve with larger contexts—suggesting that the PFN prior generalizes beyond its nominal pre-training length.</p> </li> </ol>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Tabular foundation models struggle with large datasets due to the quadratic attention. While methods like FlashAttention promise scalability, practical challenges persist in their application to tabular foundation models. Our work resolves these hurdles, enabling efficient attention, and reveals that contrary to the eariler reports, TabPFN's performance improves with larger contexts, highlighting its inherent robustness and minimal fine-tuning needs when scaling to complex, long datasets from the TabArena benchmark.]]></summary></entry><entry><title type="html">Defining and quantifying compositional structure</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/compositionality/" rel="alternate" type="text/html" title="Defining and quantifying compositional structure"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/compositionality</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/compositionality/"><![CDATA[<p>What is compositionality? For those of us working in AI or cognitive neuroscience this question can appear easy at first, but becomes increasingly perplexing the more we think about it. We aren’t short on intuitions: we know that compositionality has something to do with reuse of parts, combinatorial expressive power, systematic generalization, and that natural language is a paradigmatic case, among other things. We seem to be able to glance at some data and say “yes, <em>that</em> is compositional!”, and we largely seem to agree on these judgments. But what is compositionality <em>really</em>, on a mathematical level, and can we quantify it?</p> <p>Formalisms of intuitive concepts have proven incredibly useful in science, and for good reason. Humanity was long aware of things like gravity and drag, but it is only by understanding their deeper mathematical nature that we were able to build things like airplanes. We’re familiar with this phenomenon in AI as well. Since the early days of deep learning, people knew that building the structure of a modality and its symmetries into a model would be helpful, and were making some progress largely through intuition alone — for instance, designing convolutional architectures that are invariant to image translation <d-cite key="fukushima_neocognitron_1980"></d-cite>. However, it was only through the formalizations of Geometric Deep Learning <d-cite key="bronstein_geometric_2021"></d-cite>, which was built on the foundations of abstract algebra, topology, and group theory, that we were able to do this successfully in more general modalities such as graphs and manifolds. In contrast, when it comes to compositionality, we’re largely in the dark. We recognize its computational significance and its potential to address longstanding challenges like out-of-distribution generalization or continual learning, but we fumble around trying to make it emerge in our models without the proper theoretical tools needed to detect it, let alone build it in by design.</p> <p>The purpose of this blog post is to <strong>introduce a formal definition of compositionality</strong>. We don’t promise that it’s perfect — we have no experimental results and none of this has been vetted up to this point by peer review. Nevertheless, we feel like there’s enough here to warrant putting the ideas out in writing for the rest of the community to digest and criticize. As we hope you’ll find, the formal definition that we’re going to propose is both simple and wide-reaching, capturing disparate notions of compositionality within a single equation that applies to any form of data (a neural representation, a dataset, a piece of art or music, a physical object, etc.). We’ll also say quite a bit about <strong>what this definition of compositionality is useful for</strong> in AI, since it has deep implications for how we should build neural architectures and, even more importantly, how we should go about training them through the design of data curricula that <em>maximize compositional structure</em>.</p> <p>This blog post can broadly be divided into three sections. In the <a href="#section-1--compositionality-as-the-emergence-of-novel-shared-structure">first part</a>, we’ll introduce the ideas at a purely <em>intuitive</em> level. In the <a href="#section-2--a-formal-definition-for-quantifying-compositionality">second part</a>, we’ll make these ideas formally precise by expressing them through the mathematics of algorithmic information theory and compression, culminating in a single succinct equation that quantifies compositional structure. In the <a href="#section-3--implications-and-use-cases-for-ai">third and final section</a>, we’ll discuss some practical implications of these ideas for AI, touching on topics such as how to model hierarchical structure and construct data curricula from which knowledge can grow compositionally into the future, in a boundless and open-ended way.</p> <hr/> <h1 id="section-1--compositionality-as-the-emergence-of-novel-shared-structure">Section 1 — Compositionality as the emergence of novel shared structure</h1> <p>Paradigmatic examples of compositional data are easy to think of: a piece of music that recombines nested motifs and themes, an image dataset in which any given scene is made up of a combination of objects, a program that maximizes code reuse by defining a network of functions and classes, and of course natural language which can express an infinite set of ideas using a relatively small set of words and grammatical rules. Clearly, these examples have a lot in common, namely the notion of “parts” or “modules” which interact in complex cascades and at multiple scales to form the “whole” of the object.</p> <p>This sort of multi-scale parts-based structure is what we’re going to try to quantify. To do so, we want to first drill down on the example of <em>computer programs</em> as a backdrop to our discussion that will extend more or less throughout the blog post. There are a few reasons for this. For one, programs are extremely general: any object we can think of can be described through a set of instructions (i.e., a program). Even more important, programs are paradigmatic cases of compositional objects that are familiar to all computer scientists. Once we’ve introduced all of the ideas in the context of computer programs, we’ll abstract them back out so that they apply to <em>any</em> object, be it a piece of music, a painting, an <em>iid</em> dataset, a nonstationary stream of data, a function, etc. — essentially, anything that can be thought of as <em>information</em> expressed in bits.</p> <h2 id="programs-and-libraries">Programs and libraries</h2> <p>We consider some program compositional when it defines and reuses the same structures again and again in novel ways. Crucially, this is a property of the program’s <em>library</em> — the functions, classes, and data structures that it defines in order to optimize code reuse and modularity. By “library” here we want to stress that we do <em>not</em> mean an external package that one might import; we’re considering a self-contained program that makes no reference to external code, and the “library” refers to the reusable structures defined within the program itself — see the example in Figure 1. A compositional program’s library is rich, defining a number of functions and classes that serve as the “parts” which are recomposed. The structures in the library must also be broadly reused across the entire code base rather than only in local regions, otherwise we’re better off talking about multiple independent and non-compositional programs rather than a single compositional one. In addition, the libraries of compositional programs are themselves densely networked: functions and classes build hierarchically and laterally on top of existing ones.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-compositionality/fig1-480.webp 480w,/2026/assets/img/2026-04-27-compositionality/fig1-800.webp 800w,/2026/assets/img/2026-04-27-compositionality/fig1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-compositionality/fig1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: left; line-height: 1.5;"> <strong>Figure 1: Signatures of compositional programs.</strong> Programs are compositional in virtue of their *libraries:* the classes, functions, and other reusable code that they define. The libraries of compositional programs involve reuse of shared code broadly across the program and have a densely-networked structure. </div> <p>We don’t think any of these high-level signatures of compositionality are particularly controversial. Formalizing these intuitions is more difficult, however: how do we move from <em>qualitative</em> and vague statements like the high-level signatures we outlined above to more <em>quantitative</em> ones that can be precisely expressed in mathematical expressions?</p> <p>One of the key challenges is to specify <em>which</em> library we’re talking about, since any program can be implemented using an infinite number of possible libraries while remaining functionally identical. Is there any way to do this in a non-arbitrary way? For a given program that one might want to implement, is there a notion of the “correct” library one should write, or a library that is intrinsic to the program?</p> <p>It turns out that there is, and it can be arrived at through the goal of <em>compression</em>. Think of what any good programmer would try to do: they would write the reusable code like functions and classes in a way that makes the total length of the program <em>shortest</em>. Other than for reasons of clarity, no programmer would insist on writing a function for some piece of code that is only going to be executed a single time in the program, because doing so wouldn’t help make the program more concise as a whole.</p> <h2 id="growing-and-refactoring-libraries">Growing and refactoring libraries</h2> <p>We now have a way of talking about a program’s <em>intrinsic</em> library as the one that leads to the <em>best compression</em>. To talk about compositionality, we want to quantify the degree to which this library is <em>modular</em> — whether it decomposes into <em>multiple</em> functions and classes that get reused in many places. We also want to quantify the degree to which this modularity is densely <em>networked and hierarchical</em> — whether some elements of the library are used to define <em>other</em> elements inside it, like some functions being recombined to define new ones.</p> <p>If we can see the internals of the library and all of its components in detail, it might be possible to define some graph-theoretic metrics to quantify these things, but they risk being heuristic in nature and difficult to justify as truly general or theoretically-grounded. More problematically, while they may work for the specific case of quantifying compositional structure in programs, we’re primarily using programs as an intuition pump that we’ll later abstract out from. We’re looking for an approach that can just as easily be applied to arbitrary sorts of data like music, art, or datasets used for machine learning. In these other cases, the “library” that we’ll be talking about is less cleanly delineated into distinct modules and parts; clear boundaries might be fuzzy or non-existent, like the boundary between a solid or a liquid phase of matter, and we need our definition of compositionality to be robust here, too.</p> <p>We’re therefore going to take another approach to defining the compositionality of a program that asks how the library <em>grows</em> or <em>changes</em> when we consider increasingly large segments of the program. Basically, if you imagine chunking up a program into some <em>parts</em> (i.e., non-overlapping segments of code), compositional structure exists when the best library for compressing those parts is <em>different</em> from the best library for compressing the whole. Why must this be true? If the library of the whole is different from the libraries of the parts (e.g., it has additional functions), it necessarily means that there were <em>new</em> shared pieces of code among those parts that could be placed in the library as new modules to improve compression. Conversely, if the library of the whole is identical to the libraries of the parts, then the whole program necessarily can’t be more or less compositional than its parts (compositionality is a property of the library, and the libraries are identical); it is just a longer program. It’s only when the library changes, or is refactored, that new compositional structure necessarily emerges. This definition of compositionality in the case of computer programs is summarized below:</p> <blockquote class="notice--info" style="font-size:1.0em !important"> <p><strong>Compositional program</strong> (in English)</p> <p>A program is compositional with respect to some division into parts if the library that best compresses the whole differs from the libraries that best compress the parts.</p> </blockquote> <p>We want to quickly clarify a few things before moving on. First, even when we consider joining parts to make a whole, we’re asking whether or not <em>new</em> compositional structure emerges; even if it does not, the parts themselves might already have compositional structure. This brings us to our second point: this definition of compositionality can be recursively applied to the parts themselves in order to investigate compositionality hierarchically <em>at multiple scales</em>. Finally, this recursive property raises the question of <em>which</em> hierarchical decomposition(s) we should consider, which we’ll have more to say about later in <a href="#section-3--implications-and-use-cases-for-ai">section 3</a>.</p> <h2 id="illustrative-examples">Illustrative examples</h2> <p>For a very abstract definition such as this one, there’s no substitute for concrete examples that illustrate paradigmatic cases of both compositional and non-compositional structure. In some sense this is the entire goal of defining things in the first place — to include all positive cases while leaving out all negative ones using a simple expression — so lets put this one to the test. As before, we’ll make use of programs and libraries to build these concrete examples.</p> <p><strong>Brief notation</strong></p> <p>To avoid things getting to cumbersome, it’s time to introduce a tiny bit of notation. We’ll call a program $x$ and its best library (the one that best compresses it) $m_x$. We’ll denote the parts that we decomposed the program into with subscripts. We’ll just consider splitting the program into two parts for the moment, so that gives us $x_a$ and $x_b$ as well as their corresponding best libraries $m_{x_a}$ and $m_{x_b}$. We also said that we’re quantifying novel compositional structure as the degree to which the library of the whole <em>changes</em> from the libraries of the parts, which implies some sort of distance metric. We’ll call this distance $K(m_x \mid m_{x_a}, m_{x_b})$ for reasons that will become clearer in <a href="#section-2--a-formal-definition-for-quantifying-compositionality">section 2</a>.</p> <p><strong>Novel shared structure: compositional</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-compositionality/fig2-480.webp 480w,/2026/assets/img/2026-04-27-compositionality/fig2-800.webp 800w,/2026/assets/img/2026-04-27-compositionality/fig2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-compositionality/fig2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: left; line-height: 1.5;"> <strong>Figure 2: Example of novel shared structure.</strong> Left block shows a program and right blocks show that program split into parts. To more clearly show shared code and how a program is split into parts, each program is written both with and without its library (bottom and top of a block, respectively). </div> <p>If $x_a$ and $x_b$ share some novel structure — for instance, pieces of code that could be turned into a function — then it makes sense to say that there’s additional compositional structure in $x$ that wasn’t already present in $x_a$ or $x_b$. This is exactly what is happening in the example of Figure 2, where in order to clearly show how a program is split into parts and which parts of the program share structure we have written them both with and without their libraries. In both $x_a$ and $x_b$, there was no sense in adding a <code class="language-plaintext highlighter-rouge">norm()</code> function to their respective libraries because there would have been no reuse; the overall programs would have been slightly longer if we had. Both the part libraries $m_{x_a}$ and $m_{x_b}$ are therefore empty. However, when we consider these parts together as a whole in $x$, suddenly it makes sense to write a <code class="language-plaintext highlighter-rouge">norm()</code> function to shorten the program because it will be used once.</p> <p>The definition we’ve proposed accounts for this: $m_{x_a}$ and $m_{x_b}$ in the example are both empty because there is no code reuse that would benefit from being wrapped in a function, but $m_x$ on the other hand is not empty, making $K(m_x \mid m_{x_a}, m_{x_b}) &gt; 0$. This is the most minimal case of compositionality that we can construct — one “part” that is reused twice — and the definition correctly identifies it. Of course, the definition would also pick up on more interesting cases in which $m_{x_a}$ and $m_{x_b}$ might not be empty, and $x_a$ and $x_b$ share more interesting structure (e.g., multiple segments of code reuse that result in multiple new functions). In general, what happens in these sorts of cases is that the library of the whole $m_x$ <em>grows</em> with respect to the libraries of the parts $m_{x_a}$ and $m_{x_b}$, and this is always reflected in $K(m_x \mid m_{x_a}, m_{x_b}) &gt; 0$.</p> <p>It’s important to clarify once again what we mean by “novel shared structure” here. Clearly, the code that is shared between $x_a$ and $x_b$ was already present in each individually. However, when looking at those parts individually, the shared code does not show up in their libraries because it would not help us better compress them individually. It is only when we look at the <em>combination</em> $x = [x_a, x_b]$ that the shared code counts as compositional structure because it now helps us <em>better compress the whole</em>. The shared code itself is not what is novel, then, but rather the fact that this shared code now newly gets added to the library of the whole.</p> <p><strong>No shared structure: not compositional</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-compositionality/fig3-480.webp 480w,/2026/assets/img/2026-04-27-compositionality/fig3-800.webp 800w,/2026/assets/img/2026-04-27-compositionality/fig3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-compositionality/fig3.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: left; line-height: 1.5;"> <strong>Figure 3: Example of no shared structure.</strong> Left block shows a program and right blocks show that program split into parts. To more clearly show shared code and how a program is split into parts, each program is written both with and without its library (bottom and top of a block, respectively). </div> <p>A clear case that <em>lacks</em> compositional structure is when the best library we can build for $x$ just trivially combines those that were used for $x_a$ and $x_b$, such that $m_x = [m_{x_a}, m_{x_b}]$. This happens when $x_a$ and $x_b$ are unrelated to each other, such as in the example of Figure 3 where the two parts of a program serve entirely different functional purposes — in other words, they are <em>algorithmically independent</em>. Despite $m_x$ being larger and having more functions than either $m_{x_a}$ or $m_{x_b}$, it does not make sense to talk about $x$ as being more compositional than its parts $x_a$ and $x_b$ because those functions do not interact in any way. Instead, it makes more sense to talk about $x$ as two separate compositional parts that join together but do not compositionally interact with each other, almost like two separate subprograms that have nothing to do with each other.</p> <p>Our definition accounts for this case because the work involved in constructing $m_x$ from $m_{x_a}$ and $m_{x_b}$, which is quantified in $K(m_x \mid m_{x_a}, m_{x_b})$, is trivially small: the two libraries just need to be concatenated.</p> <p><strong>Fully shared structure: not compositional</strong></p> <p>Another case lacking compositional structure is when the library for $x$ is identical to the one used for either $x_a$ or $x_b$. Again, we need to emphasize here that we’re talking about <em>additional</em> compositional structure in the combination $x = [x_a, x_b]$; it is entirely possible that $x_a$ or $x_b$ themselves have rich libraries of reusable functions. But if the library for $x$ is identical to that of either $x_a$ or $x_b$, there is no reason to think of $x$ as being <em>more</em> compositional than its parts. This is the case when $x$ is nothing more than a longer program than $x_a$ or $x_b$ that is in fact reusing the same structures as them throughout.</p> <p>Once again, our definition easily accounts for this case: $m_x$ is identical to one of $m_{x_a}$ or $m_{x_b}$, so it is trivially compressible from them.</p> <blockquote class="notice--primary" style="font-size:0.9em !important"> <p><strong>Box: Compositional generalization</strong></p> <p>We want to give another quick example along these same lines, but this time in the realm of computer vision, since it has important connections to machine learning and generalization. While we’ve just spoken about programs up to this point, this brief digression will foreshadow how we’ll soon generalize the ideas up to this point to arbitrary kinds of data. Imagine that $x_a$ is a dataset of (concatenated) scene images and that $x_b$ is one additional image (nothing says the two objects have to be the same size). For the moment, we can think of the “libraries” in this case as “models” or collections of concepts (objects, possible relations between objects, etc.), although we’ll make this much more precise in <a href="#section-2--a-formal-definition-for-quantifying-compositionality">section 2</a>.</p> <p>If $m_x = m_{x_a}$, it means that the new image $x_b$ consists entirely of known concepts, such that there is no additional structure it could provide. The new image $x_b$ contains the same objects, subparts, backgrounds, textures, and all other reusable structures that were already present in $m_{x_a}$. In this case, the best model of data $x_a$ is <em>also</em> the best explanation of $x = [x_a, x_b]$. This provides very general conditions under which we can meaningfully talk about <em>compositional generalization</em> and when it is even possible: a model can only compositionally generalize to new data, without undergoing additional learning, if that new data provides no additional compositional structure that would serve to change the model.</p> </blockquote> <p><strong>Building on top of existing structure: compositional</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-compositionality/fig4-480.webp 480w,/2026/assets/img/2026-04-27-compositionality/fig4-800.webp 800w,/2026/assets/img/2026-04-27-compositionality/fig4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-compositionality/fig4.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: left; line-height: 1.5;"> <strong>Figure 4: Example of building on top of existing structure.</strong> Left block shows a program and right blocks show that program split into parts. To more clearly show shared code and how a program is split into parts, each program is written both with and without its library (bottom and top of a block, respectively). </div> <p>Lets now consider a more interesting example that will showcase the reach of the definition we’ve provided. A powerful benefit of compositionality is that it can allow us to describe some concepts as simple functions of others, whereas describing the concept from scratch might be pretty expensive. For instance, defining the concept of a “throne” without prior knowledge of concepts such as “chair” and “royalty” can be quite messy.</p> <p>What does this look like for programs and libraries? Consider the example in Figure 4, where in the first part of a program $x_a$ we were doing a lot of linear algebra, and as such our library $m_{x_a}$ needed to define a function for matrix multiplication <code class="language-plaintext highlighter-rouge">def matmul(a, b): ...</code>. Now, lets say that for the second part of our program $x_b$ we were doing some geometry that benefited from a vector norm function <code class="language-plaintext highlighter-rouge">def norm(v): sqrt(sum([val ** 2 for val in v]))</code>. What will the optimal library $m_x$ look like when we consider $x_a$ and $x_b$ jointly? Crucially, the <code class="language-plaintext highlighter-rouge">norm</code> function <em>will change</em> in the new library because of novel shared structure between $x_a$ and $x_b$. In particular, $m_x$ will now define the function as <code class="language-plaintext highlighter-rouge">def norm(a): sqrt(matmul(a, a))</code> because this is shorter once we already have code for implementing matrix multiplication. We have compositionality here because we get to re-express some concept (the norm of a vector) in terms of other concepts (matrix multiplication and square root). Crucially, in this case, it’s not that the library $m_x$ grew with respect to $[m_{x_a}, m_{x_b}]$, but that it was <em>refactored</em> from them because novel shared structure resulted in a better strategy for compression.</p> <p>How does our definition account for this notion of compositionality? Effectively, we’ll have that $K(m_x \mid m_{x_a}, m_{x_a}) &gt; 0$ because the implementation of the <code class="language-plaintext highlighter-rouge">norm</code> function in $m_x$ is a new component of the library — it was implemented differently in $m_{x_b}$, and has to be rewritten. Granted, in this example $K(m_x \mid m_{x_a}, m_{x_a})$ will be quite small since the new <code class="language-plaintext highlighter-rouge">norm</code> function is very easy to write given the <code class="language-plaintext highlighter-rouge">matmul</code> function in $m_{x_a}$, but the point we’re making is more general. Whenever novel structures or concepts can be succinctly described in terms of others, our definition correctly identifies this as a case of compositionality.</p> <h2 id="generalization-to-any-object-good-libraries-as-occams-razor-models">Generalization to any object: good libraries as Occam’s razor models</h2> <p>We hope that the above discussion of programs and libraries has been helpful for building intuition about compositionality, but ultimately we need to generalize outside of this particular case. For this, we’ll be replacing the notion of a program with that of <em>any</em> arbitrary data that can be expressed in bits. This encompasses things like music, images, <em>iid</em> datasets, non-stationary data streams, neural representations, functions, natural language utterances, computer programs — basically, anything that is scientifically interesting. The bigger question is: what should we replace the notion of the program’s <em>library</em> with? The analogous concept turns out to be a <em>model</em> of the data.</p> <p>A model, like a program’s library, captures <em>structure</em> in the data: patterns of information that repeat in some shape or form, either in a quite literal sense (like a repeating subsequence of bits) or in more abstract ways (like a rule or template with adjustable parameters). Just like in a computer program, the patterns of information captured by a model can be interdependent and hierarchically-defined, like in a deep neural network where representations are built upon each other.</p> <p>What all this means is that a model, like a program’s library, can <em>compress</em> the data in ways we’ll make more precise in <a href="#section-2--a-formal-definition-for-quantifying-compositionality">section 2</a>. A model can be thought of as a shorter explanation of a more complex object. If we give you a pattern of bits like <code class="language-plaintext highlighter-rouge">01001100011100001111...</code> and ask you what’s going on, you’ll probably say something like “alternate groups of 0’s and 1’s and increase the group size by one each time” — that’s a model of the data (in this case a generative one) that can help us compress the string with far fewer bits because the model is simple and easy to encode. Even if the model isn’t perfect and has some degree of error (e.g., imagine corrupting the above string with a bit of noise), it can still help with compression because encoding the model along with a few error-correcting bits can be easier than encoding the entire string verbatim.</p> <blockquote class="notice--primary" style="font-size:0.9em !important"> <p><strong>Box: Models of individual objects rather than datasets</strong></p> <p>We want to quickly head off a potential confusion for the machine learning audience. We’re accustomed to modeling <em>datasets</em> (often <em>iid</em> ones), and so the idea of modeling an individual object or a nonstationary stream of data might seem strange. But rest assured that there is nothing wrong with modeling individual objects, and indeed it is not entirely abnormal to see this done in practice. For instance, Compositional Pattern-Producing Networks (CPPNs) <d-cite key="stanley_compositional_2007"></d-cite>, Neural Radiance Fields (NeRFs) <d-cite key="mildenhall_nerf_2021"></d-cite>, and various compressors <d-cite key="Balle_2025_CVPR"></d-cite><d-cite key="liao2025arcagiwithoutpretraining"></d-cite> all aim to model an individual objet like an image or a scene. Granted, the ordinary paradigm of sampling datapoints from the training set and minimizing the loss through gradient methods sometimes has to be adapted in such cases, but none of this is essential to modeling anyway.</p> </blockquote> <p>What model should we consider, though, given that we could select among an infinite number of models for any given data? Earlier we considered the library that <em>best compresses</em> a program, and we can take the same approach here. Many will have heard of the principle of Occam’s razor, where we say that the simplest explanation of some data is the best. In machine learning we follow this principle too, whether we realize it or not, when we search for models that achieve low training error (i.e., models that explain the data) but still generalize to the test set (i.e., models that aren’t more complex than they need to be, which would result in overfitting). The Occam’s razor model is an ideal. It is the <em>simplest</em> model that helps us <em>best compress</em> some data, and it is a non-arbitrary way to talk about some data’s “intrinsic” or “true” model in the same way that the most compact implementation of a program gave us a meaningful notion of its intrinsic library.</p> <p>We’re now ready to state our definition for novel compositional structure in the general case. Instead of being a program, $x$ now represents <em>any</em> data that can be expressed in information — in bits. Instead of talking about the library that best compresses a program, $m_x$ is now the Occam’s razor model of the data that allows us to <em>best compress</em> $x$. Since we’ve spoken enough about compression up to this point, we’re also ready to clarify what $K(m_x \mid m_{x_a}, m_{x_b})$ means: it is the cost of trying to compress the model of the whole given the models of the parts — a quantity that we’ll later formalize using Kolmogorov complexity. Below is the succinct definition, in English:</p> <blockquote class="notice--info" style="font-size:1.0em !important"> <p><strong>Compositionality</strong> (in English)</p> <p>An object is compositional with respect to some division into parts if the model that best compresses the whole isn’t easily derived from the models that best compress the parts.</p> </blockquote> <p>Notice that all of the illustrative examples covered earlier for computer programs still straight-forwardly apply in the general case. Let’s take $x$ to be a pair of images, for instance. Novel shared structure (compositional) might involve the two images $x_a$ and $x_b$ sharing an object that appears in neither image individually. A case of no shared structure (uncompositional) might involve two images with entirely different semantic and structural content, like an image of cells under a microscope and an image of the Rocky Mountains (although the example isn’t perfect, as there is still shared low-level structure). A case of fully shared structure (uncompositional) might be two different images of cells under a microscope. Finally, we already gave an example of structures building on top of others earlier: an image of a throne on its own might involve a complex model, but when joined together with images whose models include the notions of a chair and of royalty, the right way to model a throne now becomes to define it in terms of those pre-existing concepts. Analogous examples can easily be constructed for other kinds of data as well, reflecting the generality of this definition of compositionality.</p> <hr/> <h1 id="section-2--a-formal-definition-for-quantifying-compositionality">Section 2 — A formal definition for quantifying compositionality</h1> <p>In this section, we’ll be formalizing all of the things we’ve said up to this point and making our definition of compositional structure mathematically precise. In particular, we’ll be clarifying the notion of an “Occam’s razor model” $m_x$ of data $x$, as well as the distance metric $K(m_x \mid m_{x_a}, m_{x_b})$ that we’ve been using to quantify novel compositional structure. Many might already have a very solid intuitive understanding of our definition at this point without the need for more formalisms, and this is no accident: the definition is built on the foundations of <em>algorithmic information theory</em>, which is one of the most intuitive yet powerful branches of mathematics we’ve encountered. Some parts of this section may feel tedious — we’ll be introducing a lot of background and new notation — but if you stick with it, we think that you’ll come away with not only a sharper understanding of compositionality, but also a deeper grasp of far-reaching concepts like information, complexity, structure, modeling, Occam’s razor, and compression.</p> <h2 id="kolmogorov-complexity-and-optimal-compression">Kolmogorov complexity and optimal compression</h2> <p>Kolmogorov complexity <d-cite key="Kolmogorov01011968"></d-cite> — the most important concept in algorithmic information theory — is a formal way to quantify information. Most people are familiar with the Shannon notion of information, so we’ll briefly start there. Shannon information quantifies the amount of information contained in an object $x$ as the length of a coded message that a speaker would need in order to communicate $x$ to a listener. Assuming that $x$ is drawn from some distribution $p$ that is known to both the speaker and the listener, it turns out that the optimal coding scheme that achieves the minimal message length in expectation encodes $x$ using $-\log_2 p(x)$ bits — intuitively, we assign shorter codes to events that are more frequent.</p> <p>Kolmogorov complexity goes one step beyond Shannon information by dropping the assumption that the distribution $p$ is known to both the speaker and listener, and in fact drops the assumption that $x$ is drawn from any distribution at all. In Kolmogorov complexity, we instead only ask one thing: how <em>compressible</em> is $x$? The way that we do this is that we fix a Turing-complete programming language (Python, for instance), and we ask <em>what is the length of the shortest program that I can write which outputs $x$.</em> We denote this quantity $K(x)$.</p> <blockquote class="notice--info" style="font-size:1.0em !important"> <p><strong>Kolmogorov complexity</strong></p> <p>Given some finite string $x$ and a universal Turing machine $U$, the Kolmogorov complexity $K(x)$ is the length $l(r)$ (in bits) of the <strong>shortest</strong> binary program $r$ that prints $x$ and halts:</p> \[K(x) = \min_r \{l(r) : U(r) = x, r \in \{0, 1\}^* \}\] </blockquote> <p>Kolmogorov complexity has many intuitive properties that make it attractive as a measure of information quantity. The smaller and the more structure an object has — regularity, patterns, rules, etc. — the more easily it can be compressed using a short program and the lower its Kolmogorov complexity. For instance, a sequence with repeating patterns or a dataset that spans a low-dimensional subspace can be significantly compressed relative to its original size, and this results in low Kolmogorov complexity. In contrast, a random string devoid of any structure cannot be compressed at all and must in effect be “hard-coded”, making its Kolmogorov complexity equal to its original size in bits.</p> <p>There’s also a conditional notion of Kolmogorov complexity that will be useful, denoted $K(y \mid x)$, which is equal to the length of the shortest program <em>which takes $x$ as input</em> and outputs $y$. Intuitively, this measures the amount of leftover information in $y$ given that we already know $x$. Conditional Kolmogorov complexity $K(y \mid x)$ is of course always less than or equal to $K(y)$ given that we have the option of simply ignoring the input $x$, and it can be significantly smaller than $K(y)$ when $x$ and $y$ share a lot of structure.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-compositionality/fig5-480.webp 480w,/2026/assets/img/2026-04-27-compositionality/fig5-800.webp 800w,/2026/assets/img/2026-04-27-compositionality/fig5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-compositionality/fig5.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: left; line-height: 1.5;"> <strong>Figure 5: Kolmogorov complexity and conditional Kolmogorov complexity.</strong> Kolmogorov complexity is the length of the shortest program that outputs an object, and quantifies information through the lens of compression. Conditional Kolmogorov complexity is the length of a shortest program that takes one or more objects as input and outputs another. </div> <h2 id="sophistication-and-occams-razor">Sophistication and Occam’s razor</h2> <p>While powerful, Kolmogorov complexity isn’t completely satisfying as a universal measure of information quantity because it makes no distinction between <em>meaningful,</em> <em>structural</em> information and <em>random, unstructured</em> information. This is easiest to explain through examples. Consider a binary string $x$ that consists exclusively of a repeating sequence of $1$’s: it’s intuitively quite a simple object, and indeed $K(x)$ is quite low because we can print $x$ using a simple for-loop. Now, consider the opposite case of a binary string $y$ that consists of an entirely random sequence of $0$’s and $1$’s: $K(y)$ is maximally large because we have no other choice but to hard-code $y$. In one respect this makes sense — $y$ is incompressible, so in that respect it is indeed “complex”. But there is also a sense in which $y$ is strikingly simple, and in fact just as simple as a constant string like $x$. In particular, neither $y$ nor $x$ can really be said to have complex <em>structure</em>. They are equally boring. Even though $y$ must be described with a very large program, that program itself doesn’t contain much interesting logic outside of simply hard-coding bits, and the distribution from which $y$ might have been drawn would just take a few lines of code to define. In contrast, we can easily imagine a string $z$ that is also difficult to compress with high $K(z)$, but because it contains significant structure (i.e., interesting and sophisticated decompression code) rather than arbitrary randomness.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-compositionality/fig6-480.webp 480w,/2026/assets/img/2026-04-27-compositionality/fig6-800.webp 800w,/2026/assets/img/2026-04-27-compositionality/fig6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-compositionality/fig6.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: left; line-height: 1.5;"> <strong>Figure 6: Shortcoming of Kolmogorov complexity in measuring structural information.</strong> Kolmogorov complexity does not distinguish between *structural/meaningful* information and *unstructured/random* information. </div> <p>This distinction between <em>structural</em> and <em>random</em> information was important to Kolmogorov himself, who quantified it through a new metric called <em>sophistication</em>. The idea is that we can consider strategies for compressing an object that work in two stages: first we encode some model of the object, and then second we encode the remaining bits of random information unaccounted for by the model. These compression strategies are called “two-part codes”. Let’s consider the model class of computable probability distributions as an example (although this also works with more general model classes). An optimal “two-part code” for an object $x$ that achieves the best possible compression is one that satisfies <d-cite key="grunwald_algorithmic_2008"></d-cite><d-cite key="vitanyi_meaningful_2006"></d-cite>:</p> \[K(x) = K(p_x) - \log_2 p_x(x)\] <p>Here, $p_x$ is the probability distribution that we are using to model $x$ and $K(p_x)$ is the minimum number of bits that it takes to implement this probability distribution in a concrete computer program. Because $p_x$ might not be a perfect model of $x$, we also have to account for error correction bits, which in this case correspond to the ordinary Shannon information $-\log_2 p_x(x)$ (recall that Shannon information assumes access to the probability distribution from which $x$ is drawn, which we have accounted for in $K(p_x)$). There is always <em>at least</em> one model that satisfies this equality. For instance, $p_x$ can trivially put all its probability mass on $x$, in which case it takes $K(p_x) = K(x)$ bits to encode and achieves an error of $-\log_2p_x(x) = 0$ bits. However, in general, there can be <em>many</em> solutions. An interesting model to prefer above the others, though, is the one that is simplest:</p> \[p_x = \mathop{\arg\,\min}\limits_{p_x'} \{ K(p_x') : K(x) = K(p_x') - \log_2 p_x'(x) \}\] <p>Note that in the above equation and for the rest of the blog post, the distribution $p_x$ does <em>not</em> just represent <em>any</em> model of $x$: it represents the <em>simplest one that best compresses it</em>. This particular model is not arbitrary, but rather is intrinsically defined purely in terms of $x$, which is why we have chosen to denote it $p_x$ where the subscript indicates the dependence on $x$.</p> <blockquote class="notice--primary" style="font-size:0.9em !important"> <p><strong>Box: Distributions as models of individual objects?</strong></p> <p>By this point, for some readers a tension will have emerged. On the one hand, we’ve stated that Kolmogorov complexity is about individual objects and that $x$ represents an <em>instance</em> rather than a random variable. On the other hand, we’ve also been saying that we can use a probability distribution to model $x$, but probability distributions are generally models of distributions and their random variables.</p> <p>We are indeed still talking about individual objects in this section, and we are making no assumptions whatsoever that $x$ has been drawn from a distribution. Nevertheless, for the purposes of compression, it can be convenient to pretend <em>as if</em> $x$ was drawn from a distribution. In particular, $x$ might look complex, but it might be possible to specify a <em>simple</em> distribution under which $x$ looks “typical” (i.e., it has high likelihood under this distribution, and is easily encoded through it).</p> <p>We also want to close this discussion by emphasizing that even though $x$ is as single object, nothing stops us from considering a single object consisting of <em>multiple</em> draws from a distribution. Kolmogorov complexity is defined over strings, and we can easily represent something like an <em>iid</em> dataset by, for instance, concatenating individual datapoints together.</p> </blockquote> <p>There’s a nice parallel to machine learning theory here, and in particular the principle of Occam’s razor: we are looking for a simple model with low $K(p_x)$ that nevertheless accurately explains the data with low error $-\log_2 p_x(x)$. The complexity of this model $K(p_x)$ is what Kolmogorov described as the <em>sophistication</em> of the string $x$, and the Occam’s razor model $p_x$ is sometimes called the <em>algorithmic minimal sufficient statistic</em> <d-cite key="grunwald_algorithmic_2008"></d-cite><d-cite key="vitanyi_meaningful_2006"></d-cite>. Sophistication is precisely the quantity that we are looking for in order to distinguish between structured and unstructured information, where $K(x)$ alone was insufficient. If a string is too simple (e.g., a repeating pattern of $1’s$), it can be best compressed by a simple model with low $K(p_x)$. On the other hand, if a string is complex because of random noise rather than structure, $K(p_x)$ is <em>still</em> low because random noise distributions are easy to implement in just a few lines of code. It is only when a string has interesting and complex structure that $K(p_x)$ is high, meaning that the string is best compressed by a complex model.</p> <p>We said earlier that sophistication is defined with respect to some class of models, and that computable probability distributions are just one option. For the rest of the post, to remain more general, we’ll therefore switch to the notation $m_x$ instead of $p_x$ to denote the Occam’s razor model of string $x$.</p> <h2 id="defining-compositionality-through-algorithmic-information-theory">Defining compositionality through algorithmic information theory</h2> <p>We now have all the tools that we need to formally define compositional structure in data, and we can do so essentially just by replacing some of the language in our earlier definition with more precise mathematics. To briefly revisit this intuition, our central argument was that compositionality emerges from <em>novel</em> structure that is <em>shared</em> between an object’s parts. This is what that looks like in math:</p> <blockquote class="notice--info" style="font-size:1.0em !important"> <p><strong>Compositionality</strong></p> <p>An object $x$ is compositional with respect to some division into parts $x = [x_a, x_b]$ if:</p> \[K(m_x \mid m_{x_a}, m_{x_b}) &gt; 0\] <p>where the Occam’s razor model of the data $m_x = \mathop{\arg\,\min}\limits_{m_x’} \{ K(m_x’) : K(x) = K(m_x’) + l_{m_x}(x) \}$, and both $m_{x_a}$ and $m_{x_b}$ are defined similarly. The <em>degree</em> of compositional structure is $K(m_x \mid m_{x_a}, m_{x_b})$.</p> <p><em>Note</em>: $l_{m_x}(x)$ represents the unstructured information in $x$ left unspecified by the model $m_x$. For instance, for the model class of computable probability distributions $l_{m_x}(x) = -\log_2 m_x(x)$.</p> </blockquote> <p>Once again, we need to emphasize that this definition only considers novel compositional structure in $x$ that wasn’t already present in $x_a$ or $x_b$ because it conditions on $m_{x_a}$ and $m_{x_b}$ — for instance, novel shared structure between two images, such as objects that appear in both but not in either individually. This definition does <em>not</em> preclude the possibility that the individual substrings $x_a$ and $x_b$ might themselves have compositional structure, and indeed this is an advantage. As we’ll show in <a href="#section-3--implications-and-use-cases-for-ai">section 3</a>, this helps us easily think about hierarchical compositionality at <em>different scales</em>, since we can simply consider further divisions of the substrings $x_a$ and $x_b$ themselves.</p> <hr/> <h1 id="section-3--implications-and-use-cases-for-ai">Section 3 — Implications and use-cases for AI</h1> <p>We’ve proposed a definition where compositionality is the emergence of novel structure shared between parts. While mathematically rigorous, is it useful? We believe this definition does more than just quantify; it offers a normative framework for understanding why current AI succeeds, how natural intelligence grows, and how we might architect the next generation of open-ended systems.</p> <h2 id="modeling-hierarchical-structure-in-the-real-world">Modeling hierarchical structure in the real world</h2> <p>Real-world data is rarely flat; it is compositional all the way down. Consider an image: simple edges form shapes, shapes form textures, textures form objects, and objects form scenes.</p> <p>Our definition captures this recursion naturally. If we split an image patch $x$ into smaller patches $x_a$ and $x_b$, we find $K(m_x \mid m_{x_a}, m_{x_b}) &gt; 0$ because the patches share low-level structure (like edges). But when we consider joining two of those larger patches themselves, we can <em>again</em> detect novel compositional structure because the new model will benefit from describing things like simple textures and shapes which reoccur among the two larger patches.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-compositionality/fig7-480.webp 480w,/2026/assets/img/2026-04-27-compositionality/fig7-800.webp 800w,/2026/assets/img/2026-04-27-compositionality/fig7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-compositionality/fig7.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: left; line-height: 1.5;"> <strong>Figure 7: Hierarchical compositional structure.</strong> In real compositional data, novel shared structure at larger scales is built off of other structure at smaller scales. </div> <p>We can keep playing this game of joining larger segments of data to detect compositionality at increasingly large scales. In the compositionality literature, people typically only consider shared structure between individual datapoints (e.g., by modeling “objects” that get reused among images). We just showed how our definition can account for compositionality at smaller scales, such as <em>within</em> an individual image, but it can also account for compositionality at larger scales that consider <em>sequences</em> of observations in a non-stationary data stream.</p> <p><strong>The alignment with deep learning</strong> This perspective offers a theoretical grounding for the success of deep learning. Neural networks essentially mirror the hierarchical structure of natural data.</p> <ul> <li><strong>Layer-wise isomorphism:</strong> Each layer in a deep net can be viewed as a model of the data at a specific scale.</li> <li><strong>Capacity for novelty:</strong> Successive layers do not merely copy information; they compute novel models of the data built off of interactions between lower-level models.</li> </ul> <p>This suggests that to model the world at increasingly large scales — moving from single images to long-form video or life-long streams — we may need architectures that can deepen unboundedly, continuously capturing new structure as it emerges.</p> <h2 id="the-time-dimension-open-endedness-and-data-curricula">The time dimension: open-endedness and data curricula</h2> <p>While standard machine learning focuses on <em>iid</em> datasets, humans are generally confronted with data that extends deep in time and grows in complexity. Knowledge in the real world isn’t just static; it <em>grows</em> compositionally. New concepts (like calculus) are not learned in isolation but are refactored on top of existing ones (like algebra).</p> <p>Our definition provides a rigorous test for whether a data curriculum is truly open-ended. Consider a data stream $x$ decomposed into history $x_a$ and a new observation $x_b$:</p> <ul> <li><strong>Redundancy ($m_x = m_{x_a}$):</strong> If $x_b$ adds no new patterns, learning stalls.</li> <li><strong>Disorder ($m_x = [m_{x_a}, m_{x_b}]$):</strong> If $x_b$ is unrelated to history (random noise or isolated facts about the world), the model effectively resorts to brute-force memorization.</li> <li><strong>Compositional growth ($K(m_x \mid m_{x_a}, m_{x_b}) &gt; 0$):</strong> This occurs only when the new observation $x_b$ shares structure with history $x_a$ in a way that forces the global model $m_x$ to evolve or refactor. Such data allows a world model to grow in a hierarchical way that efficiently leverages past experience, and allows new experiences to shape its understanding of old ones.</li> </ul> <blockquote class="notice--primary" style="font-size:0.9em !important"> <p><strong>Box: Fractured/Entangled vs. Unified/Factored representations</strong></p> <p>This perspective has implications for a fascinating new paper “Questioning Representational Optimism in Deep Learning: The Fractured Entangled Representation Hypothesis” by Kumar et al. <d-cite key="kumar_questioning_2025"></d-cite>, which found that neural representations which arise from open-ended curricula demonstrate significantly more compositional structure than those of models trained only on the final endpoints of these curricula.</p> <p>One conclusion the authors draw is that compression, while important, is not the only thing that a model should strive for — after all, the neural network they train on the final endpoint of a curriculum does a good job in terms of compression, but learns poor representations nonetheless. Our theory of compositionality suggests a different conclusion, however: optimal compression is <em>always</em> what we should strive for when modeling, but crucially a <em>curriculum might have significantly more compositional structure than its endpoint, and its optimal compression might therefore capture this compositional structure far better</em>.</p> <p>Take school curricula again, for instance. If our understanding of calculus builds off of prior concepts in algebra, we’ll be better able to jointly compress <em>both</em> through concept reuse. In contrast, if our only goal was to develop a compressed model of calculus in isolation, who knows what it would look like (imagine having learned calculus without knowing algebra first; you would certainly think of it in a very different way). There’s a crucial difference between compressing an entire curriculum versus compressing its endpoint alone. All of these interpretations of Kumar et al.’s fascinating findings are testable, and have important consequences for machine learning and the role of open-ended curricula.</p> </blockquote> <h2 id="visualizing-and-defining-intrinsic-structure">Visualizing and defining intrinsic structure</h2> <p>To move beyond abstract equations, we can map these concepts into <strong>compositional structure diagrams</strong>. These are tree-based visualizations where branches represent the merging of parts, and the height of a branch represents the quantity of novel structure $K(m_x \mid m_{x_a}, m_{x_b})$ emerging at that merge.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-compositionality/fig8-480.webp 480w,/2026/assets/img/2026-04-27-compositionality/fig8-800.webp 800w,/2026/assets/img/2026-04-27-compositionality/fig8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-compositionality/fig8.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption" style="text-align: left; line-height: 1.5;"> <strong>Figure 8: Compositional structure diagrams.</strong> A diagram that shows how compositional structure emerges at different scales in an object, according to a recursive decomposition (legend in the top left of the figure). A given node represents a substring of the entire object, and branches represent substrings merging to form a larger string. The height of a branch above a merge point represents the amount of novel compositional structure that emerges from that combination of substrings (heights of branches below a merge point do not represent anything, and just vary to improve spacing in the diagram). Note: in order to keep the diagram clean, we’re not showing the intermediate binary joins that don’t generate significant novel structure. </div> <p>This kind of visualization of our definition shows us how larger structures are built off of novel structure shared at smaller scales — like how concepts build off of others — and it shows us how this happens recursively through the object. There’s a nice analogy to be made to phylogenetic trees in biology, but whereas phylogenetic trees show a plausible story of how genetic structure diverges over time and splits off into new species, compositional structure diagrams show a plausible story of how structures merge to form novel ones (which, according to a recent theory of evolution called symbiogenesis <d-cite key="arcas_what_2025"></d-cite>, might actually be how we should think of phylogenetic histories in the first place).</p> <p><strong>The “maximal tree” hypothesis</strong> A central question remains: <em>which</em> hierarchical decomposition is the “correct” one? While the answer may depend on the domain, we propose that the <strong>maximal tree</strong> decomposition — the one that maximizes the sum of novel structure at every node — is the most scientifically interesting candidate for an object’s intrinsic structure.</p> <p>We favor the maximal tree $T_{max}(x)$ for several reasons:</p> <ul> <li><strong>Intuition and non-triviality:</strong> It inherently groups parts that maximize shared structure, avoiding arbitrary cuts. In contrast, if we were looking at <em>minimal</em> trees, we would be joining objects that are either structurally identical or algorithmically independent.</li> <li><strong>Different from sophistication:</strong> There exists objects with the same sophistication $K(m_x)$ but <em>different</em> sums of compositionality terms under the maximal tree decomposition. This means that when we consider the maximal tree, our measure of compositionality does not simply reduce to sophistication — it quantifies a <em>kind</em> of structure rather than just the total amount of structure. The opposite is true for <em>minimal</em> trees: there is always a trivial solution that merges every individual bit of a string in one single merge, resulting in a sum of compositionality terms equal to $K(m_x)$.</li> <li><strong>Consistency:</strong> It has a kind of recursive consistent: compositional substrings retain their structure under the maximal tree when embedded in larger compositional objects.</li> </ul> <h2 id="practical-use-estimation-and-design">Practical use: estimation and design</h2> <p>Finally, we must address the elephant in the room: Kolmogorov complexity is uncomputable. Does this render the definition useless? Absolutely not.</p> <ol> <li><strong>Estimation via compression:</strong> Just as we upper-bound Kolmogorov complexity with concrete compression algorithms like ZIP or PAQ, we can estimate compositionality using standard learning algorithms and compressors.</li> <li><strong>A normative north star:</strong> Even without precise measurement, the definition guides design. It shifts the focus from purely architectural inductive biases toward <strong>data-centric compositionality</strong>. It suggests that the bottleneck to general AI isn’t just how we learn from data, but ensuring we collect data streams rich enough to satisfy $K(m_x \mid m_{x_a}, m_{x_b}) &gt; 0$ continuously over time.</li> </ol> <hr/> <h1 id="conclusion">Conclusion</h1> <p>Our work on formally defining compositionality did not start out of nowhere: it arose for a particular reason. We were interested in designing models that can flexibly generalize like humans do when we think, dynamically composing concepts in order to adapt to novel situations. The further we got into these projects, however, the more we felt like we no longer understood the original goal, unable to explain what the scientific problems were or why the approaches we were pursuing would work. Eventually, we started to realize that we take compositionality for granted, using it in our vocabulary for talking about AI without having any real clue as to what it means.</p> <p>This work has filled that conceptual gap for us, and given us renewed clarity on our research and scientific interests. We hope that it can serve a similar purpose for others in the field who are reading this blog post and interested in compositionality. However, like all scientific formalisms, we think that the real significance of these ideas lies in the serendipitous directions they may lead future research. For us, having a new way to think and talk about compositionality more precisely has opened the flood gates: we see new research ideas, applications, and connections to other fields everywhere we look. We’ve tried to highlight some of these here, but our biggest hope in writing this blog post is that it has a similar stimulating effect for others, and that research into compositionality benefits from a new pool of creative ideas.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Compositionality is thought to be crucial in human cognition and AI, but we lack a scientific understanding of what it is. What kind of data is compositionally structured? Can we mathematically quantify the amount and character of compositional structure? This blog post introduces a novel approach for doing so, building off of existing tools from algorithmic information theory that formalize notions of complexity and structure. The mathematical definition of compositionality that we'll come to is rigorous, precise, and general, and the hope is that it can inspire novel research directions in AI for uncovering compositional structure in natural data.]]></summary></entry><entry><title type="html">Content Promotion as a Strategic Game: How to Design Agentic Publishers for the Evolving Search Ecosystem in the GenAI Era?</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/content-promotion-agent-design/" rel="alternate" type="text/html" title="Content Promotion as a Strategic Game: How to Design Agentic Publishers for the Evolving Search Ecosystem in the GenAI Era?"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/content-promotion-agent-design</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/content-promotion-agent-design/"><![CDATA[<h2 id="tldr">TL;DR</h2> <p>LLMs are transforming search from static ranked lists of results into interactive, chat-like experiences. LLMs also reshape the way publishers (i.e., content creators) generate content: they increasingly use LLMs and agentic tools to optimize rankings in traditional search and to improve their content visibility in the novel chat-like systems. Importantly, publishers engage in an iterative process: they observe how their content performs — whether it is highly ranked in the traditional search or visible in chat-like answers — and then strategically modify or regenerate their content in response to this feedback. As a result, the ecosystem becomes a strategic game with incomplete information, evolving feedback, and repeated interactions. In this strategic ecosystem, the publishers act as players striving to promote their content, competing for rankings and visibility under mechanisms that are only partially known to them. This blog outlines why game theory offers a natural framework to model these settings and to design agents acting on behalf of publishers. We highlight several frameworks such as Game theory and Multi-agent Reinforcement learning and show how they contribute to model and design effective agents. Our aim is to provide principled lens for content promotion in the LLM era.</p> <h2 id="llms-reshape-both-sides-of-the-search-ecosystem">LLMs Reshape Both Sides of the Search Ecosystem</h2> <p>Large language models have fundamentally reshaped the search experience <d-cite key="metzler_rethinking_2021"></d-cite>. While users once typed short keyword queries and scanned a ranked list of links, modern systems now offer chat-like interfaces that feel more like interacting with an assistant than browsing a directory. These systems provide conversational question-answering (QA) capabilities by generating responses synthesized from multiple sources. Search is no longer about “links” — it is about responses. In the service of providing answers to questions, paradigms such as Retrieval Augmented Generate (RAG) <d-cite key="gao_retrieval_augmented_2024"></d-cite> have been developed. Similar paradigms were suggested for integrate commercial content in the answer <d-cite key="feizi_online_2024, mordo_sponsored_2024"></d-cite>. The new chat-like setting introduces new challenges, such as providing users with pointers to sources of information, leading to techniques known as source attribution <d-cite key="bohnet_attributed_2023"></d-cite>. Interestingly, effective retrieval has become a necessity in this new setting: augmenting an LLM with the ability to retrieve relevant information during answer generation or to retrieve the most relevant source to attribute to related pieces of information.</p> <p>But the transformation is not limited to the user side of the ecosystem.</p> <p>LLMs are also reshaping the publisher side. In the traditional Web era, publishers focused on keyword and hyperlink optimization, and content structure to influence ranking <d-cite key="gyongyi_web_2005"></d-cite>. Today, publishers increasingly rely on LLMs:</p> <ul> <li>to generate or refine content;</li> <li>to adapt their content to the behavior of LLM-driven retrieval and QA systems;</li> <li>to strategically shape their content so that they are selected, quoted, or incorporated into the ranked list of results or the generated answer.</li> </ul> <p>The paradigm of helping publishers in improving their content visibility in generative engine responses is often called Generative Engine Optimization (GEO) <d-cite key="aggarwal_geo_2024"></d-cite>.</p> <h2 id="publisher-incentives-across-coexisting-ecosystems">Publisher Incentives Across Coexisting Ecosystems</h2> <p>The search ecosystem is not transitioning overnight — it is evolving. Today, publishers operate in a hybrid world where traditional search engines and LLM-driven chat interfaces coexist. Users still issue keyword queries and click ranked results, yet they also increasingly engage with conversational systems that generate responses. As a result, publishers must navigate two overlapping incentive structures at once.</p> <h3 id="publishers-incentives-in-the-traditional-setting">Publishers’ Incentives in the Traditional Setting</h3> <p>In the traditional setting, publisher goals are well-defined and widely understood. Visibility depends on ranking, which in turn hinges on factors, some of which are familiar:</p> <ul> <li>content relevance;</li> <li>content quality and structure;</li> <li>signals (e.g., PageRank <d-cite key="page_pagerank_1999"></d-cite>).</li> </ul> <p>Metrics such as impressions and CTR (Click Through Rate) offer interpretable objectives.</p> <h3 id="publishers-incentives-in-the-chat-like-settings">Publishers’ Incentives in the Chat-Like Settings</h3> <p>By contrast, the objectives in the chat-like settings are far less defined. Publishers know that visibility is tied to whether the chat-like setting:</p> <ul> <li>retrieves their content;</li> <li>cites or incorporates it into the generated response;</li> <li>and positions it prominently in the generated response.</li> </ul> <p>But unlike in traditional search, where there is a clear notion of a ranked results page, LLM-based systems provide no public ranking, and often little transparency about how sources are selected for retrieval or attribution.</p> <p>The publisher’s utility is therefore inherently vague: Is success defined by:</p> <ul> <li>citation count?</li> <li>contribution to the generated answer?</li> <li>similarity of the generated answer to the publisher’s content?</li> <li>some combination of the above?</li> </ul> <p>The field has not yet converged on a clear success criterion, and the objective itself is still ill-defined, making strategic optimization an open problem.</p> <h2 id="leveraging-llms-for-content-optimization">Leveraging LLMs for Content Optimization</h2> <p>Publishers — or agents acting on behalf of them — can leverage LLMs as powerful content optimization tools (See Figures <a href="#fig1">1</a> and <a href="#fig2">2</a>). The publishers are incentivized to have effective automatic generation to promote their content. However, any such agentic systems should be carefully designed:</p> <ul> <li>address the tradeoff between “content promotion” (e.g., being selected in the search results or being visible in the generated response) and “faithfulness” to the original intent (e.g., not deviating from the content before optimization);</li> <li>address the uncertainty about the ranker or the content selection/generation mechanism;</li> <li>address competing agents which aim to optimize contents of other stakeholders.</li> </ul> <div class="row mt-3"> <div class="col-sm-12" id="fig1"> <figure> <img src="/2026/assets/img/2026-04-27-content-promotion-agent-design/banana-search.jpg" class="img-fluid rounded z-depth-1" alt="Content optimization in a traditional search setting"/> <figcaption class="caption" style="color: inherit;"> <strong>Figure 1:</strong> Content optimization in a traditional search setting. </figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm-12" id="fig2"> <figure> <img src="/2026/assets/img/2026-04-27-content-promotion-agent-design/banana-chat-fixed-with-gpt.png" class="img-fluid rounded z-depth-1" alt="Content optimization in a chat-like setting"/> <figcaption class="caption" style="color: inherit;"> <strong>Figure 2:</strong> Content optimization in a chat-like setting. </figcaption> </figure> </div> </div> <h2 id="content-promotion-as-a-game">Content Promotion as a Game</h2> <p>We argue that game theory provides a major and essential framework for analyzing the resulting competitive dynamics and for designing agents that assist publishers to strategically promote their content. More precisely, repeated games with incomplete information <d-cite key="aumann_repeated_1995"></d-cite>, induced by the system (search or chat-like system). Actions correspond to content generation (or modification). Strategies are history dependent content selection/generation algorithms and strategy profiles capture joint activity of agents which are active in the ecosystem. The utility function of an agent is the accumulated payoff along time, taking into account aspects such as ranking (for traditional search), visibility (for the chat-like systems) and faithfulness to the original content before any modification. As the selection/generation mechanism is only partially known, the setting is of incomplete information. Since the publisher may keep improving the content once she receives feedback from the system, we argue that the game should be treated as a repeated game. Figures <a href="#fig3">3</a> and <a href="#fig4">4</a> present schematic illustrations of the resulting games for search systems and chat-like systems, respectively.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="fig3"> <figure> <img src="/2026/assets/img/2026-04-27-content-promotion-agent-design/search-schema-nbg.png" class="img-fluid rounded z-depth-1" alt="Schematic illustration of a traditional search game"/> <figcaption class="caption text-center" style="color: inherit;"> <strong>Figure 3:</strong> Schematic illustration of a traditional search game. </figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0" id="fig4"> <figure> <img src="/2026/assets/img/2026-04-27-content-promotion-agent-design/chat-schema-nbg.png" class="img-fluid rounded z-depth-1" alt="Schematic illustration of a chat-like game"/> <figcaption class="caption text-center" style="color: inherit;"> <strong>Figure 4:</strong> Schematic illustration of a chat-like game. </figcaption> </figure> </div> </div> <p>The viewpoint just presented is aligned with a growing body of research that treats content promotion as a strategic process. During recent years, several works have applied game-theoretic approaches to model competitive search <d-cite key="kurland_competitive_2022"></d-cite>, diversity-based ranking <d-cite key="mordo_ameliorating_2025"></d-cite>, to study the convergence of long term behavior to equilibrium in retrieval settings <d-cite key="madmon_search_2024, madmon_convergence_2024"></d-cite> and to design equilibrium in ranking games using corpus enrichment <d-cite key="nachimovsky_power_2025"></d-cite>. <d-cite key="ye_llm_empowered_2025"></d-cite> explored strategic document modifications in response to ranking feedback, revealing rich and sometimes counterintuitive dynamics. Yao et al.’s work provides deep, game-theoretic insight into how creators behave strategically in recommender systems and offers actionable designs for systems to align creator incentives with user welfare <d-cite key="yao_how_2023, yao_user_2024, yao_human_2024"></d-cite>. In the context of strategic retrieval systems, <d-cite key="mordo_sponsored_2024"></d-cite> presented the first formal game-theoretic analysis of a sponsored QA system where advertisers bid to have their sponsored content fused in the generated answer of question-answering systems. <d-cite key="mordo_sponsored_2024"></d-cite> showed how this new ad-integration model introduces strategic bidding behavior and derived equilibrium properties and welfare implications in this setting.</p> <p>However, the technological transformation of the last few years — particularly the rise of LLM-based chat interfaces — calls for re-examination and adaptation of the assumptions in classical game theoretic models. Traditional work often assumes fixed ranking functions and explicit publishers’ incentives. In contrast, chat-like systems generate personalized and dynamic answers <d-cite key="metzler_rethinking_2021"></d-cite>; the objective of publishers becomes much harder to define. These shifts motivate the development of new models that account for generative selection mechanisms, opaque feedback, and high adaptivity — all of which strengthen the case for game-theoretic reasoning and for new solution concepts.</p> <h2 id="agent-design-aspects-for-traditional-search-and-chat-like-systems">Agent Design Aspects for Traditional Search and Chat-like Systems</h2> <p>We delve into three important aspects that should be considered when designing agents that assist publishers promote their content.</p> <p>The first ingredient that can (and will) be naturally used, is to exploit the significant power of LLMs, e.g., use clever prompt engineering to promote content and maintain faithfulness in such games.</p> <p>A second ingredient one should be aware of is incomplete information about the selection or generation mechanism applied by the system. Using, for example, information about the decision made by the mechanism (e.g., past responses) can help to train and fine tune agents.</p> <p>The third ingredient is the need to be aware of competitors’ behavior. With limited access to annotated data — and only partial observability of the decisions made by these systems — this calls for carefully designed simulation tools. here again LLMs can be used as effective generative tools.</p> <p>Heretofore, we have described content promotion as a publishers game, shaped by evolving search and chat-based ecosystems and by the increasing role of generative AI. We now discuss different frameworks that can be employed to model and design effective agents in such settings.</p> <h2 id="frameworks">Frameworks</h2> <h3 id="multi-agent-reinforcement-learning-marl">Multi-Agent Reinforcement Learning (MARL)</h3> <p>This framework models each player as an autonomous learning agent that optimizes its policy through repeated interaction with the environment and with other agents. MARL enables us to study how strategic behaviors emerge end-to-end from experience, without imposing strong assumptions about how these systems (i.e., search or chat-like systems) operate internally <d-cite key="gemp_developing_2022, guo_large_2024, shoham_if_2025, zhou_multi_agent_2025"></d-cite>. It also allows us to analyze coordination, competition, and non- stationarity — core properties of competitive games under a principled RL formulation. In our setting, the policy of the agent is the LLM itself, and reinforcement learning is the mechanism that aligns this LLM to the objectives of the game, given whatever information the agent can observe; different RL alignment algorithms may be adopted; e.g., <d-cite key="schulman_proximal_2017, rafailov_direct_2024, calandriello_multi-turn_2024, zhou_wpo_2024, shao_deepseekmath_2024"></d-cite>. If we had access to a full log of games — for example, historical queries and the ranking of pages in traditional search, or user questions and the resulting responses in chat-like systems — then we could directly formulate a supervised or RL-based learning problem. With such logs, the agent could be trained to optimize performance with respect to well-defined metrics (e.g., rank position, click-through, user engagement, rejection rates, etc.). Beyond the standard trade-offs encountered in reinforcement learning, our settings introduce unique design tensions that must be addressed:</p> <h4 id="a-short-term-wins-vs-long-term-information-leakage">(a) Short-term wins vs. long-term information leakage</h4> <p>An agent may be able to win an individual round of a game by aggressively optimizing its content. However, doing so may reveal its strategy to competing agents, enabling them to quickly imitate <d-cite key="raifer_information_2017"></d-cite> or counter it. Since the objective is to maximize cumulative reward across many rounds, the agent must balance immediate gains with strategic opacity — sometimes deliberately avoiding overly distinctive or extreme actions to prevent being copied <d-cite key="aumann_repeated_1995"></d-cite>.</p> <h4 id="b-on-policy-vs-off-policy-training">(b) On-policy vs. off-policy training</h4> <p>A crucial design question is whether the agent should continue learning during the game (on-policy) or rely solely on offline training before the game begins (off-policy).</p> <ul> <li><strong>Online adaptation</strong> allows the agent to track shifting competitor behaviors or evolving ranking/generation mechanisms.</li> <li><strong>Offline training</strong> offers stability and avoids overfitting to short-term noise or adversarial manipulations. Choosing between online and offline modes — or combining them — depends on the environment’s stability, observability, and the computational budget during gameplay.</li> </ul> <h4 id="c-constructing-the-loss-function">(c) Constructing the loss function</h4> <p>A core challenge is designing a loss function that balances ranking promotion or content visibility with faithfulness to the original content. On one hand, the agent should modify content to improve performance; on the other hand, excessive deviation risks harming credibility, violating constraints, or triggering penalties. The loss function must encode this balance explicitly — penalizing harmful drift while incentivizing strategic, beneficial content modifications.</p> <h3 id="synthetic-dataset-generation">Synthetic Dataset Generation</h3> <p>Sometimes real-world data presents substantial limitations: it may be inaccessible, noisy, incomplete, or simply ill-suited for the specific questions we wish to study. For this reason, we propose constructing synthetic datasets with controlled statistical properties. Synthetic data enables us to systematically probe targeted hypotheses — for example, how diversity constraints, noise structure, or shifts in the query or question distribution influence agent behavior and system-level outcomes. Moreover, synthetic datasets provide repeatability and experimental control that are often impossible with logs of production systems. They allow for clean ablation studies, controlled perturbations, and the isolation of causal factors, all of which are essential for understanding and improving agents operating in competitive ranking and chat-like ecosystems. Importantly LLMs now make it feasible to generate high-fidelity synthetic datasets at scale, enabling richer and more flexible experimental environments than were previously possible.</p> <h3 id="simulation-environment">Simulation Environment</h3> <p>A simulation environment that models the rules of the game, the ranking or generation function, and the interaction protocol between agents is essential in the discussed strategic ecosystems <d-cite key="kovarik_game_2024"></d-cite>. Such simulation can be used for generating strategic synthetic training datasets and for evaluating agents under diverse conditions. Together with the increasing proliferation over the Web of generated AI content <d-cite key="spennemann_delving_2025"></d-cite>, we suggest using simulators of the discussed competitions. Such simulators support rapid iteration and controlled experimentation across a wide range of configurations — rankers, prompts, datasets, reward structures, and hyperparameters. By isolating specific components of the system, the simulation environment helps uncover causal relationships, identify sensitivities to design choices, and systematically analyze how agents behave under different competitive scenarios.</p> <h3 id="self-play">Self-Play</h3> <p>Self-play is a method for iteratively training agents to be improved by competing (via simulation) against evolving versions of themselves <d-cite key="fang_serl_2025, fu_improving_2023, zhang_survey_2025"></d-cite>. This approach is particularly useful in settings where optimal strategies are not known a priori and where the competitive landscape shifts over time. Self-play helps uncover equilibrium behaviors, stability properties, and potential failure modes of learning algorithms in adversarial or co-adaptive settings.</p> <h3 id="meta-games-and-learning-equilibrium">Meta Games and Learning Equilibrium</h3> <p>Agent design itself introduces an additional strategic layer: designers may choose between different LLM architectures, training procedures, datasets, prompting strategies, or RL frameworks. In this broader perspective, agents are not only competing within a single ranking or generation game, but the designers are competing in a meta-game over agent designs <d-cite key="li_meta_game_2024, nachimovsky_multi_agent_2025, shapira_glee_2025"></d-cite>. Long-run success therefore depends on performing well not only within the fixed game, but also across the distribution of possible design choices made by other participants. In such meta-games, no single deterministic design may be universally optimal.</p> <p>Instead, probabilistic ensembles of designs — mixtures over algorithms, prompting strategies, or training regimes — may provide better robustness and theoretical guarantees. These ensemble-based strategies reduce exploitability, adapt better to heterogeneous opponents, and align with classical results in game theory showing that mixed strategies often outperform pure ones in competitive environments.</p> <p>Within meta-games over agent designs, a natural solution concept is learning equilibrium <d-cite key="monderer_learning_2007"></d-cite>. Rather than focusing on equilibria of the underlying content-generation game — where actions correspond to the content produced — learning equilibrium concerns the design algorithms themselves being in equilibrium. That is, each designer’s strategy for constructing agents is optimal given the strategies of other designers, forming a stable configuration at the meta-level.</p> <p>This distinction is subtle but important: learning equilibrium ensures that agent designs collectively converge to a stable distribution, even as the content-generation game continues to evolve. Such a concept becomes particularly relevant in scenarios where a platform or cloud provider offers content-generation services to multiple competing parties. In this setting, providing agent designs that satisfy learning equilibrium naturally mitigates exploitability, promotes fairness, and guarantees robust performance across heterogeneous competitors.</p> <h2 id="preliminary-results">Preliminary Results</h2> <p>We now turn to discuss several papers that illustrate components (or the absence thereof) of the frameworks discussed above.</p> <h3 id="prompt-based-agent">Prompt-based Agent</h3> <p> <d-cite key="bardas_automatic_2025"></d-cite> introduced a set of prompt-based agents designed to modify a human-authored document so that the updated version is ranked higher in the next round of the ranking. Their prompting strategy explicitly constrains the agent to produce revisions that remain faithful — at least to some extent — to the original document. <d-cite key="bardas_automatic_2025"></d-cite> introduced prompt- engineering techniques that, in an offline evaluation, outperformed human-authored documents without requiring any learning. Their evaluation was conducted using data from a ranking competition previously organized among human document authors.</p> <h3 id="rlrf-based-agent">RLRF-based Agent</h3> <p> <d-cite key="mordo_rlrf_2025"></d-cite> proposed a ranker-aligned agent, trained via a reinforcement- learning-from-ranker-feedback (RLRF) framework. They constructed a synthetic dataset generated through a simulated ranking competition and used it to train the agent with the DPO algorithm <d-cite key="rafailov_direct_2024"></d-cite>. Notably, the resulting agent learned to adapt both to the ranking mechanism and to the behavior of competing agents. Their simulation-based evaluation demonstrates robustness with respect to various environmental conditions, including the number of competitors and the choice of ranking function. The simulation assumed that the competing agents are nonoptimized agents; this setup can be viewed as a variant of self-play, where agents iteratively interact under stable, nonlearning strategies.</p> <h3 id="lemss">LEMSS</h3> <p> <d-cite key="mordo_lemss_2025" block=""></d-cite> also introduced LEMSS, a large-scale simulation framework for ranking competitions among LLM-based agents. Their results show that LLM-driven agents exhibit behavioral patterns like those of humans in iterative ranking games. LEMSS provides fine-grained control over nearly all aspects of the competition — such as the number of competitors, the ranking mechanism, and the reward structure — and enables comprehensive analysis of the induced repeated game. The simulation models a traditional ranking competition. To design agents capable of promoting content in a chat-like interaction setting, a simulation of such a mechanism is called for.</p> <h3 id="attacking-rag-based-systems">Attacking RAG-based systems</h3> <p>Recent work highlights the growing vulnerability of retrieval-augmented generation (RAG) systems to targeted manipulation. PoisonedRAG <d-cite key="zou_poisonedrag_2024"></d-cite> shows that inserting only a few malicious documents into a RAG system’s knowledge base can steer model outputs with high success, revealing a potent “knowledge-corruption” attack surface. In contrast, RIPRAG <d-cite key="xi_riprag_2025"></d-cite> use reinforcement learning to craft effective poisoned documents without access to the system’s internals. <d-cite key="wu_what_2025"></d-cite> suggests to cooperatively optimize content for generative search engines by learning their preference rules. However, all these papers overlook the competitive nature of real-world environments, where multiple strategic content creators adaptively improve their content — potentially using agents — in a repeated game. This missing game-theoretic perspective limits their ability to capture the dynamics and equilibria that emerge when many actors simultaneously attempt to influence RAG- based systems.</p> <h2 id="tools">Tools</h2> <p>Designing agents for ranking and content promotion competition benefits from a growing ecosystem of LLM-oriented development tools. Copilot Studio, Vertex AI, and LangChain provide the infrastructure needed to orchestrate complex LLM workflows, connect models to external data, and manage iterative agent-environment interactions. Copilot Studio enables rapid prototyping of agent behaviors within controlled interfaces, while Vertex AI offers scalable model hosting, fine-tuning, and evaluation pipelines suited for experimentation at scale. LangChain adds modular abstractions — such as memory, tools, retrieval, and multi- step reasoning — that make it easier to construct agents capable of interacting with dynamic environments. In addition, there are many python packages that may help to design and simulate RL-based agents such as <d-cite key="feng_agile_2024, liu_agentlite_2024"></d-cite>.</p> <h2 id="conclusion">Conclusion</h2> <p>In this blog post, we discussed how LLMs are reshaping the search ecosystem — both in traditional search and in emerging chat-like question-answering systems. Beyond transforming how users search, LLMs also change how publishers operate, enabling them to leverage LLMs to strategically optimize and promote iteratively their content.</p> <p>We discussed several frameworks for modelling and designing such agents, including game- theoretic models and reinforcement learning approaches. These frameworks help account for multiple layers of uncertainty: the opaque ranking model or response-generation mechanisms of retrieval systems, the dynamic nature of evolving user queries/questions, and the competitive behavior of other strategic publishers or human content creators.</p> <p>To build robust and adaptable agents, we emphasized that simulation plays a crucial role. Simulations enable the creation of strategic synthetic datasets and provide controlled environments for evaluating agent behavior, stress-testing policy robustness, and studying long-term dynamics in repeated competitive scenarios.</p> <p>Overall, as the ecosystem continues to evolve, combining insights from game theory, reinforcement learning, and simulation offers a principled path toward designing agents capable of succeeding in both current and future search paradigms.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[With the rise of LLMs, publishers now operate in a dual world where traditional search and chat-like systems coexist. We propose a unified, game-theoretic view of this environment and highlight different tools, such as Multi-Agent Reinforcement Learning, that support the development of competitive content-optimization agents.]]></summary></entry></feed>