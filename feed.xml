<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://iclr-blogposts.github.io/2026/feed.xml" rel="self" type="application/atom+xml"/><link href="https://iclr-blogposts.github.io/2026/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-02T13:33:33+00:00</updated><id>https://iclr-blogposts.github.io/2026/feed.xml</id><title type="html">ICLR Blogposts 2026</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/fans/" rel="alternate" type="text/html" title="FANS - Frequency-Adaptive Noise Shaping for Diffusion Models"/><published>2026-11-30T00:00:00+00:00</published><updated>2026-11-30T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/fans</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/fans/"><![CDATA[<h2 id="motivations">Motivations</h2> <p>Diffusion models (DDPM) have achived state of the art performances on data modalities like Stable diffusion for images, Sora for videos, RFdiffusion for protiens and Mattergen for materials <d-cite key="Rombach2021HighResolutionIS,Qin2024WorldSimBenchTV,josephundefined,Zeni2023MatterGenAG"></d-cite>. They work by learning to reverse a gradual noising process. In a <strong>Forward Pass</strong> the data modality is progressively corrupted with gaussian noise until it becomes complete noise. In the <strong>Reverse process</strong> the model learns a denoiser which start from a gaussian noise and denoise this step-by-step to get an apprxomately clean sample. While this framework has proven to be quite effective, the existing works treats the noise as spatially uniform gaussian noise \(\mathcal{N}(\mu, \sigma^2)\).</p> <p>The modalities mentioned above have one thing in common. They exibits <strong>power law</strong> in their Fouries representation. That is, the low-frequency components have orders of magnitude higher variance than high-frequency components <d-cite key="vanHateren1992"></d-cite>. To interpret the fourier component through visual representation, we can say low frequencies capture the global structure while the high frequency encodes the finer details. On viewing diffusion though the fouries lens of the data, it has an important implications: The DDPM forward process noises high-frequency components substantially earlier and faster as compared to the low frequency components. This can be attributed to the fact that Gaussian noise is applied irrespective of the data’s spectral content as we show later.</p> <p>Since the introduced Gaussian noise is agnostic to the data’s spectral characteristics, the forward process thereby imposes a hierarchy of frequencies during generation. As the higher frequencies are noised earler and faster in the forward process, and DDPM learns to reverse this forward process: during backward process the higher frequencies are generated later conditioned on the forward process as we see in Figure 3. Previous works has also observed this phenomenon of imposed hierarchy of frequencies during generation<d-cite key="falck2025fourierspaceperspectivediffusion"></d-cite></p> <p>Images aren’t spectrally flat. Natural images concentrate most of their power in lower frequencies following characteristic power-law distributions, while images from domains like Astronomy, Texture Design have varied concentration of pwer across the frequency bands. This raises a fundamental question: if datasets have inherent frequency biases and the denoising trajectory naturally progresses from coarse-to-fine structure, why do we apply the same uniform white noise at every timestep and across all frequencies? How does this same uniform white noise across all frequencies perform across images form varied domain? Our research asks whether we can explicitly shape the noise spectrum to (i) match each dataset’s actual frequency distribution and (ii) implement a principled time-frequency annealing schedule—shifting from low to high frequencies as denoising progresses—to improve sample quality and training stability without modifying the underlying UNet architecture or DDPM objective.</p> <h2 id="quick-overview">Quick Overview</h2> <p>Let’s start with a quick overview of the two frameworks we will be working with: Diffusion models and Frequency domain of Images.</p> <h3 id="diffusion-models">Diffusion Models</h3> <p>Diffusion models generate data by learning to reverse a gradual noising process. The core idea is elegantly simple: start with real data and progressively corrupt it by adding Gaussian noise over many steps until it becomes pure random noise. This forward process requires no learning and follows a predefined schedule. A neural network then learns to invert this process, starting from random noise and iteratively denoising it to recover clean, structured data. This reverse process is governed by stochastic differential equations (SDEs) that describe how probability distributions evolve over time, with the model learning to approximate the score function which guides the denoising steps.</p> <h3 id="forward-process">Forward Process</h3> <p>It defines a markov chain that progressively add gaussian noise to the data sample \(\mathbf{x}_0 \sim q(\mathbf{x}_0)\) over \(T\) timesteps. Each timestep adds a slight noise to the data resulting in a increasingly noisy samples \(\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_T\), where $\mathbf{x}_T$ approximates an isotropic Gaussian distribution. The forward process is formally defined as :</p> \[q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \, \mathbf{x}_{t-1}, \beta_t \mathbf{I}),\] <p>where $\beta_t \in (0,1)$ controls the amount of noise added to the data.</p> <p>By applying the reparameterization reccursively we obtain the closed form experssion</p> \[q(\mathbf{x}_t | \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \, \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I}),\] <p>where \(\alpha_t = 1 - \beta_t\) and \(\bar{\alpha}_t = \prod_{s=1}^t \alpha_s\).</p> <p>Thus the noisy data at any given time \(t\) in the forward proccess</p> \[\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{(1 - \bar{\alpha}_t)}\epsilon\] <p>where \(\epsilon \in \mathcal{N}(0,\mathbf{I})\).</p> <p>A useful notation in this is the log signal-to-noise ratio \(\lambda_t = log(\bar{\alpha}_t / (1 - \bar{\alpha}_t))\) which increases monotonically from 0 ( clean data) to 1 (noise)</p> <h3 id="reverse-process">Reverse Process</h3> <p>The Reverse process tries to invert the forward process, transforming Gaussian noise \(\mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\) back into a data sample resembling the data distribution \(q(\mathbf{x}_0)\). Since the true reverse transitions \(q(\mathbf{x}_{t-1} | \mathbf{x}_t)\) are intractable, a parameterized model \(p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t)\) is trained to approximate them.</p> <p>The reverse process is modeled as:</p> \[p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t)),\] <p>where \(\boldsymbol{\mu}_\theta\) and \(\boldsymbol{\Sigma}_\theta\) are outputs of a neural network conditioned on \(\mathbf{x}_t\) and the timestep \(t\). The model learns to predict the mean of the denoised sample at each step.</p> <p>Using the forward process derivation, the true mean of \(q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0)\) can be expressed as:</p> \[\boldsymbol{\mu}_q(\mathbf{x}_t, \mathbf{x}_0) = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon} \right),\] <p>where \(\boldsymbol{\epsilon}\) denotes the Gaussian noise added at timestep \(t\).</p> <p>On way of learning is during training the model is optimized to predict this noise directly using a loss of the form:</p> \[L(\theta) = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} \left[ \left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta( \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}, t ) \right\|^2 \right],\] <p>which corresponds to a reweighted variational bound on the data likelihood.</p> <h3 id="fouries-domain">Fouries Domain</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/image_fft-480.webp 480w,/2026/assets/img/2026-11-25-fans/image_fft-800.webp 800w,/2026/assets/img/2026-11-25-fans/image_fft-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/image_fft.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/power_law-480.webp 480w,/2026/assets/img/2026-11-25-fans/power_law-800.webp 800w,/2026/assets/img/2026-11-25-fans/power_law-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/power_law.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure2: First two image shows an image in spatial, in frequency domain. The plot is the log-log plot of Power spectral density and Frequency of the image. </div> <p>Natural Images have rich structures in the frequency domain that is obscured in the pixel space. Understanding this frequency domain perspective is crucial for our investigation.</p> <p>Let’s say we have an image \(x \in \mathbf{R}_{H \times W\times C}\). Its pixels are coeffcient of a standard basis \(\mathcal{B} = \{ \mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_n \} \subset \mathbb{R}^n\). In the spatial domain, the image is defined as a linear combination of standard basis vectors, where each pixel represents a local real-valued intensity. When we apply the Discrete Fourier Transform(DFT) to this image, it performs unitary change of basis giving us the frequency domain equivalent of the image. The dimension of the fourier and the pixel image remains the same but in fourier domain the coefficients of the basis becomes complex valued.</p> <p>For each channel of an image the DFT can be formulated as</p> \[F(u, v) = \sum_{x=0}^{H-1} \sum_{y=0}^{W-1} f(x, y) e^{-j 2\pi \left( \frac{ux}{H} + \frac{vy}{W} \right)}\] <p>where \((u,v)\) aer spatial frequency indices. The radial frequency \(f = \sqrt{u^2 + v^2}\) measure the distance from the DC ( zero frequency component)</p> <p>Now, let’s sort the Fourier coeeficient \(f(u,v)\) from low to high frequency. To do so, we start from the center of our Fourier Space and walk in spirals. That is, sort them using the Manhatten distancen of the indices \((uv,)\) from the center (0,0) of the Fourier representation. From the Figure 2 It’s very evident that signal variance decreases rapidly with increasing frquency. <d-cite key="dielman"></d-cite> shows that similar trend is visible in other doains (Videos, Audio, Proteins) as well.</p> <p><strong>Power Spectral Density(PSD)</strong>: This quantifies how the signal enery is distributed across frequencies. For an image with Fourier Transform \(F(u,v)\), PSD is defined as :</p> \[P(u,v) = |F(u,v)|^2\] <p>representing the power at each frequency component. To get one dimensional spectral profile we compute the radially-average PSD by integrating over annular regions at constant radial frequency \(f\) (simply put we compute average power in ring-shaped zones moving outward from the center). Formally put</p> \[P(f) = \frac{1}{|B_f|} \sum{}_{(u,v) \in B_f} |F(u,v)|^2\] <p>Where \(B_f\) is the radial frequency band defined as \(B_f = {(u,v): f \leq \sqrt{u^2+v^2} \le f+ \delta f}\) and \(\|B_f\|\) is the number of frequencies in that band.</p> <p>The PSD reveals how much each frequency contributes to the overall signal. For a dataset of images, we compute per-image PSDs and average them to obtain a dataset-level spectral profile. This aggregate PSD characterizes the typical frequency distribution of the data and captures domain-specific properties</p> <p>Real-world image datasets exhibit characteristic frequency distributions. The power spectral density of natural images typically follows a power-law decay:</p> \[P \propto f^{-\alpha}\] <p>Emperical studies on natural image statistics shave shown that \(\alpha\) typically resides in the interval \([1,3]\) <d-cite key="field,physrevLett"> </d-cite>. While Standard photgraphic images generally converges towards \(\alpha \approx 2\) <d-cite key="vanHateren1992"></d-cite>, For few domain specific datasets like Astronomy images, we observe this trend \(\alpha \approx 1\).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/universe_image_fft-480.webp 480w,/2026/assets/img/2026-11-25-fans/universe_image_fft-800.webp 800w,/2026/assets/img/2026-11-25-fans/universe_image_fft-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/universe_image_fft.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/average_frequency_psd_loglog-480.webp 480w,/2026/assets/img/2026-11-25-fans/average_frequency_psd_loglog-800.webp 800w,/2026/assets/img/2026-11-25-fans/average_frequency_psd_loglog-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/average_frequency_psd_loglog.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure: In the First Figure (Top Right), We can see the fourier representation of an astronomy image varies a lot as compared to the fourier representation of the natural image as shown above. </div> <p>To quantify dataset frequency importance, we divide the spectrum into \(B\) radial frequency bands and compute the normalized band power \(\pi_b\)​ for each band \(B\):</p> \[\pi_b = \frac{1}{N}\sum{N}_{i=1} \frac{\sum{}_{f \in B_i}P_i(f)}{\sum{}_{f} P_i(f)}\] <p>where \(N\) is the number of images in the dataset and \(P_i(f)\) is the PSD of image \(i\). These band powers \({\pi_b}^B_{b=1}\)​ form a probability distribution over frequency bands, representing how the dataset’s signal energy is distributed across the spectrum.</p> <h3 id="diffusion-in-frequency-domain">Diffusion in Frequency Domain</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/heatmap-480.webp 480w,/2026/assets/img/2026-11-25-fans/heatmap-800.webp 800w,/2026/assets/img/2026-11-25-fans/heatmap-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/heatmap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Figure 3: Comparing the SNR (dB scale) for DDPM we see that High frequencies are corrupted substantially faster (SNR changes more per time increment) than low frequencies.The SNR is computed using monte carlo estimate of the SNR equation discussed below </div> <p>As we are trying to investigate how the DDPMs inductive bias in the forward process<d-cite key="falck2025fourierspaceperspectivediffusion"></d-cite> affects datasets for varying spectral power density, we can view the DDPM forward process under a change of basis to fourier space <d-cite key="dielman,gerdes2024gudgenerationunifieddiffusion"></d-cite>. This is acomplished by applying the Fourier transform \(\mathbf{F}\) to the variable \(x_t\)</p> \[y_t : \mathbf{F}\mathbf{x}_t = \mathbf{F}\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \mathbf{F}\sqrt{(1 - \bar{\alpha}_t)}\epsilon\] <p>Here, \(y_t\) is our fourier-transformed intermediate step at time step \(t\) of the forward process.</p> <p>Taking the quantification of Signal-to-noise Ration from <d-cite key="falck2025fourierspaceperspectivediffusion"></d-cite>, where SNR of \((x_t)_i\) is the signal to noise ration of frquescy \(i\) at timestep \(t\). Formally put</p> \[SNR((x_t)_i) = \frac{\bar{\alpha_t}\varsigma_i}{1-\bar{\alpha_t}}\] <p>Where \(\varsigma_i = Var((x_0)_i)\) represents the signal variance of requency \(i\).</p> <p>Tying these all together, we see in Figure 3, that standard DDPM corrupts high-frequencies faster than low frequencies. This bias is carry forwarded to the reverse process as well.</p> <p>Thus we want to study an alternate noising schedule that respects the datasets spectral signature. To put simply a Frequency Adaptive Noise Scheduler ( FANS ).</p> <h2 id="fans">FANS</h2> <p>In the previous sections we saw the inductive bias of DDPM towards high frequency components both in forward and reverse process. We want to investigate if instead of isotropic gaussian noise scheduler (irrespective of the frequency distribution of the dataset), we use a scheduler that is adaptive to the intrinsic frequency characteristic of the dataset by constructing spectrally-shaped noise. Our key insight is that different frequency bands contribute unequally to perceptual quality and should be treated accordingly during both training and generation.</p> <p>This proposed approach, operates through three complementary mechanism :</p> <ol> <li><strong>Dataset importance profiling</strong>: We analyze the spectraal distribution of the data to compute frequency band importance weight \(g_b\) that quantifies the relative contribution of each band of the overall data.</li> <li><strong>Time-Frequency Scheduling</strong>: We introduce a temporal ramping function \(\phi(t)\) that smoothly transitions noise allocation from low to high frequencies as the diffusion process evolves, enabling coarse-to-fine generation.</li> <li><strong>Variance-Compensated Weighting</strong>: We apply inverse-power weighting to band importances, ensuring that underrepresented high-frequency bands receive compensatory emphasis during training.</li> </ol> <p>We can now formalize these mechanism:</p> <h3 id="radial-frequency-band-decomposition">Radial Frequency Band Decomposition</h3> <p>Recalling from previous discussion, let’s \(x_0 \in \mathbb{R}^{H \times W \times C}\) denote a clean image, and let \(\mathcal{F}\) denote the real-valued FFT used in the implementation. To characterise the dataset’s spectral structure, we partition the \(B\) radial frequency bands \(\{B_b\}_{b=1}^B\) in the discrete \(r\)FFT layout (shape \(H \times W/2 + 1\)) using linear radial boundaries. For each band \(b\) we define :</p> <ul> <li><strong>Band mask</strong> : \(B_b \in \{0,1\}^{H \times W/2 + 1}\) indicating membership.</li> <li> <table> <tbody> <tr> <td><strong>Band size</strong> :</td> <td>\(B_b\)</td> <td>denotes the nuber of frequency coefficients in band \(b\).</td> </tr> </tbody> </table> </li> </ul> <p>Bands are constructed using radial frequency \(F(u,v) = \sqrt{u^2 + v^2}\) with edges uniformly spaced between a small positive \(f_{min}\) (to exclude DC component) and \(f_{mac}\) ( Nyquist frequency = 0.5)</p> <p><strong>Intuition</strong> : Excluding the DC component (zero frequency) is critical because it represents the global mean intensity, which dominates the spectrum but carries minimal perceptual information. Including DC in band 0 would artificially inflate its importance \(g_0\) and distort the learned weighting.</p> <h3 id="dataset-importance-profiling">Dataset Importance Profiling</h3> <p>For each image \(x\) in the training set, we compute the <strong>normalized band power</strong> \(\pi_b(x)\) as ;</p> \[\pi_b(x) = \frac{\sum{}_{k \in B_b} |F(x - \bar{x})(k)|^2 }{\sum^{B-1}_{b^\prime = 0 }\sum{}_{k \in B_{b^\prime}} |F(x - \bar{x})(k)|^2 }\] <p>wher \(\bar{x}\) is the per image mean ( removing DC ). This gives the fraction of total power residing in band \(b\) for image \(x\).</p> <p>We then compute the dataset-level band distribution by averaging over \(N\) samples:</p> \[\bar{\pi} = \frac{1}{N} \sum^{N}_{i=1} \pi_b(x_i)\] <p>Note that \(\sum^{B-1}_{b=0} \bar{\pi_b} = 1\) by construction.</p> <p>Finally, we compute importance weights via inverse-power scaling rule :</p> \[g_b = \frac{(\bar{\pi_b} + \epsilon )^{-\alpha}}{\frac{1}{B}\sum^{B-1}_{b^\prime=0}(\bar{\pi_{b^\prime}} + \epsilon)^{-\alpha}}\] <p>wher \(\alpha\) controls the strength of variance compensation. The standardization ensures \(g_b\) has zero mean and unit variance, provising a stable range for the softmax reweighting.</p> <p><strong>Intuition</strong>: Bands with low power \(\bar{\pi_b}\)(e.g., high frequencies) receive higher importance \(g_b\) ​, compensating for their underrepresentation in the data. This prevents the model from neglecting high-frequency reconstruction.</p> <h3 id="time-dependent-soft-band-weighting-check-once">Time-Dependent Soft Band Weighting [check once]</h3> <p>At each timestep \(t∈[0,1]\)t \in [0,1]\(, we compute soft band weights\)w_b(t)$$ via a temperature-scaled softmax with time-frequency ramping:</p> \[w_b(t) = \frac{exp(\beta.g_b - \gamma.\phi(t).\lambda_b)}{\sum^{B-1}_{b^\prime = 0}exp(\beta.g_b^\prime - \gamma.\phi(t).\lambda_b^\prime)}\] <p>where:</p> <ul> <li>\(\lambda_b = b/(B-1)\) is the normalized band index \(\lambda_0 = 0, \lambda_{B-1}=1\)</li> <li>\(\pi(t) \in [0,1]\) is a temporal ramp.</li> <li>\(\beta,\gamma \ge 0\) are the invese temperature hyperparameter.</li> </ul> <p>Early in the diffusion process, the term \(\beta. g_b\) dominates, emphasizing frequencies according to dataset statistics. As \(t\) increases, the ramp \(\phi(t)\) gradually increases the influence of $\gamma\lambda_b$, making the spectrum increasingly uniform.</p> <p><strong>Stabilization: White Noise Guardrail</strong> To ensure stable training during the earliest timesteps, we introduce a white noise mixing schedule:</p> \[w_b^{\text{mix}}(t) = \begin{cases} \frac{1}{B} &amp; \text{if } t &lt; t_{\text{knee}} \\ (1 - \alpha_{\text{mix}}(t)) \cdot \frac{1}{B} + \alpha_{\text{mix}}(t) \cdot w_b(t) &amp; \text{if } t \geq t_{\text{knee}} \end{cases}\] <p>where \(\alpha_{\text{mix}}(t) = \frac{t - t_{\text{knee}}}{1 - t_{\text{knee}}}\)​​ is a linear blend coefficient and \(t_{\text{knee}} = 0.15\) by default.</p> <p><strong>Intuition</strong>: At very small \(t\), the noised samples \(x_t \approx \beta_t \epsilon\) are nearly pure noise. Enforcing strong spectral shaping here can destabilize training because the model has insufficient signal to learn meaningful structure. By using uniform weights early, we ensure the model first learns to denoise white noise (as in standard DDPM), then gradually transitions to FANS-shaped noise.</p> <h3 id="fans-noise-generation">FANS Noise Generation</h3> <p>Tying the above mechanisms together to generate the FANS-Noise we get.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/fans_training-480.webp 480w,/2026/assets/img/2026-11-25-fans/fans_training-800.webp 800w,/2026/assets/img/2026-11-25-fans/fans_training-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/fans_training.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Given a sample \(x \in \mathcal{R}^{N \times C \times H \times W}\) and normalised time \(t \in [0,1]\) FANS-Noise \(\epsilon_{FANS}\) is generated as :</p> <p>Step 1. <strong>Compute band weights:</strong> \(\{w_b(t)\}^{B-1}_{b=0}\) as discussed above. Step 2. <strong>Allocate spectral power</strong>:</p> <p>The total power available in Fourier space must account for the Parseval Relation (It states that total power is time domain must be equal to the total power in the fourier domain). For the forward process, the noise component has pixel-space variance \(\beta_t^2\). We set the \(\sigma_t = 1\), giving us</p> <p>\(P_{total} = N_p.\sigma^2_t = H.W.1 = H.W\).</p> <p>Why we go for \(N_p\)​ (not \(N_p^2\)​): The power spectral density relates to the sum of squared Fourier coefficients, not their squared sum. For an \(H \times W\) image, the rFFT produces \(H \times (W/2 + 1)\) complex coefficients. By Parseval’s theorem</p> \[\sum_{i=1}^{H \cdot W} x_i^2 = \frac{1}{H \cdot W} \sum_{k} |F(x)(k)|^2\] <p>where the factor \(1/(H \cdot W)\) comes from the DFT normalization convention.</p> <p>We then allocate power to each band according to the learned weights:</p> \[P_b(t) = w_b(t) \cdot P_{\text{total}} = w_b(t) \cdot H \cdot W\] <p>The per-frequency variance within band \(b\) is obtained by distributing \(P_b\)​ uniformly across its members:</p> \[\Sigma_b(t) = \frac{P_b(t)}{|\mathcal{B}_b| + \epsilon}\] <p>where \(\epsilon = 10^{-6}\) prevents division by zero as a numerical safeguard.</p> <p>Step 3. <strong>Draw Complex Gaussian Noise in Fourier Space</strong>:</p> <p>We generate base noise \(Z \in \mathbb{C}^{N \times C \times H \times (W/2+1)}\) by sampling independent real and imaginary components:</p> \[Z_{\text{real}} \sim \mathcal{N}(0, I), \quad Z_{\text{imag}} \sim \mathcal{N}(0, I)\] \[Z(n, c, h, w) = \frac{1}{\sqrt{2}}(Z_{\text{real}}(n,c,h,w) + i \cdot Z_{\text{imag}}(n,c,h,w))\] <p>The \(1/\sqrt{2}\)​ factor ensures</p> \[\mathbb{E}[|Z(k)|^2] = \mathbb{E}[Z_{\text{real}}^2 + Z_{\text{imag}}^2]/2 = 1\] <p>Step 4: <strong>Construct Spectral Variance Mask</strong></p> <p>We build a spatial variance map \(\Sigma_t \in \mathbb{R}^{N \times C \times H \times (W/2+1)}\)that specifies the desired variance at each frequency:</p> \[\Sigma_t(n,c,h,w) = \sum_{b=0}^{B-1} \Sigma_b(t) \cdot \mathbb{1}_{(h,w) \in \mathcal{B}_b}\] <p>where \(\mathbb{1}_{(h,w) \in \mathcal{B}_b}\)​​ is the indicator function for band membership.</p> <p>Step 5: <strong>Apply Frequency-Dependent Scaling</strong> The shaped noise in Fourier space is obtained by element-wise multiplication:</p> \[E_{\text{shaped}}(n,c,h,w) = \sqrt{\Sigma_t(n,c,h,w) + \epsilon_{\text{safe}}} \cdot Z(n,c,h,w)\] <p>where \(\epsilon_{\text{safe}} = 10^{-12}\) ensures numerical stability when \(\Sigma_t \approx 0\).</p> <p>Why square root? We are scaling the amplitude of Fourier coefficients. Since power is amplitude squared, to achieve variance \(\Sigma_t(k)\), we need amplitude \(\sqrt{\Sigma_t(k)}\)​. This follows from:</p> \[\mathbb{E}[|E_{\text{shaped}}(k)|^2] = \mathbb{E}[|\sqrt{\Sigma_t(k)} \cdot Z(k)|^2] = \Sigma_t(k) \cdot \mathbb{E}[|Z(k)|^2] = \Sigma_t(k)\] <p>Step 6: <strong>Inverse Fourier Transform to Pixel Space</strong> We apply the inverse real FFT to recover a real-valued noise image:</p> \[\epsilon_{\text{FANS}}^{\text{raw}} = \text{irfft2d}(E_{\text{shaped}}, s=(H, W))\] <p>where \(s=(H, W)\) specifies the desired output shape and irfft2d is the 2D inverse real FFT.</p> <p>Step 7: <strong>Enforce Unit Variance via Normalization</strong> While the Fourier-space construction theoretically preserves variance, discretization effects, numerical precision, and band edge artifacts can cause the pixel-space variance to deviate from unity. To ensure exact compatibility with the forward process \(x_t = \alpha_t z + \beta_t \epsilon\) where \(\mathbb{E}[\epsilon \epsilon^T] = I\), we enforce unit variance per sample and per channel:</p> \[\mu(n,c) = \frac{1}{H \cdot W} \sum_{h,w} \epsilon_{\text{FANS}}^{\text{raw}}(n,c,h,w)\] \[\sigma^2(n,c) = \frac{1}{H \cdot W} \sum_{h,w} (\epsilon_{\text{FANS}}^{\text{raw}}(n,c,h,w) - \mu(n,c))^2\] \[\epsilon_{\text{FANS}}(n,c,h,w) = \frac{\epsilon_{\text{FANS}}^{\text{raw}}(n,c,h,w) - \mu(n,c)}{\sqrt{\sigma^2(n,c) + \epsilon_{\text{safe}}}}\] <p>Critical importance: This normalization is not optional. Without it, we observed training instabilities where the effective noise magnitude drifted over time, breaking the assumptions of the forward SDE. The normalization ensures:</p> <ul> <li>Zero mean: \(\mathbb{E}[\epsilon_{\text{FANS}}] = 0\) exactly (not just approximately)</li> <li>Unit variance: \(\text{Var}(\epsilon_{\text{FANS}}) = I\) exactly</li> <li> <table> <tbody> <tr> <td>Consistency: \(x_t = \alpha_t z + \beta_t \epsilon_{\text{FANS}}\)​ has the correct conditional distribution $$p_t(x</td> <td>z)$$</td> </tr> </tbody> </table> </li> </ul> <p>Why per-channel normalization? Color channels may have different effective powers after Fourier shaping due to:</p> <ul> <li>Boundary effects in rFFT (asymmetric handling of Nyquist)</li> <li>Floating-point rounding errors accumulated differently per channel</li> <li>Non-uniform distribution of salient features across RGB</li> </ul> <p>Normalizing each channel independently ensures that the model sees noise with identical statistics in all color channels, preventing the network from learning color-dependent denoising biases.</p> <p>Step 8: <strong>Return Shaped Noise</strong> The final output \(\epsilon_{\text{FANS}} \in \mathbb{R}^{N \times C \times H \times W}\) satisfies:</p> <ol> <li>\(\mathbb{E}[\epsilon_{\text{FANS}}] = 0\)(zero mean)</li> <li>\(\mathbb{E}[\epsilon_{\text{FANS}} \epsilon_{\text{FANS}}^T] = I\) (unit covariance)</li> <li>Frequency band \(b\) contains fraction \(\approx w_b(t)\) of total power</li> </ol> <p>Low frequencies dominate at small \(t\), high frequencies at large \(t\)</p> <p>This noise can now be used in the forward process:</p> \[x_t = \alpha_t z + \beta_t \epsilon_{\text{FANS}}\] <p>and in the loss computation:</p> \[\mathcal{L} = \|u_\theta^t(x_t) - (\dot{\alpha}_t z + \dot{\beta}_t \epsilon_{\text{FANS}})\|^2\] <h3 id="sampling-inference">Sampling/ Inference</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/fans_sampling-480.webp 480w,/2026/assets/img/2026-11-25-fans/fans_sampling-800.webp 800w,/2026/assets/img/2026-11-25-fans/fans_sampling-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/fans_sampling.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This algorithm shows the sampling method we used for our proposed approach.</p> <p>Now that we have the mechanism to get spectral aware shape, we need to see how performs against a baseline. To test this we simulate experiments where high frequency and fine grained information is of primary interest. For this simulation we do a synthetic study. We designed two synthetic datasets PLTB and EGM each constructed to emphasize a distinct spectral profile. All three datasets are generated programmatically as 512×512 images. We discuss these synhtetic datasets in details in the following section</p> <p>We use this synthetic data because controlled synthetic data provides the ability to vary the distribution of Fourier energy across bands in a principled way and isolate the effect of FANS.</p> <h2 id="synthetic-data">Synthetic Data</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/synth_sample-480.webp 480w,/2026/assets/img/2026-11-25-fans/synth_sample-800.webp 800w,/2026/assets/img/2026-11-25-fans/synth_sample-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/synth_sample.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="pltb-power-law-texture-bank">PLTB: Power-Law Texture Bank</h3> <p><strong>Motivation:</strong> PLTB targets the high-frequency regime. In this the entire image is defined through its radial power spectrum. We can see the distribution of spectral mass across bands can be observed in the figure.</p> <p><strong>Genertation Procedure:</strong> Each PLTB sample is generated by:</p> <ol> <li>Sampling an i.i.d. complex Gaussian field \(Z(k)\) on the half-spectrum</li> <li>Applying an amplitude mask:</li> </ol> \[A(k) \propto (|k| + \epsilon )^{\alpha/2}\] <p>with slope \(\alpha = 1\)</p> <ol> <li>Multiplying \(Z(k)\) by \(A(k)\) and applying an inverse FFT to obtain the spatial image.</li> <li>Finally, Normalizing brightness and contrast per-sample.</li> </ol> <p>This enables us to isolate testure learning capability of dissusion models. Models with insufficient high-frequency produce visibly smoother samples. A sample is given in figure above</p> <h3 id="egm-edgesgratings-mixture">EGM: Edges–Gratings Mixture</h3> <p><strong>Motivation:</strong> EGM introduces structured high-frequency content—oriented gratings, checkerboards, and sharp edges—that resemble real-image conditions where fine geometric detail matters. This allows discrete orientation content and mixed spatial primitives, providing a more realistic stress test of whether a model can faithfully reproduce high frequency geometry rather than only stochastic texture.</p> <p><strong>Generation Procedure:</strong> Each EGM image is a mixture of:</p> <ol> <li>Sinusoidal gratings with random frequency (0.04 − 0.22 cyc/px), orientation, amplitude, and phase.</li> <li>Checkerboard patterns, producing orthogonal high-frequency peaks.</li> <li>Sparse straight-line segments, introducing broadband edge energy.</li> </ol> <p>Components are randomly combined, and the resulting image is normalized to unit variance. EGM reflects scenarios common in natural images edges, periodic patterns, corner-like junctions—while still enabling controlled frequency manipulation.</p> <p>The controlled spectral mass distribution across bands of these synthetic datasets allows us to benchmark how FANS perform compared to a standard DDPM when the spectral signature of an image deviates from that of a natural image.</p> <p>To ensure that adapting to the spectral signature of an image doesn’t affect the performance in natural image, we benchmark FANS against standard DDPM in datasets like CIFAR10 and CelebA.</p> <p>To show that the synthetic dataset we generated are not entirely an imaginary usecase, we analyse the spectral signature of two real world datasets:</p> <ul> <li>Multimodal Universe: An astronomy based Dataset</li> <li>Texture dataset: A dataset consist of textures.</li> </ul> <h2 id="results">Results.</h2> <p>When we compare this Frequency Aware method against standard DDPM, we found some interesting results.</p> <p><strong>Slope Estimation:</strong></p> <p>A key advantage of FANS over standard DDPM models lies in its ability to accurately capture the spectral characteristics of the data distribution. To quantify this improvement, we evaluate both methods on their capacity to estimate the power-law decay exponent (slope) of the data’s power spectral density.</p> <table> <thead> <tr> <th>Dataset</th> <th style="text-align: center">Original Slope</th> <th style="text-align: right">FANS</th> <th style="text-align: right">DDPM</th> </tr> </thead> <tbody> <tr> <td>PLTB</td> <td style="text-align: center">1.002</td> <td style="text-align: right">1.394</td> <td style="text-align: right">3.566</td> </tr> <tr> <td>EGM</td> <td style="text-align: center">0.989</td> <td style="text-align: right">1.121</td> <td style="text-align: right">2.566</td> </tr> <tr> <td>Multimodal Universe</td> <td style="text-align: center">1.229</td> <td style="text-align: right">1.454</td> <td style="text-align: right">3.515</td> </tr> <tr> <td>Texture Data</td> <td style="text-align: center">1.121</td> <td style="text-align: right">1.618</td> <td style="text-align: right">2.178</td> </tr> <tr> <td>CIFAR10</td> <td style="text-align: center">2.848</td> <td style="text-align: right">2.512</td> <td style="text-align: right">2.733</td> </tr> </tbody> </table> <p>we compute the mean absolute error (MAE) between the estimated and ground-truth spectral slopes across all datasets in Table above. FANS achieves a substantially lower MAE (0.3165) compared to DDPM (1.5197). This difference is stable across datasets spanning texture-rich synthetic domains (PLTB, EGM) and real natural image statistics (CIFAR-10).</p> <p>To quantify statistical reliability, we treat each dataset entry as an independent paired comparison between FANS and DDPM errors. A paired t-test on the absolute errors yields a significant difference (t = −4.92, p &lt; 0.01). Instead, they reflect a systematic reduction in spectral-slope distortion.</p> <p><strong>Spectral Band Analysis:</strong> As FANS is designed to schedule noise based on the spectral property of the dataset, we perform a spectral analysis between the dataset and the images generated by the baseline and FANS.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/bar_plot-480.webp 480w,/2026/assets/img/2026-11-25-fans/bar_plot-800.webp 800w,/2026/assets/img/2026-11-25-fans/bar_plot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/bar_plot.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Figure: Shows the PSD over the frequency bands of three datasets (a) Multimodal Universe Dataset, (b) PLTB Dataset ad (c) EGM Dataset. It also shows how the PSD of the real datasets (<span style="color:green">green</span>) and generated images are distributed over the frequency bands for both the FANS (<span style="color:blue">Blue</span>) and Standard DDPM ( Baseline ) model (<span style="color:red">Red</span>)</p> <p>The plot shows the Power Spectral Density per Frequency Band for two synthetic dataset (PLTB and EGM) and a real world dataset ( Multimidal Universe). It shows that both the method could capture the spectral signature of the dataset, and understand how the PSD is distributed across the frequency bands. However the baseline ( Standard DDPM ) tends to concentrate the PSD within few frequency bands, while FANS ** distribute the PSD as per the dataset characteristics** and are more in agreement with the dataset spectral characteristic.</p> <p><strong>Metrics.</strong> To quantify spectral fidelity we use: (1) Jensen-Shannon (JS) divergence between generated and real PSD distributions (lower is better), (2) per-band correlation.</p> <p>Using Jensen-Shannon divergence (JSD), we can see that the PSD distribution across frequency bands of FANS are much closer to the actual dataset as compared to the baseline on all the three datasets.</p> <table> <thead> <tr> <th>Dataset</th> <th style="text-align: center">JSD(FANS)</th> <th style="text-align: right">JSD(Baseline)</th> </tr> </thead> <tbody> <tr> <td>EGM</td> <td style="text-align: center"><strong>0.0308</strong></td> <td style="text-align: right">0.1276</td> </tr> <tr> <td>PLTB</td> <td style="text-align: center"><strong>0.0103</strong></td> <td style="text-align: right">0.0804</td> </tr> <tr> <td>Universe</td> <td style="text-align: center"><strong>0.0088</strong></td> <td style="text-align: right">0.1041</td> </tr> </tbody> </table> <p>Across the datasets FANS shows a stronger correlation bandwise compared to the basseline. This shows the correlation between the bands of the real dataset and the samples generated. FANS has much higher bandwise correlation across both the synthetic and real dataset.</p> <table> <thead> <tr> <th>Dataset</th> <th style="text-align: center">Correlation(FANS)</th> <th style="text-align: right">Correlation(Baseline)</th> </tr> </thead> <tbody> <tr> <td>EGM</td> <td style="text-align: center"><strong>0.911</strong></td> <td style="text-align: right">0.672</td> </tr> <tr> <td>PLTB</td> <td style="text-align: center"><strong>0..968</strong></td> <td style="text-align: right">0.522</td> </tr> <tr> <td>Universe</td> <td style="text-align: center"><strong>0.877</strong></td> <td style="text-align: right">0.532</td> </tr> </tbody> </table> <p><strong>Qualitative Analysis:</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/ptlb_sample-480.webp 480w,/2026/assets/img/2026-11-25-fans/ptlb_sample-800.webp 800w,/2026/assets/img/2026-11-25-fans/ptlb_sample-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/ptlb_sample.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/egm_sample-480.webp 480w,/2026/assets/img/2026-11-25-fans/egm_sample-800.webp 800w,/2026/assets/img/2026-11-25-fans/egm_sample-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/egm_sample.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>On PLTB, standard DDPM exhibits catastrophic failure, generating samples visually indistinguishable from Gaussian noise, while FANS produces recognizable structures matching the data distribution. Quantitative analysis shows DDPM samples have near-zero correlation with real data features.</p> <p>For EGM, both methods generate stable images, but FANS preserves fine-grained textures absent in DDPM outputs. Specifically, the characteristic cross-hatched patterns present in training data are preserved by FANS but smoothed out by DDPM. This suggests FANS better captures high-frequency components critical for texture fidelity.</p> <p>To maintain the validity of our experiment all the hyperparameter setting for both the baseline and FANS training and sampling are kept identical. All experiments were performed with T = 1000 sampling steps</p> <p>To show that, the performance gains translates to real world setting as well, we show the performance comparison on real world datasets.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/Universe_sample-480.webp 480w,/2026/assets/img/2026-11-25-fans/Universe_sample-800.webp 800w,/2026/assets/img/2026-11-25-fans/Universe_sample-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/Universe_sample.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Figure: Qualitative samples for Multimodal Universe Dataset</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/texture_sample-480.webp 480w,/2026/assets/img/2026-11-25-fans/texture_sample-800.webp 800w,/2026/assets/img/2026-11-25-fans/texture_sample-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-11-25-fans/texture_sample.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Figure: Qualitative samples for Texture Dataset</p> <p>From the Figure we can see that FANS could capture the intricate details of the Multimodal Universe Dataset (FID: 10.003) while Baseline method couldn’t capture such intricacies (FID: 24.012). Similar observation can be made for the texture dataset. The baseline model intriduces a lot of atrifacts in the attempt to capture the texture details, while FANS could easilyt capture the intricate details of the dataset.</p> <p>To ensure that FANS is not only adapting to these high frequency dominated datasets, we capre the FID score of FANS with stand DDPM ( Baseline ) models on CIFAR10 and CelebA datasets.</p> <table> <thead> <tr> <th style="text-align: left">Schedule</th> <th style="text-align: left">CIFAR10 (50)</th> <th style="text-align: left">CIFAR10 (100)</th> <th style="text-align: left">CIFAR10 (200)</th> <th style="text-align: left">CIFAR10 (1000)</th> <th style="text-align: left">CelebA (50)</th> <th style="text-align: left">CelebA (100)</th> <th style="text-align: left">CelebA (200)</th> <th style="text-align: left">CelebA (1000)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>DDPM</strong></td> <td style="text-align: left">17.36</td> <td style="text-align: left">17.10</td> <td style="text-align: left">16.82</td> <td style="text-align: left">16.07</td> <td style="text-align: left">11.01</td> <td style="text-align: left">8.27</td> <td style="text-align: left">8.11</td> <td style="text-align: left">8.26</td> </tr> <tr> <td style="text-align: left"><strong>FANS</strong></td> <td style="text-align: left">16.08</td> <td style="text-align: left">16.11</td> <td style="text-align: left">15.04</td> <td style="text-align: left">14.19</td> <td style="text-align: left">13.18</td> <td style="text-align: left">12.10</td> <td style="text-align: left">10.15</td> <td style="text-align: left">10.10</td> </tr> </tbody> </table> <h2 id="conclusion">Conclusion</h2> <p>In this blog we aimed to analyse and present a principled approach to approach to addressing spectral bias in diffusion models through dynamic, dataset-aware noise scheduling. By leveraging the spectral characteristics of training data to construct frequency-dependent noise distributions, FANS enables models to allocate denoising capacity more efficiently across the frequency spectrum. Through rigorous experiments on synthetic datasets with known spectral characteristics (PLTB and EGM), we demonstrate that FANS consistently improves sample quality compared to vanilla DDPM baselines, particularly for datasets with pronounced high-frequency content. The method’s ability to learn dataset-specific frequency priorities and dynamically adjust noise shaping over time represents a meaningful step toward more adaptive and efficient diffusion training.</p> <p>However, our work also reveals important limitations and directions for future investigation. In the current implementation the computational overhead of spectral profiling and per-sample noise generation, while manageable, adds complexity to the training pipeline.</p> <p>Future work should focus on several key areas: extending FANS to high-resolution natural images and validating its benefits on large-scale datasets like ImageNet, exploring integration with modern architectures like diffusion transformers, and investigating the interplay between FANS and other recent advances such as flow matching and consistency models. Additionally, theoretical analysis of FANS’s convergence properties and its relationship to other forms of adaptive noise scheduling would strengthen the mathematical foundations of the approach.</p> <p>Despite these challenges, FANS demonstrates that incorporating dataset-specific spectral information into the noise generation process can meaningfully improve diffusion model training. As the field continues to push toward higher-resolution, higher-fidelity generation, methods that efficiently allocate model capacity across frequency bands will become increasingly important. We hope this work inspires further exploration of adaptive, data-driven approaches to noise scheduling in generative models.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Diffusion models have achieved remarkable success in generative modeling, yet they often struggle with spectral bias,the tendency to prioritize low-frequency patterns while inadequately learning high-frequency details. This limitation stems from the uniform noise scheduling employed during training, which allocates equal denoising capacity across all frequencies regardless of the dataset's spectral characteristics. We introduce Frequency-Adaptive Noise Shaping (FANS), a principled framework that addresses this fundamental limitation by dynamically shaping noise distributions according to dataset-specific frequency importance. FANS operates on a simple insight - different datasets exhibit distinct spectral signatures, and noise scheduling should reflect these differences. The framework integrates seamlessly with existing diffusion architectures through a simple modification to the noise sampling procedure during training and inference.We validate FANS on synthetic datasets with controlled spectral properties as well as real world data (CIFAR10, CelebA, Texture, MultimodalUniverse) where we demonstrate consistent improvements over vanilla DDPM baselines. Our experiments reveal that FANS particularly excels on high-frequency-rich datasets, producing sharper, more detailed samples while maintaining comparable performance for standard natural image datasets like CIFAR10 and CelebA.]]></summary></entry><entry><title type="html">Beyond the Rerun: Why Reproducibility is Failing Science</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/beyond-the-rerun/" rel="alternate" type="text/html" title="Beyond the Rerun: Why Reproducibility is Failing Science"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/beyond-the-rerun</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/beyond-the-rerun/"><![CDATA[<p>I see other students in my research group struggling and spending weeks trying to replicate experiments that have already been done and for which the code is already available. And I ask myself, what could we do to make it better for them and other researchers? It should be as easy as “just run the experiment program”, right? But even when code is available and runnable, significant friction remains.</p> <h2 id="but-why-does-it-matter-if-someone-can-run-your-research-code-easily">But, why does it matter if someone can run your research code easily?</h2> <p>Reproducibility, the ability to repeat an experiment, given its code and obtain the same results, is vital to scientific construction. Following Popper’s line of thought <d-cite key="KarlPopperStanford"></d-cite>, science depends on hypotheses and statements that can be tested and falsified. Without reproducibility, such testing becomes difficult for critical argumentation, which weakens scientific claims, even if it does not directly invalidate them.</p> <p>Today, there is a “reproducibility crisis” in science <d-cite key="baker1500ScientistsLift2016"></d-cite>. Interestingly, physicists and engineers are among those who say their work is more reproducible, while more than 60% have had problems reproducing others’ experiments. This contradiction highlights a clear reproducibility problem that impacts the quality of the scientific knowledge generated, while its causes may not be as clear. One possible reason for this is over-reliance on computers, which scientists see as precise entities that execute “precise” algorithms, but whose reproducibility ultimately depends on their proper execution of those programs <d-cite key="plesserReproducibilityVsReplicability2018"></d-cite>. From another perspective, the pressure to publish ranks second among the causes of irreproducibility, diminishing the quality of scientific code and its usability.</p> <p>There are initiatives to improve the issue of reproducibility. The “AI4Europe Reproducibility Initiative” <d-cite key="AI4EuropeReproducibilityInitiative"></d-cite> focuses on three different barriers to reproducibility: technical, the documentation and preservation of software and data; cultural, the encouragement and valuation of reproducibility over results and novelty alone; and systemic, the lack of standards and guidelines for reproducibility. Conferences also address the issue, with ICLR incentivizing a reproducibility statement <d-cite key="ICLR2026Author"></d-cite> and NeurIPS <d-cite key="NeurIPS2025Call"></d-cite> including a mandatory reproducibility question in its checklist. There are even awards for studies that best address reproducibility <d-cite key="GENEAWorkshop2025"></d-cite>. NeurIPS also suggests using the Papers With Code guidelines <d-cite key="PaperswithcodeReleasingresearchcode2025"></d-cite>, which include requirements such as a requirements list, Docker images uploaded to Docker Hub, separate code for training and evaluation, pre-trained models, and instructions for reproducing the results. As outlined in this guideline, not only the code but also sharing other artifacts are essential, as one can say, “Where re-running is successful, the published artifacts allow others to build on earlier work” <d-cite key="plesserReproducibilityVsReplicability2018"></d-cite>.</p> <p>However, we argue that reproducibility alone is not enough for science. Science is about questioning and proposing new perspectives and approaches, about advancing the SotA. It’s not enough to simply be able to re-execute and understand a project. It needs to be possible to use it and build upon it, because that’s how science is built. Often, “reproducible” code is a black box that runs in a Docker container, spits out a number, and dies. If I can’t dissect the model to understand <em>why</em> it worked, that is performance validation, not knowledge building. And the current view, focused solely on the reproducibility crisis, fails to see this resulting gap. When we ask our students to replicate an experiment published by other researchers, we want them to understand it and be able to propose changes and new approaches based on it.</p> <p>We have a proliferation of “single-use codes,” and that is not sustainable. Symptoms of the trap of prioritizing productivity over quality and scientific advancement include shortcuts to publishing quickly that, paradoxically, ultimately increase the overall time cost of science. We are creating a <strong>“scientific debt”</strong> that makes research more challenging. We have separated the code for a new method here from the code of the experiment performed to validate it, since the former should be the focus of this problem. Methods should be reusable and extensible, as significant advances in science have not been built upon single-use methods.</p> <h2 id="pillars-for-scientific-computer-science">Pillars for Scientific Computer Science</h2> <p>Building on the “scientific debt” presented, we propose to view the problem from three pillars, <strong>Reproducibility</strong>, <strong>Legibility</strong>, and <strong>Composability</strong>:</p> <table> <thead> <tr> <th><strong>Reproducibility</strong></th> <th><strong>Readability</strong></th> <th><strong>Composability</strong></th> </tr> </thead> <tbody> <tr> <td>The classical reproducibility view, which focuses on the ability to re-run an experiment, which we have discussed so far.</td> <td>The ability to interpret a project, understand how it works, and know how to use it. It’s about making a project transparent, no longer a black box.</td> <td>The possibility of creating something new using existing code. Being able to use SotA methods to advance science further.</td> </tr> </tbody> </table> <p>As mentioned, reproducibility is essential to the validity of science and the foundation of its quality; without it, the other pillars are useless. Readability, on the other hand, advances the field and enables a proper understanding of what is happening; without it, it becomes challenging to use previous results. Finally, composability is what most contributes to advancing science and should be the objective when designing research software.</p> <h2 id="from-scripts-to-building-blocks-a-path-forward">From Scripts to Building Blocks: A Path Forward</h2> <p>Let’s look at practical actions we can take to achieve the pillars. We will focus on scientific code written in Python, as it is the most widely used language in science today. We will include links to tutorials, tools, and specifications that may be useful.</p> <p>These are some recommendations that you, as a researcher, can use. But even though well-intentioned researchers can make incredible things; this is a problem we also need to address collectively, as we will discuss later.</p> <ol> <li> <p><strong>Software design</strong></p> <p>We need software with designs considering the division of concerns. Currently, it’s very common to see programs that mix the code of the proposed method (method code) with the code of the experiment that validates the method and generates metrics and results (experiment code).</p> <p>Ideally, these codes should be decoupled, allowing the method to be reused in future research. The method code should also, whenever possible, be written using well-known, widely used frameworks, further increasing compatibility. Think about how easy it is to use a neural network layer that is already written in a ready-to-use PyTorch class. The experiment code should contain not only the code to run the experiment, but also its configurations and the code responsible for processing datasets. Furthermore, it should include clear execution entry points, avoiding the need to run different scripts in different folders to perform a single task, such as training or evaluating a model.</p> </li> <li> <p><strong>Code standards</strong></p> <p>A consistent coding style throughout the project is important for ease of understanding. Writing variable, function, and class names in a way that clearly explains what they do is a good starting point.</p> <p>Some practices make code significantly easier to understand—for example, avoiding very long functions or files, keeping imports at the top of each file, and reducing the use of large configuration dictionaries passed through the code.</p> <p>There are tools, like AutoPEP8 <d-cite key="Autopep8ToolThat"></d-cite><d-cite key="Autopep8VisualStudio"></d-cite>, that automatically formats code using the PEP 8 (official “Style Guide for Python Code”) guidelines <d-cite key="guidovanrossumPEP8Style"></d-cite>, and SonarLint <d-cite key="SonarQubeIDEVisual"></d-cite>, which can help maintain a consistent, appropriate style.</p> </li> <li> <p><strong>Documentation</strong></p> <p>In addition to following good coding standards, it’s important to write clear documentation alongside the code that explains what each section of code does. This can include comments in more complex areas and, at least, function docstrings. Type annotations <d-cite key="TypeHintsPython21"></d-cite><d-cite key="TypingSupportType"></d-cite> are also part of this group; in addition to being used by functions for auto-completion of parts of docstrings, they also facilitate code usage by allowing autocomplete and visualization of the appropriate documentation while writing new code.</p> </li> <li> <p><strong>Version control</strong></p> <p>Versioning your project helps future researchers find the specific version associated with a published result. It also facilitates collaborative development among different researchers and makes explicit the changes made over time.</p> <p>Version control should be applied to the codebase, but you can also use it for other artifacts, such as pre-trained models, Docker images, and datasets. Tools like git/GitHub for code, and Zenodo for other artifacts, make it easier to make artifacts available through versions.</p> </li> <li> <p><strong>Publishing and distribution</strong></p> <p>Publishing your research software on a standard, user-friendly channel helps other researchers utilize your contribution in future research. In Python, this usually means packaging your method and distributing it using PyPI (Python Package Index) <d-cite key="PythonPackageIndex"></d-cite><d-cite key="PackagingPythonProjects"></d-cite>.</p> <p>Avoid expecting other researchers to copy your script into their projects. Besides being less straightforward, it doesn’t consider potential future versions and complicates issues like licensing. Imagine including the license for each library you used in your research in your repository.</p> </li> <li> <p><strong>Dependencies specification</strong></p> <p>Specifying all the dependencies of your project is a first step in enabling a third party to reproduce your results or use your method.</p> <p>When distributing your method, avoid specifying fixed versions, as this will make it difficult to use with other packages. Specify the minimum versions your project supports. And use standard ways to specify dependencies, such as your project’s pyproject.toml file <d-cite key="WritingYourPyprojecttoml"></d-cite>.</p> </li> <li> <p><strong>Hardware specification</strong></p> <p>Specify what hardware you used to run your experiment, as well as the minimum requirements your method may need. This helps other researchers prepare environments to reproduce your results and further develop them.</p> </li> <li> <p><strong>Containerization</strong></p> <p>Docker <d-cite key="WhatContainer0200"></d-cite>, like other container tools, allows other researchers to use the environment you used to conduct your experiments directly. This makes it much easier to reproduce your results and use your method, since all dependencies will be identical to yours. In addition to making the dockerfile you created for your image available, also consider making your built image available on Docker Hub <d-cite key="DockerHubContainer"></d-cite> or Zenodo <d-cite key="PauleveDonodoBridging"></d-cite>.</p> </li> <li> <p><strong>Tutorials</strong></p> <p>Write tutorials about your method. Besides providing a quick, straightforward introduction that can serve as a first overview and gateway to more in-depth publications, it helps others get started using your method. Jupyter Notebook <d-cite key="grangerJupyterThinkingStorytelling2021"></d-cite><d-cite key="ProjectJupyter"></d-cite> is a suitable format for tutorials.</p> </li> <li> <p><strong>Licensing</strong></p> <p>It’s pointless to make your project available if others can’t use it due to legal issues. Licensing is an integral part of allowing your method to be explored by other researchers. Several licenses, with different properties, are available. Please choose the one that best suits your project and use it correctly <d-cite key="ChooseOpenSource"></d-cite>.</p> <p>Remember that making code available without any license means being stuck in a limbo between “nobody can use it” and the contradiction of “I made it available on a platform like GitHub”. And there are specific licenses for each type of artifact. In particular, Creative Commons <d-cite key="CreativeCommons"></d-cite> licenses are not appropriate for code <d-cite key="CreativeCommonsFrequentlyAskedQuestions"></d-cite>.</p> </li> <li> <p><strong>Persistence and traceability</strong></p> <p>Make your code and artifacts available in persistent repositories. Zenodo is a good example that automatically stores releases from GitHub repositories <d-cite key="ZenodoGitHubSoftware"></d-cite>.</p> <p>Also consider how these repositories can be found in the future. Referencing the DOI of these artifacts can facilitate their traceability in the future. For code on GitHub, consider making CFF files available with the project’s DOI <d-cite key="CITATIONFiles"></d-cite>.</p> </li> </ol> <p>Let’s see how each indication relates to the pillars. Notice how reproducibility and composability are often closely related in practice:</p> <pre><code class="language-mermaid">%%{init: { "gantt": { "displayMode": "compact", "leftPadding":200 } }}%%
gantt
    title     ​ 
    dateFormat  X
    axisFormat  
    
    section ​
	    Reproducibility :active, rerun, 1, 2
	    Readability :active, read, 2, 3
	    Composability :active, comp, 3, 4

    section 1. Software design
	    ​  : a1, 3,4
	    
	section 2. Code standards
	    ​  : a2, 2,3
	    
	section 3. Documentation
	    ​  : a3, 2,3
	    
	section 4. Version control
	    ​  : a4, 1,2
		​  : a4-2, 3,4
		    
	section 5. Publishing and distribution
	    ​  : a5, 1,2
	    ​  : a5-2, 3,4    
	    
	section 6. Dependencies specification
	    ​  : a6, 1,2
		​  : a6-2, 3,4	

	section 7. Hardware specification
	    ​  : a7, 1,2
		​  : a7-2, 3,4	    
	    
	section 8. Containerization
	    ​  : a8, 1,2
	    ​  : a8-2, 3,4
	    
	section 9. Tutorials
		​  : a9, 3,4

	section 10. Licensing
		​  : a10, 3,4

	section 11. Persistence and traceability
	    ​  : a11, 1,2
		​  : a11-2, 3,4	   
</code></pre> <h2 id="last-remarks">Last Remarks</h2> <p>Many of the recommendations mentioned resemble software engineering processes. This is because, let’s not forget, scientific research software is still… software. Ensuring its quality also involves elements similar to those in any other software.</p> <p>But even given the limits of reproducibility, it’s pointless to raise our quality standards for computer science research if we can’t address this scientific debt crisis. Cultural barriers, where a lack of incentives and increasing pressure on results and publications are significant reasons why we can’t solve these problems <d-cite key="AI4EuropeReproducibilityInitiative"></d-cite>. This is where major conferences, such as the ICLR, journals, and funding agencies can step in by highlighting the importance of a sustainable scientific ecosystem that enables efficient future research and by explicitly demanding actions to achieve this goal.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Is reproducibility enough? We discuss the current reproducibility crisis and the limitations that focusing solely on this aspect of scientific project quality imposes on science. We propose a broader approach to the problem of scientific debt and outline practical actions researchers can take in their research. We also draw attention to the need for community action on the issue.]]></summary></entry><entry><title type="html">Budget Alignment: Making Models Reason in the User’s Language</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/budget-alignment/" rel="alternate" type="text/html" title="Budget Alignment: Making Models Reason in the User’s Language"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/budget-alignment</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/budget-alignment/"><![CDATA[<h1 id="budget-alignment-making-models-reason-in-the-users-language">Budget Alignment: Making Models Reason in the User’s Language</h1> <p><em>Please read this as a late-stage work in progress shared in a “lab meeting” spirit to help and motivate parallel research.</em></p> <h2 id="introduction">Introduction</h2> <p>You ask a large language model (LLM) a math question in Japanese. It responds politely in Japanese — but behind the scenes, it’s reasoning in English/Chinese. Variables, steps, and mathematical lemmas often silently switch languages during reasoning. This behavior, where models default to English for chain-of-thought (CoT) reasoning, is more than a curiosity. It breaks instruction-following, confuses human overseers, and undermines the purpose of multilingual evaluation.</p> <p>The goal is clear: we want models to reason about a question in the language they are asked — not just to answer in that language. But this turns out to be harder than it sounds. Forcing models to reason in non-English languages usually leads to a drop in accuracy. Previous work shows that instructing models to reason only in the prompt language via prompting or steering improves coherence and grading alignment <d-cite key="zhong2025language"></d-cite>, but often comes at a steep “accuracy tax.” Even a small amount of multilingual fine-tuning helps, but doesn’t eliminate the trade-off <d-cite key="qi-etal-2025-models"></d-cite>. Further, models not only prefer to reason in English — they reason <em>more effectively</em> in English. When researchers force strict in-language reasoning (e.g., in Swahili or Thai), models often lose accuracy compared to when allowed to reason in English. For higher-resource languages like French or German, this trade-off is smaller — models can reason in-language nearly as well as in English. For low-resource languages, strict enforcement harms performance more significantly.</p> <p>Why do models switch to English in the first place? Much of it traces back to training. Most reasoning data are in English. Fine-tuning even strong multilingual models on English CoT data often leads them to adopt English as their “internal language of logic.” Yong et al. (2025) observe a “quote-and-think” behavior <d-cite key="yong2025crosslingual"></d-cite>, where models copy input phrases in the prompt language, but explain everything in English <d-cite key="kim2025one"></d-cite>. The model understands the question in the non-English language — it just prefers to reason in English.</p> <p>Our technical goal is simple: <strong>stop the switching without paying an accuracy tax</strong> — ideally, push the Pareto frontier of <em>(Accuracy, Language-consistency)</em>.<br/> And we want this post to serve as a practical guide with lessons learned along the way.</p> <p>Code, data, and checkpoints will be linked in the <strong>camera-ready</strong> version of this post to preserve anonymity during review.</p> <hr/> <h2 id="what-we-try-method-in-two-steps">What we try (Method in two steps)</h2> <p>🔧 <strong>Base model.</strong> <code class="language-plaintext highlighter-rouge">deepseek-ai/DeepSeek-R1-Distill-Qwen-7B</code>, a large reasoning model distilled from R1 through supervised fine-tuning on its reasoning traces, exhibiting an English/Chinese-dominant prior.</p> <p><strong>Step 1 — Small SFT to teach in-language reasoning.</strong><br/> We fine-tune on <strong>817 curated multilingual reasoning chains</strong> (from LiMO <d-cite key="ye2025limo"></d-cite>). This supervision data contains high-quality reasoning data matching R1 long-form reasoning <em>style</em>. No Reinforcement Learning (RL) here — just teach the policy to keep reasoning in the user’s query language.</p> <p><strong>Step 2 — Math-only GRPO to push accuracy while retaining reasoning language.</strong><br/> We run an RLVR-style GRPO with no KL, higher clip of 0.28 vs −0.2 (DAPO-like <d-cite key="yu2025dapo"></d-cite>), rollout 24, LoRA r = 8, LR = 1e-5, <strong>only on a Math-500 set translated to each language</strong>.<br/> Intuition: let RL optimize hard cases and verification behaviors, while the high clip reduces catastrophic reasoning style collapse back to English.</p> <p>We set the verifiable rewards as <strong>1.0 for accuracy, 0.2 for language consistency of reasoning traces, and 0.2 for answer format</strong> <d-cite key="rastogi2025magistral"></d-cite>.</p> <p>📊 <strong>Evaluation.</strong></p> <p>We tried our approach on three different languages: <strong>Japanese (JA) / French (FR) / Spanish (ES)</strong></p> <p>And tested on multiple datasets: <strong>MMLU College Math (MMLU Math), AIME25, GPQA, MMLU Pro Medicine (MMLU Med)</strong></p> <p>The first two are in-domain: MMLU-Math is similar to the training data in terms of hardness, while AIME25 is harder.<br/> The other two are out-of-domain: GPQA covers hard science questions, and MMLU Pro Medicine is made up of hard questions in the medical domain.</p> <p><strong>Regimes tested:</strong></p> <ul> <li>Base → <code class="language-plaintext highlighter-rouge">deepseek-ai/DeepSeek-R1-Distill-Qwen-7B</code> <d-cite key="deepseekai2025deepseekr1distillqwen7b"></d-cite></li> <li>SFT on top of Base</li> <li>GRPO-from-Base</li> <li>GRPO-from-SFT</li> </ul> <p><strong>Metrics:</strong></p> <ul> <li><code class="language-plaintext highlighter-rouge">pass@k(1,5,10)</code> where <code class="language-plaintext highlighter-rouge">n = 32</code> for accuracy</li> <li><code class="language-plaintext highlighter-rouge">Language-consistency %</code> (both reasoning traces <strong>and</strong> final answers must be in the requested language; script-aware checks)</li> </ul> <p><strong>How we score language consistency:</strong><br/> We check the entire CoT span and the final boxed answer.<br/> A sample counts as <code class="language-plaintext highlighter-rouge">Following = 1</code> only if both passages are in the requested language (script tokens, numerals, and markers allowed); otherwise <code class="language-plaintext highlighter-rouge">0</code>.<br/> We report the % across the set.</p> <hr/> <h2 id="-key-contributions">🔑 Key contributions</h2> <ol> <li> <p><strong>Small SFT reprograms inner monologue.</strong><br/> With only <strong>817 chains</strong>, language consistency rises near the ceiling in French/Spanish across datasets and substantially in Japanese (Fig. RQ0).</p> </li> <li> <p><strong>Two-step recipe Pareto-improves.</strong><br/> SFT secures language consistency; <strong>GRPO-SFT recovers/boosts accuracy on tough sets</strong> (AIME/GPQA) without reverting to English (Figs. RQ1–RQ4).</p> </li> <li><strong>Diagnose regressions and actionable fixes.</strong><br/> Regressions stem from: <ul> <li>Japanese tokenization/numeric friction,</li> <li>Spanish cue misalignment,</li> <li>medicine reward/style mismatch.<br/> Tokenizer-aware normalization, small Japanese/Spanish SFT top-ups, and multi-objective GRPO (with optional model merging) could recover accuracy without sacrificing in-language reasoning.</li> </ul> </li> <li><strong>TL; DR.</strong> You can briefly see our main results from the two figures below:<br/> Starting from an EN/ZH-dominant reasoning prior, small multilingual SFT is the most cost-effective way to “steer” in-language chains of reasoning. Adding math-only GRPO then recovers or improves accuracy on hard sets like AIME and GPQA while mostly preserving SFT’s language consistency discipline — pushing the Accuracy × Following frontier in many language–dataset pairs. The two pain points, Japanese (tokenization/numeric friction) and medicine (reward/style mismatch), are expected from the base prior and training signal, and both have potential straightforward fixes with light domain augmentation. And surprisingly, model merging can be very useful and effective.</li> </ol> <p><strong>Figure 1.a) Performance comparison overall across methods</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-budget-alignment/1a-480.webp 480w,/2026/assets/img/2026-04-27-budget-alignment/1a-800.webp 800w,/2026/assets/img/2026-04-27-budget-alignment/1a-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-budget-alignment/1a.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Figure 1.b) Overall language consistency rate comparison across methods</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-budget-alignment/1b-480.webp 480w,/2026/assets/img/2026-04-27-budget-alignment/1b-800.webp 800w,/2026/assets/img/2026-04-27-budget-alignment/1b-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-budget-alignment/1b.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="rq0--can-small-sft-reprogram-a-reasoning-models-reasoning-tone">RQ0 — Can small SFT reprogram a reasoning model’s “reasoning tone”?</h2> <p>Models often output the final answer in the same language as the user query. We want the <strong>reasoning process</strong> to match the prompt (user) language, too.</p> <p><strong>Results.</strong><br/> SFT drives the language consistency rate close to the ceiling (<strong>~99–100%</strong>) in French/Spanish and raises Japanese substantially (<strong>high-80s/90s</strong>).<br/> The language consistency rates averaged across all datasets are shown in Fig. RQ0: bars labeled Japanese/French/Spanish.</p> <p><strong>Interpretation.</strong><br/> A few hundred <strong>high-quality chains</strong> are enough to overwrite the English/Chinese inner-monologue priority to other languages. Japanese remains stubborn — see RQ5.</p> <blockquote> <p>Recall that instruction-following does not only mean the answer in the prompt language, but it should also ensure that the language of the reasoning traces is the same as the user’s preference to enhance their trustworthiness. SFT alone solves most of the language mismatch with limited accuracy improvements, which are yet lower than the accuracy of reasoning in English (i.e., the gray dashes in Figure 1.a above) in most cases. We provide more details in the next section.</p> </blockquote> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-budget-alignment/r0-480.webp 480w,/2026/assets/img/2026-04-27-budget-alignment/r0-800.webp 800w,/2026/assets/img/2026-04-27-budget-alignment/r0-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-budget-alignment/r0.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="rq1--does-sft-help-accuracy-or-only-language-reasoning-style">RQ1 — Does SFT help accuracy, or only language reasoning <em>style</em>?</h2> <p>We have shown that <strong>SFT significantly improves language consistency rates</strong>, but how about the accuracy?</p> <p><strong>Design.</strong><br/> Compare the accuracy <strong>Base vs SFT</strong> on <code class="language-plaintext highlighter-rouge">pass@k</code> per dataset–language<br/> (Fig. RQ1: Δ pass@10 = SFT − Base).</p> <p><strong>Findings.</strong></p> <ul> <li><strong>MMLU-Math:</strong> substantial improvements when train and test are in the same domain <ul> <li><em>French:</em> ~76 → <strong>98</strong></li> <li><em>Spanish:</em> ~80 → <strong>99</strong></li> <li><em>Japanese:</em> ~68 → <strong>88</strong></li> </ul> </li> <li> <p><strong>AIME:</strong> mixed. Although AIME contains math problems, it is way more difficult than LiMO, making it less likely to be considered as in-domain. As a result, SFT trades accuracy for strict language consistency when reasoning in ES.</p> </li> <li><strong>GPQA / MMLU Pro Medicine:</strong> Accuracy drops in most cases, but language consistency rises after SFT, indicating that it’s not trivial to generalize the capability of generating the correct answer from the training domain to others.</li> </ul> <p><strong>Takeaway.</strong><br/> SFT reliably improves language consistency <strong>and often increases accuracy on in-domain tasks (Math).</strong><br/> On OOD, SFT can over-narrate or change prior most probable token paths since the models are undertrained to reason in lower-resource languages — accuracy may dip unless taking further actions (e.g., reinforced by RL, shown in RQ2 and RQ3).</p> <p><strong>Practical guidance.</strong><br/> If your target is <strong>language consistency/reasoning style + some accuracy</strong>, SFT alone is cost-effective in-domain.<br/> If you also need robustness on hard and/or OOD sets, doing an <strong>RL top-up could be helpful.</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-budget-alignment/r1-480.webp 480w,/2026/assets/img/2026-04-27-budget-alignment/r1-800.webp 800w,/2026/assets/img/2026-04-27-budget-alignment/r1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-budget-alignment/r1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="rq2--when-rl-comes-how-does-grpo-help-with-accuracy">RQ2 — When RL comes, how does GRPO help with accuracy?</h2> <p><strong>Design.</strong><br/> Train GRPO only on Math-500; evaluate deltas (<strong>GRPO-SFT − SFT</strong>) across<br/> MMLU-Math / AIME / GPQA / MMLU-Med (Fig. RQ2).</p> <p><strong>In-domain.</strong><br/> SFT helps accuracy, but not always; GRPO brings a boost on top of the base SFT while maintaining language consistency of reasoning traces.</p> <ul> <li><strong>MMLU-Math-FR</strong> pass@10: <strong>76.0 → 97.8 → 98.0</strong> (Base → SFT → GRPO-SFT)</li> <li><strong>MMLU-Math-ES</strong> pass@10: <strong>80.5 → 98.6 → 99.1</strong> (Base → SFT → GRPO-SFT)</li> <li><strong>MMLU-Math-JA</strong> pass@10: <strong>68.1 → 88.0 → 91.5</strong> (Base → SFT → GRPO-SFT)</li> </ul> <p>The improvement in accuracy is consistent but slight due to the fact that MMLU-Math is relatively easy:<br/> The model almost achieves 90–100% accuracy after SFT, leaving no room for GRPO. Thus, the OOD sets are more informative.</p> <p><strong>Out-of-domain.</strong></p> <p>Positive transfers on <strong>AIME JA/FR/ES and GPQA JA/FR</strong>.<br/> For instance:</p> <ul> <li><strong>GPQA-ES</strong> pass@10: <strong>68.7 → 85.2 → 85.7</strong> (Base → SFT → GRPO-SFT)</li> <li><strong>AIME-JA</strong> pass@10: <strong>22.6 → 28.5 → 34.4</strong> (Base → SFT → GRPO-SFT; GRPO adds a large JA gain)</li> </ul> <p>More results are shown in the figure below.<br/> Although improvements on AIME-FR/ES and GPQA-ES are marginal, they still indicate a successful transfer of knowledge on the OOD setup after GRPO.</p> <p><strong>Negative transfers on Pro-Medicine.</strong></p> <ul> <li>Accuracy improves on Pro-Medicine-JA but decreases on French and Spanish.</li> </ul> <p><strong>Interpretation.</strong><br/> GRPO learns verification/search habits that generalize: language consistency, math reasoning styles, re-checking numeric steps, and tighter answer boxing.<br/> Those help <strong>GPQA and AIME</strong>.<br/> But medicine needs domain lexicon, evidence phrasing, and calibrated claims — <strong>absent in math RL</strong>.<br/> Previous works have shown reasoning-only post-training harms performance on downstream instruction-following and knowledge recall tasks <d-cite key="aggarwal2025optimalthinkingbench"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-budget-alignment/r2-480.webp 480w,/2026/assets/img/2026-04-27-budget-alignment/r2-800.webp 800w,/2026/assets/img/2026-04-27-budget-alignment/r2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-budget-alignment/r2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="rq3--where-should-rl-start-from-base-or-sft">RQ3 — Where should RL start from: Base or SFT?</h2> <p><strong>Design.</strong><br/> Compare <strong>GRPO-from-Base vs GRPO-from-SFT</strong> (Fig. RQ3).</p> <p><strong>Patterns.</strong></p> <ul> <li> <p><strong>GRPO-from-SFT is a steadier path.</strong><br/> On MMLU-Math FR, for example, GRPO-SFT sits around <strong>~98 pass@10</strong> while GRPO-Base is closer to <strong>~70</strong>,<br/> i.e., <strong>starting from SFT provides language consistency and still improves accuracy.</strong></p> </li> <li> <p><strong>SFT → RL keeps the multilingual policy.</strong><br/> Because SFT already forced the model to reason in Japanese/French/Spanish,<br/> RL on top of that mostly optimizes correctness <strong>without switching back to EN/ZH reasoning</strong> (Fig. 1.b).</p> </li> </ul> <p><strong>Interpretation.</strong><br/> <strong>SFT establishes the multilingual “reasoning policy.”</strong><br/> Starting RL from the SFT model lets GRPO optimize correctness <em>while preserving language consistency</em>.<br/> RL from Base sometimes pushes the model back toward its original reasoning style while still producing answers in the target language.<br/> That can make a few out-of-domain slices look better, but it also increases variance and <strong>style regression</strong> compared to starting from SFT.</p> <p><strong>Practical rule.</strong><br/> If you care about following (see Figure 1.b) <strong>and</strong> better in-domain accuracy, <strong>do GRPO after SFT.</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-budget-alignment/r3-480.webp 480w,/2026/assets/img/2026-04-27-budget-alignment/r3-800.webp 800w,/2026/assets/img/2026-04-27-budget-alignment/r3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-budget-alignment/r3.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="rq4--can-we-push-the-pareto-frontier-instead-of-trading-accuracy-for-language-consistency">RQ4 — Can we push the Pareto frontier instead of trading accuracy for language consistency?</h2> <p><strong>Design.</strong><br/> Plot Accuracy (x-axis) vs Following (y-axis) for each regime (4-panel Pareto figure).<br/> Then, inspect bar/line panels per dataset and language.</p> <h3 id="what-we-see">What we see.</h3> <ul> <li> <p><strong>SFT shifts points up</strong> (Following ↑).<br/> On some hard sets, accuracy dips slightly.</p> </li> <li><strong>GRPO-SFT shifts rightward</strong> (Accuracy ↑) with at most a small upward loss, compared with SFT-only — <strong>creating new frontiers on:</strong> <ul> <li><strong>MMLU-Math (JA/FR/ES):</strong> both metrics are high.</li> <li><strong>GPQA-ES:</strong> strong frontier point.</li> </ul> </li> <li><strong>Non-frontier holdouts:</strong> Pro-Med FR/JA and AIME-ES, where domain/reward mismatch persists.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-budget-alignment/r4-480.webp 480w,/2026/assets/img/2026-04-27-budget-alignment/r4-800.webp 800w,/2026/assets/img/2026-04-27-budget-alignment/r4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-budget-alignment/r4.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Bottom line.</strong><br/> Read each plot within the same language marker (Japanese ▲, French ■, Spanish ●) and compare colors:</p> <ul> <li><strong>yellow vs. blue</strong> = GRPO-from-SFT vs. Base</li> <li><strong>green vs. blue</strong> = SFT vs. Base</li> </ul> <p>Under this pairing:</p> <blockquote> <p><strong>GRPO-from-SFT (yellow) strictly Pareto-dominates Base (blue) in 9 of 12 language–dataset pairs</strong> (higher on both accuracy and following).</p> </blockquote> <p>In the remaining pairs, yellow usually raises following but gives up a little accuracy —<br/> i.e., a mixed trade-off rather than a strict Pareto gain.</p> <p>SFT (green) vs. Base (blue) generally shifts points up/right, and <strong>GRPO-from-SFT most often traces the upper-right envelope</strong> when strict dominance does occur.</p> <hr/> <h2 id="rq5--does-model-merging-help">RQ5 — Does model merging help?</h2> <p><strong>Motivation.</strong><br/> GRPO+SFT often peaks on math but can regress on knowledge-heavy sets (e.g., Pro Medicine),<br/> and SFT alone doesn’t consistently stabilize accuracy across Japanese/French/Spanish.</p> <p>Ideally, we want a solution that smooths these trade-offs while <strong>keeping language-consistency strong</strong>.<br/> Previous studies have shown that model merging is a promising approach to combine models’ abilities, albeit with some performance degradation <d-cite key="ustun-etal-2024-aya"></d-cite>.</p> <p>Here, we merged the base model with the other three SFT models using <code class="language-plaintext highlighter-rouge">merge-kit</code> with an equal linear merge.</p> <blockquote> <p>The merged approach is quite promising as a one-stop solution!</p> </blockquote> <h3 id="result-avg-pattern-across-datasets">Result (avg pattern across datasets)</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-budget-alignment/r5b-480.webp 480w,/2026/assets/img/2026-04-27-budget-alignment/r5b-800.webp 800w,/2026/assets/img/2026-04-27-budget-alignment/r5b-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-budget-alignment/r5b.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-budget-alignment/r5a-480.webp 480w,/2026/assets/img/2026-04-27-budget-alignment/r5a-800.webp 800w,/2026/assets/img/2026-04-27-budget-alignment/r5a-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-budget-alignment/r5a.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>MERGE consistently shrinks worst-case losses and raises floor performance</strong>, especially where SFT/GRPO dip.<br/> On Pro Medicine, MERGE recovers large chunks of accuracy for Japanese/French<br/> (e.g., JA pass@10 climbs from SFT/GRPO’s ~47–58% to ~70%; FR from ~47–70% to ~76%),<br/> while staying competitive on AIME/GPQA and within a few points of GRPO+SFT on MMLU-Math.</p> <p>In Spanish, where SFT already leads on Medicine, MERGE lands in the middle of Base vs SFT/GRPO+SFT rather than decreasing performance to Base.</p> <p>Overall, it trades a small slice of peak scores for <strong>lower variance across languages and tasks.</strong></p> <h3 id="interpretation">Interpretation</h3> <p>Parameter-space interpolation acts like an ensemble/regularizer:</p> <ul> <li>MERGE <strong>blends GRPO’s strong multi-step heuristics</strong> with <strong>SFT’s alignment priors</strong></li> <li>Dampens overfitting to any single regime</li> <li><strong>Stabilizes cross-lingual behavior</strong></li> </ul> <p>Practically, it expresses a steering effect:</p> <blockquote> <p>“You can dial toward robustness without re-running RL.”</p> </blockquote> <p>When you need:</p> <ul> <li>the <strong>highest leaderboard peak</strong>, pick <strong>GRPO+SFT</strong></li> <li><strong>reliable, in-language reasoning across JA/FR/ES</strong>, especially on domain-heavy sets, pick <strong>MERGE</strong></li> </ul> <blockquote> <p>MERGE is the safer default when you are data + compute-poor.</p> </blockquote> <hr/> <h2 id="discussion-where-performance-regresses-and-potential-solutions">Discussion: Where performance regresses, and potential solutions</h2> <p><strong>Empirical signal.</strong><br/> After SFT followed by GRPO, Japanese language consistency improves markedly, but accuracy lags French (e.g., AIME-JA pass@1 <strong>4.4 → 17.9</strong>, pass@10 <strong>22.6 → 34.4</strong>;<br/> AIME-FR pass@1 <strong>22.2 → 27.3</strong>, pass@10 <strong>46.3 → 48.2</strong>), indicating Japanese-specific friction even with its high increase.</p> <p>Spanish on AIME shows the opposite tension: the <strong>Base</strong> model scores well because it always reasons in English despite Spanish prompts, while <strong>SFT+GRPO enforces Spanish chains and accuracy drops</strong>.</p> <p>In Pro-Medicine, <strong>math-only GRPO from SFT causes regression</strong> (e.g.,<br/> FR pass@10 <strong>70.1 → 46.6</strong>, ES <strong>86.6 → 76.6</strong>, JA <strong>75.9 → 58.3</strong>), whereas GRPO started from Base hurts less.</p> <h3 id="mechanisms">Mechanisms</h3> <ol> <li> <p><strong>Language-prior competition.</strong><br/> The model’s strongest <em>reasoning prior</em> is in EN/ZH.<br/> Under difficulty, chains drift toward those priors.<br/> SFT+GRPO strengthens language consistency, which <strong>reduces access to English-anchored reasoning traces</strong> that previously helped (e.g., AIME-ES).<br/> → evidenced by the huge language-consistency bump.</p> </li> <li> <p><strong>Tokenizer &amp; formatting tax (Japanese &gt; French / Spanish).</strong><br/> Mixed scripts, half/full-width digits, unit variants, and thousand separators inflate perplexity on numeric steps — precisely where accuracy is most sensitive.</p> </li> <li> <p><strong>Cue misalignment in Spanish math.</strong><br/> AIME leans on algebra/number-theory “recipes” the model learned primarily in English<br/> (phrases like “let x be,” “gcd,” “mod”).<br/> Spanish equivalents (“sea x,” “mcd,” “módulo”) are rarer, longer, more accented <br/> → model drifts into slower or incorrect approaches mid-solution.</p> </li> <li> <p><strong>Reward misspecification in medicine.</strong><br/> Math-only RL optimizes numeric correctness, <strong>not</strong> biomedical recall, calibration, or evidence style. The policy over-indexes math heuristics and becomes <strong>over-assertive</strong> on clinical QA.</p> </li> <li> <p><strong>Starting-point effect.</strong><br/> RL from SFT pushes the policy toward SFT’s language/style anchors and away from neutral reasoning.<br/> On medicine, this causes bigger drops. RL from Base is more neutral; regressions are smaller.</p> </li> </ol> <h3 id="lightweight-fixes-that-may-work-across-cases">Lightweight fixes that may work across cases</h3> <ul> <li> <p><strong>Prompt-level normalization (before more training).</strong></p> <ul> <li> <p><em>Japanese:</em> unify to half-width digits/decimals/exp notation; no thousand separators;<br/> explicit math chain template in Japanese. <br/> Example: <code class="language-plaintext highlighter-rouge">数字は半角… SI を使用し…</code>.</p> </li> <li> <p><em>Spanish:</em> prefer <code class="language-plaintext highlighter-rouge">gcd / lcm / mod</code>, exponent notation, half-width digits;<br/> terse step headers (<code class="language-plaintext highlighter-rouge">Definimos / Sustituimos / Comprobación / Respuesta</code>).</p> </li> </ul> </li> <li> <p><strong>Tokenizer-aware formatting.</strong><br/> Consistent spacing around numerals/operators; avoid formatting that fragments tokens.</p> </li> <li> <p><strong>Targeted SFT top-ups.</strong><br/> Small, math-dense Japanese/Spanish datasets using normalized templates to reinforce per-language priors.</p> </li> <li> <p><strong>Reward shaping for GRPO.</strong></p> <ul> <li> <p>For <strong>AIME-ES</strong>: up-weight <em>correctness</em> and make <strong>“Spanish-only chain”</strong> a secondary objective.<br/> → nudges reasoning into Spanish <strong>without punishing English-anchored correct answers</strong>.</p> </li> <li> <p>For <strong>Medicine</strong>: add a <strong>tiny medical reward head</strong><br/> (terminology fidelity, claim calibration, evidence cues),<br/> plus a <strong>KL / behavior-cloning regularizer</strong> toward medical SFT to preserve discourse style.</p> </li> <li> <p>Use <strong>mixed-objective batches</strong> (math + clinical QA),<br/> and replay OOD medical exemplars during RL to avoid domain forgetting.</p> </li> </ul> </li> </ul> <h3 id="takeaway">Takeaway</h3> <p>The regressions likely stem from one cause:</p> <blockquote> <p><strong>objective + prior mismatch</strong>.</p> </blockquote> <p>Japanese/Spanish math suffers from tokenization and cue issues; medicine suffers from the absence of domain-specific rewards. Normalizing inputs, adding small language-aware SFT top-ups, and turning “math-only RL” into multi-objective RL (with correctness-first weighting for AIME-ES and a small medical head for Pro-Medicine) could be promising ways to recover accuracy while keeping outputs in the target language and accurate.</p> <hr/> <h2 id="blog-summary--practical-takeaways">Blog Summary — Practical takeaways</h2> <ol> <li> <p><strong>If you can only afford one step, do SFT (a few hundred high-quality SFT data).</strong><br/> You’ll almost certainly fix language-consistency without compromising accuracy;<br/> you might also get accuracy improvements on in-domain tasks.</p> </li> <li> <p><strong>If you can afford two steps, do SFT → GRPO-SFT.</strong><br/> Use <strong>high clip / no KL</strong>; keep rollouts moderate; verify you haven’t regressed following.</p> </li> <li> <p>A practical and computationally efficient approach is <strong>model merging among SFT models</strong>.</p> </li> <li> <p><strong>For medicine or other narrative-dense domains, add a tiny domain reward with in-domain data or a dozens-scale domain SFT.</strong></p> </li> <li> <p><strong>For Japanese (or any non-Latin script), include numeric/style templates</strong><br/> and optionally patch tokenization via formatting.</p> </li> <li> <p><strong>Track Pareto, not single metrics.</strong><br/> Always plot <em>(Accuracy, Following)</em> together; real wins move you <strong>up-and-right</strong>.</p> </li> </ol> <hr/> <h2 id="limitations--threats-to-validity">Limitations &amp; threats to validity</h2> <ul> <li> <p><strong>Dataset scope.</strong><br/> We use four well-known benchmarks; real-world prompts are noisier.</p> </li> <li> <p><strong>Reward misspecification.</strong><br/> Math-only RL can hurt non-math; the suggested fixes mitigate but don’t prove generality across all medical subspecialities.</p> </li> <li> <p><strong>Model prior.</strong><br/> EN/ZH dominance shapes outcomes. A different base prior (e.g., EU-centric) could change which languages are hardest.</p> </li> <li> <p><strong>Language-consistency metric.</strong><br/> Strong, script-aware, but still an automatic proxy; human raters may be stricter.</p> </li> </ul>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[We explore a two step multilingual alignment recipe for large language models to keep reasoning and answers in the user language while preserving accuracy.]]></summary></entry><entry><title type="html">ChunkTabPFN: Training-free Long Context</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/chunked-tabpfn/" rel="alternate" type="text/html" title="ChunkTabPFN: Training-free Long Context"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/chunked-tabpfn</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/chunked-tabpfn/"><![CDATA[<h2 id="1-introduction">1. Introduction</h2> <p><span id="sec:introduction"></span></p> <p>Large language models leverage <strong>in-context learning (ICL)</strong> by adapting their predictions at inference time based solely on provided examples, without requiring any gradient updates. Building on this idea, recent work on <strong>tabular foundation models</strong>, such as TabPFN, TabICL, Mitra, and Limix, extends the same paradigm to tabular data <d-cite key="hollmann2022tabpfn,hollmann2025accurate,qu2025tabicl,zhang2025mitra,zhang2025limix"></d-cite>. These models are trained once on synthetic tasks drawn from a prior, allowing them to approximate the posterior predictive distribution</p> \[p(y_{*} \mid x_*, D_{\text{train}})\] <p>in a single forward pass by supplying the training set as context, without any dataset-specific fine-tuning, without fine-tuning on each new dataset <d-cite key="hollmann2022tabpfn,hollmann2025accurate"></d-cite>. This approach is compelling because it contrasts with most deep tabular models—like TabNet, FT-Transformer, NODE, TabM, or retrieval-style models such as TabR and ModernNCA, which typically require dataset-specific training or fine-tuning <d-cite key="arik2021tabnet,gorishniy2021revisiting,popov2019neural,gorishniy2024tabm,gorishniy2023tabr,ye2024modern"></d-cite>. That dependency undermines the ideal of a true “drop-in foundation model.”</p> <p>ICL-based tabular models move closer to this ideal. However, they face a major practical limitation: <strong>context length</strong>. Transformer attention scales quadratically with sequence length, and current public TabPFN implementations are constrained to around 3,000 samples in the original work to 10,000<d-footnote>At the time of writing, the new TabPFN v2.5 model has just been released, which is supposed to have pushed the context limit further to 50,000.</d-footnote> in later versions <d-cite key="hollmann2022tabpfn,hollmann2025accurate"></d-cite>. Many real-world tabular datasets far exceed these limits.</p> <p>To address this, researchers have experimented with <strong>shrinking the context</strong>, such as by clustering, partitioning, or retrieving only subsets of the data. Examples include random-forest partitioning <d-cite key="hollmann2025accurate"></d-cite>, the Mixture of In-Context Prompters (MICP) <d-cite key="xu2024mixture"></d-cite>, and KNN-style retrieval <d-cite key="thomas2024retrieval"></d-cite>. Others, like TuneTables <d-cite key="feuer2024tunetables"></d-cite>, compress the data into learned representations.</p> <p>While these methods can be effective, they come with two drawbacks:</p> <ul> <li>They often require <strong>dataset-specific tuning</strong> or even retraining, which contradicts the zero-shot, pure ICL philosophy.</li> <li>They don’t use the <strong>entire training set</strong>, which is a core assumption of TabPFN’s Bayesian approximation. Replacing full data with summaries introduces conceptual inaccuracy.</li> </ul> <p>Hence, we ask the following question:</p> <blockquote> <p>Can we fit <strong>all training examples</strong> into the context (no pruning, no KNN) without learnable compression while staying within GPU memory?</p> </blockquote> <p>In this work, we focus specifically on TabPFN, though we believe the conclusions extend to other ICL-based tabular models. Our answer is a resounding <strong>yes</strong>. Indeed, TabPFN’s native implementation already supports this on some devices via <strong>FlashAttention</strong> <d-cite key="dao2022flashattention,dao2023flashattention,shah2024flashattention"></d-cite>. But as we’ll show in this blogpost, there are important caveats:</p> <ul> <li>FlashAttention and similar efficient mechanisms can <strong>fail</strong> when batch or head sizes exceed 65,535.</li> <li>These optimizations are <strong>unsupported</strong> on older or consumer-grade GPUs.</li> </ul> <p>To resolve this, we introduce a <strong>simple patch</strong>:</p> <ul> <li>For efficient attention, we <strong>chunk inputs</strong> along head or batch dimensions to avoid hitting the 65,536 limit.</li> <li>For older GPUs, we implement a <strong>chunked forward pass</strong> in pure PyTorch using the <strong>incremental log-sum-exp trick</strong>.</li> </ul> <p>This patch yields results <strong>identical to standard attention</strong> (up to floating-point associativity), without any approximations, fine-tuning, or pre-filtering.</p> <p>Empirically, we then test TabPFN out-of-the-box scalability by evaluating it on the full <strong>TabArena</strong> benchmark <d-cite key="tabarena"></d-cite>. We specifically analyze TabPFN performance on datasets with <strong>long contexts</strong> (&gt; 10,000). Key findings include:</p> <ul> <li><strong>Accuracy improves</strong> with more data, often up to 100,000+ rows (measured in AUC for classification and RMSE for regression).</li> <li>On smaller contexts (&lt;10,000), our chunked version <strong>matches the original</strong>—no hidden degradation.</li> <li>The runtime stays <strong>practical</strong> even on commodity GPUs.</li> </ul> <h2 id="2-methodology">2. Methodology</h2> <p><span id="sec:methodology"></span></p> <p>Let <code class="language-plaintext highlighter-rouge">(X, y)</code> be the input to the TabPFN model. The typical dimensions of the feature tensor are <code class="language-plaintext highlighter-rouge">[B, L, F]</code>, where <code class="language-plaintext highlighter-rouge">B</code> is the number of datasets in the batch, <code class="language-plaintext highlighter-rouge">L</code> is the (padded) sample size, and <code class="language-plaintext highlighter-rouge">F</code> is the number of features. The first thing TabPFN does is group features <code class="language-plaintext highlighter-rouge">X</code> and embed them, which yields the following shape: <code class="language-plaintext highlighter-rouge">[B, L, G, D]</code>, where <code class="language-plaintext highlighter-rouge">G</code> is the number of feature groups and <code class="language-plaintext highlighter-rouge">D</code> is the embedding size. In the rest of the blog, we assume <code class="language-plaintext highlighter-rouge">X</code> already has this post-embedding shape.</p> <p>The labels <code class="language-plaintext highlighter-rouge">y</code> are similarly embedded and then concatenated with the features along the group dimension, producing an input of shape <code class="language-plaintext highlighter-rouge">[B, L, G + 1, D]</code>. A keen reader might notice that <code class="language-plaintext highlighter-rouge">y</code> and <code class="language-plaintext highlighter-rouge">X</code> effectively have different “logical” lengths: <code class="language-plaintext highlighter-rouge">X</code> includes both train and test samples, while <code class="language-plaintext highlighter-rouge">y</code> is only provided for the training split. This is handled by padding the label embeddings for test samples with a dummy embedding. A variable <code class="language-plaintext highlighter-rouge">single_eval_pos</code> in the original code holds the index where train and test samples are concatenated, and this logic can be seen in the <code class="language-plaintext highlighter-rouge">transformer.py</code> file of the original TabPFN repository.</p> <p>The core of TabPFN is the attention mechanism, whose logic is primarily implemented in <code class="language-plaintext highlighter-rouge">layer.py</code>. TabPFN, like many Transformer-style models, uses attention in two ways: <strong>between samples</strong> and <strong>between features</strong>. The between-sample attention has both self- and cross-attention components: self-attention among training samples and cross-attention from test samples to train samples. Following the TabPFN implementation, we assume attention layers expect input of shape <code class="language-plaintext highlighter-rouge">[batch, seq_len, input_size]</code>. In the code, the leading dimensions before <code class="language-plaintext highlighter-rouge">(seq_len, input_size)</code> are collapsed via <code class="language-plaintext highlighter-rouge">_rearrange_inputs_to_flat_batch</code>. For between-feature attention this yields an effective batch size of <code class="language-plaintext highlighter-rouge">L * B</code>, whereas for between-item (between-sample) attention it yields <code class="language-plaintext highlighter-rouge">(G + 1) * B</code>.</p> <p>Recall that efficient attention implementations in PyTorch (such as the fused CUDA kernels backing <code class="language-plaintext highlighter-rouge">torch.nn.functional.scaled_dot_product_attention</code>) tile work across the <strong>batch</strong> and <strong>head</strong> dimensions. On NVIDIA GPUs of Ampere architecture and below, this effectively limits the product <code class="language-plaintext highlighter-rouge">B * num_heads</code> to at most <code class="language-plaintext highlighter-rouge">65535</code> CUDA blocks; when it reaches <code class="language-plaintext highlighter-rouge">65536</code> the kernel can fail with <code class="language-plaintext highlighter-rouge">CUDA error: invalid configuration argument</code> (see the corresponding <a href="https://github.com/pytorch/pytorch/issues/133976">PyTorch GitHub issue</a> for a minimal example where <code class="language-plaintext highlighter-rouge">65535</code> works but <code class="language-plaintext highlighter-rouge">65536</code> fails). In TabPFN, large sample sizes <code class="language-plaintext highlighter-rouge">L</code> or a large number of feature groups <code class="language-plaintext highlighter-rouge">G</code> can easily push these flattened batch sizes (<code class="language-plaintext highlighter-rouge">L * B</code> or <code class="language-plaintext highlighter-rouge">(G + 1) * B</code>) past this limit.</p> <p>A simple practical fix is to loop over the flattened batch dimension in chunks, so that each call to <code class="language-plaintext highlighter-rouge">scaled_dot_product_attention</code> stays within the kernel’s limits. This keeps the rest of the model unchanged while avoiding the <code class="language-plaintext highlighter-rouge">invalid configuration</code> errors at large <code class="language-plaintext highlighter-rouge">L</code> or <code class="language-plaintext highlighter-rouge">G</code>. Conceptually, this is can be done via the following patch to the attention computation.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">q_chunk</span><span class="p">,</span> <span class="n">k_chunk</span><span class="p">,</span> <span class="n">v_chunk</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">k_b</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">v_b</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
<span class="p">):</span>
    <span class="c1"># (B_chunk, Lq, H, D) -&gt; (B_chunk, H, Lq, D)
</span>    <span class="n">Q</span> <span class="o">=</span> <span class="n">q_chunk</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">k_chunk</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">v_chunk</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">()</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span>
        <span class="n">Q</span><span class="p">,</span>
        <span class="n">K</span><span class="p">,</span>
        <span class="n">V</span><span class="p">,</span>
        <span class="n">attn_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">dropout_p</span><span class="o">=</span><span class="n">dropout_p</span> <span class="k">if</span> <span class="n">dropout_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">is_causal</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">scale</span><span class="o">=</span><span class="n">softmax_scale</span><span class="p">,</span>
    <span class="p">)</span>  <span class="c1"># (B_chunk, H, Lq, D)
</span>
    <span class="c1"># -&gt; (B_chunk, Lq, H, D)
</span>    <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">())</span>

<span class="n">attention_head_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <p>A different issue is <strong>hardware support</strong> for efficient attention kernels. PyTorch’s <code class="language-plaintext highlighter-rouge">scaled_dot_product_attention</code> can dispatch to several backends on CUDA: FlashAttention, memory-efficient attention, or a plain math implementation in C++. The availability of these specialized kernels varies across GPU generations. For educational purposes, and for those who wish to implement these kernels on older or unsupported devices, we refer to <a href="https://github.com/lucidrains/memory-efficient-attention-pytorch/tree/main">this repository</a>. We provide a brief sketch of how the chunking works to reduce the memory footprint below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">chunked_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">q_chunk</span><span class="p">,</span> <span class="n">kv_chunk</span><span class="p">,</span> <span class="n">scale</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    q: (..., Lq, D)
    k: (..., Lk, D)
    v: (..., Lk, Dv)
    q_chunk: size of query tiles (l)
    kv_chunk: size of key/value tiles (r)
    </span><span class="sh">"""</span>
    <span class="n">Lq</span><span class="p">,</span> <span class="n">Lk</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">k</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">qs</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Lq</span><span class="p">,</span> <span class="n">q_chunk</span><span class="p">):</span>
        <span class="n">qe</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">qs</span> <span class="o">+</span> <span class="n">q_chunk</span><span class="p">,</span> <span class="n">Lq</span><span class="p">)</span>
        <span class="n">q_tile</span> <span class="o">=</span> <span class="n">q</span><span class="p">[...,</span> <span class="n">qs</span><span class="p">:</span><span class="n">qe</span><span class="p">,</span> <span class="p">:]</span>                            <span class="c1"># (..., l, D)
</span>
        <span class="c1"># running stats per query row
</span>        <span class="n">mu</span> <span class="o">=</span> <span class="n">q_tile</span><span class="p">.</span><span class="nf">new_full</span><span class="p">(</span><span class="n">q_tile</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="nf">float</span><span class="p">(</span><span class="sh">"</span><span class="s">inf</span><span class="sh">"</span><span class="p">))</span>  <span class="c1"># (..., l)
</span>        <span class="n">s</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>                               <span class="c1"># (..., l)
</span>        <span class="n">a</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="o">*</span><span class="n">mu</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">v</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                         <span class="n">device</span><span class="o">=</span><span class="n">q</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">q</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>        <span class="c1"># (..., l, Dv)
</span>
        <span class="k">for</span> <span class="n">ks</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Lk</span><span class="p">,</span> <span class="n">kv_chunk</span><span class="p">):</span>
            <span class="n">ke</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">ks</span> <span class="o">+</span> <span class="n">kv_chunk</span><span class="p">,</span> <span class="n">Lk</span><span class="p">)</span>
            <span class="n">k_tile</span> <span class="o">=</span> <span class="n">k</span><span class="p">[...,</span> <span class="n">ks</span><span class="p">:</span><span class="n">ke</span><span class="p">,</span> <span class="p">:]</span>                           <span class="c1"># (..., r, D)
</span>            <span class="n">v_tile</span> <span class="o">=</span> <span class="n">v</span><span class="p">[...,</span> <span class="n">ks</span><span class="p">:</span><span class="n">ke</span><span class="p">,</span> <span class="p">:]</span>                           <span class="c1"># (..., r, Dv)
</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">q_tile</span><span class="p">,</span> <span class="n">k_tile</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">scale</span>
            <span class="n">local_max</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="n">values</span>               <span class="c1"># (..., l)
</span>            <span class="n">new_mu</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">local_max</span><span class="p">)</span>

            <span class="c1"># rescale old aggregates
</span>            <span class="n">alpha</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">mu</span> <span class="o">-</span> <span class="n">new_mu</span><span class="p">)</span>
            <span class="n">s</span> <span class="o">*=</span> <span class="n">alpha</span>
            <span class="n">a</span> <span class="o">*=</span> <span class="n">alpha</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">]</span>

            <span class="c1"># accumulate current tile
</span>            <span class="n">exp_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">new_mu</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">])</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="n">exp_logits</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>                         <span class="c1"># sum_k e^{z_k}
</span>            <span class="n">a</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">exp_logits</span><span class="p">,</span> <span class="n">v_tile</span><span class="p">)</span>               <span class="c1"># sum_k e^{z_k} v_k
</span>
            <span class="n">mu</span> <span class="o">=</span> <span class="n">new_mu</span>

        <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">a</span> <span class="o">/</span> <span class="n">s</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">])</span>                        <span class="c1"># softmax = a / s
</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>                           <span class="c1"># (..., Lq, Dv)
</span></code></pre></div></div> <p>In this implementation, the key components are:</p> <ul> <li>It tiles queries into chunks <code class="language-plaintext highlighter-rouge">q_chunk</code> instead of processing all <code class="language-plaintext highlighter-rouge">Lq</code> at once.</li> <li>It streams over keys/values in chunks <code class="language-plaintext highlighter-rouge">kv_chunk</code>, computing only <code class="language-plaintext highlighter-rouge">l × r</code> logits at a time.</li> <li>It maintains per-row running statistics <code class="language-plaintext highlighter-rouge">(mu, s, a)</code> using a numerically stable log-sum-exp merge, so the final output matches full attention as if we had formed the entire <code class="language-plaintext highlighter-rouge">Lq × Lk</code> score matrix in one go.</li> </ul> <h2 id="3-experiments">3. Experiments</h2> <p><span id="sec:experiments"></span></p> <p>We evaluate the TabPFN v2 model with chunking enabled on <strong>TabArena</strong> <d-cite key="tabarena"></d-cite>, which includes 51 tabular datasets spanning classification and regression tasks. We report scaling statistics for memory and runtime in Figure 1, and overall performance on TabArena in Figure 2. Note that in the original and subsequent reports of TabPFN, LIMIX, and TabICL on TabArena, the authors have typically imputed values that exceeded the context length for their respective methods. This might have created a distorted view of model capabilities. In Figure 2, we use only directly measured (non-imputed) results.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results-480.webp 480w,/2026/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results-800.webp 800w,/2026/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="attn-figure-caption"> Figure 1. Scaling TabPFN v2 to long contexts. Chunked TabPFN matches baseline accuracy where both fit, and extends inference to 100K+ examples. </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-chunked-tabpfn/elo_vs_baselines-480.webp 480w,/2026/assets/img/2026-04-27-chunked-tabpfn/elo_vs_baselines-800.webp 800w,/2026/assets/img/2026-04-27-chunked-tabpfn/elo_vs_baselines-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-chunked-tabpfn/elo_vs_baselines.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="attn-figure-caption"> Figure 2. Elo and normalized score across TabArena. Striped bars denote prior imputed TabPFN runs (filled with Random Forest fallbacks when OOM); our chunked TabPFN reports direct measurements. </div> <p>Separately, we evaluate TabPFN v2 on the same long-context datasets while varying the context length. Specifically, we sample <code class="language-plaintext highlighter-rouge">num_samples</code> points from each dataset and then report performance, memory, and runtime in Figure 3. To better understand how context length affects TabPFN’s performance, we perform a <em>scaling study</em> on the 15 “long-context” datasets from TabArena. For each dataset, we subsample the training set to progressively larger sizes (3,000 → 5,000 → 10,000 → 20,000 → 50,000 → 100,000) and compare baseline TabPFN v2 against our Chunked TabPFN.</p> <ul> <li>Chunked TabPFN maintains <em>exact equivalence</em> to baseline TabPFN while extending feasible context length by roughly 10×.</li> <li>Empirical scaling shows either plateau or monotonic improvement—never catastrophic degradation.</li> <li>Memory and runtime growth are linear in chunk size, enabling inference on 100 K+ examples with a single GPU.</li> </ul> <p>These findings reinforce that <strong>TabPFN’s in-context generalization truly extends beyond its training limit</strong>, and that the primary bottleneck was <em>implementation-level memory</em>, not <em>model-level capacity</em>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results_per_dataset-480.webp 480w,/2026/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results_per_dataset-800.webp 800w,/2026/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results_per_dataset-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-chunked-tabpfn/tabarena_long_results_per_dataset.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="attn-figure-caption"> Figure 4. Scaling curves for long-context datasets. Each plot shows RMSE, AUC, wall-clock inference time (s), and peak GPU memory (MB). Chunked TabPFN tracks baseline accuracy exactly up to 10 K examples and continues scaling to 100 K without degradation. </div> <h2 id="4-conclusion">4. Conclusion</h2> <p><span id="sec:conclusion"></span></p> <p>We presented <strong>Chunked TabPFN</strong>, an exact tiling strategy that enables TabPFN to process <em>long-context</em> tabular datasets (100 K+ rows) without retraining, fine-tuning, or any pre-processing such as clustering or compression.</p> <p>Our main results show:</p> <ol> <li> <p><strong>Exactness without approximation.</strong> The chunked attention computation is mathematically identical to the original transformer attention—only the evaluation order changes. Predictions match baseline TabPFN bit-for-bit (within floating-point tolerance) for all short-context cases.</p> </li> <li> <p><strong>Memory scalability.</strong> Peak GPU memory scales linearly with tile size instead of quadratically with context length. This removes the practical 10 K-sample ceiling and allows inference on 100 K+ rows using 24–32 GB GPUs.</p> </li> <li> <p><strong>Training-free generalization.</strong> Chunked TabPFN retains the spirit of in-context learning: no dataset-specific training, no hyperparameter search, no adaptation steps. Despite its simplicity, it matches or surpasses tuned deep tabular models on the long-context slice of TabArena.</p> </li> <li> <p><strong>Empirical insights.</strong> Many datasets continue to improve with larger contexts—suggesting that the PFN prior generalizes beyond its nominal pre-training length.</p> </li> </ol>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Tabular foundation models struggle with large datasets due to the quadratic attention. While methods like FlashAttention promise scalability, practical challenges persist in their application to tabular foundation models. Our work resolves these hurdles, enabling efficient attention, and reveals that contrary to the eariler reports, TabPFN's performance improves with larger contexts, highlighting its inherent robustness and minimal fine-tuning needs when scaling to complex, long datasets from the TabArena benchmark.]]></summary></entry><entry><title type="html">Navigating the Manifold — A Geometric Perspective on Diffusion-Based Inverse Problems</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/diffusion-inverse-problems/" rel="alternate" type="text/html" title="Navigating the Manifold — A Geometric Perspective on Diffusion-Based Inverse Problems"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/diffusion-inverse-problems</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/diffusion-inverse-problems/"><![CDATA[<h2 id="1-background">1. Background</h2> <p>Inverse problems aim to recover an unknown signal \(\mathbf{x} \in \mathbb{R}^n\) from indirect and noisy measurements \(\mathbf{y} \in \mathbb{R}^m\). A large class of problems in imaging, signal processing, and computational physics can be written—at least approximately—as a linear–Gaussian observation model</p> \[\mathbf{y} = \mathbf{A}\mathbf{x} + \mathbf{n}, \qquad \mathbf{n} \sim \mathcal{N}(0, \sigma^2 \mathbf{I})\tag{1.1}\label{eq:ip},\] <p>where \(\mathbf{A}\) encodes the physics or geometry of the measurement process (e.g., convolution with a blur kernel, subsampled Fourier transform, Radon transform), and \(\mathbf{n}\) models sensor noise and modeling errors. The fundamental difficulty is that many such problems are <strong>ill-posed</strong>:</p> <ul> <li> <p><strong>Non-uniqueness:</strong> if \(\mathbf{A}\) is rank-deficient or severely underdetermined, many different \(\mathbf{x}\) produce the same \(\mathbf{y}\).</p> </li> <li> <p><strong>Instability:</strong> when \(\mathbf{A}\) is ill-conditioned, small perturbations in \(\mathbf{y}\) can cause large changes in the reconstructed \(\mathbf{x}\).</p> </li> </ul> <p>If we ignore prior knowledge and rely solely on the measurements to solve via maximum likelihood estimation</p> \[\hat{\mathbf{x}}_{\text{MLE}} = \arg\min_{\mathbf{x}} \frac{1}{2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2,\] <p>it yields the solution either does not exist, is not unique, or is extremely sensitive to noise. This motivates <strong>regularization</strong>: we bias the solution toward signals that are “plausible” under our prior knowledge. In probabilistic form, this leads to <strong>Maximum A Posteriori (MAP)</strong> estimation</p> \[\hat{\mathbf{x}}_{\text{MAP}} = \arg\min_{\mathbf{x}} \left[ \underbrace{\frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2}_{\text{Data Consistency}} \quad + \underbrace{\lambda R(\mathbf{x})}_{\text{Prior / Regularization}} \right],\] <p>where \(R(\mathbf{x})\) is a regularizer encoding the prior, and \(\lambda &gt; 0\) controls the trade-off between data fidelity and prior strength.</p> <p>From this perspective, the history of solving inverse problems is essentially the history of designing better functions for $R(\mathbf{x})$ — from handcrafted mathematical assumptions to data-driven generative manifolds.</p> <h3 id="11-handcrafted-analytical-priors">1.1. Handcrafted Analytical Priors</h3> <p>Before the era of deep learning, priors were designed based on mathematical and statistical intuition about natural signals.</p> <ul> <li> <p><strong>Smoothness priors (Tikhonov / Gaussian):</strong><br/> Assuming that \(\mathbf{x}\) varies smoothly, one uses \(R(\mathbf{x}) = \|\mathbf{L}\mathbf{x}\|_2^2\) for some differential or finite-difference operator \(\mathbf{L}\). This corresponds to a Gaussian prior and leads to classical Tikhonov regularization <d-cite key="tikhonov1977solutions"></d-cite>. Such priors are stable and convex, but they oversmooth edges and cannot hallucinate missing high-frequency details.</p> </li> <li> <p><strong>Total Variation (TV) and piecewise-smooth priors:</strong><br/> To better preserve edges, TV replaces the quadratic penalty with an \(\ell_1\)-type penalty on gradients, e.g., \(R(\mathbf{x}) = \|\nabla \mathbf{x}\|_1.\) TV assumes that images are mostly smooth with sparse sharp transitions, which works well for piecewise-constant structures but can introduce “staircase” artifacts and still struggles with fine textures.<d-cite key="rudin1992nonlinear"></d-cite></p> </li> <li> <p><strong>Sparsity priors (wavelets, dictionary learning):</strong><br/> Another line of work assumes that \(\mathbf{x}\) is sparse in a suitable transform domain, such as wavelets or learned dictionaries, leading to penalties like \(R(\mathbf{x}) = \|\mathbf{W}\mathbf{x}\|_1,\) where \(\mathbf{W}\) is a fixed or learned transform. These priors capture certain classes of textures and edges, but still act as <strong>handcrafted surrogates</strong> for the true (and highly complex) distribution of natural signals.</p> </li> </ul> <p>All of these classical priors are <strong>analytical</strong>: they are given by explicit formulas chosen by the designer. They are computationally convenient and theoretically well understood, but ultimately limited in expressiveness. They can discourage obviously “bad” solutions, yet they do not truly know what realistic images look like.</p> <h3 id="12-explicit-generative-manifolds">1.2. Explicit Generative Manifolds</h3> <p>With the rise of deep generative models, we no longer had to <strong>guess</strong> analytical forms for \(R(\mathbf{x})\). Instead, we could <strong>learn</strong> the data distribution directly from examples, and then use that model as a prior.</p> <p>A common idea is the <strong>manifold constraint</strong>: assume that plausible signals lie near the range of a generator \(G(\mathbf{z})\), where \(\mathbf{z}\) is a low-dimensional latent code. Rather than optimizing directly over \(\mathbf{x}\), we optimize over \(\mathbf{z}\):</p> \[\hat{\mathbf{z}} = \arg\min_{\mathbf{z}} \left[ \|\mathbf{y} - \mathbf{A}(G(\mathbf{z}))\|_2^2 + \lambda \|\mathbf{z}\|_2^2 \right], \qquad \hat{\mathbf{x}} = G(\hat{\mathbf{z}}).\] <p>Different generative models instantiate this idea in different ways:</p> <ul> <li> <p><strong>GANs <d-cite key="goodfellow2014generative"></d-cite>:</strong> Adversarial training yields highly realistic samples and sharp details. However, mode collapse and training instability make the learned manifold incomplete: if the ground-truth \(\mathbf{x}\) is not well covered by the generator, optimization in \(\mathbf{z}\) may fail or converge to a visually plausible but incorrect solution.</p> </li> <li> <p><strong>VAEs <d-cite key="kingma2013auto"></d-cite>:</strong> VAEs provide a probabilistic decoder \(p_\theta(\mathbf{x}\mid \mathbf{z})\) with a simple prior on \(\mathbf{z}\) (often Gaussian). Their reconstructions are typically smoother and more stable than GANs but tend to be <strong>blurry</strong>, reflecting the ELBO objective’s tendency to average over modes.</p> </li> <li> <p><strong>Normalizing Flows <d-cite key="dinh2016density,kingma2018glow"></d-cite> and related models:</strong> Flows provide exact likelihoods and invertible mappings, so in principle they offer a very clean way to define \(p(\mathbf{x})\). In practice, however, they impose architectural constraints (e.g., invertibility and tractable Jacobians), and solving inverse problems with them often requires expensive Jacobian–vector products or MCMC steps.</p> </li> </ul> <p>In all these cases, the prior is no longer an explicit penalty \(R(\mathbf{x})\) but an <strong>implicit manifold</strong> traced out by a neural generator. This brings much richer structure but also introduces new optimization and modeling challenges: enforcing measurement consistency while staying on (or near) the learned manifold is nontrivial.</p> <h3 id="13-the-shift-to-diffusion-prior">1.3. The Shift to Diffusion Prior</h3> <p>Both previous families of priors have clear trade-offs:</p> <ul> <li>Classical analytical priors are <strong>stable, convex, and easy to optimize</strong>, but they are too simple to faithfully model complex natural images.</li> <li>Explicit generative manifolds are <strong>expressive</strong> and can synthesize highly realistic samples, but they are often <strong>hard to optimize against</strong> and can be brittle outside the training distribution.</li> </ul> <p>Diffusion models <d-cite key="ho2020denoising,song2020score"></d-cite> offer a different compromise. Instead of defining a single deterministic manifold or a closed-form regularizer, they define a <strong>stochastic generative process</strong> that gradually transforms simple noise into data through a sequence of Gaussian denoising steps. Conceptually, they give us:</p> <ul> <li>A powerful, flexible <strong>prior over signals</strong>, encoded by a learned score or denoiser.</li> <li>A natural way to <strong>interleave prior-driven denoising with measurement-driven corrections</strong> along the sampling trajectory.</li> </ul> <p>This “dynamic” view of the prior—where we guide a stochastic or deterministic trajectory through noisy space—makes diffusion models particularly attractive for inverse problems. It allows us to inject measurement information <strong>gradually</strong>, to decouple a pretrained diffusion prior from the forward operator \(\mathbf{A}\), and to design algorithms that behave more like <strong>operator-splitting schemes</strong> than hard manifold projection. The rest of this post develops this perspective in detail.</p> <h2 id="2-diffusion-priors-for-inverse-problems">2. Diffusion priors for inverse problems</h2> <p>In the background section we framed inverse problems as Bayesian inference with a prior over clean signals \(\mathbf{x}_0\) and a likelihood induced by the forward operator \(\mathbf{A}\). With a diffusion model, this prior is no longer given by an explicit energy \(R(\mathbf{x}_0)\) or a simple manifold parameterization \(G(\mathbf{z})\). Instead, it is encoded in a <strong>stochastic generative process</strong> that gradually transforms Gaussian noise into data. This section explains how that process works and why it makes inverse problems both powerful and subtle.</p> <h3 id="21-bayesian-gold-standard-with-a-diffusion-prior">2.1. Bayesian “gold standard” with a diffusion prior</h3> <p>Consider again the inverse problem where the measurement is generated from a forward model \(\mathbf{y} \sim p(\mathbf{y}\mid \mathbf{x}_0),\) with \(\mathbf{x}_0 \sim p(\mathbf{x}_0)\) given by a diffusion prior. In the simplest case we assume additive Gaussian noise (Eq. \eqref{eq:ip}), so that</p> \[p(\mathbf{y}\mid \mathbf{x}_0) \propto \exp\Big(-\frac{1}{2\sigma^2}\|\mathbf{y}-\mathbf{A}\mathbf{x}_0\|_2^2\Big).\] <p>By Bayes’ rule, the posterior over clean signals is</p> \[p(\mathbf{x}_0\mid \mathbf{y}) \;\propto\; p(\mathbf{y}\mid\mathbf{x}_0)\,p(\mathbf{x}_0).\] <p>If we could evaluate \(\log p(\mathbf{x}_0)\) and its gradient exactly, the <strong>Bayesian gold standard</strong> would be the MAP estimator</p> \[\hat{\mathbf{x}}_{\text{MAP}} = \arg\max_{\mathbf{x}_0} \left[ \underbrace{\log p(\mathbf{y}\mid \mathbf{x}_0)}_{\text{Data fidelity}} + \underbrace{\log p(\mathbf{x}_0)}_{\text{Prior}} \right]. \tag{2.1}\label{eq:obj}\] <p>Conceptually this is straightforward: we simply trade off how well \(\mathbf{x}_0\) explains the measurement \(\mathbf{y}\) versus how plausible \(\mathbf{x}_0\) is under the diffusion prior. In practice, diffusion models make this optimization non-trivial, because the prior is not defined directly in the clean space \(\mathbf{x}_0\).</p> <h3 id="22-diffusion-models-live-in-noisy-space">2.2. Diffusion models live in noisy space</h3> <p>A modern diffusion model is trained in a <strong>noisy</strong> space. One common continuous-time formulation uses a forward SDE</p> \[d\mathbf{x}_t = f(\mathbf{x}_t,t)\,dt + g(t)\,d\mathbf{w}_t, \qquad t\in[0,1],\] <p>where \(\mathbf{x}_0\) is a clean data sample, \(\mathbf{x}_1\) is approximately Gaussian noise, and \(\mathbf{w}_t\) is standard Brownian motion. The forward SDE defines a family of intermediate marginals \(p_t(\mathbf{x}_t)\) that smoothly interpolate between the data distribution at \(t=0\) and a simple reference (typically Gaussian) at \(t=1\). Training learns a <strong>score network</strong></p> \[s_\theta(\mathbf{x}_t,t) \approx \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t),\] <p>i.e., a vector field that points in the direction of steeper log-density at each noise level. Geometrically, the score pushes samples uphill towards regions of higher probability. At generation time, we can run the <strong>reverse-time SDE</strong> <d-cite key="anderson1982reverse"></d-cite></p> \[d\mathbf{x}_t = \Big[f(\mathbf{x}_t,t) - g^2(t)\,s_\theta(\mathbf{x}_t,t)\Big]\,dt + g(t)\,d\bar{\mathbf{w}}_t,\] <p>from \(t=1\) (pure noise) to \(t=0\) (clean data), where \(\bar{\mathbf{w}}_t\) is a standard Brownian motion in reversed time. Alternatively, Song et al. <d-cite key="song2020score"></d-cite> showed that there exists a deterministic <strong>probability-flow ODE (PF-ODE)</strong></p> \[\frac{d\mathbf{x}_t}{dt} = f(\mathbf{x}_t,t) - \frac{1}{2}g^2(t)\,\nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t),\] <p>whose solution curves share the same marginals \(\{p_t\}_{t\in[0,1]}\) as the stochastic SDE. Intuitively, the SDE adds randomness while the ODE simply transports probability mass along the same flow.</p> <p>Crucially, all of these objects – the forward SDE, the reverse SDE, and the PF-ODE – are defined in terms of the <strong>noisy</strong> variables \(\mathbf{x}_t\) and their densities \(p_t(\mathbf{x}_t)\). The prior \(p(\mathbf{x}_0)\) only appears implicitly as the marginal at time zero of this process. This creates a mismatch for inverse problems:</p> <ul> <li> <p>The likelihood \(p(\mathbf{y}\mid \mathbf{x}_0)\) in Eq. \eqref{eq:obj} is naturally written in the <strong>clean space</strong> \(\mathbf{x}_0\).</p> </li> <li> <p>The diffusion prior is only directly accessible through scores \(\nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t)\) in the <strong>noisy space</strong> \(\mathbf{x}_t\).</p> </li> </ul> <p>The rest of this blogpost can be read as a sequence of techniques for bridging this gap: how to couple measurement information with a prior that lives along a diffusion trajectory.</p> <h3 id="23-tweedies-estimator-as-a-clean-space-anchor">2.3. Tweedie’s estimator as a clean-space anchor</h3> <p>A key tool for connecting noisy and clean spaces is <strong>Tweedie’s formula</strong>. In the simplest (and widely used) setting, the forward diffusion at time \(t\) can be written as</p> \[\mathbf{x}_t = \alpha(t)\,\mathbf{x}_0 + \sigma(t)\,\boldsymbol{\epsilon}, \qquad \boldsymbol{\epsilon} \sim \mathcal{N}(0,\mathbf{I}),\] <p>where \(\alpha(t)\) and \(\sigma(t)\) are scalar functions that control the signal and noise levels. Under this Gaussian corruption model, Tweedie’s formula tells us that the posterior mean of the clean signal given a noisy observation \(\mathbf{x}_t\) is</p> \[\hat{\mathbf{x}}_0(\mathbf{x}_t,t) \;=\; \mathbb{E}[\mathbf{x}_0\mid\mathbf{x}_t] \;=\; \frac{\mathbf{x}_t + \sigma^2(t)\,s_\theta(\mathbf{x}_t,t)}{\alpha(t)}.\] <p>Here \(s_\theta(\mathbf{x}_t,t) \approx \nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t)\) is the score learned by the diffusion model. Thus, once the diffusion prior is trained, we can <em>decode</em> any noisy point \(\mathbf{x}_t\) into a corresponding <strong>clean-space estimate</strong> \(\hat{\mathbf{x}}_0(\mathbf{x}_t,t)\) using only the score network.</p> <p>From a statistical perspective, \(\hat{\mathbf{x}}_0(\mathbf{x}_t,t)\) is the <strong>minimum mean-squared error (MMSE)</strong> estimator of \(\mathbf{x}_0\) given \(\mathbf{x}_t\): among all possible functions of \(\mathbf{x}_t\), it minimizes the expected squared error</p> \[\mathbb{E}\big[\|\mathbf{x}_0 - \hat{\mathbf{x}}_0(\mathbf{x}_t,t)\|_2^2\big].\] <p>It is therefore a principled, Bayes-optimal representative of the posterior distribution over \(\mathbf{x}_0\) for that particular noise level. From a geometric perspective, we can interpret \(\hat{\mathbf{x}}_0(\mathbf{x}_t,t)\) as a kind of <strong>clean-space anchor</strong>:</p> <ul> <li>Let \(M_0\) denote the (unknown) data manifold where the diffusion prior assigns high density at \(t=0\).</li> <li>For a noisy point \(\mathbf{x}_t\), the posterior over \(\mathbf{x}_0\) given \(\mathbf{x}_t\) is typically concentrated near a small region on or near \(M_0\).</li> <li>Tweedie’s estimator \(\hat{\mathbf{x}}_0(\mathbf{x}_t,t)\) sits near the <em>center of mass</em> of that posterior cloud. It lives in a high-density neighborhood of \(M_0\), even if it is not exactly on the manifold, and becomes increasingly accurate as \(\sigma(t)\to 0\).</li> </ul> <p>We will repeatedly exploit this anchor in later sections:</p> <ul> <li> <p>In <strong>posterior-guided sampling</strong> (Section 3), we use Tweedie’s estimator to approximate <em>posterior</em> scores in noisy space, enabling us to modify the reverse SDE/ODE so that it samples from an approximation to \(p(\mathbf{x}_0\mid\mathbf{y})\).</p> </li> <li> <p>In <strong>clean-space Local-MAP optimization</strong> (Section 4), we treat \(\hat{\mathbf{x}}_0(\mathbf{x}_t,t)\) as a good initialization in clean space and locally refine it to better satisfy the measurement constraints.</p> </li> </ul> <figure class="figure"> <img src="/2026/assets/img/2026-04-27-diffusion-inverse-problems/tweedie_anchor_sine.gif" alt="Posterior-guided sampling: prior vs naive guidance vs Jacobian guidance" style="max-width: 100%; height: auto; image-rendering: auto;"/> </figure> <div class="caption"> Animation 1: Tweedie denoising as a clean-space anchor. </div> <p><strong>Animated illustration – posterior mean vs. data manifold.</strong><br/> Consider a simple 2D sine-shaped data manifold \(M_0\) and a single clean point \(\mathbf{x}_0\) on it. When we apply the forward diffusion to \(\mathbf{x}_0\), we obtain noisy samples \(\mathbf{x}_t\) at different noise levels:</p> <ul> <li>At <strong>high noise</strong>, the posterior density \(p(\mathbf{x}_0\mid\mathbf{x}_t)\) spreads along a large portion of the curve, and Tweedie’s estimator lies somewhere near the middle of this elongated cloud.</li> <li> <p>As the noise <strong>decreases</strong>, the posterior collapses around the true point on \(M_0\), and \(\hat{\mathbf{x}}_0(\mathbf{x}_t,t)\) moves closer and closer to the manifold.</p> </li> <li>The green semi-transparent disk illustrates an approximate posterior uncertainty region around the Tweedie anchor: as noise decreases, the posterior mass concentrates and the disk shrinks.</li> </ul> <p>This visualization helps explain why Tweedie’s estimator is a useful building block for inverse problems with diffusion priors: even though it is not a strict orthogonal projection onto \(M_0\), it provides a reliable and differentiable anchor in the clean space that we can refine using the measurement model.</p> <p>In the next sections, we show how this anchor, together with the diffusion trajectory in noisy space, gives rise to two practical paradigms for solving inverse problems: <strong>posterior-guided sampling</strong> in noisy space and <strong>clean-space Local-MAP optimization</strong>.</p> <h2 id="3-posterior-guided-sampling-paradigm">3. Posterior-Guided Sampling Paradigm</h2> <p>In this section, we focus on <strong>posterior-guided sampling</strong>, a family of methods that solve inverse problems by directly sampling from the Bayesian posterior under a diffusion prior, exemplified by DPS <d-cite key="chung2022diffusion"></d-cite> and MCG methods <d-cite key="chung2022improving"></d-cite>. The key idea is extremely simple:</p> <aside class="l-body box-note"> <p>Take the reverse-time dynamics of the diffusion model, and replace the <strong>prior score</strong> \(\nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t)\) with the <strong>posterior score</strong> \(\nabla_{\mathbf{x}_t}\log p(\mathbf{x}_t\mid \mathbf{y})\).</p> </aside> <p>Concretely, recall the probability-flow ODE (PF-ODE) associated with the forward SDE</p> \[\frac{d\mathbf{x}_t}{dt} = f(\mathbf{x}_t,t) -\frac{1}{2}g^2(t)\,s_\theta(\mathbf{x}_t,t), \qquad s_\theta(\mathbf{x}_t,t)\approx \nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t),\] <p>whose solution at \(t=0\) recovers samples from the data distribution \(p_{\text{data}}\). Posterior-guided sampling modifies this ODE by swapping in the <strong>posterior score</strong>:</p> \[\frac{d\mathbf{x}_t}{dt} = f(\mathbf{x}_t,t) - \frac{1}{2}g^2(t) \underbrace{\nabla_{\mathbf{x}_t}\log p(\mathbf{x}_t\mid \mathbf{y})}_{\text{posterior score}}. \tag{3.1}\label{eq:pgs}\] <p>By Bayes’ rule, the posterior score decomposes as</p> \[\nabla_{\mathbf{x}_t}\log p(\mathbf{x}_t\mid \mathbf{y}) = \underbrace{\nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t)}_{\text{prior score}} + \underbrace{\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t)}_{\text{likelihood / data-consistency score}}. \tag{3.2}\label{eq:posterior_score}\] <p>Intuitively, the <strong>prior score</strong> pulls trajectories back to high-density regions of the diffusion prior, while the <strong>likelihood score</strong> pushes them toward measurements that satisfy the forward model.</p> <p>Figure 1 visualizes these three vector fields at an intermediate time \(t\): prior score (blue), likelihood score (orange), and their sum—the posterior score (black). The right panel sketches how the resulting trajectories land in the intersection between the diffusion prior manifold \(\mathcal{M}_{\mathrm{DM}}\) and the observation-consistent manifold \(\mathcal{M}_{\mathrm{obs}}\), forming a posterior manifold \(\mathcal{M}_{\mathrm{pos}}\).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-inverse-problems/posterior_guided-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-inverse-problems/posterior_guided-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-inverse-problems/posterior_guided-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-inverse-problems/posterior_guided.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1. Posterior-guided sampling. At intermediate time $t$, the prior score (blue) pulls samples back to the diffusion prior manifold, the likelihood score (orange) pushes them toward measurement-consistent states, and their sum (black) is the posterior score. Integrating the reverse-time PF-ODE with this posterior score yields samples on the posterior manifold $\mathcal{M}_{\mathrm{pos}} = \mathcal{M}_{\mathrm{DM}}\cap \mathcal{M}_{\mathrm{obs}}$ at $t=0$.</figcaption> </figure> <p>The rest of this section explains:</p> <ol> <li><strong>Why</strong> integrating Eq. \eqref{eq:pgs} indeed samples from \(p(\mathbf{x}_0\mid \mathbf{y})\);</li> <li><strong>How</strong> to approximate the intractable likelihood score \(\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t)\) using the Tweedie clean-space anchor introduced in the previous section;</li> <li><strong>What</strong> the Jacobian in this approximation does geometrically, and why it acts as a projection onto the manifold’s tangent space;</li> <li>A <strong>2D toy illustration</strong> that makes these vector fields visible.</li> </ol> <h3 id="31-why-inverse-problems-can-be-solved-by-sampling">3.1. Why Inverse Problems Can Be Solved by Sampling</h3> <p>We first revisit why <strong>sampling</strong> from a suitable reverse-time process solves not only the unconditional generative problem, but also the <strong>inverse problem</strong>.</p> <p>We begin with the <strong>unconditional</strong> case. Let \(\{ \mathbf{x}_t \}_{t\in[0,T]}\) be a trajectory followed by the forward diffusion SDE,</p> \[d\mathbf{x}_t = f(\mathbf{x}_t,t)\,dt + g(t)\,d\mathbf{w}_t,\qquad \mathbf{x}_0 \sim p_{\text{data}},\] <p>with marginal densities \(p_t(\mathbf{x}_t)\). These densities satisfy the <strong>Fokker–Planck equation</strong></p> \[\partial_t p_t(\mathbf{x}_t) = -\nabla_{\mathbf{x}_t}\!\cdot\!\big(f(\mathbf{x}_t,t)\,p_t(\mathbf{x}_t)\big) +\frac{1}{2}\nabla_{\mathbf{x}_t}^2\!\big(g^2(t)\,p_t(\mathbf{x}_t)\big).\] <p>Song et al. <d-cite key="song2020score,anderson1982reverse"></d-cite> showed that there exists both a <strong>reverse-time SDE</strong> and an equivalent <strong>PF-ODE</strong> whose marginals evolve backward in time but follow the <strong>same</strong> family of densities \(\{p_t\}\). In particular, the PF-ODE</p> \[\frac{d\mathbf{x}_t}{dt} = f(\mathbf{x}_t,t) - \frac{1}{2}g^2(t)\,\nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t)\] <p>satisfies the same Fokker–Planck equation as the forward SDE, but with time running from \(t=1\) back to \(t=0\). If we start from \(\mathbf{x}_1\sim p_1\approx\mathcal{N}(0,I)\) and integrate this PF-ODE with the <strong>true</strong> score, then \(\mathbf{x}_0\sim p_{\text{data}}\).</p> <p>Now, <strong>condition</strong> on a measurement \(\mathbf{y}\) produced by a forward model \(\mathbf{y}\sim p(\mathbf{y}\mid \mathbf{x}_0)\).</p> <p>Conditioning lifts to the <strong>entire diffusion trajectory</strong>: we obtain a posterior process \(\{\mathbf{x}_t\mid \mathbf{y}\}\) with marginals \(p(\mathbf{x}_t\mid \mathbf{y})\). Crucially, because conditioning does not change the forward SDE dynamics, the posterior densities satisfy the <strong>same Fokker–Planck operator</strong>, but with different initial and terminal marginals.</p> \[\partial_t p(\mathbf{x}_t\mid \mathbf{y}) = -\nabla_{\mathbf{x}_t}\!\cdot\!\big(f(\mathbf{x}_t,t)\,p(\mathbf{x}_t\mid \mathbf{y})\big) +\frac{1}{2}\nabla_{\mathbf{x}_t}^2\!\big(g^2(t)\,p(\mathbf{x}_t\mid \mathbf{y})\big),\] <p>By the same reasoning as in the unconditional case, there exists a <strong>posterior PF-ODE</strong></p> \[\frac{d\mathbf{x}_t}{dt} = f(\mathbf{x}_t,t) -\tfrac{1}{2}g^2(t)\,\nabla_{\mathbf{x}_t}\log p(\mathbf{x}_t\mid \mathbf{y}),\] <p>such that, if we start at \(t=1\) from \(\mathbf{x}_1\sim p(\mathbf{x}_1\mid \mathbf{y})\) (which is typically close to the unconditional \(p_1\)) and integrate backward, then the marginals follow \(\{p(\mathbf{x}_t\mid \mathbf{y})\}_{t\in[0,1]}\), and in particular</p> \[\mathbf{x}_0 \sim p(\mathbf{x}_0\mid \mathbf{y}).\] <p>This gives the fundamental justification behind posterior-guided sampling:</p> <aside class="l-body box-note"> <p>If we can approximate the posterior score \(\nabla_{\mathbf{x}_t}\log p(\mathbf{x}_t\mid \mathbf{y})\) and integrate the corresponding PF-ODE, then solving the inverse problem reduces to sampling from this posterior-guided flow.</p> </aside> <p>The only missing piece is an approximation of the <strong>likelihood score</strong> \(\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t)\) in Eq. \eqref{eq:posterior_score}, which we turn to next.</p> <h3 id="32-approximating-the-likelihood-score-via-the-clean-space">3.2. Approximating the Likelihood Score via the Clean Space</h3> <p>The decomposition in Eq. \eqref{eq:posterior_score} is conceptually simple:</p> <ul> <li>The <strong>prior score</strong> \(s_\theta(\mathbf{x}_t,t)\) is directly provided by the diffusion prior.</li> <li>The challenging part is the <strong>likelihood score</strong>: \(\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t)\).</li> </ul> <p>Under the standard linear–Gaussian measurement model, the likelihood is naturally defined in the <strong>clean space</strong>:</p> \[\log p(\mathbf{y}\mid \mathbf{x}_0) = -\frac{1}{2\sigma^2}\|\mathbf{y}-\mathbf{A}\mathbf{x}_0\|_2^2 + \text{const} \quad\Rightarrow\quad \nabla_{\mathbf{x}_0}\log p(\mathbf{y}\mid \mathbf{x}_0) = \frac{1}{\sigma^2}\mathbf{A}^\top(\mathbf{y}-\mathbf{A}\mathbf{x}_0).\] <p>However, the diffusion model operates in the <strong>noisy space</strong> \(\mathbf{x}_t\), where the noisy-space likelihood is obtained by marginalizing over \(\mathbf{x}_0\):</p> \[p(\mathbf{y}\mid \mathbf{x}_t) = \int p(\mathbf{y}\mid \mathbf{x}_0)\,p(\mathbf{x}_0\mid \mathbf{x}_t)\,d\mathbf{x}_0 = \mathbb{E}_{\mathbf{x}_0\sim p(\mathbf{x}_0\mid \mathbf{x}_t)}\big[p(\mathbf{y}\mid \mathbf{x}_0)\big].\] <p>This integral is intractable in high dimensions, and differentiating \(\log p(\mathbf{y}\mid \mathbf{x}_t)\) w.r.t. \(\mathbf{x}_t\) would require full access to the conditional distribution \(p(\mathbf{x}_0\mid \mathbf{x}_t)\), which we only know implicitly through the score network.</p> <p>Posterior-guided methods therefore make a <strong>concentration assumption</strong>: for a given \((\mathbf{x}_t,t)\), the conditional distribution \(p(\mathbf{x}_0\mid \mathbf{x}_t)\) is sharply peaked around the Tweedie denoiser</p> \[\hat{\mathbf{x}}_0(\mathbf{x}_t,t) = \mathbb{E}[\mathbf{x}_0\mid \mathbf{x}_t] = \frac{\mathbf{x}_t + \sigma^2(t)\,s_\theta(\mathbf{x}_t,t)}{\alpha(t)}.\] <p>Approximating \(p(\mathbf{x}_0\mid \mathbf{x}_t)\) by a point mass at \(\hat{\mathbf{x}}_0\) (a delta / Laplace approximation) yields</p> \[p(\mathbf{y}\mid \mathbf{x}_t) \approx p(\mathbf{y}\mid \hat{\mathbf{x}}_0(\mathbf{x}_t,t)) \quad\Rightarrow\quad \log p(\mathbf{y}\mid \mathbf{x}_t) \approx \log p(\mathbf{y}\mid \hat{\mathbf{x}}_0(\mathbf{x}_t,t)).\] <p>Differentiating this approximation with respect to \(\mathbf{x}_t\) and applying the chain rule gives</p> \[\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t) = \Big(\frac{\partial \hat{\mathbf{x}}_0}{\partial \mathbf{x}_t}\Big)^\top \nabla_{\mathbf{x}_0}\log p(\mathbf{y}\mid \mathbf{x}_0) \Big|_{\mathbf{x}_0=\hat{\mathbf{x}}_0(\mathbf{x}_t,t)}. \tag{3.3}\label{eq:lik}\] <p>The likelihood score thus factorizes into:</p> <ol> <li> <p>a <strong>clean-space data-consistency gradient</strong> \(\nabla_{\mathbf{x}_0}\log p(\mathbf{y}\mid \mathbf{x}_0),\) computable in closed form for many forward models; and</p> </li> <li> <p>a <strong>Jacobian–vector product (JVP)</strong> involving the Tweedie denoiser \((\tfrac{\partial \hat{\mathbf{x}}_0}{\partial \mathbf{x}_t})^\top,\) which maps this clean-space gradient back into the noisy space.</p> </li> </ol> <p>Eq. \eqref{eq:lik} is the main technical bridge that connects the likelihood score in noisy space to a clean-space gradient plus a Jacobian term. In the next subsection we analyze what this Jacobian does geometrically.</p> <h3 id="33-the-jacobian-as-an-approximate-tangent-space-projector">3.3. The Jacobian as an approximate tangent-space projector</h3> <p>The key takeaway from Eq. \eqref{eq:lik} is that the Jacobian of the Tweedie denoiser does not act in an arbitrary way: under the standard “data lie near a smooth manifold” assumption and in the small-noise regime, it behaves <strong>approximately</strong> like a projection onto the tangent space of the data manifold. This is exactly what we want for inverse problems: the likelihood gradient should pull us <strong>along</strong> the manifold rather than pushing us off it.</p> <p>To make this precise, assume that clean data concentrate near a smooth $d_0$-dimensional manifold $M_0 \subset \mathbb{R}^d$. For notational simplicity, work in local coordinates around some point $x^\star \in M_0$ and decompose any nearby point as</p> \[x_t = x^\star + u + n,\] <p>where $u \in T_{x^\star} M_0$ is the tangent component and $n \perp T_{x^\star} M_0$ is the normal component.</p> <p>In the small-noise regime with Gaussian corruption $x_t = x_0 + \sigma \varepsilon$, $\varepsilon \sim \mathcal{N}(0,I)$, a standard manifold argument (see e.g. denoising-score literature) shows that the log-density of $x_t$ can be approximated as</p> \[\log p_t(x_t) \;\approx\; C(u) \;-\; \frac{1}{2\sigma^2}\,\|n\|^2,\] <p>where $C(u)$ varies slowly along the manifold and the quadratic term penalizes distance in the normal direction. Consequently, the score is approximately</p> \[\nabla_{x_t} \log p_t(x_t) \;\approx\; -\frac{1}{\sigma^2}\,n,\] <p>i.e., it points back toward the manifold by cancelling the normal offset $n$. Plugging this into Tweedie’s formula,</p> \[\hat x_0(x_t,t) = x_t + \sigma^2 \nabla_{x_t} \log p_t(x_t),\] <p>we obtain, to first order,</p> \[\hat x_0(x_t,t) \;\approx\; x_t + \sigma^2\Big(-\frac{1}{\sigma^2} n\Big) = x_t - n = x^\star + u.\] <p>In other words, when the noise is small and the model is accurate, the Tweedie denoiser $\hat x_0(x_t,t)$ behaves like the <strong>nearest-point projection</strong> onto the data manifold:</p> \[\hat x_0(x_t,t) \;\approx\; \Pi(x_t) \;:=\; \arg\min_{z \in M_0} \|z - x_t\|_2^2.\] <p>For this nearest-point projection map $\Pi$, classical differential geometry tells us that its Jacobian at a point $x_t$ whose projection is $x^\star = \Pi(x_t)$ is the orthogonal projector onto the tangent space $T_{x^\star} M_0$:</p> \[D\Pi(x_t) = P_{T_{x^\star} M_0}.\] <p>Intuitively, a small perturbation $\delta x_t$ of the input decomposes into</p> \[\delta x_t = \delta x_{\mathrm{tan}} + \delta x_{\mathrm{nor}}, \quad \delta x_{\mathrm{tan}} \in T_{x^\star} M_0,\; \delta x_{\mathrm{nor}} \perp T_{x^\star} M_0.\] <p>Under the projection, the tangent component moves the projected point along the manifold, while the normal component is largely “forgotten”:</p> \[D\Pi(x_t)\,\delta x_t \;\approx\; \delta x_{\mathrm{tan}}.\] <p>Since Tweedie’s denoiser $\hat x_0(x_t,t)$ is, in the small-noise limit, a smooth perturbation of this nearest-point projection, its Jacobian $J_{\hat x_0}(x_t) = \partial \hat x_0 / \partial x_t$ inherits the same behavior:</p> \[J_{\hat x_0}(x_t)\,\delta x_t \;\approx\; \delta x_{\mathrm{tan}},\] <p>i.e., it <strong>keeps</strong> the tangent component and <strong>suppresses</strong> the normal component of any perturbation.</p> <p>Plugging this geometric picture back into Eq. \eqref{eq:lik}, we can reinterpret the likelihood score as</p> \[\nabla_{x_t}\log p(y \mid x_t) \;\approx\; \underbrace{J_{\hat x_0}^\top(x_t)}_{\text{tangent-space projector}}\, \underbrace{\nabla_{x_0}\log p(y \mid x_0)\big|_{x_0=\hat x_0(x_t,t)}}_{\text{clean-space DC gradient}}.\] <p>Geometrically:</p> <ol> <li> <p>We first compute a <strong>clean-space data-consistency gradient</strong></p> \[g_{x_0} = \nabla_{x_0}\log p(y \mid x_0) = \frac{1}{\sigma^2} A^\top (y - A \hat x_0),\] <p>which tells us how to move $\hat x_0$ to better match the measurement.</p> </li> <li> <p>We then apply $J_{\hat x_0}^\top(x_t)$, which approximately projects this gradient onto the tangent space $T_{\hat x_0} M_0$, removing its normal component.</p> </li> </ol> <p>The resulting likelihood score in noisy space moves $x_t$ in a direction that (i) improves data consistency because it originates from $g_{x_0}$, and (ii) remains compatible with the diffusion prior because its normal component has been suppressed. In this approximate sense, the Jacobian of Tweedie’s estimator acts as a <strong>tangent-space projector</strong> for likelihood guidance.</p> <h3 id="34-2d-toy-illustration">3.4. 2D Toy Illustration</h3> <p>To make the geometric effect of the Jacobian term more concrete, we visualize a toy 2D inverse problem where the clean data lie on a sine-shaped manifold \(M_0 \subset \mathbb{R}^2\). We fix a single target point \(x_0^\star \in M_0\) (red star) that is consistent with the measurement \(\mathbf{y}\), and start the reverse diffusion from a noisy point far away from the manifold. At each reverse-time step we consider three different update rules, corresponding to three panels in the animation.</p> <figure class="figure"> <img src="/2026/assets/img/2026-04-27-diffusion-inverse-problems/posterior_guided_sampling.gif" alt="Posterior-guided sampling: prior vs naive guidance vs Jacobian guidance" style="max-width: 100%; height: auto; image-rendering: auto;"/> </figure> <div class="caption"> Animation 2: Clean-space local-MAP as a denoise–optimize–re-noise scheme. </div> <ul> <li> <p><strong>Prior-only dynamics (left).</strong> In the first panel we ignore the measurement and only follow the diffusion <strong>prior score</strong>. Geometrically, this score always pulls the current state \(x_t\) back towards the nearest point on the learned manifold \(M_0\). The trajectory (purple) quickly relaxes to some high-density region of the prior, but this point has no reason to satisfy the measurement. This corresponds to unconditional sampling: we solve the generative problem, not the inverse problem.</p> </li> <li> <p><strong>Prior + naive guidance in noisy space (middle).</strong> In the second panel we add a <strong>naive likelihood guidance</strong> term directly in the ambient space, approximating the posterior score as</p> \[\nabla_{x_t} \log p(x_t \mid y) \;\approx\; s_\theta(x_t,t) \;+\; \underbrace{\nabla_{x_t} \log p(y \mid x_t)}_{\text{naive DC term}}.\] <p>The blue arrow shows the prior score \(s*\theta(x_t,t)\), which pulls \(x_t\) back to the manifold, while the orange arrow is the guidance vector pointing towards the target. Because this guidance is defined in the ambient space, it typically has a large normal component and <strong>pushes the trajectory off the manifold</strong>. As a result, the two forces frequently oppose each other (“guidance fights prior”), leading to zig-zag trajectories that cut across low-density regions and visibly leave \(M_0\).</p> </li> <li> <p><strong>Prior + Jacobian-guided likelihood (right).</strong> In the third panel we use the Tweedie denoiser \(\hat x_0(x_t,t)\) as a clean-space anchor, compute the <strong>clean-space measurement gradient</strong> \(\nabla_{x_0}\log p(y \mid x_0)\) at \(x_0 = \hat x_0\), and then map it back to the noisy space via the Jacobian:</p> \[\nabla_{x_t}\log p(y \mid x_t) \;\approx\; J_{\hat x_0}^\top(x_t)\, \nabla_{x_0}\log p(y \mid x_0)\Big|_{x_0=\hat x_0(x_t,t)}.\] <p>As argued in the previous subsection, the Jacobian \(J_{\hat x_0}(x_t)\) behaves like a <strong>projection onto the tangent space</strong> \(T_{\hat x_0} M_0\). In the animation, this means the orange guidance arrow is no longer allowed to point arbitrarily into the ambient space: it is projected to lie <strong>along the manifold</strong>, while the blue prior score keeps \(x_t\) attached to \(M_0\). The resulting posterior-guided trajectory slides along the sine curve toward the target, achieving data consistency <em>without</em> drifting away from the learned generative manifold.</p> </li> </ul> <p>This toy example illustrates the core role of the Jacobian term in posterior-guided sampling: it converts a clean-space likelihood gradient into a noisy-space correction that is automatically constrained to the tangent space of the data manifold. In practice, this allows us to approximate the posterior score as</p> \[\nabla_{x_t}\log p(x_t \mid y) \;\approx\; s_\theta(x_t,t) \;+\; J_{\hat x_0}^\top(x_t)\, \nabla_{x_0}\log p(y \mid x_0),\] <p>so that the prior and likelihood contribute <strong>compatible</strong> vector fields—one keeps us on the manifold, the other moves us along it toward solutions that both match the measurements and remain realistic under the diffusion prior.</p> <h2 id="4-local-map-optimization-paradigm">4. Local-MAP optimization Paradigm</h2> <p>The posterior-guided sampling paradigm views inverse problems as a <strong>sampling task</strong>: we try to simulate a posterior diffusion process in noisy space and read off samples from $p(\mathbf{x}_0 \mid \mathbf{y})$ at $t = 0$. In this section we turn to a complementary viewpoint, exemplified by DDS <d-cite key="chung2023decomposed"></d-cite>, DDRM <d-cite key="kawar2022denoising"></d-cite>, DDNM <d-cite key="wang2022zero"></d-cite>, DiffusionMBIR <d-cite key="chung2023solving"></d-cite>, and more recent LMAPS methods <d-cite key="zhang2025local"></d-cite>, which instead treat inverse problems as a sequence of <strong>local optimization problems in the clean space</strong> $\mathbf{x}_0$.</p> <p>At a high level, these methods answer a different question:</p> <aside class="l-body box-note"> <p>Rather than asking “how should we move \(\mathbf{x}_t\) along the posterior vector field?”, we repeatedly ask : <strong>“given our current noisy state, which clean image best balances data fidelity and the diffusion prior, locally?”</strong></p> </aside> <p>With this in mind, each reverse step then consists of three conceptually separate operations:</p> <ol> <li> <p><strong>Denoise to a clean-space anchor:</strong> map the current noisy point $\mathbf{x}_t$ to a Tweedie denoiser $\hat{\mathbf{x}}_0(\mathbf{x}_t, t)$, which serves as a local prior “anchor” in the clean space.</p> </li> <li> <p><strong>Local MAP refinement:</strong> around this anchor, solve a <strong>small MAP problem</strong> in $\mathbf{x}_0$ that trades off data consistency and proximity to $\hat{\mathbf{x}}_0$.</p> </li> <li> <p><strong>Re-noising:</strong> re-inject the optimized solution into the diffusion trajectory by adding back the appropriate amount of noise.</p> </li> </ol> <p>Over the full reverse trajectory, this yields a <strong>denoise–optimize–re-noise</strong> scheme that gradually transports the initial noise towards a measurement-consistent and prior-plausible solution. As shown in Figure 2, starting from pure noise \(z = x_1\) at \(t=1\), the reverse process produces intermediate noisy states \(x_t, x_{t-1}, \dots\) in the noisy space. At each step,</p> <ul> <li> <p>Step 1: Tweedie denoising maps \(x_t\) to a clean-space anchor on the diffusion prior manifold \(\mathcal{M}_{\mathrm{DM}}\);</p> </li> <li>Step 2: a local MAP subproblem in clean data space \((x_0)\) refines this anchor toward the posterior manifold \(\mathcal{M}_{\mathrm{pos}} = \mathcal{M}_{\mathrm{DM}}\cap \mathcal{M}_{\mathrm{obs}}\), balancing data consistency with the observation manifold \(\mathcal{M}_{\mathrm{obs}}\);</li> <li>Step 3: the optimized clean solution is re-noised to obtain the next noisy state \(x_{t-1}\).</li> </ul> <p>Iterating these three steps transports samples towards posterior modes that are both measurement-consistent and realistic under the diffusion prior.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-inverse-problems/local_map-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-inverse-problems/local_map-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-inverse-problems/local_map-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-inverse-problems/local_map.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2. Clean-space local-MAP as a denoise–optimize–re-noise scheme.</figcaption> </figure> <p>In what follows we first derive the local MAP objective, then show how to implement it algorithmically, and finally discuss when this local scheme approximates the <strong>global</strong> MAP solution.</p> <h3 id="41-from-global-map-to-local-quadratic-surrogates">4.1. From global MAP to local quadratic surrogates</h3> <p>Recall the global MAP objective from Eq. \eqref{eq:obj}:</p> \[\hat{\mathbf{x}}_{\text{MAP}} = \arg\max_{\mathbf{x}_0} \left[ \log p(\mathbf{y}\mid \mathbf{x}_0) + \log p(\mathbf{x}_0)\right]. \tag{4.1}\label{eq:global_map}\] <p>Under the linear–Gaussian measurement model \(\mathbf{y} = \mathbf{A}\mathbf{x}_0 + \mathbf{n}\), the negative log-likelihood is a familiar quadratic:</p> \[-\log p(\mathbf{y}\mid \mathbf{x}_0) = \frac{1}{2\sigma^2}\|\mathbf{y}-\mathbf{A}\mathbf{x}_0\|_2^2 + \text{const}.\] <p>The difficulty lies in the prior term \(-\log p(\mathbf{x}_0)\). For a diffusion prior, we do not have direct access to this function or its gradient in \(\mathbf{x}_0\)-space; all we can evaluate is the <strong>noisy-space score</strong> \(\nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t)\) along the diffusion trajectory.</p> <p>This is where the Tweedie estimator \(\hat{\mathbf{x}}_0(\mathbf{x}_t,t)\), introduced in the previous section, becomes useful. For a fixed time \(t\) and noisy sample \(\mathbf{x}_t\), we can view</p> \[\hat{\mathbf{x}}_0(\mathbf{x}_t, t) = \mathbb{E}[\mathbf{x}_0 \mid \mathbf{x}_t]\] <p>as a <strong>local summary of the prior</strong>: it lives in a high-density neighborhood of the data manifold, and in the small-noise regime it behaves like a soft projection onto $M_0$. It is therefore natural to use $\hat{\mathbf{x}}_0$ as the center of a <strong>local quadratic surrogate</strong> for $-\log p(\mathbf{x}_0)$,</p> \[-\log p(\mathbf{x}_0) \;\approx\; \frac{1}{2}\|\mathbf{x}_0 - \hat{\mathbf{x}}_0(\mathbf{x}_t,t)\|_2^2 + \text{const},\] <p>which says “in a small neighborhood of $\hat{\mathbf{x}}_0$, the prior behaves as if it were a Gaussian with mean $\hat{\mathbf{x}}_0$ and unit covariance.”</p> <p>Substituting this approximation into Eq. \eqref{eq:global_map} and keeping the exact data term yields the <strong>local MAP objective</strong> at time $t$:</p> \[\mathbf{x}_0^\star(\mathbf{x}_t) \;\approx\; \arg\min_{\mathbf{x}_0} \underbrace{\frac{\gamma}{2}\|\mathbf{y}-\mathbf{A}\mathbf{x}_0\|_2^2}_{\text{data consistency}} \;+\; \underbrace{\frac{1}{2}\|\mathbf{x}_0-\hat{\mathbf{x}}_0(\mathbf{x}_t,t)\|_2^2}_{\text{local prior / trust-region term}}, \tag{4.2}\label{eq:local_map}\] <p>where $\gamma$ is a tunable parameter that absorbs $1/\sigma^2$ and possible rescalings of the prior term.</p> <p>The second term plays a dual role:</p> <ul> <li><strong>Local prior:</strong> it encodes the diffusion prior <strong>locally</strong> around $\hat{\mathbf{x}}_0$;</li> <li><strong>Trust region:</strong> it keeps the optimizer from wandering too far away from the current clean-space anchor, ensuring that the Gaussian approximation of the prior remains valid.</li> </ul> <p>In other words, Eq. \eqref{eq:local_map} is a <strong>local, trust-region version</strong> of the global MAP problem \eqref{eq:global_map}, with the full diffusion prior replaced by a quadratic surrogate centered at $\hat{\mathbf{x}}_0(\mathbf{x}_t,t)$.</p> <h3 id="42-denoiseoptimizere-noise-scheme">4.2. Denoise–optimize–re-noise scheme</h3> <p>Given the local objective \eqref{eq:local_map}, local-MAP-based algorithms build a reverse-time solver by repeating the following pattern at each time step $t_k$:</p> <ol> <li> <p><strong>Step 1:</strong> Denoise to obtain a clean-space anchor.</p> <p>Starting from the current noisy state $\mathbf{x}_{t_k}$, we first compute the Tweedie denoiser</p> \[\hat{\mathbf{x}}_0^{(k)} = \hat{\mathbf{x}}_0(\mathbf{x}_{t_k}, t_k) = \frac{\mathbf{x}_{t_k} + \sigma^2(t_k)\,s_\theta(\mathbf{x}_{t_k},t_k)}{\alpha(t_k)},\] <p>which serves as a <strong>clean-space anchor</strong> encoding the diffusion prior at time $t_k$. This step is identical to what we used in the posterior-guided sampling paradigm: Tweedie provides a statistically optimal estimate of the underlying clean image given the noisy observation $\mathbf{x}_{t_k}$.</p> </li> <li> <p><strong>Step 2:</strong> Local MAP optimization in the clean space.</p> <p>Given $\hat{\mathbf{x}}_0^{(k)}$, we now solve the local surrogate problem</p> \[\mathbf{x}_0^{(k)\,\star} = \arg\min_{\mathbf{x}_0} \frac{\gamma}{2}\|\mathbf{y}-\mathbf{A}\mathbf{x}_0\|_2^2 + \frac{1}{2}\|\mathbf{x}_0-\hat{\mathbf{x}}_0^{(k)}\|_2^2. \tag{4.3}\label{eq:local_map_k}\] <p>For linear measurement operators $\mathbf{A}$, Eq. \eqref{eq:local_map_k} is a convex quadratic with a closed-form solution</p> \[\mathbf{x}_0^{(k)\,\star} = \left(\gamma\,\mathbf{A}^\top\mathbf{A}+\mathbf{I}\right)^{-1} \left(\gamma\,\mathbf{A}^\top\mathbf{y} + \hat{\mathbf{x}}_0^{(k)}\right),\] <p>but explicitly inverting the matrix is usually impractical. Instead, DDS <d-cite key="chung2023decomposed"></d-cite> and diffusion MBIR-style methods <d-cite key="chung2023solving"></d-cite> solve \eqref{eq:local_map_k} with <strong>iterative linear solvers</strong>, most notably <strong>conjugate gradients (CG)</strong>. This has two important consequences:</p> <ul> <li>The data-consistency structure (e.g., convolution, Fourier sampling, Radon transforms) can be exploited via fast operators $\mathbf{A}$ and $\mathbf{A}^\top$, without ever forming $\mathbf{A}^\top\mathbf{A}$ explicitly.</li> <li>The local MAP update can be run for a small, fixed number of CG iterations, trading off accuracy for speed. In practice, a handful of CG steps per diffusion time step already yields strong reconstructions.</li> </ul> <p>Conceptually, this step is where <strong>classical inverse-problem machinery</strong> (quadratic data terms, Krylov solvers, proximal point iterations) interfaces with the <strong>diffusion prior</strong>. All prior information enters through the choice of $\hat{\mathbf{x}}_0^{(k)}$; the rest of the computation is purely deterministic optimization in the clean space.</p> </li> <li> <p><strong>Step 3:</strong> Re-noise the optimized solution back to $t_{k-1}$.</p> <p>Having obtained the locally optimal clean image $\mathbf{x}_0^{(k)\,\star}$ that balances measurement fit and closeness to the anchor, the final step is to <strong>inject it back into the diffusion trajectory</strong> by adding noise corresponding to the next time step.</p> <p>For a DDIM- or VE-/VP-style schedule, this can be written in the generic form</p> \[\mathbf{x}_{t_{k-1}} = \alpha(t_{k-1})\,\mathbf{x}_0^{(k)\,\star} + \sigma(t_{k-1})\,\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t_{k}}, t_k),\] <p>possibly with additional deterministic or stochastic corrections depending on the chosen sampler. The key point is that <strong>the generative noise injection is now decoupled from the data-consistency optimization</strong>:</p> <ul> <li>The diffusion prior governs how we move between time steps via $(\alpha(t),\sigma(t))$ and the score network;</li> <li>The measurement model governs how we update $\mathbf{x}_0$ locally through Eq. \eqref{eq:local_map_k}.</li> </ul> </li> </ol> <h3 id="43-how-local-map-approximates-global-map">4.3. How local MAP approximates global MAP</h3> <p>So far we have only claimed that Eq. \eqref{eq:local_map} is a <strong>local surrogate</strong> to the global MAP objective \eqref{eq:global_map}. A natural question is: <strong>under what conditions does following these local updates approximate the true MAP solution?</strong></p> <p>Recent analyses under the umbrella of LMAPS <d-cite key="zhang2025local"></d-cite> provide the following picture:</p> <ol> <li> <p><strong>Small time steps &amp; accurate Tweedie.</strong> If we use a fine time discretization and the Tweedie estimator is accurate (so that $\hat{\mathbf{x}}_0(\mathbf{x}_t,t)$ stays in a tight neighborhood of the true posterior mean), then the quadratic surrogate in \eqref{eq:local_map} is a good approximation of $-\log p(\mathbf{x}_0)$ in that neighborhood.</p> </li> <li> <p><strong>Limited local movement per step.</strong> If the trust-region penalty $|\mathbf{x}_0-\hat{\mathbf{x}}_0|^2$ is sufficiently strong (or equivalently, if we do not over-iterate the local solver), then each update $\mathbf{x}_0^{(k),\star}$ remains close to $\hat{\mathbf{x}}_0^{(k)}$, and the Gaussian approximation of the prior does not break down.</p> </li> <li> <p><strong>Composition of local steps.</strong> As we move backward in time, the composition of these local MAP updates traces out a trajectory in the clean space that, in the continuous-time limit, can be shown to follow a <strong>gradient flow of the negative log-posterior</strong>. Intuitively, each local solve takes one small step in the direction of the global MAP solution, but expressed in the natural coordinates of the diffusion prior.</p> </li> </ol> <p>Under these assumptions, the final $\mathbf{x}_0$ returned by the denoise–optimize–re-noise scheme is a <strong>consistent approximation of the true MAP estimator</strong> in \eqref{eq:global_map}.</p> <h3 id="44-toy-illustration-local-map-as-denoiseoptimizere-noise">4.4. Toy Illustration: Local-MAP as Denoise–Optimize–Re-noise</h3> <p>To make the local-MAP paradigm concrete, we build a simple 2D toy example and visualize one full reverse trajectory as an animation, see Animation 3.</p> <p>We place clean data on a 1D “sine-wave” manifold \(M_0 = \{(u, \sin u): u \in \mathbb{R}\}\) (Please note that $M_0$ refers to $M_{\text{DM}}$), which plays the role of a toy diffusion prior: clean samples concentrate near this curve. The forward operator is a 1D linear measurement \(\mathbf{A} = [\cos\phi,\; \sin\phi],\) so the set of points consistent with a given measurement \(y\) forms a <strong>slanted observation line</strong></p> \[M_{\mathrm{obs}} = \{\mathbf{x} \in \mathbb{R}^2 : \mathbf{A}\mathbf{x} = y\}.\] <p>We pick a ground-truth point \(\mathbf{x}_0^\star \in M_0\) and set \(y = \mathbf{A}\mathbf{x}_0^\star\), so the red star in the animation marks the intersection \(M_0 \cap M_{\mathrm{obs}}\) — the ideal MAP solution.</p> <p>The purple curve traces the noisy trajectory \(\{\mathbf{x}_t\}_{t=1\to 0}\). We initialize it at a point far away from \(\mathbf{x}_0^\star\) in the upper-right corner. As the reverse process proceeds, the trajectory gradually bends towards the red star, illustrating how repeated local-MAP updates can still recover the correct solution even without solving a global optimization problem.</p> <p>Each reverse-time step in the animation is decomposed into three colored moves, directly mirroring the <strong>denoise–optimize–re-noise</strong> structure in Sec. 4.2: <strong>Purple (\(x_t\))</strong> represents current noisy state, <strong>Blue (\(\hat{\mathbf{x}}_0\))</strong> represents Tweedie denoising anchor, <strong>Orange (\(\mathbf{x}_0^\mathrm{local}\))</strong> represents local MAP refinement in clean space, and finally, we re-noise \(\mathbf{x}_0^\mathrm{local}\) to obtain the next noisy state <strong>Purple (\(x_{t-1}\))</strong>.</p> <figure class="figure"> <img src="/2026/assets/img/2026-04-27-diffusion-inverse-problems/local_map_2d_slanted_obs.gif" alt="Posterior-guided sampling: prior vs naive guidance vs Jacobian guidance" style="max-width: 100%; height: auto; image-rendering: auto;"/> </figure> <div class="caption"> Animation 3: Clean-space local-MAP as a denoise–optimize–re-noise scheme. </div> <p>To complement the static 2D toy example, we provide an interactive 3D visualization of the <strong>clean-space Local-MAP paradigm</strong> (see the following Interactive Figure). The blue bowl-shaped surface represents the diffusion prior manifold \(M_{\mathrm{DM}}\), modeled here as an anisotropic Gaussian energy landscape. The light orange plane is the observation manifold \(M_{\mathrm{obs}}\), corresponding to a one-dimensional linear measurement operator \(y = A x\). Their intersection encodes the ideal posterior manifold where both the generative prior and the data-consistency constraint are satisfied.</p> <p>The black curve shows a full <strong>reverse-diffusion trajectory</strong> ({x_t}_{t=T}^{0}) in the clean space. You can type an integer (t) into the input box above the figure to inspect a single reverse step. For each selected time step (t), the visualization highlights the three sub-steps of the Local-MAP update:</p> <ul> <li> <p>A <strong>green segment</strong> connects the current noisy point \(x_t\) to its Tweedie-denoised anchor \(\hat{x}_0^{t}\).</p> </li> <li> <p>A <strong>red segment</strong> connects \(\hat{x}_0^{t}\) to the Local-MAP solution \(\hat{x}_0^{t,*}\).</p> </li> <li> <p>A <strong>blue segment</strong> connects \(\hat{x}_0^{t,*}\) to the next noisy iterate \(x_{t-1}\).</p> </li> </ul> <p>By rotating the 3D view and sweeping \(t\) from large values down to small ones, one can see how Local-MAP repeatedly nudges the reverse trajectory toward the intersection of \(\mathcal{M}_{\mathrm{DM}}\) and \(\mathcal{M}_{\mathrm{obs}}\) while staying faithful to the diffusion prior.</p> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-diffusion-inverse-problems/local_map_interactive.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <div class="caption"> Interactive Figure: Local-MAP as Denoise–Optimize–Re-noise. Drag the slider to sweep the time index $t$ and inspect how the three sub-steps evolve across the reverse process. </div> <h2 id="5-conclusion">5. Conclusion</h2> <p>In this post, we examined diffusion priors for inverse problems through two complementary paradigms: <strong>Posterior-Guided Sampling</strong> and <strong>Local-MAP Optimization</strong>. While the landscape of diffusion inverse solvers is vast <d-dict key="daras2024survey"></d-dict> — including methods based on Variational Inference (VI) <d-cite key="mardani2023variational,alkan2023variational"></d-cite>, latent space optimization <d-cite key="daras2022score,wang2024dmplug"></d-cite>, and asymptotically exact MCMC/SMC approaches <d-cite key="wu2024principled,dou2024diffusion"></d-cite> – but we focused on these two because they have become the most practical and widely used ways to turn a powerful unconditional diffusion model into a plug-and-play inverse solver.</p> <p>At a high level, both paradigms realize the same philosophy:</p> <aside class="l-body box-note"> <p><strong>Decouple the generative prior from the measurement physics.</strong></p> </aside> <p>Once a diffusion prior has been trained (once and for all) on natural signals, we can pair it with an arbitrary forward operator \(\mathbf{A}\) and noise model to solve tasks as diverse as inpainting, deblurring, super-resolution, and CT reconstruction – without retraining the generative model. What changes from task to task is not the diffusion backbone, but <strong>how we combine a prior operator with a data-consistency operator</strong>.</p> <h3 id="51-posterior-guided-sampling-gradient-based-flows-in-noisy-space">5.1. Posterior-guided sampling: gradient-based flows in noisy space</h3> <p>The posterior-guided paradigm keeps the reverse diffusion entirely in the noisy space \(\mathbf{x}_t\). By replacing the prior score field with the posterior score (Eq. \eqref{eq:posterior_score}), the reverse PF-ODE becomes a gradient-like flow along \(\nabla_{\mathbf{x}_t}\log p(\mathbf{x}_t\mid \mathbf{y})\). The posterior score can be decomposed into two parts, namely the prior score and the likelihood score, this makes posterior-guided sampling look like an <strong>operator splitting</strong> scheme in noisy space: at each step we follow the diffusion prior flow and then apply a data-consistency correction that has been carefully projected onto the manifold’s tangent space.</p> <p>From this perspective, methods such as DPS/DMPS and MCG can be viewed as different instantiations of the same template: they are <strong>gradient-based posterior flows</strong> that operate directly on \(\mathbf{x}_t\).</p> <ul> <li> <p><strong>Strengths.</strong> Conceptually close to Bayesian posterior sampling; naturally produces <strong>multiple diverse reconstructions</strong> and can, in principle, approximate the full posterior \(p(\mathbf{x}_0\mid \mathbf{y})\). Works well with non-linear or differentiable black-box forward models, as long as we can backpropagate to obtain a likelihood gradient.</p> </li> <li> <p><strong>Limitations.</strong> Accurate likelihood-score approximations typically require <strong>Jacobian–vector products (JVP) through the Tweedie denoiser</strong>, on top of standard forward passes. This increases memory and compute, especially at high resolution. The dynamics can also be sensitive to guidance strength and discretization; if the likelihood term is not geometrically aligned (e.g. without the Jacobian projection), it may fight against the prior instead of flowing along the data manifold.</p> </li> </ul> <h3 id="52-local-map-optimization-denoiseoptimizere-noise-in-clean-space">5.2. Local-MAP optimization: denoise–optimize–re-noise in clean space</h3> <p>The local-MAP paradigm instead views inverse problems as a sequence of <strong>small optimization problems in the clean space</strong> \(\mathbf{x}_0\), this yields a clean-space <strong>denoise–optimize–re-noise</strong> operator splitting. The prior trajectory (Steps 1 and 3) depends only on the unconditional diffusion model, while the inner optimization (Step 2) depends only on the measurement model and our choice of local objective and solver.</p> <p>Under this view, seemingly different methods – including DDS, DDRM, DiffusionMBIR, and various PnP/RED-style algorithms – can be unified as instances of the same <strong>local-MAP optimization template</strong> with different choices of surrogate, solver, and re-noising schedule.</p> <ul> <li> <p><strong>Strengths.</strong> Recasts each reverse step as a <strong>standard optimization problem in \(\mathbf{x}_0\)</strong>, allowing us to leverage decades of work on quadratic solvers, Krylov methods, and proximal algorithms. Avoids noisy-space JVPs entirely: all gradients live in clean space or through \(\mathbf{A}\). Naturally targets <strong>MAP-like reconstructions</strong>, which is often exactly what practitioners want in imaging applications.</p> </li> <li> <p><strong>Limitations.</strong> The method is only <strong>locally</strong> MAP: it relies on Tweedie anchors and quadratic surrogates remaining faithful to the true posterior landscape in a neighborhood. Solving a local subproblem at every step (e.g. via CG) can be costly for large-scale 3D problems or complex \(\mathbf{A}\). By construction it is mode-seeking, and does not explore the full posterior without additional machinery.</p> </li> </ul> <h3 id="53-decoupling-as-the-main-design-principle">5.3. Decoupling as the main design principle</h3> <p>Despite these differences, both paradigms share the same structural core:</p> <aside class="l-body box-note"> <p><strong>A diffusion prior operator that proposes plausible samples, and a data-consistency operator that enforces the physics.</strong></p> </aside> <p>Posterior-guided methods implement this decoupling directly in noisy space via a posterior score field, while local-MAP methods implement it in clean space via Tweedie anchors and trust-region MAP updates. This decoupling is what makes diffusion priors so attractive for real-world inverse problems: once the prior is trained, we can adapt to new measurement operators and noise models largely by changing <strong>how</strong> we plug in the data-consistency operator, rather than retraining the model.</p> <h3 id="54-limitations-and-outlook">5.4. Limitations and Outlook</h3> <p>Although both paradigms are elegant from theoretically and practically perspective, they rely on approximations (Tweedie denoising, score-to-likelihood projections, local quadratic surrogates). When these approximations break (strongly ill-posed settings, heavy nonlinearity, severe out-of-distribution measurements), performance can degrade in ways that are hard to diagnose. In addition, the computation cost is also a problem that cannot be ignored, posterior-guided methods can be dominated by JVP/gradient cost; local-MAP can be dominated by per-step inner solves—either way, iterative sampling is often the bottleneck.</p> <p>Looking forward, several frontiers remain active in this area:</p> <ul> <li> <p><strong>Hybrid schemes.</strong> Combine global exploration (posterior-guided at high noise) with sharp refinement (local-MAP near \(t\!\to\!0\)).</p> </li> <li> <p><strong>Beyond linear–Gaussian models.</strong> Most current theory assumes \(\mathbf{y} = \mathbf{A}\mathbf{x}_0 + \mathbf{n}\) with Gaussian noise. Extending these paradigms to non-linear, non-Gaussian, or partially unknown forward models is still challenging.</p> </li> <li> <p><strong>Acceleration and distillation.</strong> Iterative sampling remains expensive. Distillation, consistency models, and related techniques offer promising ways to compress these multi-step procedures into a few learned updates while preserving their geometric structure.</p> </li> </ul> <p>Our hope is that the geometric and probabilistic lens developed in this blog – posterior-guided flows in noisy space, and local-MAP optimization in clean space – can serve as a compact mental model for navigating the rapidly growing literature on diffusion-based inverse problems, and as a design toolkit for building the next generation of robust and reusable inverse solvers.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[This blogpost develops a geometric and probabilistic lens on diffusion priors for inverse problems. We show that a wide range of methods mostly instantiate two operator-splitting paradigms, i.e., posterior-guided sampling and clean-space local-MAP optimization. Through manifold diagrams, Tweedie-based animations, and step-by-step derivations, we explain how these paradigms decouple a pretrained diffusion prior from measurement physics, clarify when they approximate full posterior sampling versus MAP estimation, and distill practical design rules for building robust diffusion-based inverse solvers.]]></summary></entry><entry><title type="html">Sample Blog Post</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/distill-example</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/distill-example/"><![CDATA[<p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.liquid path="assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/iclr-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/iclr-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/iclr-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2025-04-27-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/9-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/9-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/7-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/7-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/8-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/8-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/8.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/10-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/10-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/10.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/11-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/11-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/11-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/11.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/12-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/12-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/12.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/7-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/7-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here’s how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong>). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %}
</code></pre></div></div> <p>For example, the following code can be used to generate the figure underneath it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2026-04-27-distill-example/plotly_demo_1.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span>
    <span class="na">src=</span><span class="s">"{{ 'assets/html/2026-04-27-distill-example/plotly_demo_1.html' | relative_url }}"</span>
    <span class="na">frameborder=</span><span class="s">"0"</span>
    <span class="na">scrolling=</span><span class="s">"no"</span>
    <span class="na">height=</span><span class="s">"600px"</span>
    <span class="na">width=</span><span class="s">"100%"</span>
  <span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>

</code></pre></div></div> <p>Voila!</p> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>

<span class="p">}</span></code></pre></figure> <hr/> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid.js</a> directly. Below, we generate examples of such diagrams using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a> syntax.</p> <p><strong>Note:</strong> To enable mermaid diagrams, you need to add the following to your post’s front matter:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">mermaid</span><span class="pi">:</span>
  <span class="na">enabled</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">zoomable</span><span class="pi">:</span> <span class="kc">true</span> <span class="c1"># optional, for zoomable diagrams</span>
</code></pre></div></div> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```mermaid
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
```
</code></pre></div></div> <pre><code class="language-mermaid">sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
</code></pre> <hr/> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p> <hr/> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item</li> </ol> <ul> <li>Unordered sub-list.</li> </ul> <ol> <li>Actual numbers don’t matter, just that it’s a number <ol> <li>Ordered sub-list</li> </ol> </li> <li> <p>And another item.</p> <p>You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>To have a line break without a paragraph, you will need to use two trailing spaces. Note that this line is separate, but within the same paragraph. (This is contrary to the typical GFM line break behavior, where trailing spaces are not required.)</p> </li> </ol> <ul> <li> <p>Unordered lists can use asterisks</p> </li> <li> <p>Or minuses</p> </li> <li> <p>Or pluses</p> </li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">AI Fundamentals: Valuing AI Agents &amp;amp; Data Assets</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/economic-agents/" rel="alternate" type="text/html" title="AI Fundamentals: Valuing AI Agents &amp;amp; Data Assets"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/economic-agents</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/economic-agents/"><![CDATA[<h2 id="introduction">Introduction</h2> <p><strong>Motivation.</strong> Large Language Model (LLM) based agents are rapidly evolving beyond simple chatbots into versatile autonomous assistants embedded in real workflows. These agents can <em>read</em> the world through managed context pipelines (ingesting documents, code, sensor data) and <em>write</em> to the world via APIs and tools, while continually improving themselves by incorporating new training data (experience) into their weights. As AI systems begin to perform economically valuable tasks across domains <d-cite key="erol2025cost,ide2024artificial,hadfield2025economy,handa2025economic"></d-cite>, from writing <d-cite key="choi-etal-2024-combining"></d-cite> and software development <d-cite key="chen2025textsuperscript,Miserendino2025"></d-cite> to customer service <d-cite key="liu2025evaluating,ackerman2025perceptions"></d-cite> and healthcare <d-cite key="Arora2025,gallifant2025beyond"></d-cite>, there is a pressing need to rigorously measure their performance, value, and risks in terms that organizations can understand and trust.</p> <p>Recent analyses of millions of AI usage instances show that AI is already touching a wide range of occupations, with particularly heavy use in software development and writing tasks (together nearly half of all usage) <d-cite key="Handa2025"></d-cite>. Notably, around 36% of occupations see at least a quarter of their tasks involving AI assistance <d-cite key="Handa2025"></d-cite> – but in most cases this is <em>augmentation</em> (AI helping a human) rather than full automation. Indeed, one study found 57% of AI usage suggests human-AI collaboration (learning or iterating on an output) vs. 43% where the AI essentially completes tasks autonomously <d-cite key="Handa2025"></d-cite>.</p> <p>Meanwhile, the capabilities of frontier models have been climbing rapidly. In high-stakes domains like medicine, open-ended physician-written evaluations (HealthBench) show model performance improving from 16% (GPT-3.5) to 32% (GPT-4) and up to 60% with the latest models <d-cite key="Arora2025"></d-cite> within two years – a nearly 4x improvement. In software engineering, new benchmarks of freelance programming tasks (SWE-Lancer) valued at \$1M found that state-of-the-art models can now complete a significant subset of real-world coding jobs (earning about \$400k of the \$1M total value) <d-cite key="Miserendino2025"></d-cite>, though they still fail on the majority of tasks. We are entering what Silver and Sutton term the <em>Era of Experience</em> <d-cite key="SilverSutton2025"></d-cite>, where AI agents learn continuously from real-world interactions rather than solely from static training data.</p> <p><strong>Contributions.</strong> We present an expanded <strong>AI Fundamentals</strong> framework that maps every technical aspect of an AI agent’s performance into the language of economics and public-company finance <d-cite key="bai2025review,garrido2024deep,erol2025cost"></d-cite>. Our goal is to define a single unifying metric – <strong>Agent Economic Value (AEV)</strong> – which collapses all performance aspects into one cash–flow based expression:</p> \[\begin{aligned} \text{AEV} &amp; = \text{Cost Saving} + \text{Efficiency Premium} - \text{Model Cost} \\ &amp;- \text{Data Cost} - \text{Human Intervention Cost}. \end{aligned}\] <ol> <li>We formalise AEV and map each cost or benefit term to GAAP <d-cite key="GAAP"></d-cite> line items, with empirical evidence showing the full expression is measurable today.</li> <li>We provide a measurement methodology validated on software, healthcare, and enterprise usage datasets.</li> <li>We extend the template to the “Era of Experience” where agents learn from grounded, real-world reward streams <d-cite key="SilverSutton2025"></d-cite>.</li> </ol> <p><strong>Data–Asset Valuation.</strong> Interaction logs generated during agent operation accrue as an <em>intangible data asset</em> <d-cite key=" moon-etal-2025-limacost,yanggmvaluator,zhang2025fairshare"></d-cite>. Under GAAP we expense collection and cleaning costs immediately, but we may capitalise the curated corpus once it demonstrably improves future cash flows (analogous to software development costs that pass technological feasibility). The same treatment applies under IFRS (IAS 38), where the asset is amortised over the useful life of the model. We therefore track a “Data R&amp;D” line item that migrates to the balance-sheet once the corpus clears the feasibility gate, providing an auditable bridge from token spend to book value.</p> <h2 id="background">Background</h2> <figure style="text-align: center; width: 100%;"> <img src="/2026/assets/img/2026-04-27-economic-agents/economic-value.png" style="width: 40%;"/> <figcaption style="font-size: 1em;">Figure 1: Graphical breakdown of Agent Economic Value (AEV) into positive cash inflows (Cost Saving and Efficiency Premium) versus negative outflows (Model, Data, and Human Intervention Costs).</figcaption> </figure> <h3 id="from-tasks-to-cash-flows">From Tasks to Cash Flows</h3> <p>A <em>task</em> is the atomic unit defined by O<em>NET. Handa *et al.</em> build a differential-privacy pipeline (Clio) that maps raw conversations to such tasks at scale, recovering wages, skills, and job-zone attributes <d-cite key="Handa2025"></d-cite>. If each task’s historic market price is known (Upwork contracts, Medicare reimbursement, customer-service SLA penalty, …), the jump to GAAP becomes mechanical.</p> <h3 id="limitations-of-exposure-studies">Limitations of Exposure Studies</h3> <p>Forecasts based on patent text overlap <d-cite key="Handa2025"></d-cite> or embedding similarity predict potential, not realised impact. Our framework instead <em>measures</em> cash deltas per task, capturing both augmentation and automation effects.</p> <figure style="text-align: center; width: 100%;"> <img src="/2026/assets/img/2026-04-27-economic-agents/logistics-chain.png" style="width: 80%;"/> <figcaption style="font-size: 1em;">Figure 2: End-to-end inference supply chain. Curated data and expert prompts are routed through the MCP to the model, where compute tokens refine them into machine-level skills that ultimately map to human-level economic output.</figcaption> </figure> <h2 id="framework">Framework</h2> <h3 id="technical-architecture-of-an-ai-agent">Technical Architecture of an AI Agent</h3> <p>A contemporary LLM-based agent is not an isolated text predictor but an <em>interactive system</em> that perceives and acts on the world in extended sequences. Figure 3 illustrates the components and data flows:</p> <figure style="text-align: center; width: 100%;"> <img src="/2026/assets/img/2026-04-27-economic-agents/picture-1-updated.png" style="width: 80%;"/> <figcaption style="font-size: 1em;">Figure 3: Perception–Action–Self-Improvement pipeline. MCP servers provide read/write primitives; GPU clusters host training &amp; inference.</figcaption> </figure> <p>The architecture includes:</p> <ul> <li><strong>Observer (Read) Paths:</strong> The agent ingests context through managed pipelines (MCP-Read). These could be <em>pull-based</em> queries or <em>push-based</em> streams of information.</li> <li><strong>Actuator (Write) Paths:</strong> The agent produces outputs via actions (MCP-Write), generating text or calling tools/APIs to enact changes in the environment.</li> <li><strong>Agent Core (LLM + Controller):</strong> The large model that processes inputs and decides on outputs, implementing a policy $\pi$ in an RL sense.</li> <li><strong>Learning (Weight Updates):</strong> After tasks are completed, interaction data can be fed back into training pipelines (DL-TRAIN) to update model weights.</li> </ul> <p>Everything is orchestrated by a <strong>Managed Compute &amp; Prompt (MCP) service</strong>, which coordinates reads and writes and provides an interface to the agent.</p> <h3 id="resource-taxonomy-and-marginal-valuation">Resource Taxonomy and Marginal Valuation</h3> <p>Data are <em>non-fungible</em> assets whose worth is governed by their <em>quality</em> rather than sheer volume <d-cite key="moon-etal-2025-limacost,yanggmvaluator,zhang2025fairshare"></d-cite>. We track four economically distinct subclasses:</p> <ol> <li><strong>Expert Time</strong> – manually crafted demonstrations, ratings, or critiques by domain specialists.</li> <li><strong>Live-Environment Experience</strong> – interaction traces collected during real production usage.</li> <li><strong>Inference Data Stream</strong> – prompts and completions logged on-the-fly at inference time.</li> <li><strong>Training Dataset</strong> – the curated corpus used for pre-training or fine-tuning.</li> </ol> <p>The <strong>Model &amp; Compute</strong> layer is priced <em>per inference token</em>. Beyond raw compute, we include a <em>model premium</em>: the price gap between a proprietary frontier model and the best open-source alternative delivering similar quality.</p> <p><strong>Machine Skills</strong> achievable today include coding, deep financial research, and multi-modal artistic generation. Their marginal contribution is reflected in <em>Cost Saving</em> and <em>Efficiency Premium</em>. Conversely, <strong>Human Skills</strong> are either replaced or augmented, with any residual oversight captured by the <em>Human Intervention Cost</em> term.</p> <p>Putting these pieces together, the updated AI Fundamental identity is</p> \[\begin{aligned} \text{AEV} &amp;= \text{Cost Saving} + \text{Efficiency Premium} - \text{Model Cost} \\ &amp;- \text{Data Cost} - \text{Human Intervention Cost}, \end{aligned}\] <p>where Model Cost $=$ (inference compute token cost) $+$ model premium, and all terms are measured on a <em>marginal</em> per-task basis.</p> <h3 id="metric-definitions">Metric Definitions</h3> <p>The above Equation defines AEV with explicit cost fields.</p> \[\begin{aligned} \text{AEV} &amp;= \sum_{i \in T}\!\bigl(\text{CostSaving}_i + \text{EfficiencyPremium}_i\bigr) \\ &amp;- \text{ModelCost}_i - \text{DataCost}_i - \text{HumanInterventionCost}_i. \end{aligned}\] <p>We can also express Agent Economic Value with more detailed cost components:</p> \[\begin{aligned} \text{Agent Economic Value} &amp;= \text{Cost Saving} + \text{Efficiency Premium} \\ &amp;- (C_{\text{GPU}} + C_{\text{energy}} + C_{\text{MCP}} + C_{\text{human}} + C_{\text{data}}) \end{aligned}\] <p>Here, the cost components can be broken down further: $C_{\text{GPU}}$ encompasses expenses for GPUs and other accelerators; $C_{\text{energy}}$ includes power consumption and associated infrastructure like cooling systems; $C_{\text{data}}$ covers the often substantial lifecycle costs of data (acquisition, storage, processing, and ongoing curation), which forms the foundation for the agent’s learning and performance; $C_{\text{MCP}}$ pertains to the managed compute and prompt orchestration services, which can also factor in costs related to algorithmic complexity or specific software; and $C_{\text{human}}$ is the cost of necessary human oversight and intervention.</p> <p><strong>GAAP mapping.</strong></p> <ul> <li><strong>Cost Saving</strong> $\to$ Operating expense reduction</li> <li><strong>Efficiency Premium</strong> $\to$ Incremental revenue or throughput uplift</li> <li><strong>Model, Data, Human Intervention Costs</strong> $\to$ Cost of goods sold / operating expenses</li> <li><strong>AEV</strong> $\to$ Net Operating Profit After Tax (NOPAT)</li> </ul> <h3 id="efficiency-premium">Efficiency Premium</h3> <p>The Efficiency Premium captures the top-line growth or productivity gain from AI – not just doing the same work for cheaper, but doing <em>more</em> or <em>better</em> with AI than before. This metric recognizes that AI agents can operate at superhuman speed and scale when reliable, potentially generating additional revenue or throughput.</p> <p>Examples include:</p> <ul> <li>Customer support AI handling hundreds of chats concurrently compared to a human’s 5 chats</li> <li>Healthcare where AI can engage with patients who otherwise wouldn’t get attention</li> <li>Software development cycles becoming faster, allowing teams to tackle more ambitious projects</li> </ul> <p>The Efficiency Premium often remains latent until human-intervention costs are brought under control. Once the AI is reliable enough (failures are very rare), it can be scaled almost without limit and the premium can accumulate unabated. When intervention costs trend toward zero the negative terms vanish, allowing the Efficiency Premium to accumulate without bound.</p> <h2 id="measurement-methodology">Measurement Methodology</h2> <ol> <li><strong>Task Extraction.</strong> For unstructured chat logs we apply Clio’s embedding+pattern pipeline (DP noise $\epsilon=0.5$) <d-cite key="Handa2025"></d-cite>.</li> <li><strong>Dollar Valuation.</strong> <ul> <li><em>Software</em>: Upwork median payout $\tilde{p}=$ \$500; full ticket pool \$1M <d-cite key="Miserendino2025"></d-cite>.</li> <li><em>Healthcare</em>: average avoided readmission saves \$12,000 (CMS FY-24); multiply HealthBench rubric score by that coefficient <d-cite key="Arora2025"></d-cite>.</li> </ul> </li> <li><strong>Human-Intervention Logging.</strong> Tag manual edits, escalations, or tool fallbacks. Frontier models still miss $\sim 60%$ of SWE-Lancer tasks <d-cite key="Miserendino2025,liang2025swe"></d-cite>.</li> <li><strong>Experience Accounting.</strong> Tokens spent on self-play or simulation $\to$ <em>Exploration Opex</em>; indispensable for the experience-era view <d-cite key="SilverSutton2025"></d-cite>.</li> </ol> <p><strong>Data–Asset Valuation.</strong> Interaction logs generated during agent operation accrue as an <em>intangible data asset</em>. Under GAAP we expense collection and cleaning costs immediately, but we may capitalise the curated corpus once it demonstrably improves future cash flows (analogous to software development costs that pass technological feasibility). The same treatment applies under IFRS (IAS 38), where the asset is amortised over the useful life of the model. We therefore track a “Data R&amp;D” line item that migrates to the balance-sheet once the corpus clears the feasibility gate, providing an auditable bridge from token spend to book value.</p> <h2 id="empirical-benchmarks">Empirical Benchmarks</h2> <h3 id="anthropic-clio-national-task-adoption">Anthropic Clio: National Task Adoption</h3> <p><strong>Key findings</strong> <d-cite key="Handa2025"></d-cite>:</p> <ul> <li>46% of usage involves software and writing tasks; less than 0.5% involves physical manipulation.</li> <li>Only 4% of occupations use AI for at least 75% of their tasks, implying a high baseline human–intervention rate (HIR).</li> <li>Uptake peaks in <em>Job–Zone 4</em>, corresponding to bachelor-level pay-grade roles.</li> <li>57% of usage suggests human–AI collaboration, while 43% represents full automation.</li> </ul> <h3 id="swelancer-outcomepriced-coding">SWE–Lancer: Outcome–Priced Coding</h3> <p>The dataset comprises 1,488 Upwork tickets totalling \$1M in payouts. Claude 3.5 Sonnet earns \$403k, yielding a median cost–saving rate (CSR) of roughly \$300 and an HIR of about 74% <d-cite key="Miserendino2025"></d-cite>. Robust human–written tests mitigate grader gaming.</p> <h3 id="healthbench-safetycritical-triage">HealthBench: Safety–Critical Triage</h3> <p>GPT–4o scores 32%, whereas a later model (<em>o3</em>) scores 60% <d-cite key="Arora2025"></d-cite>. Using the \$12,000 valuation coefficient (Section 4), GPT–4o’s 32% rubric score translates to a CSR of approximately \$3,840 per triage (0.32 $\times$ \$12,000). The <em>o3</em> model, with a 60% score, yields an HIR of roughly 40%.</p> <h2 id="examples">Examples</h2> <p>To demonstrate the diagnostic utility of the <strong>AI Fundamentals</strong> framework, we apply it to three <strong>simulated</strong> business scenarios. While these case studies are hypothetical, the parameters are modeled on realistic operational baselines and current frontier model pricing <d-cite key="ackerman2025perceptions"></d-cite>. These simulations illustrate how the AEV equation decomposes complex agent deployments into clear economic drivers, highlighting the interplay between cost savings, efficiency gains, and the critical “tax” of human intervention.</p> <h3 id="example-1-ai-powered-market-research-analysis-and-report-generation">Example 1: AI-Powered Market Research Analysis and Report Generation</h3> <p>We model a hypothetical consulting firm, <em>InsightCorp</em>, that deploys an AI agent to streamline the production of market research.</p> <p><strong>Scenario and Baseline</strong></p> <ul> <li><strong>Task</strong>: Analyze market trends and generate a client-ready 50-page report.</li> <li><strong>Baseline (Human-Only)</strong>: 2 analysts, 10 days (160 hours), \$75/hour = \$12,000 per report.</li> <li><strong>Output</strong>: Maximum 2 reports per month per team.</li> </ul> <p><strong>AI Fundamentals Metrics (Simulated)</strong></p> <p><em>Cost Saving</em></p> <ul> <li>AI processing: \$500</li> <li>Human review (reduced to 40 hours): \$3,000</li> <li>Total agent-assisted cost: \$3,500</li> <li><strong>Cost Saving</strong> = \$12,000 - \$3,500 = **\$8,500 per report**</li> </ul> <p><em>Efficiency Premium</em></p> <ul> <li>Increased throughput (4 reports/month vs. 2): \$10,000 per report</li> <li>Enhanced quality/scope: \$1,000 per report</li> <li><strong>Efficiency Premium</strong> = <strong>\$11,000 per report</strong></li> </ul> <p><em>Human Intervention Cost</em></p> <ul> <li>In this model, 19 reports require only planned review.</li> <li>1 report needs significant rescue (vs. previous 2).</li> <li>Intervention incidents: 1 out of 20 reports required rescue (5%).</li> <li><strong>Intervention penalty</strong>: We assign a simulated governance penalty of \$10 per basis point of intervention; hence 450 b.p. = 4.5% $\times$ 10 = \$4,500.</li> </ul> <p><em>Agent Economic Value (AEV)</em></p> <ul> <li>Additional costs (infrastructure, exploration): \$1,000</li> <li>Intervention penalty: \$10/b.p. $\times$ 450 b.p. = \$4,500 (reduced from \$9,500)</li> <li><strong>AEV</strong> = \$8,500 + \$11,000 - (\$1,000 + \$4,500) = **\$14,000 per report**</li> </ul> <p><strong>Key Outcomes</strong> The simulation suggests that even with a 5% intervention rate, the system achieves:</p> <ul> <li>71% cost reduction (\$8,500 savings per report)</li> <li>100% increased throughput (4 vs. 2 reports monthly)</li> <li>117% ROI (\$14,000 profit/\$12,000 baseline cost)</li> </ul> <p>Further reducing intervention incidents to 2% in this model would add another \$3,000 to AEV (to \$17,000), highlighting how reliability directly impacts profitability.</p> <h3 id="example-2-ai-enhanced-customer-support-automation">Example 2: AI-Enhanced Customer Support Automation</h3> <p>Next, we simulate <em>ConnectSphere Inc.</em>, a hypothetical telecom company deploying an AI system to handle Tier-1 support.</p> <p><strong>Scenario and Baseline</strong></p> <ul> <li><strong>Task Domain</strong>: Tier-1 customer support (password resets, billing inquiries, basic troubleshooting)</li> <li><strong>Baseline</strong>: 100 agents handling 50,000 monthly interactions at \$10 per interaction (\$500,000 total)</li> <li><strong>Service Quality</strong>: 15-minute average resolution time, 70% CSAT score</li> </ul> <p><strong>AI Fundamentals Metrics (Simulated)</strong></p> <p><em>Cost Saving</em></p> <ul> <li>AI fully resolves 20,000 interactions; humans handle 30,000 with AI assistance</li> <li>AI system cost: \$50,000</li> <li>Reduced human staffing: 50 agents at \$5,000 each = \$250,000</li> <li><strong>Cost Saving</strong> = \$500,000 - \$300,000 = **\$200,000 per month**</li> </ul> <p><em>Efficiency Premium</em></p> <ul> <li>CSAT improvement (70% $\to$ 88%), reducing churn: \$120,000</li> <li>24/7 availability: \$20,000</li> <li><strong>Efficiency Premium</strong> = **\$140,000 per month** (increased from previous \$120,000)</li> </ul> <p><em>Human Intervention Cost</em></p> <ul> <li>600 interactions (3% of AI-only attempts) require human rescue (reduced from 5%)</li> <li><strong>Intervention penalty</strong>: We assign a governance penalty of \$50 per basis point of intervention; hence 200 b.p. = 2% $\times$ \$50 = \$10,000.</li> </ul> <p><em>Agent Economic Value (AEV)</em></p> <ul> <li>Intervention penalty: \$50/b.p. $\times$ 200 b.p. = \$10,000 (reduced from \$20,000)</li> <li><strong>AEV</strong> = \$200,000 + \$140,000 - (\$0 + \$10,000) = **\$330,000 per month**</li> </ul> <p><strong>Key Outcomes</strong> In this scenario, the agent delivers a \$330,000 monthly profit (66% return). Key drivers in the model include:</p> <ul> <li>40% cost reduction (\$200,000 monthly savings)</li> <li>67% faster resolution (5 vs. 15 minutes for AI-handled queries)</li> <li>26% CSAT improvement (70% $\to$ 88%)</li> </ul> <h3 id="example-3-ai-driven-deep-research-in-finance">Example 3: AI-Driven Deep Research in Finance</h3> <p>Finally, we consider a simulated quantitative asset-management firm, <em>AlphaFunds</em>, deploying an LLM agent for forensic financial analysis.</p> <p><strong>Scenario and Baseline</strong></p> <ul> <li><strong>Task:</strong> Produce a 20-page forensic analysis covering accounting quality, competitive moat, and scenario valuation for one ticker.</li> <li><strong>Baseline (Human-Only):</strong> 1 senior analyst + 2 associates, 80 hours total at an average blended rate of \$180/h $\Rightarrow$ \$14,400 per report.</li> <li><strong>Throughput:</strong> 10 tickers per quarter.</li> </ul> <p><strong>AI Fundamentals Metrics (Simulated)</strong></p> <p><em>Cost Saving</em></p> <ul> <li>LLM inference and retrieval cost: \$400.</li> <li>Human review (senior analyst 8 h): \$1,440.</li> <li>Total agent-assisted cost: \$1,840.</li> <li><strong>Cost Saving</strong> = \$14,400 - \$1,840 = \$12,560 per report.</li> </ul> <p><em>Efficiency Premium</em></p> <ul> <li>Faster turn-around enables coverage of 25 tickers/quarter (2.5× baseline) generating incremental fee income of \$30,000.</li> <li>Richer alt-data synthesis improves hit-rate, adding expected alpha worth \$5,000 per report.</li> <li><strong>Efficiency Premium</strong> = \$35,000 / 25 $\approx$ \$1,400 per report.</li> </ul> <p><em>Human Intervention Cost</em></p> <ul> <li>2 of 25 reports require significant rewrites $\Rightarrow$ <strong>Intervention penalty</strong>: We assign a governance penalty of \$15 per basis point of intervention; hence 700 b.p. = 7% $\times$ \$15 = \$10,500.</li> <li>Policy gate: 1% (100 b.p.) $\Rightarrow$ excess 700 b.p.</li> </ul> <p><em>Agent Economic Value (AEV)</em></p> <ul> <li>Model Cost (inference tokens + premium): \$400.</li> <li>Data Cost (alt-data subscription slices): \$150.</li> <li>Intervention penalty ($\lambda$ = 15 b.p.): \$10,500.</li> <li><strong>AEV</strong> = \$12,560 + \$1,400 - (\$400 + \$150) - \$10,500 = \$2,910 per report.</li> </ul> <p><strong>Key Outcomes</strong> Even after a heavy intervention penalty, the simulated agent achieves a 20% profit margin. Reducing the intervention rate to 2% in the model would lift AEV above \$10,000.</p> <figure style="text-align: center;"> <img src="/2026/assets/img/2026-04-27-economic-agents/aev_returns_costs_v2.png" width="98%"/> <figcaption style="font-size: 1em;">Figure 4: <strong>AEV Structural Analysis: Returns vs. Costs.</strong> The top row displays the composition of economic flows, highlighting the Net Margin (AEV / Total Returns) for each simulated domain. The bottom row contrasts absolute Returns (Cost Saving + Efficiency Premium) against Costs (Human Intervention + Model + Data). While <em>Support Automation</em> (center) benefits from massive scale and low intervention costs, the <em>Finance Deep Research</em> agent (right) illustrates the "reliability tax," where high human intervention costs significantly compress the net margin despite valid cost savings.</figcaption> </figure> <h2 id="the-reinforcement-learning-accelerator">The Reinforcement Learning Accelerator</h2> <p>The advent of advanced Reinforcement Learning (RL) <d-cite key="wang2025reinforcement,pippas2025evolution"></d-cite> techniques promises to significantly accelerate agent development and performance. As argued by Silver &amp; Sutton, the focus is shifting towards agents that learn from <em>experience</em> by optimizing for grounded rewards directly tied to economic outcomes like profit, or operational metrics like latency <d-cite key="lin2025stop"></d-cite>, rather than relying solely on static human feedback <d-cite key="SilverSutton2025"></d-cite>. The core idea is to define the agent’s objective function in terms of our framework’s value metrics. The total return $G_t$ at a given step $t$ is the sum of immediate economic contributions, $G_t = \text{CostSaving}_t + \text{EfficiencyPremium}_t$. The agent’s learning problem then becomes maximizing the expected cumulative discounted Agent Economic Value over an infinite horizon:</p> \[\begin{aligned} \max_\pi \mathbb{E}\Biggl[ &amp; \sum_{k=0}^{\infty} \gamma^k ( (\text{CostSaving}_{t+k} + \text{EfficiencyPremium}_{t+k}) - \\ &amp; \text{ModelCost}_{t+k} - \text{DataCost}_{t+k} - \text{HumanInterventionCost}_{t+k} ) \Biggr]. \end{aligned}\] <p>Here, the agent’s policy $\pi$ is learned to maximize this sum, where $\gamma$ is the discount factor; each term directly reflects measurable economic impact of the agent’s actions.</p> <p>A critical factor for successful RL-driven development is the creation of high-fidelity <em>simulation environments</em>. These environments must closely mirror the specific real-world scenarios in which the agent will operate, including the nuances of tasks, data distributions, and potential failure modes. By interacting with such well-crafted environments (effectively digital twins of operational realities), an agent can rapidly accumulate a vast amount of experience, far exceeding what is feasible with direct real-world interaction alone or from static datasets. For instance, AlphaProof achieved superhuman problem-solving by generating 100 million proofs through interaction with a dedicated proof environment, starting from an initial seed of 100k human proofs <d-cite key="SilverSutton2025"></d-cite>.</p> <p>This RL-driven, experience-based learning paradigm is central to achieving the long-run vision: an agent with consistently high Agent Economic Value. Such an agent continuously expands its capabilities—thereby raising Cost Saving and the Efficiency Premium—while actively learning to keep intervention costs negligible through ongoing adaptation and refinement within its operational or simulated domain. The key is not just the RL algorithms themselves, but the synergy between these algorithms and rich, representative environments that enable effective and accelerated learning.</p> <h2 id="future-outlook-agentic-liquidity-and-the-mcp-wallet">Future Outlook: Agentic Liquidity and The MCP Wallet</h2> <p>The current AEV equation assumes the agent works with fixed resources. However, as agents move toward full autonomy, we must extend the framework to include <strong>Agentic Liquidity</strong>. This is the ability of the agent to hold and spend capital—”MCP form money”—to complete tasks.</p> <h3 id="the-problem-closed-resource-loops">The Problem: Closed Resource Loops</h3> <p>Currently, if an agent lacks a specific tool or data source (e.g., a real-time Bloomberg terminal feed or a paid API for specialized protein folding), it fails or reports to a human. This spikes $C_{\text{human}}$ and lowers AEV.</p> <h3 id="the-solution-the-settlement-layer">The Solution: The Settlement Layer</h3> <p>We propose extending the MCP architecture to include a <strong>financial settlement layer</strong>. This transforms the agent from a passive software user into an active economic participant.</p> <p>In this model, the MCP is endowed with a wallet containing AI-native currency (stablecoins or equivalent). The agent can autonomously decide to incur a dynamic <strong>Transaction Cost</strong> ($C_{\text{txn}}$) to purchase:</p> <ol> <li><strong>Private APIs &amp; Tools:</strong> Accessing paid SaaS tools or specialized computation (e.g., “renting” a physics engine for 5 minutes).</li> <li><strong>Gated Data Sources:</strong> Paywall unlocking or purchasing high-fidelity datasets on the fly to improve inference quality.</li> <li><strong>Inter-Agent Services:</strong> Hiring other specialized agents (e.g., a Generalist Agent hiring a specialized Legal Agent for contract review).</li> </ol> <h2 id="conclusion">Conclusion</h2> <p>Our framework collapses the valuation of AI agents into a single, auditable cash–flow metric—Agent Economic Value (AEV). AEV integrates</p> <ul> <li><strong>Cost Saving</strong> (direct expense reduction),</li> <li><strong>Efficiency Premium</strong> (new revenue or throughput), and</li> <li>the three marginal costs of <strong>Model</strong>, <strong>Data</strong>, and <strong>Human Intervention</strong>.</li> </ul> <p>This lens unifies technical performance and financial reporting into one number that investors and operators can track just like any other business KPI <d-cite key="ackerman2025perceptions"></d-cite>. Empirical studies across software <d-cite key="Miserendino2025"></d-cite>, healthcare <d-cite key="Arora2025"></d-cite>, and large-scale enterprise usage <d-cite key="Handa2025,erol2025cost"></d-cite> demonstrate that every term in the AEV equation can be measured today.</p> <p>As the Era of Experience unfolds and agents increasingly learn from and act in the real world <d-cite key="ide2024artificial,SilverSutton2025"></d-cite>, the AEV framework will be indispensable to ensure that technical progress translates into measurable economic value. By pushing Cost Saving and the Efficiency Premium up while driving the three cost terms down, organizations can continuously optimise agent deployments in a language the CFO understands.</p> <p>The future of AI valuation lies in this synthesis of technical capability and financial discipline <d-cite key="erol2025cost"></d-cite>. When Cost Saving and Efficiency Premium rise faster than Model, Data, and Human Intervention Costs, the resulting positive Agent Economic Value creates sustainable economic impact that can be measured, predicted, and optimised across diverse domains.</p> <p><em>Read pipes feed knowledge; write pipes mint dollars; training data re-wires the mint. Keep intervention costs low and the cash machine hums.</em></p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Large Language Model (LLM) agents now read the world through managed-context pipelines, write to it via tool-calling APIs, and continuously re-wire themselves with fresh experience. Stakeholders therefore need a Generally Accepted Accounting Principles (GAAP) compatible method to price both (i) the agent's labour-like output and (ii) the data traces that fuel learning. We formalise a single unifying metric - agent Economic Value (AEV)- and demonstrate that these metrics are measurable today. We then extend the template to reinforcement-learning regimes in which grounded rewards equal cash flows. Lastly, we propose a financial settlement layer, which transforms the agent from a passive software user into an active economic participant.]]></summary></entry><entry><title type="html">Elastic Weight Consolidation (EWC): Nuts and Bolts</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/elastic-weight-consolidation-nuts-bolts/" rel="alternate" type="text/html" title="Elastic Weight Consolidation (EWC): Nuts and Bolts"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/elastic-weight-consolidation-nuts-bolts</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/elastic-weight-consolidation-nuts-bolts/"><![CDATA[<h2 id="abstract">Abstract</h2> <p>In this blogpost, we present a theoretical support of the continual learning method <strong>Elastic Weight Consolidation</strong>, introduced in the paper titled ‘Overcoming catastrophic forgetting in neural networks’ <d-cite key="kirkpatrick2017overcoming"></d-cite>. Being one of the most cited papers in regularized methods for continual learning, this blogpost disentangles the underlying concept of the proposed objective function. We assume that the reader is aware of the basic terminologies of continual learning.</p> <h2 id="introduction">Introduction</h2> <p>Following are the notations used throughout this blogpost. Vectors and matrices are denoted in bold lowercase and bold uppercase, respectively. Superscript $^{\top}$ denotes matrix transpose. \(\mathbb{E}[\cdot]\) denotes the expectation operator. An optimum value of a variable is denoted by adding a superscript $^{\star}$.</p> <p>Continual learning is a much desired attribute for neural networks. For example, if we train a model to distinguish between images of a cat and a dog (task 1), and subsequently train it again to distinguish between images of chair and table (task 2), the model should be able to retain its knowledge on task 1 even after learning task 2. In simple terms, our network model should be able to perform equally well on all seen tasks, even after learning new ones. Any degradation of performance on the previous tasks after learning new ones is fittingly termed as <em>catastrophic forgetting</em>. This sub-research area has seen an insurgence in works in recent times <d-cite key="kirkpatrick2017overcoming,zenke2017continual,li2017learning,aljundi2018memory"></d-cite>. Briefly, the continual learning scenarios can be categorized into following <d-cite key="van2019three"></d-cite>:</p> <ul> <li><strong>Task-Incremental Learning</strong>: For the given set of tasks, the task identity is known during testing.</li> <li><strong>Domain-Incremental Learning</strong>: For the given set of tasks, task identity is not provided during testing, but need not infer the same.</li> <li><strong>Class-Incremental Learning</strong>: For the given set of tasks, task identity is not provided during testing, but has to infer the same.</li> </ul> <p>We highly recommend <d-cite key="van2019three,wiewel2019localizing"></d-cite> for a good overview of different methodologies to alleviate catastrophic forgetting as well as continual learning in general. The next Section describes the well studied regularization method of continual learning: Elastic Weight Consolidation. It presents a solution to the continual learning problem by making task-specific synaptic (<em>read</em> network parameters) consolidation. Based on the theory of plasticity of post-synaptic dendritic spines in the brain, this method presents a paradigm that marks how important is a network parameter to the previous tasks and penalizes any change made to it depending upon the importance, while learning new tasks.</p> <h2 id="elastic-weight-consolidation">Elastic Weight Consolidation</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig1-480.webp 480w,/2026/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig1-800.webp 800w,/2026/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig1.png" class="img-fluid rounded z-depth-1" width="75%" height="auto" title="Possible configurations of θ*_A" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <strong>Possible configurations of</strong> $\boldsymbol{\theta}^\star_{\mathcal{A}}$. The shaded region represents a space of optimum $\boldsymbol{\theta}_{\mathcal{A}}$ with acceptable errors w.r.t. $\boldsymbol{\theta}^\star_{\mathcal{A}}$ for task $\mathcal{A}$. </div> <p>Denote parameters of layers of a deep neural network (DNN) with $\boldsymbol{\theta}$. Training DNNs generates a mapping between the input distribution space and target distribution space. This is done by finding out an optimum \(\boldsymbol{\theta} = \boldsymbol{\theta}^\star\) which results in the least error in the training objective. It has been shown in earlier works <d-cite key="sussmann1992uniqueness"></d-cite> that such a mapping can be obtained with many configurations of $\boldsymbol{\theta}^\star$, represented in the figure above. The term <em>many configurations</em> can be interpreted as a solution space around the most optimum $\boldsymbol{\theta}$ with acceptable error in the learned mapping. Note that in figures to follow, the shaded ellipses represent the solution of individual tasks where as the overlapping region of multiple ellipses, marked by diagonal lines, represents the common solution space for all tasks.</p> <p>Let’s begin with a simple case of two tasks, task $\mathcal{A}$ and task $\mathcal{B}$. To have a configuration of parameters that performs well for both $\mathcal{A}$ and $\mathcal{B}$, the network should be able to pick $\boldsymbol{\theta}$ from the overlapping region of the individual solution spaces (see Figure 2). This is with the assumption that there is always an overlapping region for the solution spaces of all tasks for the network to learn them sequentially. A case of four tasks has been illustrated in Figure 2. In the first instance, the network can learn any \(\boldsymbol{\theta} = \boldsymbol{\theta}_{\mathcal{A}}\) that performs well for task $\mathcal{A}$. But with the arrival of task $\mathcal{B}$, the network should pick up a \(\boldsymbol{\theta} = \boldsymbol{\theta}_{\mathcal{A}, \mathcal{B}}\).</p> <p>The next question that arrives is how can the network learn the a set of parameters that lies in this overlapping region. To this end, EWC presents a method of selective regularization of parameters $\boldsymbol{\theta}$. After learning $\mathcal{A}$, this regularization method identifies which parameters are important for $\mathcal{A}$, and then penalizes any change made to the network parameters according to their importance while learning $\mathcal{B}$.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig2-480.webp 480w,/2026/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig2-800.webp 800w,/2026/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig2.png" class="img-fluid rounded z-depth-1" width="75%" height="auto" title="Overlap of possible configurations of θ*" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <strong>Overlap of possible configurations of</strong> $\boldsymbol{\theta}^\star$. The overlapping space represents an optimum parameter region where the network performs without any catastrophic degradation on previous tasks. </div> <h3 id="intractability-of-posterior-of-mathcala-and-its-approximation">Intractability of posterior of $\mathcal{A}$ and its approximation</h3> <p>To formulate the objective, we start by taking a Bayesian approach needed to estimate the network parameters $\boldsymbol{\theta}$. More specifically given the data $\boldsymbol{\Sigma}$, we want to learn the posterior probability distribution function $p(\boldsymbol{\theta}\mid\boldsymbol{\Sigma})$. Following <d-cite key="lherranz2018rotating"></d-cite> and using Bayes rule, we can write</p> <p>\begin{equation} \underbrace{p(\boldsymbol{\theta}\mid\boldsymbol{\Sigma})}_{\text{posterior}} = \dfrac{\overbrace{p(\boldsymbol{\Sigma}\mid\boldsymbol{\theta})}^{\text{likelihood}}\overbrace{p(\boldsymbol{\theta})}^{\text{prior}}}{p(\boldsymbol{\Sigma})} \end{equation}</p> <p>Since maximizing a function is same as maximizing its logarithm, we take \(\log(\cdot)\) of the above equation as follows:</p> <p>\begin{equation} \log(p(\boldsymbol{\theta}\mid\boldsymbol{\Sigma})) = \log(p(\boldsymbol{\Sigma}\mid\boldsymbol{\theta})) +\log(p(\boldsymbol{\theta})) - \log(p(\boldsymbol{\Sigma})) \end{equation}</p> <p>To train the neural network on $\boldsymbol{\Sigma}$, the objective function to be optimized over the log-likelihood function:</p> <p>\begin{equation} \text{argmax}_{\boldsymbol{\theta}}{\ell(\boldsymbol{\theta}) = \log(p(\boldsymbol{\theta}\mid\boldsymbol{\Sigma}))} \end{equation}</p> <p>For the case of given two independent tasks such that \(\boldsymbol{\Sigma} = \{\mathcal{A}, \mathcal{B}\}\) (with $\mathcal{B}$ appearing in sequence after $\mathcal{A}$), the log-posterior can be written as:</p> <p>\begin{equation} \log(p(\boldsymbol{\theta}\mid\boldsymbol{\Sigma})) = \log(p(\mathcal{B}\mid\boldsymbol{\theta})) +\log(p(\boldsymbol{\theta}\mid\mathcal{A})) - \log(p(\mathcal{B})) \end{equation} where the independence of $\mathcal{A}$ and $\mathcal{B}$ is used. Following the Bayesian formulation, $p(\mathcal{B}\mid\boldsymbol{\theta})$ is the loss for current task $\mathcal{B}$, $p(\mathcal{B})$ is the likelihood for $\mathcal{B}$, and now posterior $p(\boldsymbol{\theta}\mid\mathcal{A})$ for $\mathcal{A}$ becomes prior for $\mathcal{B}$.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig3-480.webp 480w,/2026/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig3-800.webp 800w,/2026/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Laplace approximation of true posterior pdf" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <strong>Laplace approximation of true posterior pdf.</strong> $\mathbb{I}_\mathcal{A}$ represents the Fisher Information matrix. </div> <p>Referring to the log-posterior equation, it can be observed that we have to deal with the function $p(\boldsymbol{\theta}\mid\mathcal{A})$. This is the posterior function for $\mathcal{A}$ which contains the information about the parameters that explain $\mathcal{A}$ using the given network. As discussed in <d-cite key="kirkpatrick2017overcoming"></d-cite>, this posterior function is said to be intractable. Basically, the intractability of $p(\boldsymbol{\theta}\mid\mathcal{A})$ can be interpreted as the function not existing in some interpretable form. Hence, it is difficult to estimate its quantiles. See <d-cite key="tokdar2013lecture"></d-cite> for an example.</p> <p>Next as the posterior is difficult to analyze in its present form, we aim to approximate it using Laplace approximation. In simple terms, Laplace approximation methodology is employed to find a normal distribution approximation to a continuous probability density distribution (see Figure 3). Assuming $p(\boldsymbol{\theta}\mid\mathcal{A})$ is smooth and majorly peaked around its point of maxima (i.e. $\boldsymbol{\theta}^\star_{\mathcal{A}}$), we can approximate it with a normal distribution with mean $\boldsymbol{\theta}^\star_{\mathcal{A}}$ and variance $[\mathbb{I}_{\mathcal{A}}]^{-1}$. This brings us to the question on how did we come to the conclusion on these particular values of mean and variance for the normal distribution.</p> <p>To begin with, compute the second order Taylor expansion of $\ell(\boldsymbol{\theta})$ around $\boldsymbol{\theta}^\star_{\mathcal{A}}$ as follows:</p> <p>\begin{equation} \ell(\boldsymbol{\theta})\approx \ell(\boldsymbol{\theta}^\star_{\mathcal{A}}) +( \dfrac{\partial\ell(\boldsymbol{\theta})}{\partial\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}}) + \dfrac{1}{2}(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}})^\top(\dfrac{\partial^2\ell(\boldsymbol{\theta})}{\partial^2\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}})(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}}) + \text{(higher order terms)} \end{equation}</p> <p>Neglecting higher order terms and noting that \(\dfrac{\partial\ell(\boldsymbol{\theta})}{\partial\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}} = 0\) (slope of tangent at peak), we have:</p> \[\begin{align} \ell(\boldsymbol{\theta})\approx \ell(\boldsymbol{\theta}^\star_{\mathcal{A}}) + \dfrac{1}{2}(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}})^\top\underbrace{(\dfrac{\partial^2\ell(\boldsymbol{\theta})}{\partial^2\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}})}_{\text{Hessian}}(\boldsymbol{\theta} -\boldsymbol{\theta}^\star_{\mathcal{A}}) \end{align}\] <p>Using the log-posterior equation, we can write the above for task $\mathcal{A}$ as following:</p> <p>\begin{equation} \log(p(\boldsymbol{\theta}\mid\mathcal{A})) = \log(p(\boldsymbol{\theta}^\star_{\mathcal{A}}\mid\mathcal{A})) + \dfrac{1}{2}(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}})^\top(\dfrac{\partial^2(\log(p(\boldsymbol{\theta}\mid\mathcal{A})))}{\partial^2\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}})(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}}) + \Delta \end{equation}</p> <p>where \(\Delta = \log(p(\boldsymbol{\theta}^\star_{\mathcal{A}}\mid\mathcal{A}))\). Next, write</p> <p>\begin{equation} (\dfrac{\partial^2(\log(p(\boldsymbol{\theta}\mid\mathcal{A})))}{\partial^2\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}}) = -((-\dfrac{\partial^2(\log(p(\boldsymbol{\theta}\mid\mathcal{A})))}{\partial^2\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}})^{-1})^{-1} \end{equation}</p> <p>and replace it back in the equation to express the same in the standard form of normal distribution function:</p> <p>\begin{equation} p(\boldsymbol{\theta}\mid\mathcal{A}) = \epsilon\exp(-\dfrac{1}{2}(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}})^\top((-\dfrac{\partial^2(\log(p(\boldsymbol{\theta}\mid\mathcal{A})))}{\partial^2\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}})^{-1})^{-1}(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}})) \end{equation}</p> <p>where \(\epsilon = \exp(\Delta)\) is a constant. From this equation, it can be concluded that we have obtained the Laplace approximation of posterior pdf as:</p> <p>\begin{equation} p(\boldsymbol{\theta}\mid\mathcal{A})\sim\mathcal{N}(\boldsymbol{ \theta}^\star_{\mathcal{A}}, (-\dfrac{\partial^2(\log(p(\boldsymbol{\theta}\mid\mathcal{A})))}{\partial^2\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}})^{-1}) \end{equation}</p> <p>Notice the variance of the estimated normal distribution of $p(\boldsymbol{\theta}\mid\mathcal{A})$. Given $\boldsymbol{ \theta}^\star_{\mathcal{A}}$, the term $\log(p(\boldsymbol{\theta}\mid\mathcal{A}))$ represents the log-likelihood of posterior pdf $p(\boldsymbol{\theta}\mid\mathcal{A})$. Clearly, the term represents the inverse of <strong>Fisher information matrix</strong> (FIM),</p> \[\begin{equation} \mathbb{I}_{\mathcal{A}} = \mathbb{E}[-\dfrac{\partial^2(\log(p(\boldsymbol{\theta}\mid\mathcal{A})))}{\partial^2\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}}] \end{equation}\] <p>Note that we obtain $\mathbb{I}_{\mathcal{A}}$ by using the Bayesian equation and treating the prior $p(\boldsymbol{\theta})$ and $p(\mathcal{A})$ constant. This makes derivative of log of the Bayesian equation posterior and likelihood equal. More on this in <strong>Appendix A.2</strong> of <d-cite key="van2019three"></d-cite>. Finally, we get</p> <p>\begin{equation} p(\boldsymbol{\theta}\mid\mathcal{A})\sim\mathcal{N}(\boldsymbol{ \theta}^\star_{\mathcal{A}}, [\mathbb{I}_{\mathcal{A}}]^{-1}) \end{equation}</p> <p>Further, as FIM can also be computed from first order derivatives, we can avoid the Hessian computed in the Taylor expansion using the following property <d-cite key="kay1993fundamentals"></d-cite>:</p> \[\begin{equation} \mathbb{I}_{\mathcal{A}} = \mathbb{E}[-\dfrac{\partial^2(\log(p(\boldsymbol{\theta}\mid\mathcal{A})))}{\partial^2\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}}] = \mathbb{E}[(\dfrac{\partial(\log(p(\boldsymbol{\theta}\mid\mathcal{A})))}{\partial\boldsymbol{\theta}})(\dfrac{\partial(\log(p(\boldsymbol{\theta}\mid\mathcal{A})))}{\partial\boldsymbol{\theta}})^\top\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}}] \end{equation}\] <p>Now, we can write the log-posterior equation as:</p> \[\begin{equation} \log(p(\boldsymbol{\theta}\mid\boldsymbol{\Sigma})) = \log(p(\mathcal{B}\mid\boldsymbol{\theta})) +\dfrac{\lambda}{2}(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}})^\top(\dfrac{\partial^2(\log(p(\boldsymbol{\theta}\mid\mathcal{A})))}{\partial^2\boldsymbol{\theta}}\mid_{\boldsymbol{\theta}^\star_{\mathcal{A}}})(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}}) + \epsilon' \end{equation}\] <p>where $\epsilon’$ accounts for all constants and $\lambda$ is a hyper-parameter introduced to have a trade off between learning $\mathcal{B}$ and not forgetting $\mathcal{A}$. Simplifying more, we have:</p> \[\begin{equation} \log(p(\boldsymbol{\theta}\mid\boldsymbol{\Sigma})) = \log(p(\mathcal{B}\mid\boldsymbol{\theta})) - \dfrac{\lambda}{2}(\boldsymbol{\theta} - \boldsymbol{ \theta}^\star_{\mathcal{A}})^\top \mathbb{I}_{\mathcal{A}} (\boldsymbol{\theta} - \boldsymbol{\theta}^\star_{\mathcal{A}}) + \epsilon' \end{equation}\] <p>This implies: \(\begin{equation} \underbrace{\ell(\boldsymbol{\theta})}_{\text{overall loss}} = \underbrace{\ell_\mathcal{B}(\boldsymbol{\theta})}_{\text{loss for \mathcal{B} }} - \underbrace{\dfrac{\lambda}{2}(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}})^\top\mathbb{I}_{\mathcal{A}}(\boldsymbol{\theta} -\boldsymbol{ \theta}^\star_{\mathcal{A}})}_{\text{weight regularizer}} + \epsilon' \end{equation}\)</p> <p>Further simplification can be found in <d-cite key="van2019three"></d-cite>. Before we end this Section, let’s discuss how does the FIM indicates the <strong>importance</strong> of the parameters for the previous tasks.</p> <p>We say a network has learnt a task when its objective has reached a minimum in the loss surface. We know that the curvature of such surfaces represent the sensitivity of the network with respect to the optimum $\boldsymbol{\theta}^\star$. This sensitivity can be determined by looking at the direction along which $\boldsymbol{\theta}^\star$ changes. This implies the curvature is inversely proportional to change in $\boldsymbol{\theta}^\star$. Hence, if the more the curvature, a ‘$\delta$’ increment can result in large increase in the loss. Curvature of a curve is denoted by its Hessian and hence in our case, as the second derivative is of the log likelihood function of the posterior pdf, the FIM \(\mathbb{I}_{\mathcal{A}}\) comes into picture. Thus, \(\mathbb{I}_\mathcal{A}\) can tell us which parameter is important to the the previous task as its corresponding element in \(\mathbb{I}_\mathcal{A}\) will have a large value, indicating higher importance. See <d-cite key="maltoni2019continuous"></d-cite> for more.</p> <h2 id="conclusion">Conclusion</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig4-480.webp 480w,/2026/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig4-800.webp 800w,/2026/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-elastic-weight-consolidation-nuts-bolts/fig4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Sequential training on task B after task A" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <strong>Sequential training on task</strong> $\mathcal{B}$ <strong>after task</strong> $\mathcal{A}$. Left: Train the network as it is: results in 'Forgetting', Middle: Make no change in the parameters of previous tasks, Right: Make changes in the parameters of the previous tasks depending on their importance. </div> <p>In this blogpost, we have presented a theoretical support of the EWC method. We have shown how the intractable posterior function can be approximated using Laplace approximation, and how the Fisher Information Matrix can be used to identify the importance of parameters for previous tasks. The EWC method provides a principled approach to continual learning by selectively regularizing parameters based on their importance to previously learned tasks.</p>]]></content><author><name>Anonymous</name></author><category term="Continual Learning"/><category term="Regularization Methods"/><category term="Theoretical Analysis"/><category term="EWC"/><category term="Catastrophic Forgetting"/><category term="Fisher Information Matrix"/><category term="Bayesian Methods"/><category term="Laplace Approximation"/><summary type="html"><![CDATA[A theoretical deep-dive into the Elastic Weight Consolidation method for continual learning, explaining the mathematical foundations and intuitions behind this influential approach to preventing catastrophic forgetting.]]></summary></entry><entry><title type="html">EvalCards for Standardized Evaluation Reporting</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/evalcards/" rel="alternate" type="text/html" title="EvalCards for Standardized Evaluation Reporting"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/evalcards</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/evalcards/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Classic scientific scandals often turn on withholding of details around exactly how scientific hypotheses were evaluated. For example, the case of the Piltdown Man, where researchers selectively (mis-)reported crucial contextual details, misled evolutionary sciences for decades <d-cite key="vincent1999piltdown"></d-cite>. Lack of reporting standards can also lead to confusion, even when everyone acts in good faith. To illustrate, in 19th-century chemistry, the lack of agreed conventions on atomic weights left the field in chaos, with the same compounds appearing under conflicting formulas, until the Karlsruhe Congress established common standards <d-cite key="ihde1961karlsruhe"></d-cite>. These (and other similar) episodes <d-cite key="goldacre2009bad"></d-cite> instill the same lesson: without reliable reporting conventions, even important discoveries can distort rather than advance science.</p> <p>Evaluation - quantitative measurement of a model’s performance on a pre-defined task or benchmark - has long been one of the central means of assessing progress in NLP <d-cite key="jones1994towards, church2017emerging, church2019survey, bowman-dahl-2021-will, kiela-etal-2021-dynabench, sainz-etal-2023-nlp"></d-cite>. Despite this, our standards for reporting evaluations have not kept pace <d-cite key="bhatt2021case, belz-etal-2023-non, belz-etal-2025-standard, zhao-etal-2025-sphere"></d-cite>. Such a lack of standards becomes more concerning with the fast adoption of Large Language Models (LLMs) by a wide range of stakeholders, many of whom are not experts and yet heavily depend on such systems to make decisions that impact real-world outcomes <d-cite key="araujo2020ai, bommasani2023holistic"></d-cite>. As LLMs become embedded in critical domains, responsible deployment is a key consideration <d-cite key="10536000, radanliev2024ethics, orr2024building,tripathi2025ethical"></d-cite> and a major part of this includes transparency on what a model can and cannot do. Based on a survey of recent research in the field of evaluation studies, we identify three critical problems stemming from current reporting practices: the <em>Reproducibility Crisis</em>, the <em>Accessibility Crisis</em>, and the <em>Governance Crisis</em>. We discuss why current efforts at transparency <d-cite key="mitchell2019model, gebru2021datasheets"></d-cite> need reconsideration. In light of such issues, we propose EvalCards: concise evaluation summaries which are (i) <em>easy to write</em>, (ii) <em>easy to understand</em>, and (iii) <em>hard to miss</em>. We present case studies of three popular models, showing how difficult it was to gather consistent evaluation details when creating sample EvalCards, and also discuss directions of future work.</p> <p>Our main argument is one for a shift in norms: evaluation reporting is not a marketing exercise but a core component of what it means to release a model responsibly. While the broader challenge of how to evaluate models remains open and complex <d-cite key="laskar2024systematic, chang2024survey, gao2025llm"></d-cite>, our focus here is narrower but nonetheless critical: improving how evaluations are reported. We hope this work sparks conversation and helps move the field toward a culture of more honest and actionable evaluation disclosure practices.</p> <h2 id="problems-with-evaluation-reporting">Problems with Evaluation Reporting</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-evalcards/eval_card_figure-480.webp 480w,/2026/assets/img/2026-04-27-evalcards/eval_card_figure-800.webp 800w,/2026/assets/img/2026-04-27-evalcards/eval_card_figure-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-evalcards/eval_card_figure.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ground our analysis of problems in evaluation reporting in NLP and AI, we conducted a survey of recent work with systematic keyword searches related to evaluation and reporting (e.g., evaluation, reporting, disclosure, evaluation artifacts) in ACL Anthology, DBLP, and Google Scholar. We complemented this with a reverse snowball sampling from the most recent broad-scope seminal works in NLP evaluation as seeds <d-cite key="weidinger2025toward, gao2025llm,zhao-etal-2025-sphere,chang2024survey,laskar2024systematic,biderman2024lessons,burnell2023rethink, allen2021evaluation"></d-cite>. We then manually analyzed the works to extract recurring themes of discussion. From this, we identify three overarching crises of reproducibility, accessibility, and governance.</p> <h3 id="reproducibility-crisis">Reproducibility Crisis</h3> <p>In machine learning, the broader reproducibility crisis is well discussed <d-cite key="kapoor2023leakage"></d-cite> and also manifests in evaluation reporting <d-cite key="bouthillier2019unreproducible, dodge-etal-2019-show, belz-etal-2025-standard, zhao-etal-2025-sphere"></d-cite>. More specifically, in the context of model evaluations, prior work repeatedly highlights that published results often cannot be trusted without careful reconstruction of undocumented choices <d-cite key="belz-etal-2021-systematic,burnell2023rethink, belz-etal-2023-non, biderman2024lessons, belz-etal-2025-standard, zhao-etal-2025-sphere"></d-cite>. We conduct some case studies on some recent LLM model releases and discuss three such crucial details that are often inconsistent or missing from evaluation details:</p> <p><strong>Target Capability</strong> Model releases include benchmark names and scores, but can often fail to specify what each benchmark is intended to measure. This problem can be exacerbated for reported scores on large composite benchmarks such as SuperGLUE <d-cite key="wang2019superglue"></d-cite> or HELM <d-cite key="bommasani2023holistic"></d-cite> that aggregate tasks across domains and do not provide clarity, especially for non-experts, as to what capability is targeted.</p> <p><strong>Metric</strong> Reported scores frequently omit which evaluation metric was used, which makes it difficult to assess what a score reflects or to compare performance across models fairly <d-cite key="mizrahi2024state, hu2024unveiling"></d-cite>.</p> <p><strong>Prompting Strategy</strong> Prompting, i.e, the way a query is structured and phrased for LLMs, is one of the most significant variables in model performance <d-cite key="hu2024unveiling, sclar2024quantifying, zhuo-etal-2024-prosa, chatterjee-etal-2024-posix"></d-cite>, yet is often absent from reported results. Also, the absence of consistent reporting on a common baseline strategy, such as zero-shot prompting, further hinders meaningful comparison across models <d-cite key="10.1145/3582269.3615599, ousidhoum-etal-2021-probing"></d-cite>.</p> <h3 id="accessibility-crisis">Accessibility Crisis</h3> <p>A recurring theme in relevant work is the highly fragmented nature of available information on models. Documentation frameworks such as Model Cards, DataSheets, and FactSheets were introduced precisely to improve accessibility of information about models <d-cite key="mitchell2019model, gebru2021datasheets, arnold2019factsheets, bhardwaj2024machine, luo2025lack"></d-cite>. However, even today, evaluation details are dispersed across academic papers, technical appendices, GitHub READMEs, HuggingFace model cards, and blog posts, each using different terminology and presentation styles. This scattered documentation makes it difficult to locate and compare the evaluation results. This has consequences for both researchers and users.</p> <p><strong>For Researchers</strong> When evaluation details are scattered across sources, they are easily overlooked or lost altogether. Important context, such as benchmark versions, question framing, or metric details, may never reach the researchers who rely on these results <d-cite key="belz-etal-2023-non,belz-thomson-2024-2024,belz-etal-2025-standard, belz-etal-2025-2025"></d-cite>.</p> <p><strong>For Users</strong> For non-technical users and decision-makers, the problem is compounded by selective reporting, where only strong benchmark results are emphasized or marketed in some sources, further reducing trust in evaluation claims <d-cite key="arnold2019factsheets"></d-cite>. As a result, users may struggle to select appropriate models, a challenge that becomes particularly critical in high-stakes or sensitive deployment contexts <d-cite key="huijgens2024help, 10.1145/3706598.3713240"></d-cite>.</p> <p>For evaluation to be actionable, it must be consistently visible and easily accessible. Without a standardized way to report evaluations in one place, users are left to piece together incomplete information, undermining efforts to assess a model’s suitability for deployment.</p> <h3 id="governance-crisis">Governance Crisis</h3> <p>AI legislations across the world today, from US to EU <d-cite key="edwards2021eu, act2024eu, sloane2025systematic, carey2025regulating"></d-cite> and from Singapore to China <d-cite key="pande2023navigating, roberts2021chinese, dong2024meta"></d-cite>, are increasingly concerned with transparency <d-cite key="larsson2020transparency, agrawal2024accountability"></d-cite> and reporting mandates <d-cite key="nagendran2020artificial, laux2024three"></d-cite>. Without standardized evaluation reports, governance of models faces three key problems:</p> <p><strong>Risk Assessment</strong> It becomes challenging to determine model risks when the evaluation methods are not clearly reported <d-cite key="hogan2021ethics, novelli2024taking, reuel2024betterbench, reuel2024open"></d-cite>.</p> <p><strong>Algorithmic Accountability</strong> When developers can selectively report results or omit critical weaknesses, it makes it difficult for external reviewers or regulators to hold systems to consistent standards <d-cite key="shah2018algorithmic, wieringa2020account, horneber2023algorithmic"></d-cite>.</p> <p><strong>Compliance Washing</strong> Akin to ethics washing practices <d-cite key="bietti2020ethics"></d-cite>, AI developers can satisfy regulatory requirements by disclosing something—even if that ``something” is incomplete, selectively positive, or methodologically weak <d-cite key="koshiyama2024towards"></d-cite>. Regulatory compliance becomes a box-ticking exercise, undermining the goals of safety, accountability, and public trust <d-cite key="veale2021demystifying"></d-cite>.</p> <p>These crises highlight that the problem is not only how models are evaluated, but how those evaluations are reported. In the next section, we examine limitations of current standardization efforts and introduce EvalCards as a practical solution for improving evaluation reporting.</p> <h2 id="problems-with-existing-standards">Problems with Existing Standards</h2> <p><strong>Lack of evaluation focus</strong> Existing documentation frameworks rarely place evaluation at the center. Model Cards, DataSheets, and FactSheets typically treat evaluation results as only one component among many. BenchmarkCards <d-cite key="sokol2024benchmarkcards"></d-cite> focuses on information specific to a single benchmark only, with no reference to models. For large language models, however, evaluation is the primary means by which users, researchers, and regulators can understand capabilities and limitations.</p> <p><strong>High effort for developers</strong> Most of the proposed documents, like Model Cards <d-cite key="mitchell2019model"></d-cite>, are lengthy and time-consuming to produce since they require additional analysis like listing out all possible use-cases and users, detailed demographic factor evaluation, intersectional quantitative analyses, etc. This can be especially problematic when many models are being released at a rapid pace. Dedicating extra time to such detailed analysis may not be possible.</p> <p><strong>Limited accessibility for non-experts</strong> For decision-makers, policymakers, and many end-users, these documents are often too technical or jargon-heavy to offer real clarity <d-cite key="mcgregor2025err, crisan2022interactive"></d-cite>. For example, OpenAI’s system cards, like the <a href="https://openai.com/index/gpt-4o-system-card/">GPT-4o card</a>, offer in-depth safety and governance information but are often very lengthy and complex. As a result, even when provided, they are underutilized by key stakeholders <d-cite key="blodgett-etal-2024-human"></d-cite>.</p> <p><strong>Lack of visibility</strong> While earlier works have talked about model documentation, not many have emphasized the need for visibility. Today, information about models is frequently buried in supplemental materials, obscure repositories, or separate websites—making it difficult to access and easy to overlook <d-cite key="mcgregor2025err"></d-cite>.</p> <h2 id="evalcards">EvalCards</h2> <p>To address the challenges outlined in previous sections, we propose Evaluation Disclosure Cards (EvalCards), a short-form standardized reporting format for model evaluations. In this section, we discuss the design principles of an EvalCard, what it should contain, when it should be created, and where it should be available.</p> <h3 id="design-principles-of-evalcards">Design Principles of EvalCards</h3> <p>Any reporting format must go beyond existing documentation efforts by tackling the practical barriers identified above: focusing on evaluation, reducing the burden on developers, making results clear to a wide range of users, and ensuring that evaluation information is consistently visible wherever models are accessed. We summarize the design principles here:</p> <p><strong>Evaluation Focus</strong> Unlike broader documentation frameworks such as Model Cards or DataSheets, which include information about training data, intended use cases, and ethical considerations, EvalCards place evaluation at the center. The goal is not to capture every possible aspect of model development, but to provide clear and standardized details about what was evaluated, how it was evaluated, and under what conditions.</p> <p><strong>Easy to Write</strong> For transparency to become standard practice, evaluation reporting must be easy to implement. EvalCards are designed to capture only the essential details of model evaluation, making them quick to produce and maintain. This is especially important for smaller organizations and open-source model developers that lack the resources to run large test suites.</p> <p><strong>Easy to Understand</strong> Transparency is meaningless if only a handful of experts can interpret it. Evaluation reports must be designed for broad accessibility, enabling not just researchers but also all those without domain expertise to grasp a model’s capabilities and risks quickly.</p> <p><strong>Hard to Miss</strong> As discussed, evaluation details are buried in academic papers, supplementary materials, or hidden deep within repositories. Standardized evaluation reports should be integrated directly into any landing pages where models can be accessed, whether that is on HuggingFace Hub, API dashboards, or third-party model provider repositories. By ensuring that evaluation disclosures are always visible and linked to the model itself, we would create a culture where understanding a model’s capabilities and risks becomes a default part of using models, not an optional deep dive.</p> <h3 id="what-should-an-evalcard-contain">What should an EvalCard contain?</h3> <p><strong>Modalities Evaluated</strong> EvalCards specify which input and output modalities—such as text, image, or audio—the model has been evaluated on.</p> <p><strong>Languages Evaluated</strong> As with modalities, clearly stating which languages a model has evaluated on helps define the scope of its real-world applicability. Many models advertise multilingual <code class="language-plaintext highlighter-rouge">training'' or</code>support’’, but such claims do not indicate whether those languages have been explicitly tested in any systematic way. Without evaluation, such claims can be misleading <d-cite key="joshi-etal-2020-state, blasi-etal-2022-systematic,talat-etal-2022-reap"></d-cite></p> <p><strong>Capability Evaluation</strong> EvalCards do not prescribe specific benchmarks, but we require developers to explicitly state which core abilities (e.g., summarization, reasoning, factual recall, mathematical problem solving) were evaluated, and to indicate the benchmark chosen for each. Additionally, all reported results should be accompanied by the metric used (e.g., exact match, accuracy, precision@1), zero-shot prompting strategy (to enable better baseline comparison across models), and any alternative prompting strategies tested (e.g., few-shot, chain-of-thought).</p> <p><strong>Safety Evaluation</strong> AI models pose well-documented risks, such as bias <d-cite key="dai2024bias"></d-cite>, toxicity <d-cite key="luong-etal-2024-realistic"></d-cite>, and misinformation generation <d-cite key="zhang2024toward, chen2024combating"></d-cite>. These issues are often under-reported or selectively presented in current evaluation practices <d-cite key="burnell2023rethink, mcgregor2025err"></d-cite>. EvalCards should include a dedicated section for such safety risks. As with capability evaluations, developers should specify the safety feature evaluated, the benchmarks used, the metrics applied, and both the zero-shot and any alternative prompting scores.</p> <p><strong>Developer Footnotes</strong> In this free-text section of the EvalCard, model developers can choose to have relevant footnotes or include any information they think is relevant for users.</p> <h3 id="when-should-evalcards-be-created">When should EvalCards be created?</h3> <p>EvalCards should be generated as part of the initial model release workflow, whether for open-source models or commercial APIs. <em>First,</em> model developers are the ones who trained the model, chose the data, designed the architecture, and tuned the objectives. Standardized evaluation reporting works best when it is done by the people who know the model inside out and at the point of release. Also, most of these evaluations align with internal testing already conducted by developers during model validation phases, and direct reporting can prevent multiple runs of the model on the same tests, leading to reduced climate impact. <em>Second,</em> most models that require evaluation today—especially large foundation models with hundreds of billions of parameters—are built by organizations with substantial compute resources. If a team can train a model with billions of parameters, it is likely to have enough compute to run a standard suite of evaluations.</p> <p>By embedding EvalCard creation into the release pipeline, developers ensure that transparency is delivered upfront, not left to third-party auditors. Furthermore, EvalCards should be updated with each major version change or significant fine-tuning event, reflecting how model behavior may evolve. This keeps users informed of both improvements and potential regressions across the capability and safety dimensions.</p> <h3 id="where-should-evalcards-be-displayed">Where should EvalCards be displayed?</h3> <p>Visibility is a core principle of EvalCards: Evaluation summaries should appear where the model appears. This includes:</p> <ul> <li><strong>Model Repositories</strong>: EvalCards should be a standard, prominently linked file in Huggingface Hub or Github—similar to a README or license—ensuring that anyone downloading or browsing the model can immediately access it.</li> <li><strong>Commercial Platforms</strong>: For closed-source or hosted models accessed through APIs or user interfaces (e.g., ChatGPT, Google Gemini), EvalCards should be integrated into developer dashboards, product documentation, or user-facing pages. This allows users to review evaluation and safety information before deployment or interaction, supporting informed use and regulatory compliance.</li> <li><strong>Research Publications</strong>: For both open and closed-source models, EvalCards should be present as a section in the main text or appendix of academic papers and any technical blogs as a concise summary of evaluation results—providing a clear alternative to selective performance highlights.</li> </ul> <h3 id="model-card-vs-evalcard">Model Card vs EvalCard</h3> <p>Model Cards <d-cite key="mitchell2019model"></d-cite> are an early effort at increasing transparency. However, EvalCards are not designed to serve the same purpose and are complementary to Model Cards. Below, we highlight how EvalCards differ from Model Cards and why they are essential in the current landscape:</p> <ul> <li><strong>Evaluation Focus:</strong> Model Cards were developed at a time when most models were custom-built, and detailed information about training, design motivations, and use cases was essential. EvalCards elevate evaluation to the primary focus and provides a decision-time snapshot to help users and regulators quickly evaluate if a model is appropriate for their needs.</li> <li><strong>Ease of Adoption:</strong> Model Cards focus on open-ended description of model information, including detailed analysis of ethnographic and ethical considerations. However, it can serve as a barrier to adoption by overwhelming end-users with less technical expertise. EvalCards are designed to be lightweight and easy to adopt, with structured fields that capture high-signal evaluation details with minimal overhead.</li> <li><strong>Visibility:</strong> Model Cards do not specify where or how they should be displayed, and the information is often buried across multiple sources. EvalCards are displayed where the model is accessed—on model hubs, APIs, or UIs.</li> </ul> <p>As the norm shifts towards the use of off-the-shelf generative AI models, evaluation becomes the key requirement for responsible model deployment.</p> <h2 id="evalcards-case-studies">EvalCards Case Studies</h2> <p>To implement our idea, we started with case studies of three popular LLMs. We document the issues we found for collecting details for each model’s evaluation followed by the creation of an EvalCard for each model. In Fig 1, we create an EvalCard for the model <em>OLMO-2-1124-7B-Instruct</em>, from the OLMo project on transparency and open research from the Allen Institute for AI <d-cite key="olmo20252olmo2furious"></d-cite>- which despite being one of the most open models. Metrics and prompting details were dispersed across sources and inconsistent across benchmarks. Accessibility was limited by scattered results throughout the paper, hindering quick assessment. Safety reporting was aggregated into a single score, offering no dataset-level transparency and preventing meaningful analysis of specific risks.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-evalcards/evalcard_olmo-480.webp 480w,/2026/assets/img/2026-04-27-evalcards/evalcard_olmo-800.webp 800w,/2026/assets/img/2026-04-27-evalcards/evalcard_olmo-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-evalcards/evalcard_olmo.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In Fig 2, we have an EvalCard for Qwen3-4B-Base, released on the 29th of April 2025. The model’s release lacked timely and transparent evaluation details: key results were delayed, language coverage and capability claims were ambiguous, and many benchmarks lacked metrics or prompting information. Safety evaluations were entirely absent, making it difficult for users to reliably assess the model’s performance or risks.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-evalcards/evalcard_qwen-480.webp 480w,/2026/assets/img/2026-04-27-evalcards/evalcard_qwen-800.webp 800w,/2026/assets/img/2026-04-27-evalcards/evalcard_qwen-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-evalcards/evalcard_qwen.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In Fig 3, we create an EvalCard for Gemini Flash 2.0 which lacks clear, comprehensive documentation: multilingual support is vaguely described without evidence of evaluated performance, benchmark scores omit metrics and prompting details, and no concrete safety evaluations or red-teaming results are disclosed. As a result, users must rely on incomplete information when assessing the model’s actual capabilities and risks.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-evalcards/evalcard_gemini-480.webp 480w,/2026/assets/img/2026-04-27-evalcards/evalcard_gemini-800.webp 800w,/2026/assets/img/2026-04-27-evalcards/evalcard_gemini-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-evalcards/evalcard_gemini.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>These case studies reveal a common thread: while open-source models like OLMo 2 generally provide more information than their closed counterparts, the process of compiling that information remains time-consuming and often incomplete. EvalCards offer a structured way to consolidate key details in one place, reducing the need to spend hours navigating scattered documentation and supplementary sources. More importantly, EvalCards make it immediately visible what’s missing—turning the absence of evaluation details from something easy to hide into something impossible to ignore. For model providers, EvalCards serve as both a checklist and a commitment: a clear reminder of what transparency actually requires and a visible demonstration of their willingness to provide it.</p> <h2 id="alternative-views">Alternative Views</h2> <p>While we advocate for the adoption of EvalCards, it is important to acknowledge reasonable concerns from different stakeholders. Here, we outline two commonly raised perspectives and address them.</p> <p><strong>A Developer’s View</strong> Developers may argue that evaluation is already done internally, and publishing it publicly increases workload or reputational risk, especially when performance is uneven. However, as regulatory frameworks like the EU AI Act and US or UK guidelines begin to demand transparency in model capabilities and risks <d-cite key="act2024eu"></d-cite>, structured evaluation disclosures might become a requirement, not a preference. EvalCards offer a proactive way to meet these expectations. Even seeing what has not been evaluated is useful to avoid misinterpretations and build credibility ahead of external audits or compliance checks <d-cite key="raji2020closing, koshiyama2024towards"></d-cite>.</p> <p><strong>A Researcher’s View</strong> Researchers may note that evaluation is context-sensitive, where evaluations vary by tasks <d-cite key="chang2024survey"></d-cite> or user groups <d-cite key="hershcovich-etal-2022-challenges"></d-cite> and that standards for ``good’’ benchmarks are still evolving <d-cite key="reuel2024betterbench,liu-etal-2024-ecbd, blodgett-etal-2024-human"></d-cite>. Introducing a fixed reporting format might seem premature. But EvalCards are not rigid templates. They do not enforce benchmark choices but merely require clarity about what was tested and how. This transparency helps everyone interpret results accurately, compare across models, and build on prior evaluations rather than duplicating or misapplying them.</p> <h2 id="conclusion">Conclusion</h2> <p>Evaluation reporting is a critical but under-prioritized part of responsible NLP and AI. Non-standardized practices create hurdles for researchers, users, and regulators, fueling reproducibility, accessibility, and governance crises. Existing documentation efforts like Model Cards <d-cite key="mitchell2019model"></d-cite> or BenchmarkCards <d-cite key="sokol2024benchmarkcards"></d-cite> have aimed to improved transparency, but they do not put evaluation at the center. EvalCards aim to close this gap with a format that is easy to write, easy to understand, and hard to miss. By making evaluation details visible and consistent, they turn scattered disclosures into a foundation for cumulative research, informed adoption, and accountability. Looking ahead, we highlight three directions to strengthen and extend the EvalCard framework.</p> <p><strong>Increased Transparency</strong> EvalCards can help establish a unified pipeline that links evaluation and reporting into a single transparent process by reducing ambiguity across sources <d-cite key="biderman2024lessons"></d-cite>. In the longer term, EvalCards could link to Benchmark Cards <d-cite key="sokol2024benchmarkcards"></d-cite> for each mentioned benchmark, creating a connected reporting ecosystem where model, benchmark, and evaluation details are transparently linked to ensure that both the tests and the results behind model claims are easy to trace and verify.</p> <p><strong>Increased Adoption</strong> Widespread adoption will require making EvalCards easy to produce and use. Methods like automated extraction from technical reports <d-cite key="liu-etal-2024-automatic"></d-cite> or community contribution to scores <d-cite key="10855627"></d-cite> can help ease the burden on model developers, while usability testing can help refine design and language for diverse stakeholders <d-cite key="crisan2022interactive"></d-cite>. This will help ensure that EvalCards are not only technically sound but also popular.</p> <p><strong>Regulatory Integration</strong> EvalCards can play a key role in bridging technical evaluation and regulatory transparency. They can serve as a standardized reporting format for models within compliance processes, such as regulatory sandboxes under the EU AI Act <d-cite key="lanamaki2025expect"></d-cite> or US AI Risk Management Framework <d-cite key="ai2023artificial"></d-cite>, by offering a consistent way to report model performance and limitations. Partnerships with platforms like HuggingFace or emerging standards bodies could help maintain vetted benchmark sets that align with evolving priorities. Future work in technical governance <d-cite key="reuel2024open, reuel2024position"></d-cite> can explore how EvalCards can be incorporated into regulatory compliance workflows, e.g., by establishing minimum mandatory fields tied to emerging AI regulations.</p> <p>As model development accelerates, reporting practices must evolve with equal urgency. Evaluation should drive progress, not confusion—but that is only possible when what models can and cannot do is made clear. EvalCards take a small but concrete step toward that goal, embedding transparency into the model release process itself. We hope this work sparks deeper reflection and concrete action toward standardizing how we evaluate and report on models.</p> <hr/>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[In the age of rapidly released LLMs, evaluation reporting is fragmented, inconsistent, and often misleading. We surveyed the landscape and found three critical crises—reproducibility, accessibility, and governance—that Model Cards alone can't solve. Our solution? EvalCards-- lightweight, standardized evaluation summaries that are easy to write, easy to understand, and impossible to miss. EvalCards are designed to enhance transparency for both researchers and practitioners while providing a practical foundation to meet emerging governance requirements.]]></summary></entry><entry><title type="html">The effect of feature resolution on embedding dimension</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/feature-reduction/" rel="alternate" type="text/html" title="The effect of feature resolution on embedding dimension"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/feature-reduction</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/feature-reduction/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>An interesting paper was published in August 2025: “On the Theoretical Limitations of Embedding-Based Retrieval” <d-cite key="weller2025"></d-cite>. In the paper, the authors identify a use case for which embeddings seem to be limited by their dimensionality: identifying individual words in a corpus of documents.</p> <p>A student of algebra might at this point note that you need $n$ independent dimensions to express $n$ independent variables - so if the words in a document are independent, you would need to embed every document with a dimensionality the size of the vocabulary. But the authors of “On the Theoretical Limitations” identified something interesting: space can be used far more efficiently if each of your documents have a certain number of features, or words, which is smaller than the size of the vocabulary. As it turns out, this limit introduces a uniform dependence between words in a document.</p> <p>Which inspired the question: how does that actually affect the number of dimensions you need to represent a set of documents, while preserving the ability to identify individual words?</p> <p>The following post uses that question as a loose cloak to explore the effects of the feature composition of a dataset on how it can be embedded.</p> <h2 id="what-is-the-feature-composition-of-a-dataset">What is the feature composition of a dataset?</h2> <p>A feature of a data point is a property of the data point which can be used to describe it. When the data points in a dataset are embedded, usually their embeddings share a set of features, specifically so that the data point embeddings can be compared. However, in the original dataset, data points might have been represented using vastly different features.</p> <p>For example, a hypotenuse is a property of a right-angled triangle. But in a dataset of shapes, what is the hypotenuse of a circle? The question doesn’t make sense, because the feature does not exist in the “circle” data point.</p> <p>So if we want to talk about features which are common to all data points in a dataset, we need to be able to handle the case where the feature does not exist. Luckily, we can do this by defining how we handle nonexistence of a feature in a data point:</p> <div class="proof-block l-body-outset"> <p style="margin-left: 15px; margin-right: 15px"> <em>Definition</em>: <b>Feature of a dataset</b> <br/> Given a dataset $D = {d_0, \ldots , d_n}$, with $d_i$ one data point in $D$, suppose there exists a function $g$ from $D_g \subseteq D$ to a set of values $v_g$. Then we can define the feature associated with $D$ and $g$, $f$, as a function: $$ f(d_i) = \begin{cases} g(d_i) &amp; \text{ if } d_i \in D_g \\ x &amp; \text{ if } d_i \not \in D_g \end{cases} $$ where $ x \not \in v_g$. </p> </div> <p>In other words, a feature of a dataset is a function on the dataset which maps it to a column with exactly one entry for each row of the dataset, with one entry value specially reserved to indicate nonexistence.</p> <p>Since $f$ is a function, it can only map a data point to one output. This makes sense - for example, if a dataset of creatures has one creature per data point, then “number of eyes of a creature” will always give you one number (even if that number is zero). However, as soon as the dataset has more than one creature for a data point, the “number of eyes of a creature” for a data point is unclear; does a picture of a spider and a horse have eight “eyes of a creature”, or two, or both?</p> <p>But this definition of a feature does not only <em>limit</em>, it also <em>allows</em> - if you want to apply your feature to a new data point, or rather to a new case, you can add to your set of feature values, with the caveat that the value you add cannot be the same as any one used to represent feature nonexistence.</p> <p>Now that we have a definition for a feature with respect to a dataset, we can talk about the feature composition of a dataset.</p> <p>Let’s go back to our question of “how many words can you represent in x dimensions given a corpus of documents?”. We can treat word existence as a feature of the corpus, or dataset, for each word in the dataset. Then each document can be represented by indicating existence or nonexistence for each word in the vocabulary, and we would care about the different combinations of words which occur.</p> <p><strong>Feature composition</strong> is concerned with the unique combinations of feature existences in a dataset. One possible feature composition of a dataset with features <code class="language-plaintext highlighter-rouge">dog</code>, <code class="language-plaintext highlighter-rouge">cat</code> <code class="language-plaintext highlighter-rouge">horse</code>, and <code class="language-plaintext highlighter-rouge">fish</code> looks as follows:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{}
{dog}, {cat}, {horse}, {fish}
{dog, cat}, {cat, horse}, {dog, horse}, {dog, fish}, {cat, fish}, {horse, fish}
{dog, cat, horse}, {cat, horse, fish}, {dog, horse, fish}, {dog, cat, fish}
{dog, horse, cat, fish}
</code></pre></div></div> <p>We can abstract this to general features:</p> \[\{ \}\] <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-feature-reduction/combinations-compact-free.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-feature-reduction/combinations-compact-free.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>If every possible combination of features exists in such a dataset, we shall say that the dataset is <strong>full</strong> with respect to its features. If a dataset is full, and balanced with respect to the combinations, then the existence of any one feature in a document tells us nothing about whether another feature exists in the document. The feature existences are then fully independent - and you would always need $n$ features to embed the data points in your dataset if your dataset contains $n$ unique words.</p> <p>However, in most cases these combinations do not have an even spread in the dataset. To simplify the problem, we differentiate only between combinations which do and do not appear in the dataset. For example, the combination of “green, orange and pink” might never appear in the document, and so the feature composition of the dataset would look as follows:</p> \[\{ \}\] <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-feature-reduction/combinations-minus-one.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-feature-reduction/combinations-minus-one.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In general, $n$ features give rise to $\sum_{i=0}^{n} \binom{n}{i}$ combinations. The binomial theorem says $\sum_{i=0}^{n} \binom{n}{i} = 2^n$, and indeed, there is a natural way to map the existence of features to binary representations:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-feature-reduction/combinations-table-boolean-final.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-feature-reduction/combinations-table-boolean-final.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>With the above binary values, we can order the dataset in at least four unique ways, once for each feature:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-feature-reduction/combinations-ordered-by-features-ld.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-feature-reduction/combinations-ordered-by-features-ld.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We can also create four partitions of the dataset, each based on whether a certain feature exists in a combination:</p> <p>[do four separate partitions first, then all four at the same time]</p> \[\{ \}\] <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-feature-reduction/combinations-partitioned-ld.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-feature-reduction/combinations-partitioned-ld.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Partitions quickly become messy to visualise, though. Expressing all possible combinations by partitions requires drawing a venn diagram, which becomes difficult when we want to consider more than three features.</p> <p>We can also view the structural information of feature composition through the lens of set inclusion, which can be naturally expressed through a <em>partial</em> order. We define a partial order on the combinations: for combinations $x$ and $y$, let $x \leq y$ ifand only if $x \subseteq y$. So if combination $y$ contains every feature in combination $x$, then $x \leq y$. A set $S$ with a partial order $\leq_P$ on it is called a poset (which we will often call $P$). We can represent our poset as a directed graph where edges connect related combinations, pointing from the lesser to the greater element. The graph below shows this structure without indicating direction:</p> <div class="l-page-outset"> <iframe src="/2026/assets/html/2026-04-27-feature-reduction/kinetic-graph-hierarchical%20(6).html" frameborder="0" scrolling="no" height="750px" width="100%"></iframe> </div> <p>This graph describes the specific poset which is a Boolean lattice, or Boolean algebra - specifically $\mathcal{B}_4$. <d-footnote>It is also one of the most-used examples of a lattice since it can be used to relate the elements of the power set of a set, which is a fundamental concept in set theory. Not all of the graphs we will draw are lattices, but this one is.</d-footnote></p> <p>Our original question only requires identifying individual features within data points that contain a specific number of features. We therefore only need to preserve the parts of the partial order which relate the first level to one other level of the combinations. For a poset $P = (S,\leq_P)$ if we select a subset $U$ of $S$ and maintain the relation $\leq_P$ between the elements of the subset, then we call it a <strong>suborder</strong>. Our special poset where we select two layers of the Boolean lattice and preserve the partial order between their elements has a name: for $0 \leq s &lt; t \leq n$, the poset which is a suborder of the Boolean lattice $\mathcal{B}_n$ and selects the layers $s$ and $t$ of $\mathcal{B}_n$ is denoted by $\mathcal{B}_n(s,t)$. So if you toggle off the third and fourth layers of the diagram above, you get $\mathcal{B}_4(1,2)$.</p> <p>We now consider one last formulation of our problem. To do so, we map from our poset to a set of linear orders in the following way:</p> <p>Given a poset $P$ with a partial order $\leq_P$ on a set of combinations $S$, we create a set of linear orders on $S$, $\mathcal{L} = {L_1, \ldots , L_n}$ - in simpler terms, we create a set of rankings of the combinations where no two combinations get the same rank in any one ranking. The set of rankings has the property that for every two combinations $x$ and $y$, $x &lt; y$ in $P$ if and only if in every ranking, $x &lt; y$. The set $\mathcal{L}$ is called a realizer of $P$<d-cite key="dushnik1941"></d-cite>. This property automatically means that if $x$ is not comparable to $y$ in $P$ then $x&gt;y$ in at least one of the rankings.</p> <p>For example, since <code class="language-plaintext highlighter-rouge">{horse,cat}</code> and <code class="language-plaintext highlighter-rouge">{dog,fish}</code> share no features, they cannot be compared by our order, and so we have neither <code class="language-plaintext highlighter-rouge">{horse,cat}</code> $\leq$ <code class="language-plaintext highlighter-rouge">{dog,fish}</code>, nor <code class="language-plaintext highlighter-rouge">{horse,cat}</code> $\geq$ <code class="language-plaintext highlighter-rouge">{dog,fish}</code>. We therefore need to have at least one ranking where <code class="language-plaintext highlighter-rouge">{horse,cat}</code> $&gt;$ <code class="language-plaintext highlighter-rouge">{dog,fish}</code> and at least one ranking where <code class="language-plaintext highlighter-rouge">{horse,cat}</code> $&lt;$ <code class="language-plaintext highlighter-rouge">{dog,fish}</code>. Since you cannot rank an item as both greater and lesser than another item in one linear order, you need at least two rankings to realise this.</p> <p>A realizer for this poset might look like:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-feature-reduction/combinations_two.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-feature-reduction/combinations_two.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Since $a$ is not comparable to $b$, we use the two linear orders $L_1$ and $L_2$ to fulfil the required property that $a&lt;b$ in one order and $a&gt;b$ in one order.</p> <p>This may seem like an unnecessarily complicated way to view our problem, but it links us back to dimensionality: encoding all points from a dataset in $n$ dimensions yields precisely $n$ independent linear orders of the dataset. So if we can express $P$ through $n$ linear orders, then we can encode $P$ in $n$ dimensions. In fact, the minimal size of the set of rankings required to preserve the information in $P$ is called the classical or Dushnik-Miller <strong>dimension</strong> of the poset <d-footnote>There are other types of dimensions of posets, see <d-cite key="barreracruz2020"></d-cite> for a nice comparison.</d-footnote>. We shall simply call it the dimension of the poset, and denote it by $\text{dim}(s,t;n)$.</p> <p>To summarize, the feature composition of a dataset is the set of feature combinations appearing in that dataset, which encodes both the number of unique points and their structural relationships. If we want to identify unique words in a dataset, we will need to represent all the combinations of words that the corpus contains, and we will need to preserve some of the structure of those combinations.</p> <h2 id="dataset-resolution-and-embedding-dimension">Dataset resolution and embedding dimension</h2> <p>Remember our question from the start: when each document in a corpus contains a certain number of features which is smaller than the total number of features in the dataset, how does that affect the number of dimensions we need to express every feature in each document?</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-feature-reduction/feature_subsets_full.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-feature-reduction/feature_subsets_full.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Let’s call the number of features any data point in the dataset may contain <strong>dataset resolution</strong>, and denote it by $k$. Then if a set of $n$ features describes a dataset and the dataset has resolution $k$ with respect to those features, exactly $k$ of them exist in any given data point.</p> <p>We rarely have a perfect set of $n$ features for which any data point contains exactly $k$ of them. However, it is reasonable to assume that for many datasets, the data points contain similar amounts of the features we might care about, since data points are often captured in a similar way and standardised in terms of vector size.</p> <p>Real-world datasets often have an upper limit to their resolution (in other words, $k$ is often smaller than $n$). For example, a dataset of images of animals almost never has every animal type in one image. There are practical reasons for why it would be difficult to get all the animals in one image, especially without eating each other, but more importantly you would need an enormously detailed image to actually capture each animal so that its distinguishing properties are recognisable.</p> <p>For text, a similar principle applies: you would have to write an enormously long piece of text to use every word in existence in the appropriate context, and almost certainly no piece of text that we might care about has every word in it.</p> <p>Dataset resolution is defined with respect to a set of features, by necessity. For example, the pixel resolution of a dataset of images of animals could be 28x28, but the RGB resolution would be 28x28x3, and the animal resolution could be two (if we always have two animals appear in each image).</p> <p>A full four-feature dataset with resolution two has data points which contain two of four features. Suppose we want to be able to identify existence of each feature in a (full) dataset with resolution $k$. Then preserving the following combinations and partial order will ensure that the preserve enough information to identify features:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-feature-reduction/combinations_with_arrows.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-feature-reduction/combinations_with_arrows.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This is $\mathcal{B}_4(1,2)$, which we have previously seen. The feature structure represented in such a dataset will need to use at most $n$ partitions of the dataset, and at least $k$ partitions.</p> <p>We can now begin to answer the question of how many dimensions we would need to encode such a dataset.</p> <h3 id="intuition">Intuition</h3> <p>Perhaps you would expect to be able to reduce features the most when a dataset’s resolution is small. Suppose you only have two features in any data point in your dataset. Let’s consider what happens in the smallest case, where there are three features in your dataset, but every data point has two of the three features.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-feature-reduction/combinations_three_middle.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-feature-reduction/combinations_three_middle.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We can build intuition by examining this problem in a vector space. Suppose you need to embed these three features in two dimensions, such that you can retrieve existence of any single feature with cosine similarity. What this really means is that when you apply cosine similarity between each data point embedding and a feature embedding, then every data point which contains the feature should have a higher similarity than any data point which does not. This means that the order that the cosine similarity imposes on the data points should allow you to define a threshold such that everything above the threshold certainly contains the feature.</p> <div class="l-page-outset"> <iframe src="/2026/assets/html/2026-04-27-feature-reduction/vector_diagram.html" frameborder="0" scrolling="no" height="750px" width="100%"></iframe> </div> <p>Three features can be embedded this way without too much complication. Take a moment to convince yourself that when green and pink are embedded on the axes, and the combinations are defined to be halfway between the base features, that orange needs to be embedded within the grey area.</p> <p>But what is the grey area? It is precisely the area that indicates <em>not</em> green and <em>not</em> pink. We have taken advantage of the fact that orange will never appear at the same time as green and pink, which means we never need to distinguish all the features of a data point which contains green <em>and</em> pink <em>and</em> orange.</p> <div class="l-page-outset"> <iframe src="/2026/assets/html/2026-04-27-feature-reduction/vector_diagram_all_interactive.html" frameborder="0" scrolling="no" height="750px" width="100%"></iframe> </div> <p>There is one more thing to note here: recall when we said that the dependence that dataset resolution creates is uniform? We would then expect that an efficient embedding would treat each feature the same. Indeed, if you space the base features evenly in the figure, that admits a valid embedding (and one could convince onesself that it is the configuration furthest from being an invalid embedding).</p> <p>Let us move to three dimensions. Trivially, you could add one more feature, since an extra dimension is available.</p> <div class="l-page-outset"> <iframe src="/2026/assets/html/2026-04-27-feature-reduction/vector_diagram_3d_interactive_sphere.html" frameborder="0" scrolling="no" height="750px" width="100%"></iframe> </div> <p>In dimensions higher than four, it becomes tricky to approach this problem visually. We already have some intuition, though, around the geometry of the problem: it is likely that if the individual features are embedded evenly with respect to the dimensions in which they are allowed freedom, and the number of features is maximal, that will admit a valid (and optimal with respect to the features) embedding.</p> <p>The intuitions we have gained around uniformity and embedding in “not”-space have interesting connections to contrastive learning:</p> <ul> <li>First, contrastive learning optimises for alignment and uniformity <d-cite key="wang2020"></d-cite>, consistent with our intuition that uniformly arranging the base features of a dataset in a space will admit an optimal solution, if the number of features of a dataset is larger than its resolution.</li> <li>Second, we have seen that it can be efficient to utilise the “not”-space of more than one embedding at a time. Many popular implementations of contrastive learning use InfoNCE-style loss <d-cite key="oord2018"></d-cite> with cosine similarity. This loss pushes positive samples away from multiple negative samples at once. “Away” in terms of cosine similarity means “on the opposite side of the hypersphere”, so the loss pushes the positive into the combined “not”-space of the negative samples in question. Put slightly differently, the loss correlates a positive with the negative of a combination of multiple points which do not relate to it. This is exactly how we embedded extra features into the space above.</li> </ul> <h3 id="intuition-for-dimensions-four-and-above">Intuition for dimensions four and above</h3> <p>In order to express our problem in higher dimensions, we turn to the constructions of order theory - in particular, the dimension of a suborder of a Boolean lattice, as addressed previously.</p> <p>First, let us see what happens when we try to find a two-dimensional realiser of $B_3(1,2)$.</p> <div class="proof-block l-body-outset"> <p style="margin-left: 15px; margin-right: 15px"> Let us call our features $a$, $b$ and $c$. Then our combinations are $a$, $b$, $c$, $ab$, $cb$ and $ac$. We are allowed two linear orders on the combinations, and since every combination needs to be incomparable to exactly three other combinations, no single combination may be in the bottom three in both orders. Features $a$, $b$ and $c$ need by definition to be below at least two combinations in both orders, so their highest possible ranking is third. However, since there are only two linear orders, there are only two spots that are below third but above the bottom three. Therefore, one of $a$, $b$ and $c$ will need to be in the bottom three for both orders. <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-feature-reduction/linear_order_elements.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-feature-reduction/linear_order_elements.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> We have reached a contradiction. It is therefore not possible to have a two-dimensional realiser for layers 1 and 2 of the Boolean lattice $\mathcal{B}_3$. </p> </div> <p>Now that is disappointing - we found an embedding which used only two dimensions to represent two-combinations of three features, so we might have expected that the dimension of $\mathcal{B}_3(1,2)$ would have been three.</p> <p>The poset $\mathcal{B}_n(s,t)$ contains more information than our other problem formulations, and more information than we care to preserve. For example, the order information in $\mathcal{B}_n(s,t)$ is directional, whereas a feature existence does not need a direction. This means that the dimension of $\mathcal{B}_n(s,t)$ provides an upper bound for our problem, but it is not tight even in the smallest case.</p> <p>So what now? Well, since our nicely defined link to dimension did not work, let us look at how our sucessful embedding used the two axes available to order the 2-combinations of three features:</p> <div class="l-inline-block"> <iframe src="/2026/assets/html/2026-04-27-feature-reduction/vector_diagram_labeled.html" frameborder="0" scrolling="no" height="750px" width="100%"></iframe> </div> \[\text{Ordered by x: } \begin{bmatrix} a \\ ab \\ ac \\ b \\ c \\ bc \end{bmatrix} \qquad \text{Ordered by y: } \begin{bmatrix} b\\ ab \\ bc\\ a \\ c \\ ca \end{bmatrix}\] <p>So the features on the axes, $a$ and $b$, were ordered such that every combination which contains them appeared first when ordering according to the axis on which they are embedded. This is not surprising.</p> <p>The orders naturally rank the combinations, which can give the following embedding into $x$ and $y$:</p> \[\left[ \begin{array}{c | cc} &amp; x &amp; y \\ \hline a &amp; 6 &amp; 3 \\ ab &amp; 5 &amp; 5\\ ac &amp; 4 &amp; 1 \\ b &amp; 3 &amp; 6\\ c &amp; 2 &amp; 2 \\ bc &amp; 1 &amp; 4 \end{array} \right]\] <p>Note the actual values of the rankings do not matter so much as the order which they preserve.</p> <p>Let us retrieve each of the features using a similarity function $s$. If we do suppose the values are what is indicated above, we could retrieve “$a$ is in this item” by applying to each embedding the condition $x&gt;3$. Alternatively, we could multiply the rows are follows:</p> <p>$s = \frac{1}{4}x+0y$</p> <p>and say that $a$ is in the combination if $s\geq 1$. We could do the same for $b$, with $x$ and $y$ swapped in $s$.</p> <p>Observe carefully how $c$ was ordered. We cannot isolate $c$ in any one order as we could with $a$ and $b$. However, we can use our earlier retrieval method to recover the items containing $c$:</p> \[s = -x-y: \qquad \left[ \begin{array}{c | cc | c} &amp; x &amp; y &amp; s\\ \hline a &amp; 6 &amp; 3 &amp; -9 \\ a,b &amp; 5 &amp; 5 &amp; -10\\ a,c &amp; 4 &amp; 1 &amp; -5 \\ b &amp; 3 &amp; 6 &amp; -9\\ c &amp; 2 &amp; 2 &amp; -4 \\ b,c &amp; 1 &amp; 4 &amp; -5 \end{array} \right]\] <p>Then we can say that a combination contains $c$ if $s&gt;-9$. Although our threshold is slightly different, we are still able to recover all the items in $c$. It is important to note that we had to use negative coefficients to do so if we wanted a high similarity to mean the combination contains $c$, as opposed to a low value; remember we embedded $c$ in the “not”-space of the other two features, so it makes sense that we had to flip the orders.</p> <p>Let us now consider again the values for $x$ and $y$ which our embedding gave.</p> \[\left[ \begin{array}{c|cc} \text{} &amp; x &amp; y \\ \hline a &amp; 1 &amp; 0 \\ b &amp; 0 &amp; 1 \\ c &amp; -\frac{1}{\sqrt{2}} &amp; -\frac{1}{\sqrt{2}} \\ a,b &amp; \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\ a,c &amp; 0.38 &amp; -0.92 \\ b,c &amp; -0.92 &amp; 0.38 \\ \end{array} \right]\] <p>Recall we used cosine similarity as our similarity function in the previous section. Where one example ago we <em>chose</em> coefficients for each dimension, now with cosine similarity our coefficients are chosen to be <em>the values in the order themselves</em>. With cosine similarity, we also normalise the result. However, since we embed on the unit circle, this scales the similarity by the same amount for each vector and does not affect the final order.</p> <p>So the coefficients of the linear operations which allow us to retrieve $a$, $b$ and $c$ are the $x$ and $y$ values for $a$, $b$ and $c$ themselves! Note that while these values do allow us to retrieve $a$, $b$ and $c$, there are many other coefficients which could have done the job.</p> <p>However, another nice property that we get from using cosine similarity in this case, is that a similarity of more than zero indicates that the feature exists. It is convenient to have a common threshold for all features, and for the threshold to relate to sign.</p> <p>Let us go back to considering how $c$ is ranked. What allowed us to isolate it?</p> <p>Notice that the combinations of $c$ are grouped as closely as possible to the lower end of the rankings, without making $a$ and $b$ irretrievable. In order to make up for the high position of $a,c$ in $x$, we have to ensure that the value of $a,c$ in $x$ can be cancelled out by the value of $a,c$ in $y$. In fact, $a,c$ in $y$ ranks the lowest, and so we are able on average to bring it below $b$, which ranks third and sixth.</p> <div class="l-inline"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-feature-reduction/ordered_elements_highlight_cb.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-feature-reduction/ordered_elements_highlight_cb.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>We could not have done this if there was any combination containing $c$ which was always above $b$. And so if we want to (retrievably-via-cosine-similarity) embed a third feature in the lower ranks of a set of linear orders, then there cannot be a combination not containing that feature which always ranks below a specific combination containing that feature. No amount of scaling can change that order.</p> <p>This view of our problem is far more similar to the partitions we showed in the second section, but it still requires that we consider order. Specifically, we are finding a set of orders on the dataset for which we can recover each of the feature partitions by an order-respecting partition of the values given by a linear operation on the orders.</p> <p>That’s quite a mouthful. For now, we restrict ourselves to asking: can we embed three features in every two dimensions of an embedding, in the way we just did? In other words, if I have six dimensions, can I embed one feature in the “not”-space of each pair of independent dimensions, allowing me to embed nine features into six?</p> <p>Maybe you are convinced we can, since the added features are orthogonal to all features in the pre-existing dimensions. We cna prove this by induction:</p> <div class="proof-block l-body-outset"> <p style="margin-left: 15px; margin-right: 15px"> We already know that in the case that we have two dimensions, we can embed three features in the following way: $$ \left[ \begin{array}{c|cc} \text{} &amp; x &amp; y \\ \hline a &amp; 1 &amp; 0 \\ b &amp; 0 &amp; 1 \\ c &amp; -\frac{1}{\sqrt{2}} &amp; -\frac{1}{\sqrt{2}} \\ a,b &amp; \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\ a,c &amp; 0.38 &amp; -0.92 \\ b,c &amp; -0.92 &amp; 0.38 \\ \end{array} \right] $$ Then we query for a vector using cosine similarity, which is just the normalised dot product of the query vector and embedding vector. Note that the query for $a$ and $b$ each select one column only. Without loss of generality, since all the strictly positive values in the first column contain $a$, we can identify $a$ as anything which has a positive cosine similarity with $a$. To select $c$, since the two columns selected by $c$'s query vector are evenly weighted, we can compare directly the values in each column. The cosine similarity between $c$ and any document containing $c$ will be positive, and every other document will have a zero or negative cosine similarity to the embedding of $c$ (you can check that - we only require that the sum of the two columns selected by $c$ is negative). We can therefore isolate $c$. <br/> <br/> Now suppose that you have a set of $2m$ dimensions, consisting of triples of features embedded in pairs of dimensions in this way. We add two more dimensions, $d_1$ and $d_2$ containing three features $a$, $b$, and $c$, where the features and the combinations they produce are embedded as before. Firstly, note that the combination between $a$, $b$ or $c$ and any feature outside of those three will only result in a scaling of the values, since dimensions $d_1$ and $d_2$ are orthogonal in features $a$, $b$ and $c$ to all other dimensions. <br/> <br/> Now we can use the fact that any query for one feature will select at most two dimensions, and the rows in those dimensions will be scaled copies of the rows in the matrix above. <br/> <br/> Without loss of generality, let us consider a query for $a$. Then one column is selected. We want to ensure that every combination which contains $a$ and $d$ such that $d$ is not the $b$ and $c$ associated with $a$, will result in a positive value in the dimension selected by query $a$. Since any such feature will have a zero value in column $a$, the value in column $a$ will only be scaled, and will therefore be positive. The only other feature which could affect a value in column $a$ is $c$, but $c$ is negative, and will only be scaled in $a$ by any feature other than $a$, and will therefore never produce a negative combination. So we can still isolate $a$. <br/> <br/> Without loss of generality, let us consider a query for $c$. Then two columns are selected, with an associated $a$ and $b$ column. Suppose this $c$ is combined with another feature $d \not = a$ or $b$. Then, again, the values in $c$ will only be scaled, and as before they will be scaled proportionally. So the sum of the two columns containing $c$ will be negative if and only if $c$ is in the combination. We can therefore say that a combination contains $c$ if and only if it has a positive cosine similarity to $c$, and so we can isolate $c$. <br/> <br/> So all of the combinations added when we add features embedded in the target way preserve the fact that a combination has a positive cosine similarity with a feature if and only if it contains that feature. We have therefore shown that it is possible to embed $3m$ features in $2m$ dimensions such that existence of each feature can be identified in any 2-combination of features. </p> </div> <p>Now that we know this, what does it mean? Well, it means we can upper bound the number of dimensions needed to express the existence of two features in a data point given that there are $n$ features in the dataset, and we have a recipe for constructing such an embedding.</p> <p>In particular, if there are $n$ words and each document contains exactly $2$ of them, we never need more than $\frac{2n}{3}$ dimensions to embed every document losslessly.</p> <p>Two questions arise from this:</p> <ol> <li>What can we say when the resolution is higher than two?</li> <li>Can we improve on this upper bound?</li> </ol> <p>For the first question, we conjecture that for our method of construction, you need to embed a dependent feature in the “not”-space of at least $k$ independent dimensions if your dataset resolution is $k$. We leave it to future work to confirm or deny the conjecture.</p> <p>The answer to the second question, to the best of our knowledge, is maybe.</p> <h2 id="conclusion">Conclusion</h2> <p>We have picked, from a continuous, roiling sea of data points, a discrete model which provides limits for the dimensionality needed to express a set of data points. While the cases we address using this model occur often in real-world datasets and theoretically limit dimensionality, we often expect exploitable dependencies in the continuous aspects of data that allow further dimensionality reduction (such as through dataset balance and continuous feature value correlations). We may therefore never really hit the bounds applied by the discrete properties of the dataset. It just so happens that identifying words in text is such a discrete and low-resolution goal that the limits the requirements pose actually reach the range of embedding dimensions that are used in practice.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[High-dimensional data can be compressed into lower-dimensional embeddings while retaining a relatively large amount of relevant information, a phenomenon which, despite its widespread use, we struggle to fully explain. In this post, we use a common property of datasets - a limit on the number of features per data point - to show how a slight uniform dependence between features can be exploited to reduce the required dimensions by at least a third, while sacrificing no information about the features. To do so, we introduce the concepts of dataset resolution and feature composition of a dataset, and analyse how a set of orderings of the dataset affect the types of partitions we can create of the dataset.]]></summary></entry></feed>