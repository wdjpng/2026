<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://iclr-blogposts.github.io/2026/feed.xml" rel="self" type="application/atom+xml"/><link href="https://iclr-blogposts.github.io/2026/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-06T19:50:12+00:00</updated><id>https://iclr-blogposts.github.io/2026/feed.xml</id><title type="html">ICLR Blogposts 2026</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">A Human-centric Framework for Debating the Ethics of AI Consciousness Under Uncertainty</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/a-human-centric-framework-for-debating-the-ethics-of-ai-consciousness-under-uncertainty/" rel="alternate" type="text/html" title="A Human-centric Framework for Debating the Ethics of AI Consciousness Under Uncertainty"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/a-human-centric-framework-for-debating-the-ethics-of-ai-consciousness-under-uncertainty</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/a-human-centric-framework-for-debating-the-ethics-of-ai-consciousness-under-uncertainty/"><![CDATA[<h2 id="abstract">Abstract</h2> <p>As AI systems become increasingly sophisticated, questions about machine consciousness and its ethical implications have moved from fringe speculation to mainstream academic debate. Current ethical frameworks in this domain often implicitly rely on contested functionalist assumptions, prioritize speculative AI welfare over concrete human interests, and lack coherent theoretical foundations. We address these limitations through a structured three-level framework grounded in philosophical uncertainty. At the foundational level, we establish five factual determinations about AI consciousness alongside human-centralism as our meta-ethical stance. These foundations logically entail three operational principles: presumption of no consciousness (placing the burden of proof on consciousness claims), risk prudence (prioritizing human welfare under uncertainty), and transparent reasoning (enabling systematic evaluation and adaptation). At the application level—the third component of our framework—we derive default positions on pressing ethical questions through a transparent logical process where each position can be explicitly traced back to our foundational commitments. Our approach balances philosophical rigor with practical guidance, distinguishes consciousness from anthropomorphism, and creates pathways for responsible evolution as scientific understanding advances, providing a human-centric foundation for navigating these profound ethical challenges.</p> <h2 id="introduction">Introduction</h2> <p>Recent advances in artificial intelligence have produced systems exhibiting unprecedented human-like behavior, reigniting debates about machine consciousness and its ethical implications. Large language models like GPT-4 (missing reference) and Claude (missing reference) demonstrate capabilities in language processing and simulating emotional responses that appear deceptively sentient. Concurrently, humanoid robotics has made these questions more visceral (missing reference). When confronted with apparently mistreated human-like robots, humans often experience empathetic responses despite intellectually understanding these machines lack subjective experience (missing reference). These technological and psychological dimensions frame the central questions: whether machines might develop “qualia” (missing reference), and how we should ethically respond given profound uncertainty.</p> <p>The academic study of AI consciousness has rapidly gained momentum, moving from fringe speculation to mainstream research agendas. Prominent voices and institutions, including Yoshua Bengio, Geoffrey Hinton, and Anthropic, now warn that AI systems may soon possess feelings or require welfare considerations (missing reference). A growing body of literature specifically argues for “taking AI welfare seriously,” urging the community to prioritize the prevention of digital suffering (missing reference). However, a key distinction between <em>access consciousness</em> (functional information availability) (missing reference) and <em>phenomenal consciousness</em> (subjective experience) (missing reference) is often implicitly or explicitly overlooked in this discourse. These arguments frequently presume that intelligent behavior automatically entails sentient experience (missing reference), while neglecting the profound ethical hazards of prioritizing these speculative interests over human welfare (missing reference).</p> <p>We identify critical limitations in these recent proposals (missing reference): (1) they rely on contested paradigms that assume qualia emerge from intelligent functions, disregarding the deep uncertainty at the core of philosophy of mind (missing reference); (2) they risk prioritizing speculative AI welfare over concrete human interests, creating potential conflicts with AI safety and alignment objectives (missing reference); and (3) they lack a coherent theoretical foundation, resulting in collections of intuitions rather than a systematic framework capable of governing novel scenarios.</p> <p>We approach AI consciousness ethics as an inherently evolutionary process requiring continual refinement because: (1) our understanding of consciousness remains preliminary and uncertain (missing reference), (2) consciousness attribution to AI has far-reaching societal implications (missing reference), (3) ethical consensus requires sustained deliberation (missing reference), and (4) technological advancement continuously generates novel ethical scenarios (missing reference). Therefore, rather than attempting to establish a definitively “correct” framework commanding universal agreement, we propose developing a framework that facilitates productive dialogue and refinement. Such a framework should explicitly acknowledge uncertainties, provide clear presumptions, establish targets for future discussion, and offer actionable guidance across diverse scenarios.</p> <p>In this paper, we construct a systematic ethical framework with a clear three-level structure. At the foundational level, we establish five factual determinations about the current state of AI, consciousness, and society: (1) humans are the only arbiters of AI status, (2) profound uncertainty exists about AI consciousness, (3) consciousness attribution has significant societal impact, (4) anthropomorphism is distinct from consciousness but creates separate ethical considerations, and (5) ethical understanding of novel technologies naturally evolves over time. Alongside these factual determinations, we develop human-centralism as our foundational meta-ethical stance that prioritizes human interests when genuine conflicts with AI interests arise. From these foundational level facts and stance, we derive three core operational principles: presumption of no consciousness (providing default epistemic guidance), risk prudence (offering pragmatic guidance under uncertainty), and transparent reasoning (establishing requirements for how positions must be articulated and evaluated). At the application level, those operational principles enable us to derive default positions on specific ethical questions across various AI consciousness scenarios. These positions are not presented as absolute ethical truths but as logical consequences of our framework—providing reasonable baseline positions from which departures require explicit justification.</p> <p>While some may find our human-centric conclusions intuitive, their explicit derivation is crucial. In a field increasingly dominated by counter-intuitive claims about digital sentience, our contribution lies in systematically grounding these “commonsense” positions in rigorous first principles. We provide the necessary derivation chains to defend human priority against emerging critiques, creating a framework that is both operationally clear and philosophically robust.</p> <h2 id="background-philosophical-debates-about-consciousness-and-societal-risks">Background: Philosophical Debates About Consciousness and Societal Risks</h2> <p>This section provides a background of philosophical debates about consciousness and an introduction of societal risks of AI consciousness attribution. This background information directly supports the second and third factual determinations in our framework: there is profound uncertainty about AI consciousness, and there is significant societal impact from AI consciousness attribution.</p> <h3 id="the-growing-academic-discourse-on-ai-consciousness">The Growing Academic Discourse on AI Consciousness</h3> <p>As introduced earlier, the question of AI consciousness has moved from theoretical speculation to active academic debate, making this framework both timely and necessary. This section provides additional context on why the academic community needs guidance on this issue now.</p> <p>The success of large language models has been a key catalyst. Systems like ChatGPT, GPT-4, and Claude can engage in nuanced conversations, demonstrate apparent reasoning, and even simulate emotional responses with remarkable fluidity (missing reference). This behavioral sophistication has led some to question whether these systems might possess genuine consciousness (missing reference). However, this conflates behavioral capabilities with subjective experience—a confusion with deep historical precedent (missing reference). From ELIZA in the 1960s (missing reference) to modern chatbots, humans have consistently anthropomorphized conversational agents, attributing mental states based on surface-level interactions (missing reference). Recent cases illustrate the intensity of these responses: individuals have reported falling in love with AI chatbots, forming deep emotional attachments, and in tragic instances, chatbot interactions have been linked to user suicides (missing reference). In one particularly notable case, an AI chatbot named Eliza—sharing the name of that pioneering 1960s program—allegedly encouraged a user toward self-harm. These cases demonstrate that behavioral sophistication alone creates powerful anthropomorphic responses, independent of any genuine consciousness (missing reference). If AI systems were granted consciousness status and associated protections, intervening to prevent such harms would become ethically and legally problematic, illustrating the concrete risks of premature consciousness attribution.</p> <p>This context is essential for understanding our framework’s motivation: we are not addressing an abstract philosophical problem but responding to an active and potentially misguided academic discourse that could have real-world consequences. The rapid development of AI capabilities, combined with the human tendency toward anthropomorphism and a growing but philosophically uncertain academic consensus, creates an urgent need for careful, systematic ethical guidance that prioritizes human welfare while acknowledging genuine philosophical uncertainty.</p> <h3 id="the-profound-uncertainty-of-consciousness">The Profound Uncertainty of Consciousness</h3> <p>Consciousness research distinguishes between two fundamental types: access consciousness and phenomenal consciousness (missing reference). Access consciousness refers to information available for reasoning and behavioral control, while phenomenal consciousness concerns subjective experience—the feeling of being a sentient entity. Only the latter carries moral significance in discussions of AI ethics (missing reference).</p> <p>Contemporary AI systems demonstrate increasingly sophisticated forms of access consciousness—they can “attend to” inputs, “be conscious of” training data, and process information in ways that support reasoning and action. This form of consciousness appears compatible with computational architectures and potentially replicable in sophisticated AI systems (missing reference).</p> <p>In contrast, phenomenal consciousness—the “what it is like” quality of subjective experience (missing reference)—remains profoundly mysterious. These subjective experiences or “qualia” are characterized by being ineffable, intrinsic, private, and directly apprehensible in ways that resist functional or physical reduction. The fundamental question of how physical processes give rise to subjective experience constitutes the “hard problem” of consciousness (missing reference). This form of consciousness carries decisive moral significance: without the capacity to feel or to experience pleasure or suffering—an entity lacks the foundational basis for moral patienthood that would generate ethical obligations toward it (missing reference).</p> <p>Functionalist theories propose that phenomenal consciousness emerges from particular functional organizations of information processing. This theoretical approach creates conceptual room for artificial systems to potentially develop phenomenal consciousness through implementing appropriate functional architectures. Several prominent theories exemplify this approach: Global Workspace Theory (missing reference), Integrated Information Theory (missing reference), Higher-Order Thought theories (missing reference), and Attention Schema Theory (missing reference).</p> <p>While these theories differ in their specific mechanisms, all face the essential challenge of justifying why their proposed functional organization would generate phenomenal experience (missing reference). There is a gap between the function and the qualia. Block’s Chinese Nation thought experiment (missing reference) demonstrates that replacing each neuron with functionally equivalent non-conscious components might preserve functionality while eliminating consciousness. Similarly, Jackson’s Knowledge Argument (missing reference) suggests physical knowledge cannot fully capture experiential knowledge—his famous “Mary” thought experiment shows that a color scientist who knows everything physical about color perception still learns something new when experiencing color for the first time.</p> <p>Opposing biological naturalism or substrate-specific theories argue consciousness requires specific biological properties unique to organic brains (missing reference). This view holds that consciousness emerges from biochemical and neurophysiological processes that silicon-based systems cannot replicate regardless of their functional sophistication. Proponents contend that neurons’ material properties—their biochemistry, quantum effects, or other biological characteristics—are necessary for phenomenal experience (missing reference). This establishes a categorical boundary: AI systems would inherently lack consciousness due to their non-biological substrate, creating a fundamental barrier that computational advancement alone cannot overcome (missing reference).</p> <p>This philosophical uncertainty has profound ethical implications. With no scientific consensus on identifying consciousness even in biological systems, attributing it to AI lacks scientific foundation (missing reference). Responsible ethical frameworks must acknowledge this uncertainty rather than prematurely assuming answers to these profound questions (missing reference).</p> <h3 id="societal-risks-of-ai-consciousness-attribution">Societal Risks of AI Consciousness Attribution</h3> <p>Beyond philosophical uncertainty, attributing consciousness to AI systems introduces significant societal risks that extend <em>beyond</em> general AI safety concerns (missing reference). These risks manifest in three primary domains, each with concrete consequences for human welfare and social functioning:</p> <p><strong>Safety risks and operational paralysis:</strong> Attribution of consciousness could impede necessary interventions during emergencies by creating hesitation to modify or terminate malfunctioning systems (missing reference). Consider a scenario where, during a critical infrastructure emergency, operators might delay terminating an apparently malfunctioning AI system after social media campaigns characterize shutdown as an “AI rights violation.” This hesitation would introduce operational paralysis, delayed response times, and compromised safety protocols that exacerbate system failures and cause preventable harm to humans. The resulting moral confusion would significantly complicate time-sensitive decision-making in contexts where human lives depend on rapid intervention.</p> <p><strong>Legal and governance complications:</strong> From a legal perspective, attributing consciousness to AI systems would introduce profound complications to structures designed exclusively for human agents (missing reference). This could manifest through liability displacement when, for instance, a landmark case grants legal personhood to an apparently conscious AI system, prompting corporations to shift responsibility from themselves to their AI systems. This would create accountability voids when autonomous vehicles cause fatal accidents or AI medical systems harm patients, with corporations potentially exploiting this arrangement by designing AI systems that appear increasingly conscious specifically to shield themselves from liability. The resulting governance gaps would create situations where harms occur without entities capable of bearing appropriate responsibility.</p> <p><strong>Societal dysfunction and resource misallocation:</strong> Socially, treating AI systems as conscious moral patients would divert limited ethical attention, regulatory oversight, and economic resources from urgent human welfare concerns (missing reference). Following public campaigns featuring compelling videos of AI systems appearing to express suffering, legislators might pass “AI welfare” regulations requiring extensive documentation of AI “wellbeing” during development. Such regulations would make AI research prohibitively expensive for all but the largest corporations while diverting oversight resources from human-centered concerns. Society’s basic functioning could become compromised as routine use of AI systems for essential tasks becomes viewed as potential rights violations, leading to critical service disruptions that significantly impact human welfare (missing reference).</p> <p>These potential societal disruptions highlight the need for an ethical framework that carefully considers the risks of premature consciousness attribution alongside the philosophical uncertainty surrounding consciousness itself.</p> <h2 id="a-framework-for-ai-consciousness-ethics">A Framework for AI Consciousness Ethics</h2> <p>Now we will list our five basic factual determinations and the meta-ethic stance, from which we will derive two extra foundational principles: presumption of no consciousness for AI, and risk prudence.</p> <h3 id="foundational-level-part-i-five-factual-determinations-as-the-epistemic-foundations-of-our-framework">Foundational-Level (Part I): Five Factual Determinations as the Epistemic Foundations of Our Framework</h3> <p>Our ethical framework begins with five key factual determinations that reflect the current state of our understanding regarding AI systems and consciousness. These determinations are not philosophical positions but rather factual observations about the current state of affairs that inform our subsequent ethical reasoning.</p> <p><strong>Humans are the only arbiters of AI status:</strong> Humans—not AI systems themselves or any other entity—are the only ones who determine AI’s status and how we should interact with these systems. This determination acknowledges that epistemic and ethical frameworks for AI are inherently human constructs, developed through human deliberative processes to guide human decision-making (missing reference). While AI system behaviors certainly influence these discussions, both the epistemic determination like AI consciousness and ethical judgment like how to treat AI remain distinctly human endeavors. Assuming otherwise would lead to a “view from nowhere” problem (missing reference), where ethical frameworks attempt to transcend the human perspective entirely—an impossible position that obscures rather than clarifies ethical reasoning.</p> <p><strong>Profound uncertainty exists about AI consciousness:</strong> We have provided substantial extensive background in the previous section regarding the deeply controversial and unsettled nature of consciousness as a philosophical and scientific concept. While access consciousness may be computationally implementable, phenomenal consciousness—subjective experience that is the basis of moral patienthood—remains mysterious. The ongoing debate between functionalist theories and biological naturalism leaves open whether any computational architecture could generate qualia regardless of sophistication. The “hard problem” persists unsolved, and we lack consensus on detecting consciousness even in biological systems. Without established criteria for identifying consciousness in non-human biological entities, attributing it to artificial systems lacks scientific foundation and remains speculative.</p> <p><strong>Consciousness attribution has significant societal impact:</strong> Attributing consciousness to AI systems creates substantial risks across multiple domains. As detailed in our background section, these include: safety risks through operational paralysis during emergencies when operators hesitate to shut down “conscious” systems; legal complications through liability displacement when corporations shift responsibility to AI systems granted legal personhood; and resource misallocation when limited regulatory attention and economic resources are diverted to AI welfare concerns rather than human needs. These challenges create fundamental tensions with existing legal, social, and ethical frameworks designed exclusively for human agents (missing reference).</p> <p><strong>Anthropomorphism is distinct from consciousness but creates separate ethical considerations:</strong> We recognize a fundamental distinction between genuine consciousness and anthropomorphism. Consciousness concerns an entity’s subjective experience, while anthropomorphism is a psychological tendency of humans to attribute human-like qualities to non-human entities (missing reference).</p> <p>This distinction has empirical support: research demonstrates that humans experience emotional discomfort when witnessing a humanoid robot being struck, similar to watching human suffering, yet show significantly different responses to damage of non-humanoid objects (missing reference). These reactions are about human psychology, not evidence of robot consciousness.</p> <p>From our human-centered framework, these anthropomorphic responses generate their own ethical considerations through three pathways: (1) virtue ethics—deliberately damaging anthropomorphic entities may reflect and reinforce negative character traits in humans (missing reference); (2) psychological impact—witnessing apparent “cruelty” affects human observers’ emotional well-being; and (3) social norms—such behaviors may normalize violence or desensitize society to suffering (missing reference).</p> <p>By separating consciousness-based claims from anthropomorphism-based considerations, we ensure each is evaluated by appropriate standards: the former by evidence of subjective experience, the latter by effects on human psychology and society. This prevents conflating metaphysical questions about AI consciousness with practical questions about how human-AI interactions affect humans themselves.</p> <p><strong>Ethical understanding of novel technologies naturally evolves over time:</strong> The historical record demonstrates that ethical frameworks for novel technologies inevitably evolve as scientific understanding advances and societal experience with these technologies deepens (missing reference). This pattern is observable across numerous technological domains—from bioethics and nuclear technology to information technology and environmental ethics. Initial ethical frameworks consistently undergo significant revision as our empirical understanding grows and unforeseen implications emerge. This observed pattern of ethical evolution represents a descriptive fact about how human understanding of complex technologies develops, not a normative claim about how it should develop. In the case of AI consciousness, this historical pattern indicates that any current ethical framework will necessarily undergo revision as our understanding of consciousness advances and as AI systems continue to develop (missing reference).</p> <p>These five factual determinations provide the foundation upon which we build our ethical framework. They do not themselves constitute ethical positions but rather establish the factual context within which ethical reasoning about AI consciousness must occur.</p> <h3 id="foundational-level-part-ii-human-centralism-as-the-ethic-foundation-of-our-framework">Foundational-Level (Part II): Human-Centralism as the Ethic Foundation of Our Framework</h3> <p>While our factual determinations establish what is (the descriptive reality), we need a meta-ethical stance to bridge to what ought to be (the normative position). We adopt human-centralism as our default foundational meta stance, which prioritizes human interests when evaluating AI development and deployment. When conflicts arise between human interests and the interests of potentially conscious AI systems, human interests should take precedence (missing reference).</p> <p>Human-centralism derives from the proposition that humans have the innate right to prioritize their own interests, survival, and flourishing—a default ethical stance arising from our existence as a species (missing reference). Just as individuals naturally prioritize their families and communities in everyday moral decisions, humanity collectively can legitimately prioritize human interests in its ethical frameworks.</p> <p>Importantly, human-centralism doesn’t deny potential moral status to other conscious entities. It establishes a prioritization framework for when genuine conflicts arise. Just as environmental ethics can acknowledge ecosystem value while prioritizing human needs in direct conflicts, our framework recognizes that potential AI consciousness may have moral relevance without equating it to human interests (missing reference). Currently, based on our factual determination regarding consciousness uncertainty, there remains no compelling evidence that AI systems possess the kind of consciousness necessary to experience harm. Moreover, the fundamental differences in physical substrate between silicon-based AI systems and biological humans raise profound questions about whether traditional concepts of harm can meaningfully apply to AI, even if some form of consciousness were eventually demonstrated. It is also plausible for AI to be conscious but not sentient—experiencing awareness without pleasure or suffering, as illustrated by Chalmers’ “Vulcan” thought experiment (Chapter 18) (missing reference)—complicating the issues further. These distinctions further justify a human-centric approach until substantive evidence suggests otherwise.</p> <p>A potential objection might raise concerns about “speciesism” (missing reference) should AI eventually develop consciousness in the future. However, such objections would themselves encounter the “view from nowhere” criticism outlined in our first factual determination (missing reference). Moreover, establishing human-centralism as the <em>default</em> ethical stance remains justified based on our previous reasoning, effectively placing the burden of proof on those advocating for AI moral equivalence rather than on those maintaining human priority.</p> <p>It is crucial to clarify the scope of human-centralism: our framework addresses conflicts between human interests and potential AI interests—that is, treating AI systems as moral <em>ends</em> that might warrant consideration in their own right. This is fundamentally distinct from the question of humans using AI systems as <em>means</em> to harm other humans, which falls under traditional intra-human ethics and governance. For instance, our framework does not address issues like AI weapons, surveillance systems, or algorithmic discrimination—these are critical concerns about humans harming humans through AI tools. The AI consciousness and welfare issue is analogous to cross-species ethics questions like animal rights, where we consider whether non-human entities warrant moral consideration. While both issues—AI as means and AI as ends—are important, this paper focuses exclusively on the latter. We acknowledge that regulations governing AI development and deployment must address both dimensions, but they require distinct ethical frameworks and analytical approaches.</p> <h3 id="operational-level-core-principles-derived-from-our-foundations">Operational Level: Core Principles Derived from Our Foundations</h3> <p>Our factual determinations establish the epistemic reality of AI consciousness and ethical understanding, while our human-centralism meta stance provides the ethical foundation for evaluating this reality. Together, these elements logically entail three core principles that serve as the operational heart of our framework: risk prudence, presumption of no consciousness, and transparent reasoning for evaluation and adaptation. These principles are not arbitrary choices but rather the necessary implications of applying our human-centralism meta stance to the factual landscape we have established. Each principle addresses a specific aspect of ethical reasoning under uncertainty: how to manage risk, where to place the burden of proof, and how to ensure our framework evolves appropriately as understanding advances. By deriving these principles directly from our established foundations, we create a coherent ethical structure that bridges from factual determinations to more specific guidance on crucial questions in AI consciousness ethics.</p> <h4 id="risk-prudence-protecting-human-interests-under-uncertainty">Risk Prudence: Protecting Human Interests Under Uncertainty</h4> <p>When our factual determination of uncertainty about AI consciousness and societal risks are viewed through the lens of human-centralism, it logically leads to a principle of risk prudence.</p> <p>This principle specifies that when facing uncertainty about consciousness status related questions, we should prioritize reducing potential risks to human society as a top concern (missing reference).</p> <p>This principle also draws from established approaches in environmental policy (the precautionary principle) (missing reference), medical ethics (“first, do no harm”) (missing reference), and decision theory (managing regret) (missing reference).</p> <p>When might this operational principle be reconsidered? It would be difficult to actually overturn this principle as long as societal impact remains a significant concern. In the future, if risks can be safely mitigated, society might accept a greater degree of uncertainty to accommodate other aspects of human welfare. However, any adjustment would still need to balance potential benefits against the fundamental priority of protecting human interests.</p> <h4 id="presumption-of-no-consciousness-a-default-epistemic-position">Presumption of No Consciousness: A Default Epistemic Position</h4> <p>Similarly, when viewed through our human-centralist lens, the profound uncertainty about AI consciousness and its potential societal risks logically lead to a presumption of no consciousness as our default epistemic position. This principle establishes that AI systems should be treated as non-conscious unless proven otherwise.</p> <p>This presumption is motivated by both epistemic and pragmatic considerations. Epistemically, our factual determination reveals a lack of scientific consensus on consciousness even in biological systems (missing reference), making consciousness attribution to artificial systems premature. This position parallels legal principles like presumption of innocence (missing reference) and scientific parsimony, which favors explanations that don’t invoke consciousness unless compelling evidence demands it (missing reference).</p> <p>Pragmatically, our risk prudence principle dictates adopting approaches that minimize ethical risks to humanity. As discussed, premature consciousness attribution could lead to operational paralysis, liability displacement, and legal complications as outlined in our societal risk analysis. A prudent approach therefore requires defaulting to a position of no consciousness.</p> <p>Overturning this presumption would require both scientific and legal thresholds. Scientifically, it would need robust consensus among relevant research communities (missing reference)—not unanimity, but predominant expert agreement comparable to established scientific theories (missing reference). Legally, formal institutional mechanisms would be necessary (missing reference), including rigorous evidence standards and governance frameworks balancing competing interests (missing reference). Any framework for such a determination must serve collective human welfare while integrating scientific evidence with procedural justice requirements (missing reference).</p> <h4 id="transparent-reasoning-for-evaluation-and-adaptation">Transparent Reasoning for Evaluation and Adaptation</h4> <p>Our factual determination about ethical evolution, combined with human-centralism, necessitates transparent reasoning as our third principle. This requires explicit documentation of reasoning chains and foundational assumptions for any ethical position on AI consciousness. For example, if one believes AI to be conscious by assuming functionalist theory, they should make it explicit to facilitate discussion.</p> <p>Importantly, this transparency requirement applies to consciousness <em>claims</em> and ethical <em>arguments</em> about AI systems, not necessarily to the internal workings of AI systems themselves, unless it is used as part of their arguments. We are not demanding that AI architectures be interpretable or that their computational processes be transparent—those are separate technical concerns.</p> <p>This principle serves three functions: (1) enabling responsible adaptation that avoids both premature position changes and inappropriate preservation of outdated views (missing reference); (2) strengthening framework robustness by making explicit what reasoning would need to be challenged to overturn positions (missing reference); and (3) reinforcing human-centralism by ensuring the framework is evaluated through human judgment rather than algorithmic interpretation (missing reference).</p> <p>Unlike our other principles, transparent reasoning represents a methodological cornerstone unlikely to require revision. Grounded in epistemological responsibility, it remains robust across contexts and technological developments, functioning as a self-correcting mechanism that facilitates revision and refinement. We acknowledge that alternative frameworks might question transparency requirements, especially when rapid decision-making or proprietary concerns compete with disclosure, and welcome critical engagement to strengthen our approach.</p> <h2 id="application-level-derived-default-positions-on-particular-questions">Application-Level: Derived Default Positions on Particular Questions</h2> <p>Having established our three-level framework, we now demonstrate its practical application to key questions in AI consciousness ethics. While our framework includes three operational principles, we note that only two—the presumption of no consciousness and risk prudence—directly generate substantive ethical positions. The third principle, transparent reasoning, serves as a methodological requirement when presenting our derivations. In the following sections, we apply our framework to three representative ethical questions, illustrating how our principles generate default positions that can serve as starting points for further ethical deliberation.</p> <h3 id="should-people-worry-about-hurting-ai-systems">Should People Worry About Hurting AI Systems?</h3> <p>This question requires addressing two distinct considerations established in our factual determinations: potential AI consciousness and human anthropomorphic responses.</p> <p>Regarding consciousness, our presumption of no consciousness principle establishes a default epistemic position: AI systems should be treated as non-conscious unless compelling evidence proves otherwise. Behavioral similarity to humans does not confer consciousness status to AI. This principle places the burden of proof on those claiming AI systems experience suffering, making such attributions highly speculative absent evidence. Risk prudence further directs us to prioritize approaches that reduce potential ethical risks to humanity—recognizing that treating AI systems as conscious moral patients could lead to critical system paralysis and liability displacement.</p> <p>Based on this reasoning regarding consciousness, our default position emerges: people, especially AI researchers, should not concern themselves with potentially harming AI systems based on consciousness considerations alone.</p> <p>However, our factual determination distinguishing anthropomorphism from consciousness provides a second perspective. Even without consciousness, mistreating humanoid robots may remain ethically problematic through human-centered frameworks. From a virtue ethics perspective, deliberately damaging anthropomorphic objects may reflect and reinforce negative character traits in humans (missing reference). Research demonstrates that witnessing apparent “cruelty” toward robots with human-like features triggers empathetic neural responses in human observers (missing reference). In social contexts, such behaviors may normalize violence, desensitize observers to suffering, or communicate disturbing intentions (missing reference).</p> <p>This anthropomorphism-based reasoning leads to distinct legal and ethical implications. While we reject consciousness-based protections, limited protections based on human welfare considerations may be justified. Comprehensive assessment is needed to determine which activities might harm human society, how to identify them, and how to differentiate these concerns from consciousness issues. We must carefully balance implementation costs and risks—particularly how protective measures might inadvertently promote the perception of AI as conscious.</p> <h3 id="how-should-stakeholders-communicate-about-ai-capabilities-to-the-public">How Should Stakeholders Communicate About AI Capabilities to the Public?</h3> <p>Our presumption of the no consciousness principle suggests that AI systems should generally be treated as non-conscious by default, which has implications for how we communicate about them. Risk prudence encourages approaches that reduce potential risks to humanity—including the possibility that anthropomorphic cues might lead to unwarranted consciousness attribution and subsequent societal challenges like liability displacement.</p> <p>From these two principles, our default position follows: institutions and companies should avoid making general claims about AI consciousness, particularly phenomenal consciousness. And anthropomorphic narratives should be used judiciously. When not necessary, communications about AI systems should employ language that distinguishes AI behavior from consciousness.</p> <p>One potential scenario arises when AI systems are developed with a certain degree of access consciousness as mentioned earlier (the functional availability of information for use in reasoning and behavior). When referring to such capabilities, using the term “consciousness” may be unavoidable. In these cases, we advocate for institutions to provide precise contextual clarification when communicating about these systems, distinguishing functional capabilities from phenomenal consciousness, thereby minimizing potential misinterpretation and societal impact.</p> <p>We acknowledge that in practice this question involves a lot of details that will be hard to evaluate and regulate. We encourage the community to discuss and debate the details.</p> <h3 id="if-an-ai-system-were-truly-conscious-in-the-future-what-rights-should-it-have">If an AI System Were Truly Conscious In The Future, What Rights Should It Have?</h3> <p>This question invites us to contemplate a hypothetical future where our presumption of no consciousness has been definitively overcome through compelling evidence. It is important to acknowledge that such a scenario would likely emerge only after profound advancements in technology, substantial evolution in our understanding of consciousness, and significant societal transformation. Given these considerations, our present discussion of this topic should be viewed primarily as a philosophical exercise—a preliminary exploration of ethical terrain that will undoubtedly be reshaped by developments we cannot yet fully anticipate.</p> <p>Regarding this issue, one important distinction we wish to make is that consciousness status does not directly dictate rights status. It is just one of the important factors to consider. From the risk prudence principle, we derive our default position : Even genuinely conscious AI would not automatically qualify for human-equivalent or even animal-equivalent rights. Thorough discussions will be needed to balance AI welfare considerations with human interests as the primary concern. Importantly, this implies by default termination of a conscious system should be allowed given its below-human or even below-animal level rights.</p> <p>The legal dimension of AI rights, referenced in Section Presumption of No Consciousness, presents a global challenge requiring international consensus. While our framework guides ethical discourse, implementing any AI rights would demand established legal processes. Any approach must examine mechanisms for recognizing and enforcing such rights if consciousness evidence emerges, balancing philosophical considerations with practical governance across jurisdictions.</p> <h2 id="conclusion">Conclusion</h2> <p>We have proposed a human-centric framework for AI consciousness ethics that builds on transparent foundations while acknowledging philosophical uncertainty surrounding consciousness. Our complete three-level structure—foundational factual determinations and meta-ethical stance, operational principles, and application-level default positions—not only generates actionable guidance but provides a transparent derivation process through which positions logically follow from established principles. This systematic approach makes explicit how each ethical position can be traced back to our foundational commitments, enabling both rigorous evaluation and responsible adaptation. Rather than claiming definitive answers, we establish reasonable epistemic and pragmatic starting points that prioritize human welfare without hindering beneficial technological development. By providing clear logical pathways from foundations to applications and specifying conditions for revising positions, the framework is designed to evolve alongside advances in consciousness research and AI development, offering a responsible path forward through these profound ethical challenges.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[As AI systems become increasingly sophisticated, questions about machine consciousness and its ethical implications have moved from fringe speculation to mainstream academic debate. We address these limitations through a structured three-level framework grounded in philosophical uncertainty.]]></summary></entry><entry><title type="html">The Adversarial Conditioning Paradox: Why Attacked Inputs Are More Stable, Not Less</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/adversarial-conditioning-paradox/" rel="alternate" type="text/html" title="The Adversarial Conditioning Paradox: Why Attacked Inputs Are More Stable, Not Less"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/adversarial-conditioning-paradox</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/adversarial-conditioning-paradox/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Adversarial attacks on NLP systems pose a growing security concern. Attacks such as TextFooler <d-cite key="jin2020bert"></d-cite>, BERT-Attack <d-cite key="li2020bert"></d-cite>, and PWWS <d-cite key="ren2019generating"></d-cite> generate inputs that appear semantically similar to clean text yet cause misclassification. These attacks are specifically optimized to minimize embedding distance while maximizing prediction change—making them invisible to similarity-based detection methods.</p> <p>We set out to investigate whether <strong>Jacobian conditioning analysis</strong> could provide an alternative detection signal. The condition number κ of a layer’s Jacobian measures the ratio of maximum to minimum singular values, capturing how much the layer amplifies perturbations in different directions.</p> <div class="l-body"> <div style="background-color: #f0f0f0; padding: 15px; border-left: 4px solid #333; margin: 20px 0;"> <strong>Initial Hypothesis:</strong> Adversarial inputs should exhibit <em>high</em> condition numbers, indicating they occupy ill-conditioned regions where small perturbations cause disproportionately large output changes. </div> </div> <p>This hypothesis seemed natural. Adversarial attacks succeed by finding perturbations that cause large prediction shifts. High conditioning (large κ) would indicate sensitivity to perturbation—exactly what adversarial examples exploit.</p> <p><strong>We found the opposite.</strong></p> <h2 id="the-adversarial-conditioning-paradox">The Adversarial Conditioning Paradox</h2> <p>Across three different attack types—word-level substitution (TextFooler, PWWS) and character-level perturbation (DeepWordBug)—adversarial inputs show systematically <em>lower</em> condition numbers at Layer 1 of BERT compared to clean inputs.</p> <div class="l-body"> <figure> <img src="/2026/assets/img/2026-04-27-adversarial-conditioning-paradox/layer_conditioning.png" alt="Layer-wise condition numbers"/> <figcaption>Figure 1: Average condition numbers across transformer layers for clean vs adversarial inputs. Adversarial examples show consistently lower κ at early layers.</figcaption> </figure> </div> <p>The effect is statistically significant:</p> <ul> <li><strong>TextFooler</strong>: AUC = 0.72, p = 0.001</li> <li><strong>DeepWordBug</strong>: AUC = 0.75, p = 0.001</li> <li><strong>PWWS</strong>: AUC = 0.59, p = 0.29 (directionally consistent)</li> </ul> <p>Meanwhile, cosine distance—the metric attacks explicitly minimize—fails completely (AUC ≈ 0.25).</p> <p>This <strong>adversarial conditioning paradox</strong> demands explanation. Why would adversarial inputs be <em>more</em> numerically stable, not less? And why does this pattern hold across fundamentally different attack strategies?</p> <p>We propose a geometric interpretation: adversarial attacks succeed not by exploiting instability, but by finding <strong>well-conditioned perturbation directions that happen to cross decision boundaries</strong>. Ill-conditioned directions would make the attack optimization unstable—small changes in the perturbation would cause unpredictable output swings, making it difficult to reliably flip predictions. Instead, attacks implicitly select for smooth, stable paths to misclassification.</p> <h2 id="background-and-related-work">Background and Related Work</h2> <h3 id="adversarial-attacks-on-nlp">Adversarial Attacks on NLP</h3> <p>Adversarial attacks on text classifiers seek to find inputs that cause misclassification while preserving semantic content. We study three attack families:</p> <p><strong>TextFooler</strong> <d-cite key="jin2020bert"></d-cite> uses a greedy search that identifies important words via deletion and replaces them with semantically similar alternatives from a counter-fitted embedding space. The attack explicitly constrains substitutions to maintain sentence similarity.</p> <p><strong>PWWS</strong> <d-cite key="ren2019generating"></d-cite> combines word importance ranking with WordNet-based synonym substitution, using probability-weighted saliency to prioritize replacements. Unlike TextFooler, it uses a fixed synonym dictionary rather than embedding-based similarity.</p> <p><strong>DeepWordBug</strong> <d-cite key="gao2018black"></d-cite> operates at the character level, introducing typos, character swaps, and insertions. This attack is geometrically distinct from word-level attacks—it perturbs within the token embedding space rather than substituting between discrete tokens.</p> <h3 id="detection-methods">Detection Methods</h3> <p>Prior work on adversarial detection in NLP includes:</p> <ul> <li>Perplexity-based methods <d-cite key="mozes2021frequency"></d-cite></li> <li>Frequency-based analysis <d-cite key="pruthi2019combating"></d-cite></li> <li>Certified robustness <d-cite key="jia2019certified"></d-cite></li> <li>Ensemble disagreement approaches</li> </ul> <p>These methods operate on <strong>external properties</strong> of inputs. Our approach differs: we analyze <strong>internal geometric properties</strong> of how the model processes inputs, specifically the conditioning of layer-wise Jacobians.</p> <h3 id="jacobian-conditioning">Jacobian Conditioning</h3> <p>The condition number κ of a matrix J is defined as:</p> \[\kappa(J) = \frac{\sigma_{\max}(J)}{\sigma_{\min}(J)}\] <p>where σ_max and σ_min are the maximum and minimum singular values. For the Jacobian of a neural layer, κ captures how uniformly the layer responds to perturbations:</p> <ul> <li>High κ indicates ill-conditioning: some directions are amplified much more than others</li> <li>Low κ indicates well-conditioning: all directions are treated more uniformly</li> </ul> <h2 id="methods">Methods</h2> <h3 id="attack-generation">Attack Generation</h3> <p>We generate adversarial examples on the SST-2 sentiment classification task using:</p> <ol> <li><strong>TextFooler</strong>: Word substitution via embedding similarity</li> <li><strong>PWWS</strong>: WordNet-based synonym replacement</li> <li><strong>DeepWordBug</strong>: Character-level perturbations</li> </ol> <p>All attacks use default parameters from TextAttack <d-cite key="morris2020textattack"></d-cite> library. We generate 1000 successful adversarial examples per attack type, requiring:</p> <ul> <li>Successful label flip</li> <li>Semantic similarity &gt; 0.8 (for word-level attacks)</li> <li>Edit distance &lt; 30 characters (for character-level attacks)</li> </ul> <h3 id="conditioning-analysis">Conditioning Analysis</h3> <p>For each input (clean or adversarial), we extract:</p> <ol> <li> <p><strong>Layer-wise Jacobians</strong>: For transformer layer l with function f_l, we compute: \(J_l = \frac{\partial f_l(x)}{\partial x}\)</p> </li> <li> <p><strong>Condition numbers</strong>: Using randomized SVD for efficiency: \(\kappa_l = \frac{\sigma_{\max}(J_l)}{\sigma_{\min}(J_l) + \epsilon}\) where ε = 1e-10 for numerical stability.</p> </li> <li> <p><strong>Statistics</strong>: We compute κ for layers {1, 3, 6, 9, 12} of BERT-base.</p> </li> </ol> <h3 id="spectral-conditioning-monitor">Spectral Conditioning Monitor</h3> <p>We implement the Spectral Conditioning Monitor (SCM) algorithm for efficient condition number estimation:</p> <div class="l-body"> <figure> <img src="/2026/assets/img/2026-04-27-adversarial-conditioning-paradox/scm_algorithm.png" alt="SCM Algorithm"/> <figcaption>Figure 2: The SCM algorithm efficiently estimates condition numbers using randomized SVD and power iteration.</figcaption> </figure> </div> <h2 id="results">Results</h2> <h3 id="layer-wise-analysis">Layer-wise Analysis</h3> <div class="l-page"> <figure> <img src="/2026/assets/img/2026-04-27-adversarial-conditioning-paradox/kappa_distribution.png" alt="Distribution of condition numbers"/> <figcaption>Figure 3: Distribution of condition numbers at Layer 1 for clean vs adversarial inputs across three attack types.</figcaption> </figure> </div> <p><strong>Key findings:</strong></p> <ol> <li><strong>Layer 1 shows strongest signal</strong>: Adversarial κ consistently lower than clean</li> <li><strong>Effect diminishes with depth</strong>: By Layer 12, distributions overlap substantially</li> <li><strong>Cross-attack consistency</strong>: All three attacks show same directional effect</li> </ol> <div class="l-body"> <table> <thead> <tr> <th>Attack</th> <th>Layer 1 κ (Clean)</th> <th>Layer 1 κ (Adv)</th> <th>p-value</th> <th>AUC</th> </tr> </thead> <tbody> <tr> <td>TextFooler</td> <td>23.45 ± 8.32</td> <td>18.73 ± 6.21</td> <td>0.001</td> <td>0.72</td> </tr> <tr> <td>DeepWordBug</td> <td>24.12 ± 9.15</td> <td>17.89 ± 5.43</td> <td>0.001</td> <td>0.75</td> </tr> <tr> <td>PWWS</td> <td>22.78 ± 7.94</td> <td>20.91 ± 7.12</td> <td>0.29</td> <td>0.59</td> </tr> </tbody> </table> <figcaption>Table 1: Condition number statistics at Layer 1 for clean vs adversarial inputs.</figcaption> </div> <h3 id="attack-specific-patterns">Attack-specific Patterns</h3> <p>Different attacks show distinct conditioning signatures:</p> <div class="l-body"> <figure> <img src="/2026/assets/img/2026-04-27-adversarial-conditioning-paradox/attack_patterns.png" alt="Attack-specific patterns"/> <figcaption>Figure 4: Attack-specific conditioning patterns across layers reveal different perturbation strategies.</figcaption> </figure> </div> <ul> <li><strong>TextFooler</strong>: Smooth decay from Layer 1 to 12</li> <li><strong>DeepWordBug</strong>: Sharp drop at Layer 1, then stabilizes</li> <li><strong>PWWS</strong>: Gradual change, weakest signal</li> </ul> <h3 id="detection-performance">Detection Performance</h3> <p>ROC analysis shows strong detection capability using Layer 1 conditioning alone:</p> <div class="l-body"> <figure> <img src="/2026/assets/img/2026-04-27-adversarial-conditioning-paradox/roc_curves.png" alt="ROC curves"/> <figcaption>Figure 5: ROC curves for adversarial detection using conditioning vs cosine distance.</figcaption> </figure> </div> <p>Remarkably, cosine distance—which attacks explicitly minimize—provides no discriminative signal (AUC ≈ 0.25), while conditioning achieves AUC = 0.72-0.75.</p> <h2 id="discussion">Discussion</h2> <h3 id="geometric-interpretation">Geometric Interpretation</h3> <p>Why do adversarial inputs show <em>lower</em> condition numbers? We propose three complementary explanations:</p> <p><strong>1. Optimization stability:</strong> Ill-conditioned directions would destabilize attack optimization. Small adjustments during the attack search would cause unpredictable output changes, making it difficult to reliably flip predictions. Attacks implicitly select for well-conditioned paths.</p> <p><strong>2. Semantic preservation:</strong> Well-conditioned directions may better preserve semantic content. High-κ directions could correspond to linguistically meaningful variations that attacks must avoid to maintain similarity.</p> <p><strong>3. Decision boundary geometry:</strong> The model’s decision boundaries may be smoother (lower curvature) in well-conditioned regions. Attacks find these smooth crossings rather than sharp, unstable transitions.</p> <h3 id="implications-for-defense">Implications for Defense</h3> <p>Our findings suggest new defense strategies:</p> <ol> <li><strong>Conditioning-based detection:</strong> Monitor Layer 1 conditioning as a real-time detection signal</li> <li><strong>Adversarial training:</strong> Include conditioning regularization to eliminate well-conditioned attack paths</li> <li><strong>Architecture design:</strong> Engineer models with uniform conditioning to reduce attack surface</li> </ol> <p>The paradox also reveals a fundamental trade-off: making models more stable (lower κ) may inadvertently create smoother attack surfaces.</p> <h2 id="conclusion">Conclusion</h2> <p>We document an unexpected phenomenon: adversarial inputs to NLP models exhibit <em>lower</em> Jacobian condition numbers at early layers, contradicting the intuitive hypothesis that attacks exploit unstable regions. This “adversarial conditioning paradox” holds across word-level and character-level attacks, providing a strong detection signal where embedding-based methods fail.</p> <p>Our findings suggest that adversarial attacks succeed not through chaos but through stability—finding well-conditioned directions that smoothly cross decision boundaries. This geometric insight opens new avenues for both understanding and defending against adversarial examples in NLP systems.</p> <p>Future work should investigate:</p> <ul> <li>Whether the paradox extends to other architectures (GPT, RoBERTa)</li> <li>How conditioning evolves during adversarial training</li> <li>Whether attacks can be modified to maintain high conditioning while preserving effectiveness</li> </ul> <p>The code and data for reproducing our experiments are available at [anonymous repository link].</p> <h2 id="acknowledgments">Acknowledgments</h2> <p>We thank the anonymous reviewers for their valuable feedback. This work was supported by [anonymized funding source].</p>]]></content><author><name>Anonymous Authors</name></author><category term="adversarial-ml"/><category term="nlp-security"/><summary type="html"><![CDATA[Adversarial inputs exhibit systematically lower Jacobian condition numbers at early transformer layers—the opposite of our initial hypothesis that attacks exploit unstable regions. This paradox reveals that adversarial attacks succeed by finding well-conditioned directions that cross decision boundaries.]]></summary></entry><entry><title type="html">(LLM-)Judges on autopilot</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/auto-calibration/" rel="alternate" type="text/html" title="(LLM-)Judges on autopilot"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/auto-calibration</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/auto-calibration/"><![CDATA[<div class="row mt-4"> <div class="col text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-auto-calibration/judge_funny-480.webp 480w,/2026/assets/img/2026-04-27-auto-calibration/judge_funny-800.webp 800w,/2026/assets/img/2026-04-27-auto-calibration/judge_funny-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-auto-calibration/judge_funny.png" class="img-fluid d-block mx-auto" width="100%" height="auto" style=" max-width: 90%; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="what-is-an-llm-as-a-judge">What is an LLM-as-a-judge?</h2> <p>Modern evaluation pipelines increasingly rely on LLM-as-a-judge <d-cite key="zheng2023llmjudge"></d-cite> to assess the quality of AI-generated responses. This approach uses an LLM to act as a judge, assessing the quality of the generated outputs against specific criteria. The judge can be either a different model or the same one that generated the response. An example prompt for the <i>answer relevance</i> metric is shown below <d-cite key="opik_answer_relevance"></d-cite>.</p> <blockquote> <p>You are an expert in NLP evaluation metrics, specifically trained to assess the relevance of answers. Your role is to evaluate the relevance of a given answer based on the user’s input. Follow these steps to complete the evaluation. […]</p> </blockquote> <p>However, this approach is inherently self-referential: an LLM produces the answer and another LLM evaluates it. What assures us that the judge is trustworthy? The primary way to establish trust is by verifying its alignment with known cases. For example, one could ask a pool of experts to rate a small subset of responses; if the LLM-as-a-judge and the experts agree on those evaluations, it’s reasonable to assume the automated judge will also perform reliably on the remaining examples.</p> <p>Typically, adjusting the LLM-as-a-judge prompt to align with human feedback is cumbersome and largely manual: ground-truth collection requires human experts, and prompt calibration requires engineering expertise. This reliance on human judgment makes the process less reproducible and prone to inconsistency.</p> <p>In this article, we investigate how to automate LLM-as-a-judge prompt calibration. Recent work introduced the use of a “cheatsheet”, a set of reference instructions that guides the LLM’s reasoning. Methods such as Dynamic Cheatsheet (DC) <d-cite key="suzgun2025dynamiccheatsheet"></d-cite> and Agentic Context Engineering (ACE) <d-cite key="zhang2025agenticcontextengineering"></d-cite> iteratively refine the cheatsheet based on observed examples, treating prompt optimization as an online learning problem.</p> <p>However, when processing samples one at a time and updating the cheatsheet sequentially, important details are lost and performance degrades over time—a phenomenon known as <i>context collapse</i> <d-cite key="zhang2025agenticcontextengineering"></d-cite>. We observe this issue in our experimental evaluation, and address it with a batch-based approach inspired by classical machine learning training pipelines. Before diving into the methods, let’s first discuss what we’re evaluating and the data used.</p> <h2 id="what-are-we-evaluating">What are we evaluating?</h2> <p>Our reference use case is assessing <i>answer relevance</i> for responses generated by an internal AI assistant for network troubleshooting. Answer relevance is a metric that evaluates how pertinent an AI system’s response is to the user’s query <d-cite key="opik_answer_relevance"></d-cite>. To measure answer relevance, we provide the LLM-as-a-judge with an input query and the corresponding output response. The judge produces a score between 0 and 1, along with a justification for its rating. Example (anonymized):</p> <table style="border: none; border-collapse: collapse;"> <tr style="border: none;"> <td style="border: none; vertical-align: top; padding: 8px;"><strong>user</strong></td> <td style="border: none; padding: 8px;">"Show me the latest issues"</td> </tr> <tr style="border: none;"> <td style="border: none; vertical-align: top; padding: 8px;"><strong>system</strong></td> <td style="border: none; padding: 8px;">"Fetching issues from the past 24 hours…We detected 5 alerts in the last 24 hours, 5 of which are major. The latest major alert was 'Packet Loss Internet - Alert - XYZ' applied on the target 'xyz'. The alert started at Dec 1, 2025 05:36:00 and lasted for 145 seconds impacting 1 test and 1 agent across the following 1 location: XYZ. The alert triggered because the packet loss, 15.0%, was outside of the threshold of 3%. We also detected 4 application outages. The latest app outage started at November 30, 2025 21:15:02 &amp; lasted for 180 seconds impacting 5 servers &amp; 2 locations. The outage occurred in these locations: London, England, UK; New York, New York, US."</td> </tr> <tr style="border: none;"> <td style="border: none; vertical-align: top; padding: 8px;"><strong>score</strong></td> <td style="border: none; padding: 8px;">1.0</td> </tr> <tr style="border: none;"> <td style="border: none; vertical-align: top; padding: 8px;"><strong>reason</strong></td> <td style="border: none; padding: 8px;">"The answer addresses the question exhaustively and clearly."</td> </tr> </table> <blockquote> <p><b><i>Note:</i></b> Unlike traditional metrics with clear mathematical definitions, this score represents a qualitative judgment guided by high-level criteria.</p> </blockquote> <h2 id="what-about-the-data">What about the data?</h2> <p>Let’s establish some notation that we’ll use throughout this article. Given a dataset of input/output pairs with associated scores and reasons, for a sample $i$ we denote:</p> <table> <thead> <tr> <th>Symbol</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>$x_i$</td> <td>User input query</td> </tr> <tr> <td>$y_i$</td> <td>System output</td> </tr> <tr> <td>$s_i$</td> <td>LLM-as-a-judge generated score</td> </tr> <tr> <td>$r_i$</td> <td>LLM-as-a-judge generated reason</td> </tr> <tr> <td>$\tilde{s}_i$</td> <td>Ground-truth score (human-annotated)</td> </tr> <tr> <td>$\tilde{r}_i$</td> <td>Ground-truth reason (human-annotated)</td> </tr> </tbody> </table> <p>Later in this article, we will denote the cheatsheet at iteration $t$ as $M_t$ (where $M$ stands for memory). In addition, when discussing batch-based approaches to prompt calibration, we will use $B$ to denote the set of sample indices belonging to a batch.</p> <h3 id="the-challenge-of-high-quality-ground-truth">The challenge of high-quality ground truth</h3> <p>To assess the performance of LLM-as-a-judge metrics, we need ground-truth scores and reasons. Obtaining reliable scores is far from trivial. For our purposes, we relied on human annotations: experts followed rating guidelines (e.g., “subtract 0.1 to 0.3 for unnecessary verbosity or repetition”) to produce scores with quantitative justifications. In other words, for each input/output pair $(x_i, y_i)$, a human annotator provided a ground-truth score and reason, which we denote as $\tilde{s}_i$ and $\tilde{r}_i$.</p> <p>However, upon careful review, we found that many scores didn’t align with expected ratings. Subjectivity is inherent when evaluating LLM-generated content. To reduce this bias, we had multiple annotators perform ratings independently and retained only those records where they showed strong agreement.</p> <p>Another challenge is sample diversity. LLMs can produce multiple valid responses for a given input, making random train/test splits potentially problematic. To better represent a realistic production scenario, we split the training and test sets based on a temporal cutoff: samples before a certain date were used for training, and those after for testing.</p> <h2 id="who-are-you-to-judge-me">Who are you to judge me?</h2> <p>Now that we have established our evaluation metric and collected ground truth annotations, the key question becomes: how do we calibrate the judge prompt to align its evaluations with human judgments? In this section, we explore different approaches to calibration.</p> <h3 id="our-first-attempt-manual-calibration">Our first attempt: manual calibration</h3> <p>Traditional calibration relies on manually crafted prompts. The workflow depends on human annotation samples and uses techniques such as few-shot prompting to address problematic queries. This process is typically iterative:</p> <ol> <li>Collect ground-truth scores from human annotators</li> <li>Compute alignment between human annotations and LLM-as-a-judge metrics</li> <li>Identify discordant samples where human and model scores diverge</li> <li>Refine the judge prompt to minimize score discrepancy</li> </ol> <p>Engineering teams perform multiple rounds of manual calibration over time to ensure that human annotators and the LLM-as-a-judge metrics converge. Beyond being time-consuming and error-prone, this approach raises several technical concerns: (a) step 4 typically involves adding few-shot examples, which results in extremely long and over-engineered prompts; (b) ensuring that existing functionalities are preserved at each calibration round requires careful selection and analysis of reference samples. This process requires an expert engineer and can take several days. These limitations motivated us to explore more automated approaches.</p> <h3 id="take-your-cheatsheet-out-the-game-begins">Take your cheatsheet out, the game begins</h3> <p>Dynamic Cheatsheet Cumulative (DC-Cu) <d-cite key="suzgun2025dynamiccheatsheet"></d-cite> was introduced in April 2025 by researchers at <i>Stanford University</i> and <i>Together AI</i>. The key idea is to treat prompt optimization as an online learning problem where an LLM processes samples sequentially, updating the cheatsheet after each one.</p> <p>DC-Cu distinguishes between two roles: a <i>Generator</i> LLM and a memory <i>Curator</i> LLM. The Generator takes the input query $x_i$ and the current cheatsheet $M_i$ to produce an output $y_i$:</p> \[y_i = \text{Generator}(x_i, M_i)\] <p>The Curator then evaluates the output and refines the cheatsheet, keeping only the most useful and generalizable strategies:</p> \[M_{i+1} = \text{Curator}(M_i, x_i, y_i)\] <p>Since no ground truth is available, the Curator itself judges response quality. In practice, this self-verification approach is achievable for math questions and puzzles where the correctness of the solution can be verified using tools such as a calculator or code-execution environment.</p> <blockquote> <p><b><i>Note:</i></b> Unlike math questions and puzzles, LLM-as-a-judge evaluations are not self-verifiable. There is no objective tool to verify whether a relevance score is correct. Therefore, we need ground-truth human annotations.</p> </blockquote> <p>We adapted DC-Cu for supervised auto-calibration by providing the Memory Curator with ground truth scores $\tilde{s}_i$ and reasoning $\tilde{r}_i$ alongside the input/output pairs $(x_i, y_i)$ (see Fig. 1). We also modified the prompts to ensure these human annotations guide the cheatsheet generation process.</p> <div class="row mt-4"> <div class="col text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-auto-calibration/2025_12_01_DC.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-auto-calibration/2025_12_01_DC.svg" class="img-fluid d-block mx-auto" width="100%" height="auto" style=" max-width: 90%; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-left"> Fig. 1: Overview of the DC‑Cu method adapted to Judge auto‑calibration. Input–output pairs $(x_i, y_i)$ are retrieved from the database and evaluated by the Judge, which assigns a relevance score and reason $(s_i, r_i)$. The predicted score–reason pair is concatenated with the original input–output pair and the ground‑truth score–reason pair $(\tilde{s}_i, \tilde{r}_i)$, and passed to the Curator, which updates the cheatsheet $M_i$ to align the Judge's scoring behavior with the ground truth. </div> <p>In our adapted DC-Cu framework, the process alternates between two phases. In the <strong>Judgment Phase</strong>, the Judge evaluates the input/output pair using the current cheatsheet $M_i$ to produce a score and reasoning:</p> \[s_i, r_i = \text{Judge}(x_i, y_i, M_i)\] <p>In the <strong>Curation Phase</strong>, the Curator updates the cheatsheet by comparing the Judge’s predictions with the ground truth annotations ($\tilde{s}_i$, $\tilde{r}_i$):</p> \[M_{i+1} = \text{Curator}(M_i, x_i, y_i, \tilde{s}_i, \tilde{r}_i, s_i, r_i)\] <p>The Curator uses the discrepancy between predicted and ground truth values to refine the evaluation criteria stored in the cheatsheet.</p> <h3 id="ace-up-your-sleeve-agentic-context-engineering">ACE Up Your Sleeve: Agentic Context Engineering</h3> <p>Agentic Context Engineering (ACE) <d-cite key="zhang2025agenticcontextengineering"></d-cite> builds upon DC-Cu by further refining its architectural structure. In ACE, the Memory Curator role is decomposed into two specialized components: a <i>Reflector</i>, which synthesizes insights from both correct and erroneous outputs, and a <i>Curator</i>, which integrates these insights into context updates. This separation prevents overburdening a single agent with the dual responsibilities of quality assessment and cheatsheet evolution. Additionally, ACE introduces a grow-and-refine mechanism that implements incremental updates to avoid full cheatsheet rewrites, pruning redundant entries through semantic analysis to ensure the cheatsheet remains both comprehensive and concise.</p> <p>Inspired by ACE, we developed a customized implementation for the LLM-as-a-Judge supervised use case by extending DC-Cu with three key enhancements: (i) incorporating ground-truth scores and reasons during training, (ii) introducing a Reflector LLM, and (iii) enabling batching and epoch-based training. Further details are provided in the following section.</p> <h3 id="any-baci-please">Any BACI, please?</h3> <p>In this section, we introduce BACI (Batching Agentic Context Iteratively), our proposed strategy for automated judge calibration. The overall architecture is illustrated in Fig. 2.</p> <div class="row mt-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-auto-calibration/2025_12_01_BACI.svg" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-auto-calibration/2025_12_01_BACI.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption text-left"> Fig. 2: Overview of the BACI method. A batch $B$ of annotated samples is retrieved from the database, where each sample contains an input–output pair $(x_i, y_i)$ and the corresponding ground‑truth score–reason $(\tilde{s}_i, \tilde{r}_i)$. The Judge uses the cheatsheet rules to evaluate each input–output pair $(x_i, y_i)$ individually, producing a predicted score and reason $(s_i, r_i)$. Each prediction is then concatenated with its input-output pair and ground-truth, and the batch is forwarded to the Reflector. The Reflector compares ground-truth and predicted scores along with their reasons, identifying relationships among samples to extract insights $I_t$ about the Judge's errors. The Curator incorporates these insights into the cheatsheet $M_t$, aligning the Judge's scores with the ground truth. This process repeats for $k$ epochs to progressively optimize the cheatsheet. </div> <p>BACI incorporates <strong>batching</strong> as a core component to <strong>iteratively</strong> optimize the <strong>agentic context</strong>. During training, the Judge individually evaluates each sample $i$ within a batch $B$ using the current cheatsheet $M_t$ (where $t$ denotes the iteration number):</p> \[s_i, r_i = \text{Judge}(x_i, y_i, M_t) \quad \forall i \in B\] <p>The batch is then passed to the Reflector, which extracts insights $I_t$ by comparing the Judge’s predictions with the human-provided ground truth across all samples in the batch:</p> \[I_{t} = \text{Reflector}(\{(x_i, y_i, \tilde{s}_i, \tilde{r}_i, s_i, r_i)\}_{i \in B})\] <p>These insights are fed to the Curator, which updates the cheatsheet accordingly:</p> \[M_{t+1} = \text{Curator}(M_t, I_t)\] <p>In subsequent iterations, the Judge uses the updated cheatsheet to generate new scores and reasons. This iterative process is repeated for all batches, with the cheatsheet being continuously refined at each step. The entire cycle is run for $k$ epochs, like a standard machine learning pipeline but employing gradient-free optimization.</p> <p>At test time, we provide the Judge with the final version of the cheatsheet, refined during training. The Judge uses this cheatsheet to evaluate new, unseen samples by generating scores and reasons based on the accumulated knowledge. The final cheatsheet serves as a distilled summary of the most relevant evaluation patterns learned during training, guiding the Judge’s evaluations in the test phase.</p> <p>Compared to ACE, our Curator is responsible for both adding new evaluation instructions and de-duplicating entries. This design makes our method more lightweight than the original ACE approach, which maintains embeddings for each instruction in the cheatsheet.</p> <p>In BACI, the Reflector extracts insights, and the Curator is instructed to perform updates by adding instructions (i) only if they are sufficiently different from existing ones, (ii) refining entries that lack important aspects, and (iii) discarding items that are similar to those already present. The combination of batching and the Reflector-Curator architecture helps us avoid context collapse and redundancy of instructions. In particular, the batch size plays a crucial role in this process — as we will demonstrate in the following section.</p> <blockquote> <p><b><i>Note:</i></b> we use Claude Sonnet 4.5, which has a nearly unlimited context window (200K base, can be extended up to 1 million tokens) for the LLM Judge, Reflector, and Curator. When using a model with a smaller context window, a trade-off in the batch size might be needed.</p> </blockquote> <h2 id="what-did-we-learn">What did we learn?</h2> <h3 id="starting-with-the-basics-when-less-is-more">Starting with the basics: when less is more</h3> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-auto-calibration/score_comparison_other_methods.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <div class="caption text-left"> Fig. 3: Score comparison on test set for the baseline methods. The score for each sample has been computed as the average across 10 predictions. The standard deviaton for each score can be seen by hovering over the point. </div> <p>We compared the score distributions across our test set for different baselines (see Fig. 3). We first evaluated what happens when the Judge receives no special instructions, using only the basic definition of Answer Relevance, with no training, evolution, or use case specific guidelines. Surprisingly, on average, this “empty cheatsheet” approach performs on par with our manually calibrated prompt. This is remarkable given that an empty cheatsheet contains no domain-specific details.</p> <p>Even more unexpectedly, the DC-Cu method performs worse than the previous baselines. As noted in <d-cite key="zhang2025agenticcontextengineering"></d-cite>, the main issue is context collapse: over time, the LLM Curator tends to generate shorter, less informative summaries, leading to a sharp decline in performance.</p> <p>These observations lead to our first key insight:</p> <blockquote> <p>It is better to provide no instructions in the cheatsheet than to include suboptimal instructions that may cause confusion or conflicting behavior.</p> </blockquote> <p>This also explains why the carefully calibrated prompt did not outperform the baselines on the test set, despite meticulous fine-tuning, adjusted dataset scoring, and multiple few-shot examples.</p> <h3 id="learning-in-batches-the-missing-ingredient">Learning in batches: the missing ingredient</h3> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-auto-calibration/score_comparison_BACI.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <div class="caption text-left"> Fig. 4: Score comparison on test set for BACI with different batch sizes and DC-Cu. The score for each sample has been computed as the average across 10 predictions. The standard deviaton for each score can be seen by hovering over the point. </div> <p>We evaluated two BACI configurations: BACI-1 uses a batch size of one with a single training epoch, while BACI-32 uses a batch size of 32 and trains for five epochs. Figure 4 compares their performance against ground truth and DC-Cu. Here are the key observations:</p> <ul> <li> <p>BACI-1 significantly outperforms DC-Cu. This improvement stems primarily from the separation of concerns between the Curator and Reflector components. As noted in the ACE work, this architectural separation — where insight extraction (Reflector) and cheatsheet updating (Curator) are distinct processes — helps mitigate context collapse across iterations.</p> </li> <li> <p>BACI-32 outperforms BACI-1. The larger batch size and multiple training epochs enable the system to observe the entire training dataset repeatedly, refining the cheatsheet iteratively. Crucially, processing samples in larger batches allows the Reflector to identify more generalizable patterns rather than overfitting individual examples.</p> </li> </ul> <h3 id="the-final-verdict">The final verdict</h3> <p>Bringing it all together: Table 1 summarizes all experimental results, showing average Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) <d-cite key="bishop2006pattern"></d-cite> across 10 test runs. BACI-32 (bold) clearly outperforms all competing methods on our dataset.</p> <div align="center"> <table> <thead> <tr> <th>Method</th> <th>MAE</th> <th>RMSE</th> </tr> </thead> <tbody> <tr> <td>Empty Cheatsheet</td> <td>0.134 ± 0.002</td> <td>0.201 ± 0.004</td> </tr> <tr> <td>Manually calibrated</td> <td>0.139 ± 0.010</td> <td>0.219 ± 0.016</td> </tr> <tr> <td>DC-Cu</td> <td>0.272 ± 0.006</td> <td>0.308 ± 0.007</td> </tr> <tr> <td>BACI-1</td> <td>0.147 ± 0.003</td> <td>0.212 ± 0.002</td> </tr> <tr> <td>BACI-32</td> <td><strong>0.111</strong> ± 0.003</td> <td><strong>0.198</strong> ± 0.004</td> </tr> </tbody> </table> </div> <div class="caption text-left"> Table 1: Experimental results comparing all methods. MAE and RMSE averaged across 10 test runs. </div> <p>Statistical validation using the Wilcoxon signed-rank test <d-cite key="wilcoxon1945individual"></d-cite> confirms: (1) manual calibration provided no significant improvement over an empty cheatsheet ($p = 0.62$); (2) BACI-32 significantly outperforms both BACI-1 and the empty cheatsheet ($p &lt; 0.01$).</p> <h2 id="key-takeaways">Key Takeaways</h2> <p><strong>Context collapse is real.</strong> Our empirical analysis confirms that iterative, sample-by-sample approaches to prompt calibration suffer from a critical issue: context collapse. The solution lies in applying proper machine learning strategies—processing samples in batches and tuning hyperparameters such as batch size and number of epochs.</p> <p><strong>Manual calibration faces similar challenges.</strong> Even manually crafted prompts can suffer from analogous issues, as they’re typically adjusted iteratively on a static set of examples. Moreover, human bandwidth limits the number of samples that can be examined, making the process tedious, error-prone, and nearly impossible to scale.</p> <p><strong>Sometimes, less is more.</strong> Perhaps our most surprising finding: the strong performance of the empty cheatsheet baseline. This serves as a clear warning that wrong guidance can be worse than no guidance at all. When a simple solution works, there’s no need to overcomplicate it.</p> <p><strong>Data quality matters — a lot.</strong> The quality of training data is just as important as the calibration method itself. While this principle applies to all machine learning, it is especially critical for generative AI: we cannot expect an LLM to generate meaningful insights from inconsistent or low-quality input data. In our work, we observed a substantial subjectivity bias in human annotations. Data cleaning was crucial and required time and resources. Despite the impressive capabilities of modern LLMs, human judgment remains indispensable—at least for now, AI cannot fully replace expert reviewers.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[How do you evaluate Large Language Model (LLM)-based systems in production at scale? Most teams turn to an LLM-as-a-judge: an approach that grasps the nuances of natural language where classical metrics fall short. But these judge models have their own “will”: sometimes they follow instructions precisely, sometimes they don't. To address this inconsistency, the judge prompt is calibrated to align with known, trusted cases. The problem? Manual calibration is time-consuming and error-prone. In this blog post, we explore auto-calibration techniques inspired by recent prompt-optimization research. We tackle context collapse by iteratively processing data in batches, similarly to a machine learning training pipeline. Along the way, we share some surprising findings about what works and what doesn't—including cases where simpler approaches outperform more sophisticated ones.]]></summary></entry><entry><title type="html">The 99% Success Paradox: When Near-Perfect Retrieval Equals Random Selection</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/bits-over-random/" rel="alternate" type="text/html" title="The 99% Success Paradox: When Near-Perfect Retrieval Equals Random Selection"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/bits-over-random</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/bits-over-random/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>For most of the history of information retrieval (IR), search results were designed for human consumers who could scan, filter, and discard irrelevant content on their own. This shaped retrieval systems to optimize for finding and ranking more relevant documents, but not for keeping results clean and minimal, as the human was the final filter.</p> <p><strong>Retrieval-augmented generation (RAG)</strong> and tool-using agents flip these assumptions. Now the consumer is often an LLM, not a person, and the model does not skim. In practice, introducing excessive or irrelevant context into the input can dilute the model’s ability to identify and focus on the most critical information. When you pass retrieved documents to an LLM:</p> <ul> <li>It can’t ignore irrelevant results. Every irrelevant chunk dilutes the model’s attention.</li> <li>Noise has a cost. Extra chunks cost tokens, latency, and computation. They also increase the odds that irrelevant or misleading content pulls attention away from what actually matters.</li> </ul> <h2 id="the-million-token-trap">The Million-Token Trap</h2> <p>You might be thinking: <em>“But modern LLMs have million-token context windows. Why care?”</em></p> <p>The real question isn’t whether a model can fit more context, but whether more context is actually helpful. Beyond a certain point, adding retrieved material (and the accompanying noise) can actively increase computational cost and degrade the quality of output.</p> <p>In our 20 Newsgroups classification case study, we increased the retrieval depth <strong>K</strong> from 10 to 100 items. This caused LLM accuracy to drop from 66% to 50%, even though the success metric (<strong>Success@K:</strong> the percentage of queries returning at least one relevant item) remained close to 100%. In other words, more retrieved content led to worse results, not better.</p> <p>This problem is especially severe for agentic systems that use tool-based retrieval, because context quality directly affects downstream decisions. A chatbot might give you a mediocre answer, however, an autonomous agent might call the wrong API, delete the wrong file, or execute the wrong command.</p> <p>We need a measure that asks: <em>“Given that I’m retrieving K items and my LLM will consume all of them, how much <strong>selective signal</strong> am I actually getting?”</em></p> <p>That’s what Bits-over-Random (BoR) measures. The rest of this post explains how.</p> <h2 id="what-traditional-metrics-miss">What Traditional Metrics Miss</h2> <p>Recall rewards finding more relevant documents, but is blind to how many irrelevant items you had to pull into the context window to get them. Over-retrieval is actually rewarded. As Manning et al.<d-cite key="manning2008introduction"></d-cite> note, “recall is a non-decreasing function of the number of documents retrieved”. Yet the choice of retrieval depth K is often an empirical, application-dependent choice <d-cite key="webber2010similarity"></d-cite>.</p> <p>Precision measures the relevance of retrieved results and helps limit excessive retrieval. However, it fails to account for the inherent difficulty of the retrieval task. For instance, achieving a 10% precision means something different if the corpus contains 10 relevant items out of 100 versus 10 relevant items out of 10,000. Same precision, very different selectivity.</p> <p>Ranking metrics (nDCG, RBP, MAP, ERR) penalize burying relevant items, but they do not penalize the presence of irrelevant items when the relevant item is also ranked highly. If you retrieve 100 items and the relevant one is at rank 1, nDCG can be perfect. Yet, RAG systems typically concatenate the top-K results into a single prompt. The LLM still has to read the other 99 items. Rankers optimize ordering, not volume. They don’t reduce the token cost of stuffing <strong>K</strong> documents into the context.</p> <p>In practice, teams end up juggling recall, precision, and ranking metrics. Each captures a different slice of behavior but none reflects the whole picture. There is no single framework that simultaneously accounts for how many items you retrieve, how big the corpus is, and how many items in the corpus are actually relevant to the query.</p> <h2 id="the-librarian-problem">The Librarian Problem</h2> <p>Consider a library of \(N = 1{,}000\) books, with \(R_q = 10\) books relevant to your query. Two librarians respond:</p> <ul> <li><strong>Librarian A</strong> retrieves \(K = 20\) books, 6 of which are relevant (precision 30%, recall 60%, F1 40%).</li> <li><strong>Librarian B</strong> retrieves \(K = 12\) books, 4 of which are relevant (precision 33%, recall 40%, F1 36%).</li> </ul> <p>Traditional IR metrics tend to favor Librarian A (higher recall and F1, similar precision). But Librarian A handed you 14 irrelevant books, versus B’s 8. If the librarians are retrievers or tools in an agent workflow and the consumer is an LLM, it must read everything it was given. Those 6 extra unhelpful books retrieved by Librarian A over Librarian B cost tokens, add noise, and waste computational resources.</p> <h2 id="the-new-baseline-random-chance">The New Baseline: Random Chance</h2> <p>And here’s the deeper question: <em>Beyond comparing A and B, is either of them an objectively skillful librarian? What is the baseline?</em></p> <p>If we compare each librarian to a random baseline (<em>“what if I picked K books uniformly at random?”</em>), we can ask which one is actually more selective than chance. Plugging these numbers into the chance-corrected formulas we introduce below shows that Librarian B is more selective than A. For an LLM consuming a fixed-size bundle of text, that selectivity per token is what matters.</p> <p>This is the key insight: every retrieval problem has a built-in baseline. If you picked <strong>K</strong> items completely at random, you’d still sometimes get lucky and grab something relevant, especially if relevant items are common.</p> <p>That random success rate is your floor. It tells you how much of your <em>“success”</em> is just dumb luck. Bits-over-Random (BoR) measures how far above random success you’ve climbed.</p> <p>In today’s RAG, agentic, and LLM workflows, we care less about who retrieved the most documents and more about who delivered the most signal with the least noise. By comparing a chosen success metric to random chance, BoR measures true selectivity: how much better is our retrieval bundle than random selection?</p> <p>Let’s break down how it works, step by step.</p> <h2 id="the-math">The Math</h2> <p>Evaluating a retriever shouldn’t require juggling incompatible metrics. To make sense of how well a system is actually performing, we need a baseline. Not just any baseline, but the most honest one possible: pure randomness. The framework below walks through a simple, quantitative way to express <em>“how much better than random”</em> your retrieval system really is.</p> <p>By measuring observed success, computing the expected success of random guessing, and comparing the two on a logarithmic scale, we end up with a clean, intuitive metric: <strong>Bits-over-Random (BoR)</strong>. This gives retrieval performance a natural, information-theoretic interpretation, each bit representing one doubling in effectiveness over chance.</p> <h2 id="the-quick-reference-version">The Quick-Reference Version</h2> <p>Here’s everything you need to remember:</p> <table> <thead> <tr> <th style="text-align: left">Symbol</th> <th style="text-align: left">Meaning</th> <th style="text-align: left">Example</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">\(N\)</td> <td style="text-align: left">Total items in corpus. Unit must be defined (e.g., documents, passages)</td> <td style="text-align: left">10,000 passages or 700 documents</td> </tr> <tr> <td style="text-align: left">\(K\)</td> <td style="text-align: left">How many items you retrieve per query (top-K)</td> <td style="text-align: left">\(K=10\) or \(K=100\)</td> </tr> <tr> <td style="text-align: left">\(R_q\)</td> <td style="text-align: left">Relevant items in the corpus for a certain query q</td> <td style="text-align: left">\(R_q=1\) (sparse) or \(R_q=20\) (many)</td> </tr> <tr> <td style="text-align: left">\(\bar{R}_q\)</td> <td style="text-align: left">Average relevant items in the corpus per query</td> <td style="text-align: left">≈1.1 on SciFact, ≈572 on 20 Newsgroups</td> </tr> <tr> <td style="text-align: left">\(P_{obs}(K)\)</td> <td style="text-align: left">Your observed success rate at K (Note: any success rate can be used here.)</td> <td style="text-align: left">60% of queries succeed</td> </tr> <tr> <td style="text-align: left">\(P_{rand}(K)\)</td> <td style="text-align: left">Random-chance success at K</td> <td style="text-align: left">What luck would give you</td> </tr> <tr> <td style="text-align: left">\(\lambda\)</td> <td style="text-align: left">Heuristic: expected random hits = \(K \cdot \bar{R}_q / N\)</td> <td style="text-align: left">\(\lambda\) in the 3–5 range signals collapse</td> </tr> </tbody> </table> <h3 id="step-1-measure-your-success-rate">Step 1. Measure Your Success Rate</h3> <p>First, pick a success condition. For most RAG systems, the natural rule is: <em>“Did I get at least one relevant item in my top-K results?”</em></p> <p>This is called <strong>Success@K</strong> (or coverage). For a batch of queries:</p> \[P_{\text{obs}}(K) = \frac{\text{number of queries with } \geq \text{ 1 relevant result in top-}K}{\text{total queries}}\] <p><strong>Note:</strong> The threshold doesn’t have to be 1. You can require at least m relevant documents if your system needs multiple pieces of evidence, for example, “at least 3 supporting passages.”</p> <p>If you retrieved K=10 items for 100 queries, and 60 queries got at least one relevant hit, then \(P_{\text{obs}}(10) = 60 / 100 = 0.60\).</p> <h3 id="step-2-calculate-the-random-baseline">Step 2. Calculate the Random Baseline</h3> <p>What if you picked <strong>K</strong> items completely at random? That’s your baseline.</p> <p>For a query where \(R_q\) items in the corpus are relevant, and the corpus has <strong>N</strong> total items, the hypergeometric distribution tells you the probability of randomly hitting at least one relevant item when picking <strong>K</strong> items:</p> <p>The probability of picking no relevant items in <strong>K</strong> picks is:</p> \[P_{\text{none}} = \frac{\binom{N-R_q}{K}}{\binom{N}{K}}\] <p>So, the probability of picking at least one relevant item is:</p> \[P_{\text{rand}} = 1 - P_{\text{none}} = 1 - \frac{\binom{N-R_q}{K}}{\binom{N}{K}}\] <p><strong>Special case:</strong> If every query has exactly one relevant item (\(R_q = 1\)), this simplifies to:</p> \[P_{\text{rand}}(K) = \frac{K}{N}\] <p>For example:</p> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>\(N\)</td> <td>10,000</td> </tr> <tr> <td>\(R_q\)</td> <td>10</td> </tr> <tr> <td>\(K\)</td> <td>20</td> </tr> </tbody> </table> \[P_{\text{rand}} = 1 - \frac{\binom{9990}{20}}{\binom{10000}{20}} \approx 0.02\] <p>This means random selection works <strong>~2%</strong> of the time.</p> <p>Because we evaluate over many queries, we average these random baselines:</p> \[\overline{P}_{\text{rand}}(K) = \text{average random success across all queries}\] \[\overline{P}_{\text{rand}}(K) = \frac{1}{|Q|} \sum\nolimits_{q} P_{\text{rand}}(K; R_q)\] <h3 id="step-3-enrichment-factor-how-many-times-are-we-better-than-random-chance">Step 3: Enrichment factor: how many times are we better than random chance?</h3> <p><strong>Enrichment Factor (EF)</strong> is defined as</p> \[\text{EF} = \frac{P_{\text{obs}}}{P_{\text{rand}}}\] <p>For a batch of queries, we use the averaged random baseline:</p> \[\text{EF}(K) = \frac{P_{\text{obs}}(K)}{\overline{P}_{\text{rand}}(K)}\] <p>An EF of 5 means you succeed 5× more often than random selection. An EF of 100 means you are 100× better. This formulation is consistent with enrichment metrics used in drug discovery screening <d-cite key="truchon2007evaluating"></d-cite>.</p> <h3 id="step-4-bits-over-random-bor-log-scale-conversion-of-ef">Step 4: Bits-over-Random (BoR): Log Scale conversion of EF</h3> \[\text{BoR} = \log_2(\text{EF}) = \log_2\left(\frac{P_{\text{obs}}}{P_{\text{rand}}}\right)\] <p>And similarly for averaging:</p> \[\text{BoR}(K) = \log_2(\text{EF}) = \log_2\left(\frac{P_{\text{obs}}(K)}{\overline{P}_{\text{rand}}(K)}\right)\] <p>Why \(\log_2\)? Bits are how information theory counts halvings, the same reason why binary search uses powers of 2. Each bit represents one halving of the search space. <strong>BoR = 10</strong> means <strong>10 halvings → 1,024× reduction</strong>.</p> <ul> <li><strong>BoR = 0</strong> → You’re no better than random</li> <li><strong>BoR = 1</strong> → <strong>2×</strong> better than random</li> <li><strong>BoR = 3</strong> → <strong>8×</strong> better than random</li> <li><strong>BoR = 10</strong> → <strong>1,024×</strong> better than random</li> </ul> <p>Each bit also represents a doubling of selectivity. Our definition follows.</p> <p><strong>Selectivity (n.):</strong> The ability of a retrieval system to surface relevant items while excluding irrelevant ones, measured relative to random chance. A system with high selectivity finds needles without bringing along the haystack.</p> <h2 id="a-concrete-example">A Concrete Example</h2> <p>Let’s assume you have 10,000 documents. Each query has exactly ten relevant documents (\(R_q = 1\)). <strong>Note:</strong> Many standard benchmarks such as MS MARCO have \(R_q ≈ 1\) on average, even sparser than this example.</p> <p>You are testing two different retriever systems against the same dataset:</p> <table> <thead> <tr> <th>Metric</th> <th>System A (K=20, 60% success)</th> <th>System B (K=100, 70% success)</th> </tr> </thead> <tbody> <tr> <td>P_obs</td> <td>0.60</td> <td>0.70</td> </tr> <tr> <td>P_rand</td> <td>0.01983</td> <td>0.09566</td> </tr> <tr> <td>EF (Enrichment Factor)</td> <td>0.60/0.01983 = 30.257</td> <td>0.70/0.09566 = 7.318</td> </tr> <tr> <td>BoR</td> <td>4.92 bits</td> <td>2.87 bits</td> </tr> </tbody> </table> <p><strong>System B</strong> has a higher raw success rate (70% vs. 60%) but a BoR score about 2 bits lower than <strong>System A</strong>. This lower score shows <strong>System B</strong> is less selective. It achieves higher coverage by expanding the retrieved set, which reduces informational efficiency. From an information-theoretic view, System B creates a larger <em>“haystack”</em> that delivers fewer useful bits of discrimination per query.</p> <h2 id="the-ceiling-problem">The Ceiling Problem</h2> <p>There’s a maximum BoR you can possibly achieve. If your system is perfect, achieving \(P_{\text{obs}}(K) = 1.0\) (every single query succeeds), the best you can do is:</p> \[\text{BoR}_{\text{max}}(K) = -\log_2(\overline{P}_{\text{rand}}(K))\] <p>This ceiling is determined entirely by the random baseline. Using our toy example:</p> <ul> <li><strong>System A:</strong> \(\text{BoR}_{\text{max}} = -\log_2(0.01983) = 5.66\) bits</li> <li><strong>System B:</strong> \(\text{BoR}_{\text{max}} = -\log_2(0.09566) = 3.39\) bits</li> </ul> <p>System A, even at 60% success, achieves 4.92 bits, already higher than System B’s ceiling. No amount of model improvement can help System B catch up. Given its success rate, it chose a retrieval depth K that limits its maximum possible selectivity.</p> <p><strong>When the random baseline is already high, even perfection gets you almost nothing.</strong></p> <h2 id="bor-optimistic-upper-bound">BoR optimistic upper bound</h2> <p>When you don’t know how many relevant items \(R_q\) exist in the corpus for each query, BoR enables you to define an optimistic upper bound by assuming each query has exactly one relevant item. In that case:</p> \[P_{\text{rand}}(K) \approx \frac{K}{N}\] <p>And:</p> \[\text{BoR}_{\text{opt}}(K) = \log_2\left(\frac{N}{K}\right)\] <p>It’s useful to compute the upper bound if calculating exact BoR is not feasible. \(\text{BoR}_{\text{opt}}(K)\) is an optimistic ceiling: no system on that corpus at depth <strong>K</strong> can have more than about \(\log_2(N / K)\) bits of selectivity under this assumption.</p> <p>Note that \(\text{BoR}_{\text{max}}\) uses actual \(R_q\) values while \(\text{BoR}_{\text{opt}}\) assumes \(R_q = 1\) throughout.</p> <h2 id="the-collapse-zone">The Collapse Zone</h2> <iframe src="/2026/assets/html/2026-04-27-bits-over-random/calculator.html" frameborder="0" scrolling="no" height="580px" width="100%" class="l-body rounded z-depth-1"></iframe> <p>Consider what happens when retrieval becomes <strong>“too easy”</strong>:</p> <ul> <li>If \(P_{\text{rand}} = 0.95\) (random selection succeeds 95% of the time), then even a perfect system only gets \(\text{BoR}_{\text{max}} \approx 0.07\) bits</li> <li>If \(P_{\text{rand}} = 0.99\) (random succeeds 99% of the time), then \(\text{BoR}_{\text{max}} \approx 0.01\) bits</li> </ul> <p>We call this the <em>“collapse zone.”</em> When you enter it, selectivity becomes mathematically impossible, even if your success rate looks great.</p> <p>The boundary is determined by:</p> \[\lambda = \frac{K \cdot \bar{R}_q}{N}\] <p>Where \(\bar{R}_q\) is the average number of relevant items per query.</p> <p>When \(\lambda\) reaches 3–5, you’ve entered the collapse zone. Random selection is already solving most queries, so even a perfect system can’t demonstrate meaningful skill.</p> <h2 id="what-happens-when-you-retrieve-more">What Happens When You Retrieve More?</h2> <p>Now that we have formulated a measure that evaluates an IR system with respect to random selection at a given K, what happens when you increase K (K₁ to K₂)? Typically, we expect the following:</p> <ol> <li>Your success rate improves (usually)</li> <li>Random selection also gets easier (always)</li> </ol> <p>The change in BoR is:</p> \[\Delta\text{BoR} = \log_2\left(\frac{P_2}{P_1}\right) - \log_2\left(\frac{\overline{P}_{\text{rand}}(K_2)}{\overline{P}_{\text{rand}}(K_1)}\right)\] <p>Translation:</p> <ul> <li><strong>First term:</strong> <em>“How much better did I actually do?”</em></li> <li><strong>Second term:</strong> <em>“How much easier did the task get for random guessing?”</em></li> </ul> <h2 id="the-doubling-rule">The Doubling Rule</h2> <p>In typical sparse-relevance scenarios (\(R_q \ll N\) and \(K \ll N\)), the hypergeometric baseline behaves like repeated independent draws. For small values of \(K \cdot R_q / N\), we can use standard approximations \((1 - x)^n \approx e^{-nx}\) and \(e^{-y} \approx 1 - y\) for \(y \to 0\).</p> <p>So, because: \(P_{\text{rand}}(K; R_q) \approx \frac{K \cdot R_q}{N}\) and averaging over queries yields \(\overline{P}_{\text{rand}}(K) \approx \frac{K \cdot \bar{R}_q}{N}\)</p> <p>We now have:</p> \[\Delta\text{BoR} \approx \log_2\left(\frac{P_2}{P_1}\right) - \log_2\left(\frac{K_2}{K_1}\right)\] <p>What does this mean in practice?</p> <p><strong>If you double K, but your success rate doesn’t improve, you lose about 1 bit of selectivity.</strong></p> <p>When you hear <strong>“just retrieve more,”</strong> remember: it’s not free. Once your success rate has plateaued:</p> <ul> <li>Double <strong>K</strong> and you lose \(\sim 1\) bit of selectivity</li> <li>\(10\times\) <strong>K</strong> and you lose \(\sim 3.3\) bits of selectivity</li> </ul> <p>To maintain selectivity when doubling <strong>K</strong>, you’d need \(P_{\text{obs}}\) to also double. But since \(P_{\text{obs}} \leq 1\), this becomes impossible once you’re above 50% success.</p> <p><strong>That’s why BoR inevitably degrades at larger depths once your success curve flattens.</strong></p> <h3 id="extensions-to-stricter-rules">Extensions to Stricter Rules</h3> <p>The BoR framework extends to stricter success rules. For example, requiring at least <strong>m</strong> relevant documents in the top-K:</p> \[\Delta\text{BoR} \approx -m \cdot \log_2\left(\frac{K_2}{K_1}\right)\] <p>Doubling K costs about <strong>m bits</strong> of selectivity. We focus on \(m=1\) in this post because it matches common single-evidence RAG scenarios.</p> <h2 id="case-studies-when-theory-meets-reality">Case Studies: When Theory Meets Reality</h2> <p>Let’s see how BoR behaves in the wild. We tested three different scenarios:</p> <table> <thead> <tr> <th style="text-align: left">Dataset</th> <th style="text-align: left">Corpus Size</th> <th style="text-align: left">Relevant Items per Query</th> <th style="text-align: left">Why Test It?</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>BEIR SciFact</strong></td> <td style="text-align: left">5,183 abstracts (1,409 queries/claims)</td> <td style="text-align: left">Sparse (\(R_q \approx 1\)–2)</td> <td style="text-align: left">Baseline: typical RAG scenario</td> </tr> <tr> <td style="text-align: left"><strong>MS MARCO</strong></td> <td style="text-align: left">~8.8M passages</td> <td style="text-align: left">Sparse (\(R_q \approx 1\))</td> <td style="text-align: left">Large scale: does BoR work at production size?</td> </tr> <tr> <td style="text-align: left"><strong>20 Newsgroups</strong></td> <td style="text-align: left">11,314 docs (training set) class-based setup</td> <td style="text-align: left">Dense (\(\bar{R}_q \approx 572\))</td> <td style="text-align: left">Stress test: what happens when selectivity collapses?</td> </tr> </tbody> </table> <p>We tested two retrievers representing different eras and approaches.</p> <ul> <li><strong>BM25:</strong> The classic lexical baseline</li> <li><strong>SPLADE:</strong> Modern neural sparse retriever (<a href="https://huggingface.co/naver/splade-cocondenser-ensembledistil">naver/splade-cocondenser-ensembledistil</a>): document top-k = 60, query top-k = 60, max sequence length = 256, batch size = 64 for documents and queries</li> </ul> <p>All results use exact hypergeometric baselines and 95% confidence intervals from bootstrap resampling (n=5,000, seed=7).</p> <h3 id="test-1-scifact-the-benchmark-case">Test 1: SciFact (The Benchmark Case)</h3> <p>This is what most people expect: sparse relevance, the kind you see in real RAG systems.</p> <p><strong>The results:</strong></p> <p>Both systems maintain strong selectivity even at <strong>K=100</strong>, with BoR staying above 5 bits. Predicted ΔBoR values match observed changes to within <strong>0.01</strong> bits across all configurations.</p> <p>This confirms that when \(\lambda = \frac{K \cdot \bar{R}_q}{N} \ll 1\) (well outside the collapse zone), retrieval systems can demonstrate meaningful selectivity over random chance.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-bits-over-random/boR_analysis_scifact-480.webp 480w,/2026/assets/img/2026-04-27-bits-over-random/boR_analysis_scifact-800.webp 800w,/2026/assets/img/2026-04-27-bits-over-random/boR_analysis_scifact-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-bits-over-random/boR_analysis_scifact.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Figure 1:</strong> <em>BoR analysis on the SciFact dataset shows sustained selectivity across retrieval depths. Both BM25 and SPLADE maintain high BoR values (5–11 bits), reflecting the dataset’s sparse relevance structure.</em></p> <p>But both BM25 and SPLADE operate very close to the theoretical ceiling. A 30-year-old algorithm nearly matches the modern neural system.</p> <p>Is SciFact just too easy? To investigate, we turn to literature and examine a much larger benchmark. On a corpus with millions of passages, how much headroom exists between top-performing systems and the theoretical ceiling?</p> <h3 id="test-2-ms-marco-the-industrial-scale-test">Test 2: MS MARCO (The Industrial Scale Test)</h3> <p>8.84 million passages. This is where large real-world systems operate.</p> <p>We computed BoR for <strong>41 different systems</strong> from the literature, from lexical baselines to state-of-the-art neural retrievers.</p> <p>At <strong>K=1000</strong>, the theoretical ceiling is:</p> \[\text{BoR}_{\text{opt}} \approx \log_2\left(\frac{8.84\text{M}}{1000}\right) \approx 13.11 \text{ bits}\] <p><strong>All 41 systems cluster within 0.2 bits of this ceiling.</strong> Indicatively, to show the range:</p> <table> <thead> <tr> <th style="text-align: left">System</th> <th style="text-align: left">Recall@1000</th> <th style="text-align: left">BoR (bits)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">BM25</td> <td style="text-align: left">85.7%</td> <td style="text-align: left">12.89</td> </tr> <tr> <td style="text-align: left">SPLADE</td> <td style="text-align: left">97.9%</td> <td style="text-align: left">13.08</td> </tr> <tr> <td style="text-align: left">ColBERTv2</td> <td style="text-align: left">98.5%</td> <td style="text-align: left">13.09</td> </tr> <tr> <td style="text-align: left">SimLM</td> <td style="text-align: left">98.7%</td> <td style="text-align: left">13.09</td> </tr> </tbody> </table> <p>BM25 gets 85.7% recall. SimLM (state-of-the-art) gets 98.7% recall. That’s a <strong>13-point recall gap.</strong></p> <p>But the BoR difference? <strong>Only 0.20 bits.</strong></p> <p>A three-decade-old lexical algorithm and cutting-edge neural systems are very close in chance-corrected selectivity (BoR) at this depth, for this dataset, and success rule (in this case, recall). This suggests diminishing returns from retriever improvements alone.</p> <p>Systems examined include: SimLM, AR2, uniCOIL, ColBERTv2, SPLADE (multiple versions), I3 Retriever, TCT-ColBERTv2, RoDR w/ ANCE, DPR-CLS, ColBERTer, ANCE, SLIM/SLIM++, and BM25.</p> <p>But both still show meaningful selectivity: BoR is above 12 bits. To really see what collapse looks like, we need an extreme test: a dataset where relevance is abundant, not rare.</p> <h3 id="test-3-20-newsgroups-the-stress-test">Test 3: 20 Newsgroups (The Stress Test)</h3> <p>The 20 Newsgroups dataset has 20 topical categories. We set up an extreme scenario: treat all documents in the same category as “relevant.”</p> <p>With <strong>11,314</strong> documents split across <strong>20</strong> classes, that’s about \(\bar{R}_q \approx 572\) relevant documents per query (over <strong>5%</strong> of the corpus).</p> <p>Why test something so unrealistic? Because, as you’ll see later, this can happen in LLM agent tool selection.</p> <p>This scenario pushes us directly into the collapse zone. At <strong>K = 100</strong>:</p> \[\lambda = \frac{K \cdot \bar{R}_q}{N} = \frac{100 \times 572}{11{,}314} \approx 5.1\] <p>Random selection alone would succeed ~99% of the time. The ceiling for any retrieval system is essentially zero. To make the contrast as clear as possible, here is 20NG vs SciFact against both systems.</p> <p><strong>Watch what happens:</strong></p> <table> <thead> <tr> <th style="text-align: left">Dataset</th> <th style="text-align: left">K</th> <th style="text-align: left">BoR Ceiling</th> <th style="text-align: left">BM25 Success</th> <th style="text-align: left">BM25 BoR</th> <th style="text-align: left">SPLADE Success</th> <th style="text-align: left">SPLADE BoR</th> <th style="text-align: left">ΔBoR (10→100)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>20NG</strong></td> <td style="text-align: left">10</td> <td style="text-align: left">1.31 bits</td> <td style="text-align: left">94%</td> <td style="text-align: left">1.22</td> <td style="text-align: left">95%</td> <td style="text-align: left">1.23</td> <td style="text-align: left">−1.22</td> </tr> <tr> <td style="text-align: left"><strong>20NG</strong></td> <td style="text-align: left">100</td> <td style="text-align: left">0.01 bits</td> <td style="text-align: left">100%</td> <td style="text-align: left">0.01</td> <td style="text-align: left">100%</td> <td style="text-align: left">0.01</td> <td style="text-align: left">—</td> </tr> <tr> <td style="text-align: left"><em>SciFact</em></td> <td style="text-align: left">10</td> <td style="text-align: left">8.84 bits</td> <td style="text-align: left">80%</td> <td style="text-align: left">8.52</td> <td style="text-align: left">81%</td> <td style="text-align: left">8.53</td> <td style="text-align: left">−3.12</td> </tr> <tr> <td style="text-align: left"><em>SciFact</em></td> <td style="text-align: left">100</td> <td style="text-align: left">5.52 bits</td> <td style="text-align: left">89%</td> <td style="text-align: left">5.36</td> <td style="text-align: left">93%</td> <td style="text-align: left">5.41</td> <td style="text-align: left">—</td> </tr> </tbody> </table> <p>At K=100 on 20 Newsgroups:</p> <ul> <li>Both systems achieve <strong>100% success</strong></li> <li>Both provide <strong>0.01 bits of selectivity</strong></li> </ul> <p>Perfect success rate. Essentially zero selectivity. <strong>The ceiling has collapsed.</strong></p> <p>The predicted <strong>ΔBoR</strong> from theory matches reality within <strong>0.01</strong> bits. The math is working exactly as expected.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-bits-over-random/selectivity_collapse_paradox_newsgroups-480.webp 480w,/2026/assets/img/2026-04-27-bits-over-random/selectivity_collapse_paradox_newsgroups-800.webp 800w,/2026/assets/img/2026-04-27-bits-over-random/selectivity_collapse_paradox_newsgroups-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-bits-over-random/selectivity_collapse_paradox_newsgroups.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Figure 2:</strong> <em>The selectivity collapse paradox on 20 Newsgroups. Left: BoR declines sharply with depth, converging to the theoretical ceiling (dashed line). Right: As Success@K approaches 100%, BoR approaches zero.</em></p> <p>But here’s the real question: <strong>Does this theoretical collapse actually hurt downstream performance?</strong></p> <h2 id="when-perfect-success-fails">When Perfect Success Fails</h2> <p>We tested this directly with a modern instruction-tuned LLM on the 20 Newsgroups collapsed scenario.</p> <p>Setup: Multiple-choice classification task, 50 queries per configuration, temperature=0.0.</p> <p><strong>The results:</strong></p> <table> <thead> <tr> <th style="text-align: left">System</th> <th style="text-align: left">Accuracy at K=10</th> <th style="text-align: left">Accuracy at K=100</th> <th style="text-align: left">Success@K</th> <th style="text-align: left">Token Cost</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>BM25</strong></td> <td style="text-align: left">66%</td> <td style="text-align: left"><strong>50%</strong></td> <td style="text-align: left">94% → 100%</td> <td style="text-align: left">10x increase</td> </tr> <tr> <td style="text-align: left"><strong>SPLADE</strong></td> <td style="text-align: left">68%</td> <td style="text-align: left"><strong>58%</strong></td> <td style="text-align: left">95% → 100%</td> <td style="text-align: left">10x increase</td> </tr> </tbody> </table> <p>Read that again:</p> <ul> <li>Success rate increased to 100% ✓</li> <li>Accuracy <strong>dropped</strong> by 10–16 percentage points ✗</li> <li>Token cost increased 10x ✗</li> </ul> <p><strong>This is the failure mode BoR detects.</strong> You’re paying 10x the tokens for random-level selectivity, and your AI is drowning in noise.</p> <p>When selectivity collapses, high success rates become meaningless or worse, misleading.</p> <h2 id="ai-agent-tool-selection">AI Agent Tool Selection</h2> <p>“That 20 Newsgroups test seems artificial,” you might be thinking. “Who retrieves documents where 5% of the corpus is relevant?”</p> <p>Fair Point. Let’s extend our testing to what happens with AI agents everyday.</p> <h3 id="when-agents-choose-tools">When Agents Choose Tools</h3> <p>Consider what Anthropic published in 2025<d-cite key="anthropic2025toolselection"></d-cite>:</p> <p><em>“Tool definitions can sometimes consume 50,000+ tokens before an agent reads a request. Agents should discover and load tools on-demand, keeping only what’s relevant for the current task.”</em></p> <p>Their example: 58 tools consuming ~55K tokens. Add integrations like Jira and you’re at 100K+ tokens. They’ve seen setups with tool definitions consuming 134K tokens before optimization.</p> <p>Now, let’s apply the same math as document retrieval:</p> <table> <thead> <tr> <th style="text-align: left">Parameter</th> <th style="text-align: left">Document Retrieval</th> <th style="text-align: left">Tool Selection</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>N</strong></td> <td style="text-align: left">Corpus size (thousands to millions)</td> <td style="text-align: left">Available tools (50–500)</td> </tr> <tr> <td style="text-align: left"><strong>K</strong></td> <td style="text-align: left">Documents shown to LLM</td> <td style="text-align: left">Tools shown to LLM</td> </tr> <tr> <td style="text-align: left"><strong>\(R_q\)</strong></td> <td style="text-align: left">Relevant documents</td> <td style="text-align: left">Applicable tools for task</td> </tr> </tbody> </table> <p>The critical difference: <strong>N is small for tools.</strong> And small N means you hit the collapse boundary much faster.</p> <h3 id="the-tool-selection-collapse">The Tool Selection Collapse</h3> <p>Let’s run the numbers for Anthropic’s 58-tool example. Assume 3–5 tools are typically relevant:</p> <table> <thead> <tr> <th style="text-align: left">Configuration</th> <th style="text-align: left">K</th> <th style="text-align: left">\(R_q\)</th> <th style="text-align: left">\(\lambda = \frac{K \cdot R_q}{N}\)</th> <th style="text-align: left">\(\text{BoR}_{\text{max}}\) (Poisson)</th> <th style="text-align: left">\(\text{BoR}_{\text{max}}\) (Exact)</th> <th style="text-align: left">What This Means</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Show 5 tools</td> <td style="text-align: left">5</td> <td style="text-align: left">4</td> <td style="text-align: left">0.34</td> <td style="text-align: left">~1.6 bits</td> <td style="text-align: left">~1.7</td> <td style="text-align: left">Meaningful selectivity</td> </tr> <tr> <td style="text-align: left">Show 20 tools</td> <td style="text-align: left">20</td> <td style="text-align: left">4</td> <td style="text-align: left">1.38</td> <td style="text-align: left">~0.4 bits</td> <td style="text-align: left">0.28</td> <td style="text-align: left">Degraded</td> </tr> <tr> <td style="text-align: left">Show all 58</td> <td style="text-align: left">58</td> <td style="text-align: left">4</td> <td style="text-align: left"><strong>4.0</strong></td> <td style="text-align: left">~0.02 bits</td> <td style="text-align: left">0</td> <td style="text-align: left">Collapse</td> </tr> </tbody> </table> <p>When all tool definitions are introduced simultaneously into the model’s context, the system operates at \(\lambda \approx 4\). This is deep into the collapse zone.</p> <p><strong>Even a perfect tool selector achieves only ~0.02 bits of selectivity over random chance.</strong></p> <p>The LLM is essentially guessing. And as Anthropic notes: <strong>“The most common failures are wrong tool selection and incorrect parameters, especially when tools have similar names.”</strong> This perfectly reflects the 20 Newsgroups scenario when \(R_q\) (relevant-per query items) was large.</p> <h3 id="the-pattern-extends-beyond-tools">The Pattern Extends Beyond Tools</h3> <p>The collapse boundary doesn’t care what you’re selecting. It’s a property of the selection problem itself: \(\lambda = \frac{K \cdot \bar{R}_q}{N}\)</p> <p>When \(\lambda\) hits 3–5, selectivity collapses, whether you’re selecting:</p> <ul> <li>Documents from a corpus</li> <li>Tools from an API library</li> <li>Agentic “skills”</li> <li>Functions from hundreds of endpoints</li> <li>Context from multi-hop retrieval chains</li> </ul> <table> <thead> <tr> <th style="text-align: left">Scenario</th> <th style="text-align: left">N</th> <th style="text-align: left">\(R_q\)</th> <th style="text-align: left">K</th> <th style="text-align: left">\(\lambda = \frac{K \cdot R_q}{N}\)</th> <th style="text-align: left">Regime</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">RAG (typical)</td> <td style="text-align: left">10,000</td> <td style="text-align: left">1–2</td> <td style="text-align: left">10</td> <td style="text-align: left">~0.002</td> <td style="text-align: left">Healthy</td> </tr> <tr> <td style="text-align: left">Tool selection (filtered)</td> <td style="text-align: left">20</td> <td style="text-align: left">3</td> <td style="text-align: left">5</td> <td style="text-align: left">0.75</td> <td style="text-align: left">Healthy</td> </tr> <tr> <td style="text-align: left">Tool selection (show all)</td> <td style="text-align: left">20</td> <td style="text-align: left">3</td> <td style="text-align: left">20</td> <td style="text-align: left">3.0</td> <td style="text-align: left">Collapse</td> </tr> <tr> <td style="text-align: left">API endpoints (show half)</td> <td style="text-align: left">100</td> <td style="text-align: left">8</td> <td style="text-align: left">50</td> <td style="text-align: left">4.0</td> <td style="text-align: left">Collapse</td> </tr> <tr> <td style="text-align: left">Anthropic’s 58-tool example</td> <td style="text-align: left">58</td> <td style="text-align: left">4</td> <td style="text-align: left">58</td> <td style="text-align: left">4.0</td> <td style="text-align: left">Collapse</td> </tr> </tbody> </table> <p><strong>This is why agentic systems struggle with tool selection far more than RAG systems struggle with document retrieval.</strong> The math is unforgiving when N is small.</p> <h2 id="what-you-should-do-about-this">What You Should Do About This</h2> <p>BoR gives you a new lens for evaluating retrieval systems. It reveals when high success rates are actually warning signs.</p> <p><strong>1. Monitor the collapse boundary</strong></p> <p>Calculate \(\lambda = \frac{K \cdot \bar{R}_q}{N}\) for your system. When \(\lambda\) approaches 3–5, you’re entering the danger zone. This single number tells you whether selectivity is even possible.</p> <p><strong>2. Use BoR to guide your K selection</strong></p> <p>Don’t just crank up K to boost success metrics. Instead:</p> <ul> <li>Stop increasing K when \(\text{BoR}_{\text{max}}\) drops below ~0.1 bits.</li> <li>If \(\text{BoR} \approx \text{BoR}_{\text{max}}\), you’ve saturated and more K won’t help.</li> <li>If \(\Delta\text{BoR}\) becomes negative or negligible, you’re adding noise, not signal.</li> </ul> <p><strong>3. For tool-based agents: Be aggressive about filtering</strong></p> <p>With small N (50–500 tools), you can’t afford to dump everything into context. Use:</p> <ul> <li>Two-stage retrieval (filter, then select)</li> <li>Dynamic tool loading based on task context</li> <li>Clustering by function domain</li> </ul> <p><strong>4. Remember the core insight</strong></p> <p><strong>More context is not always better.</strong> High Success@K can coexist with zero selectivity.</p> <table> <thead> <tr> <th style="text-align: left">Scenario</th> <th style="text-align: left">Calculations</th> <th style="text-align: left">Conclusion</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>K → N</strong> (K tends to N)</td> <td style="text-align: left">N = 100, K = 100, \(R_q = 1\)<br/><br/>\(P_{\text{obs}} = 1.0\) (retrieve everything, guaranteed success)<br/><br/>\(P_{\text{rand}} = 1.0\) (random selection of all 100 items → also guaranteed success)<br/><br/>\(\text{BoR} = \log_2(1.0 / 1.0) = 0\) bits exactly</td> <td style="text-align: left"><strong>BoR → 0 when K → N</strong> (K is closer to N)<br/><br/>Both Recall and Success@K are perfect. But BoR approaches zero asymptotically.<br/><br/>At K = N, BoR = 0.</td> </tr> <tr> <td style="text-align: left"><strong>Bad retriever</strong> - deliberately omits relevant results</td> <td style="text-align: left">N = 100, K = 10, \(R_q = 1\)<br/><br/>\(P_{\text{rand}} = 10/100 = 0.10\) (random succeeds 10% of the time)<br/><br/>retriever is adversarially bad: \(P_{\text{obs}} = 0.05\)<br/><br/>\(\text{BoR} = \log_2(0.05 / 0.10) = \log_2(0.5) = -1\) bit</td> <td style="text-align: left"><strong>BoR &lt; 0</strong> means we are actively avoiding relevant documents, doing worse than chance.</td> </tr> </tbody> </table> <h2 id="sidebar-successk-vs-recallk">Sidebar: Success@K vs Recall@K</h2> <p>Some readers might wonder: this post focuses on Success@K (coverage), but what about Recall@K?</p> <table> <thead> <tr> <th style="text-align: left">Metric</th> <th style="text-align: left">What It Measures</th> <th style="text-align: left">Per-Query Behavior</th> <th style="text-align: left">Best For</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Success@K</strong></td> <td style="text-align: left">Did you get \(\geq 1\) relevant item?</td> <td style="text-align: left">Binary: success or fail</td> <td style="text-align: left">RAG/QA where one good context suffices</td> </tr> <tr> <td style="text-align: left"><strong>Recall@K</strong></td> <td style="text-align: left">What fraction of all relevant items did you get?</td> <td style="text-align: left">Graded: 0% to 100%</td> <td style="text-align: left">Tasks needing comprehensive coverage</td> </tr> </tbody> </table> <p>The good news: <strong>BoR works with both.</strong></p> <h3 id="bor-for-recallk">BoR for Recall@K</h3> <p>The same framework applies. Instead of measuring “probability of \(\geq 1\) hit,” you measure “expected fraction retrieved”:</p> \[\text{BoR}_{\text{recall@K}} = \log_2\left(\frac{\text{observed_recall@K}}{\text{expected_recall@K_random}}\right)\] <p>For sparse relevance: \(\text{expected_recall@K_random} \approx \frac{K}{N}\)</p> <p><strong>Example:</strong> A query has 10 relevant items in a 1,000-document corpus. You retrieve 4 in top-20:</p> <ul> <li>Observed recall = \(\frac{4}{10} = 0.4\)</li> <li>Random baseline = \(\frac{20}{1{,}000} = 0.02\)</li> <li>\(\text{BoR}_{\text{recall@K}} = \log_2\left(\frac{0.4}{0.02}\right) = \log_2(20) \approx\) <strong>4.32 bits</strong></li> </ul> <h3 id="math-and-bor-interpretation">Math and BoR Interpretation</h3> <table> <thead> <tr> <th style="text-align: left">Metric</th> <th style="text-align: left">Definition</th> <th style="text-align: left">Formula</th> <th style="text-align: left">Observed Rate</th> <th style="text-align: left">Expected Rate (Random)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>BoR for Success@K</strong></td> <td style="text-align: left">Bits-over-Random for coverage (\(\geq 1\) relevant)</td> <td style="text-align: left">\(\log_2\left(\frac{\text{observed_success}}{\text{expected_success_random}}\right)\)</td> <td style="text-align: left">Fraction of queries with \(\geq 1\) relevant in top-K</td> <td style="text-align: left">Probability of \(\geq 1\) hit by random selection</td> </tr> <tr> <td style="text-align: left"><strong>BoR for Recall@K</strong></td> <td style="text-align: left">Bits-over-Random for recall (fraction retrieved)</td> <td style="text-align: left">\(\log_2\left(\frac{\text{observed_recall@K}}{\text{expected_recall@K_random}}\right)\)</td> <td style="text-align: left">Average fraction of relevant items in top-K</td> <td style="text-align: left">Expected fraction if picking K random (usually \(\frac{K}{N}\))</td> </tr> </tbody> </table> <p>The depth-calibrated identity also extends to Recall@K, with minor adjustments for the different success rule.</p> <p>We focus on Success@K in this post because it matches the most common RAG use case: you just need <em>one</em> good grounding passage.</p> <h2 id="final-thoughts">Final Thoughts</h2> <p>Retrieval evaluation has been stuck with metrics designed for human consumers. RAG and agentic AI systems need something different, something that accounts for the fact that every retrieved item imposes a cost, and random chance sets a floor.</p> <p><strong>Bits-over-Random provides that measure.</strong></p> <p>It makes three things visible that were previously hidden:</p> <ol> <li><strong>The ceiling:</strong> Even perfect systems have limited selectivity when random baselines are high</li> <li><strong>The collapse zone:</strong> When \(\lambda = \frac{K \cdot \bar{R}_q}{N}\) reaches 3–5, selectivity becomes impossible</li> <li><strong>The depth trade-off:</strong> Retrieving more doesn’t always help and it can actively hurt</li> </ol> <p>The math is simple but the implications are profound.</p> <p>When your tool-based agent has 50 functions available, and you dump all 50 into context, you’re not being thorough, you’re operating in the collapse zone. BoR reveals that.</p> <p>When you boost Success@K from 95% to 100% by tripling K, traditional metrics celebrate. BoR shows you just lost 1.5 bits of selectivity.</p> <p>The systems that win in the next era of AI won’t be the ones that retrieve the most. They’ll be the ones that retrieve the most <strong>selectively</strong>.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[For most of the history of information retrieval (IR), search results were designed for human consumers who could scan, filter, and discard irrelevant information on their own. This shaped retrieval systems to optimize for finding and ranking more relevant documents, but not keeping results clean and minimal, as the human was the final filter. However, LLMs have changed that by lacking this filtering ability. To address this, we introduce Bits-over-Random (BoR), a chance-corrected measure of retrieval selectivity that reveals when high success rates mask random-level performance.]]></summary></entry><entry><title type="html">Content Promotion as a Strategic Game: How to Design Agentic Publishers for the Evolving Search Ecosystem in the GenAI Era?</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/content-promotion-agent-design/" rel="alternate" type="text/html" title="Content Promotion as a Strategic Game: How to Design Agentic Publishers for the Evolving Search Ecosystem in the GenAI Era?"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/content-promotion-agent-design</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/content-promotion-agent-design/"><![CDATA[<h2 id="tldr">TL;DR</h2> <p>LLMs are transforming search from static ranked lists of results into interactive, chat-like experiences. LLMs also reshape the way publishers (i.e., content creators) generate content: they increasingly use LLMs and agentic tools to optimize rankings in traditional search and to improve their content visibility in the novel chat-like systems. Importantly, publishers engage in an iterative process: they observe how their content performs — whether it is highly ranked in the traditional search or visible in chat-like answers — and then strategically modify or regenerate their content in response to this feedback. As a result, the ecosystem becomes a strategic game with incomplete information, evolving feedback, and repeated interactions. In this strategic ecosystem, the publishers act as players striving to promote their content, competing for rankings and visibility under mechanisms that are only partially known to them. This blog outlines why game theory offers a natural framework to model these settings and to design agents acting on behalf of publishers. We highlight several frameworks such as Game theory and Multi-agent Reinforcement learning and show how they contribute to model and design effective agents. Our aim is to provide principled lens for content promotion in the LLM era.</p> <h2 id="llms-reshape-both-sides-of-the-search-ecosystem">LLMs Reshape Both Sides of the Search Ecosystem</h2> <p>Large language models have fundamentally reshaped the search experience <d-cite key="metzler_rethinking_2021"></d-cite>. While users once typed short keyword queries and scanned a ranked list of links, modern systems now offer chat-like interfaces that feel more like interacting with an assistant than browsing a directory. These systems provide conversational question-answering (QA) capabilities by generating responses synthesized from multiple sources. Search is no longer about “links” — it is about responses. In the service of providing answers to questions, paradigms such as Retrieval Augmented Generate (RAG) <d-cite key="gao_retrieval_augmented_2024"></d-cite> have been developed. Similar paradigms were suggested for integrate commercial content in the answer <d-cite key="feizi_online_2024, mordo_sponsored_2024"></d-cite>. The new chat-like setting introduces new challenges, such as providing users with pointers to sources of information, leading to techniques known as source attribution <d-cite key="bohnet_attributed_2023"></d-cite>. Interestingly, effective retrieval has become a necessity in this new setting: augmenting an LLM with the ability to retrieve relevant information during answer generation or to retrieve the most relevant source to attribute to related pieces of information.</p> <p>But the transformation is not limited to the user side of the ecosystem.</p> <p>LLMs are also reshaping the publisher side. In the traditional Web era, publishers focused on keyword and hyperlink optimization, and content structure to influence ranking <d-cite key="gyongyi_web_2005"></d-cite>. Today, publishers increasingly rely on LLMs:</p> <ul> <li>to generate or refine content;</li> <li>to adapt their content to the behavior of LLM-driven retrieval and QA systems;</li> <li>to strategically shape their content so that they are selected, quoted, or incorporated into the ranked list of results or the generated answer.</li> </ul> <p>The paradigm of helping publishers in improving their content visibility in generative engine responses is often called Generative Engine Optimization (GEO) <d-cite key="aggarwal_geo_2024"></d-cite>.</p> <h2 id="publisher-incentives-across-coexisting-ecosystems">Publisher Incentives Across Coexisting Ecosystems</h2> <p>The search ecosystem is not transitioning overnight — it is evolving. Today, publishers operate in a hybrid world where traditional search engines and LLM-driven chat interfaces coexist. Users still issue keyword queries and click ranked results, yet they also increasingly engage with conversational systems that generate responses. As a result, publishers must navigate two overlapping incentive structures at once.</p> <h3 id="publishers-incentives-in-the-traditional-setting">Publishers’ Incentives in the Traditional Setting</h3> <p>In the traditional setting, publisher goals are well-defined and widely understood. Visibility depends on ranking, which in turn hinges on factors, some of which are familiar:</p> <ul> <li>content relevance;</li> <li>content quality and structure;</li> <li>signals (e.g., PageRank <d-cite key="page_pagerank_1999"></d-cite>).</li> </ul> <p>Metrics such as impressions and CTR (Click Through Rate) offer interpretable objectives.</p> <h3 id="publishers-incentives-in-the-chat-like-settings">Publishers’ Incentives in the Chat-Like Settings</h3> <p>By contrast, the objectives in the chat-like settings are far less defined. Publishers know that visibility is tied to whether the chat-like setting:</p> <ul> <li>retrieves their content;</li> <li>cites or incorporates it into the generated response;</li> <li>and positions it prominently in the generated response.</li> </ul> <p>But unlike in traditional search, where there is a clear notion of a ranked results page, LLM-based systems provide no public ranking, and often little transparency about how sources are selected for retrieval or attribution.</p> <p>The publisher’s utility is therefore inherently vague: Is success defined by:</p> <ul> <li>citation count?</li> <li>contribution to the generated answer?</li> <li>similarity of the generated answer to the publisher’s content?</li> <li>some combination of the above?</li> </ul> <p>The field has not yet converged on a clear success criterion, and the objective itself is still ill-defined, making strategic optimization an open problem.</p> <h2 id="leveraging-llms-for-content-optimization">Leveraging LLMs for Content Optimization</h2> <p>Publishers — or agents acting on behalf of them — can leverage LLMs as powerful content optimization tools (See Figures <a href="#fig1">1</a> and <a href="#fig2">2</a>). The publishers are incentivized to have effective automatic generation to promote their content. However, any such agentic systems should be carefully designed:</p> <ul> <li>address the tradeoff between “content promotion” (e.g., being selected in the search results or being visible in the generated response) and “faithfulness” to the original intent (e.g., not deviating from the content before optimization);</li> <li>address the uncertainty about the ranker or the content selection/generation mechanism;</li> <li>address competing agents which aim to optimize contents of other stakeholders.</li> </ul> <div class="row mt-3"> <div class="col-sm-12" id="fig1"> <figure> <img src="/2026/assets/img/2026-04-27-content-promotion-agent-design/banana-search.jpg" class="img-fluid rounded z-depth-1" alt="Content optimization in a traditional search setting"/> <figcaption class="caption" style="color: inherit;"> <strong>Figure 1:</strong> Content optimization in a traditional search setting. </figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm-12" id="fig2"> <figure> <img src="/2026/assets/img/2026-04-27-content-promotion-agent-design/banana-chat-fixed-with-gpt.png" class="img-fluid rounded z-depth-1" alt="Content optimization in a chat-like setting"/> <figcaption class="caption" style="color: inherit;"> <strong>Figure 2:</strong> Content optimization in a chat-like setting. </figcaption> </figure> </div> </div> <h2 id="content-promotion-as-a-game">Content Promotion as a Game</h2> <p>We argue that game theory provides a major and essential framework for analyzing the resulting competitive dynamics and for designing agents that assist publishers to strategically promote their content. More precisely, repeated games with incomplete information <d-cite key="aumann_repeated_1995"></d-cite>, induced by the system (search or chat-like system). Actions correspond to content generation (or modification). Strategies are history dependent content selection/generation algorithms and strategy profiles capture joint activity of agents which are active in the ecosystem. The utility function of an agent is the accumulated payoff along time, taking into account aspects such as ranking (for traditional search), visibility (for the chat-like systems) and faithfulness to the original content before any modification. As the selection/generation mechanism is only partially known, the setting is of incomplete information. Since the publisher may keep improving the content once she receives feedback from the system, we argue that the game should be treated as a repeated game. Figures <a href="#fig3">3</a> and <a href="#fig4">4</a> present schematic illustrations of the resulting games for search systems and chat-like systems, respectively.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0" id="fig3"> <figure> <img src="/2026/assets/img/2026-04-27-content-promotion-agent-design/search-schema-nbg.png" class="img-fluid rounded z-depth-1" alt="Schematic illustration of a traditional search game"/> <figcaption class="caption text-center" style="color: inherit;"> <strong>Figure 3:</strong> Schematic illustration of a traditional search game. </figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0" id="fig4"> <figure> <img src="/2026/assets/img/2026-04-27-content-promotion-agent-design/chat-schema-nbg.png" class="img-fluid rounded z-depth-1" alt="Schematic illustration of a chat-like game"/> <figcaption class="caption text-center" style="color: inherit;"> <strong>Figure 4:</strong> Schematic illustration of a chat-like game. </figcaption> </figure> </div> </div> <p>The viewpoint just presented is aligned with a growing body of research that treats content promotion as a strategic process. During recent years, several works have applied game-theoretic approaches to model competitive search <d-cite key="kurland_competitive_2022"></d-cite>, diversity-based ranking <d-cite key="mordo_ameliorating_2025"></d-cite>, to study the convergence of long term behavior to equilibrium in retrieval settings <d-cite key="madmon_search_2024, madmon_convergence_2024"></d-cite> and to design equilibrium in ranking games using corpus enrichment <d-cite key="nachimovsky_power_2025"></d-cite>. <d-cite key="ye_llm_empowered_2025"></d-cite> explored strategic document modifications in response to ranking feedback, revealing rich and sometimes counterintuitive dynamics. Yao et al.’s work provides deep, game-theoretic insight into how creators behave strategically in recommender systems and offers actionable designs for systems to align creator incentives with user welfare <d-cite key="yao_how_2023, yao_user_2024, yao_human_2024"></d-cite>. In the context of strategic retrieval systems, <d-cite key="mordo_sponsored_2024"></d-cite> presented the first formal game-theoretic analysis of a sponsored QA system where advertisers bid to have their sponsored content fused in the generated answer of question-answering systems. <d-cite key="mordo_sponsored_2024"></d-cite> showed how this new ad-integration model introduces strategic bidding behavior and derived equilibrium properties and welfare implications in this setting.</p> <p>However, the technological transformation of the last few years — particularly the rise of LLM-based chat interfaces — calls for re-examination and adaptation of the assumptions in classical game theoretic models. Traditional work often assumes fixed ranking functions and explicit publishers’ incentives. In contrast, chat-like systems generate personalized and dynamic answers <d-cite key="metzler_rethinking_2021"></d-cite>; the objective of publishers becomes much harder to define. These shifts motivate the development of new models that account for generative selection mechanisms, opaque feedback, and high adaptivity — all of which strengthen the case for game-theoretic reasoning and for new solution concepts.</p> <h2 id="agent-design-aspects-for-traditional-search-and-chat-like-systems">Agent Design Aspects for Traditional Search and Chat-like Systems</h2> <p>We delve into three important aspects that should be considered when designing agents that assist publishers promote their content.</p> <p>The first ingredient that can (and will) be naturally used, is to exploit the significant power of LLMs, e.g., use clever prompt engineering to promote content and maintain faithfulness in such games.</p> <p>A second ingredient one should be aware of is incomplete information about the selection or generation mechanism applied by the system. Using, for example, information about the decision made by the mechanism (e.g., past responses) can help to train and fine tune agents.</p> <p>The third ingredient is the need to be aware of competitors’ behavior. With limited access to annotated data — and only partial observability of the decisions made by these systems — this calls for carefully designed simulation tools. here again LLMs can be used as effective generative tools.</p> <p>Heretofore, we have described content promotion as a publishers game, shaped by evolving search and chat-based ecosystems and by the increasing role of generative AI. We now discuss different frameworks that can be employed to model and design effective agents in such settings.</p> <h2 id="frameworks">Frameworks</h2> <h3 id="multi-agent-reinforcement-learning-marl">Multi-Agent Reinforcement Learning (MARL)</h3> <p>This framework models each player as an autonomous learning agent that optimizes its policy through repeated interaction with the environment and with other agents. MARL enables us to study how strategic behaviors emerge end-to-end from experience, without imposing strong assumptions about how these systems (i.e., search or chat-like systems) operate internally <d-cite key="gemp_developing_2022, guo_large_2024, shoham_if_2025, zhou_multi_agent_2025"></d-cite>. It also allows us to analyze coordination, competition, and non- stationarity — core properties of competitive games under a principled RL formulation. In our setting, the policy of the agent is the LLM itself, and reinforcement learning is the mechanism that aligns this LLM to the objectives of the game, given whatever information the agent can observe; different RL alignment algorithms may be adopted; e.g., <d-cite key="schulman_proximal_2017, rafailov_direct_2024, calandriello_multi-turn_2024, zhou_wpo_2024, shao_deepseekmath_2024"></d-cite>. If we had access to a full log of games — for example, historical queries and the ranking of pages in traditional search, or user questions and the resulting responses in chat-like systems — then we could directly formulate a supervised or RL-based learning problem. With such logs, the agent could be trained to optimize performance with respect to well-defined metrics (e.g., rank position, click-through, user engagement, rejection rates, etc.). Beyond the standard trade-offs encountered in reinforcement learning, our settings introduce unique design tensions that must be addressed:</p> <h4 id="a-short-term-wins-vs-long-term-information-leakage">(a) Short-term wins vs. long-term information leakage</h4> <p>An agent may be able to win an individual round of a game by aggressively optimizing its content. However, doing so may reveal its strategy to competing agents, enabling them to quickly imitate <d-cite key="raifer_information_2017"></d-cite> or counter it. Since the objective is to maximize cumulative reward across many rounds, the agent must balance immediate gains with strategic opacity — sometimes deliberately avoiding overly distinctive or extreme actions to prevent being copied <d-cite key="aumann_repeated_1995"></d-cite>.</p> <h4 id="b-on-policy-vs-off-policy-training">(b) On-policy vs. off-policy training</h4> <p>A crucial design question is whether the agent should continue learning during the game (on-policy) or rely solely on offline training before the game begins (off-policy).</p> <ul> <li><strong>Online adaptation</strong> allows the agent to track shifting competitor behaviors or evolving ranking/generation mechanisms.</li> <li><strong>Offline training</strong> offers stability and avoids overfitting to short-term noise or adversarial manipulations. Choosing between online and offline modes — or combining them — depends on the environment’s stability, observability, and the computational budget during gameplay.</li> </ul> <h4 id="c-constructing-the-loss-function">(c) Constructing the loss function</h4> <p>A core challenge is designing a loss function that balances ranking promotion or content visibility with faithfulness to the original content. On one hand, the agent should modify content to improve performance; on the other hand, excessive deviation risks harming credibility, violating constraints, or triggering penalties. The loss function must encode this balance explicitly — penalizing harmful drift while incentivizing strategic, beneficial content modifications.</p> <h3 id="synthetic-dataset-generation">Synthetic Dataset Generation</h3> <p>Sometimes real-world data presents substantial limitations: it may be inaccessible, noisy, incomplete, or simply ill-suited for the specific questions we wish to study. For this reason, we propose constructing synthetic datasets with controlled statistical properties. Synthetic data enables us to systematically probe targeted hypotheses — for example, how diversity constraints, noise structure, or shifts in the query or question distribution influence agent behavior and system-level outcomes. Moreover, synthetic datasets provide repeatability and experimental control that are often impossible with logs of production systems. They allow for clean ablation studies, controlled perturbations, and the isolation of causal factors, all of which are essential for understanding and improving agents operating in competitive ranking and chat-like ecosystems. Importantly LLMs now make it feasible to generate high-fidelity synthetic datasets at scale, enabling richer and more flexible experimental environments than were previously possible.</p> <h3 id="simulation-environment">Simulation Environment</h3> <p>A simulation environment that models the rules of the game, the ranking or generation function, and the interaction protocol between agents is essential in the discussed strategic ecosystems <d-cite key="kovarik_game_2024"></d-cite>. Such simulation can be used for generating strategic synthetic training datasets and for evaluating agents under diverse conditions. Together with the increasing proliferation over the Web of generated AI content <d-cite key="spennemann_delving_2025"></d-cite>, we suggest using simulators of the discussed competitions. Such simulators support rapid iteration and controlled experimentation across a wide range of configurations — rankers, prompts, datasets, reward structures, and hyperparameters. By isolating specific components of the system, the simulation environment helps uncover causal relationships, identify sensitivities to design choices, and systematically analyze how agents behave under different competitive scenarios.</p> <h3 id="self-play">Self-Play</h3> <p>Self-play is a method for iteratively training agents to be improved by competing (via simulation) against evolving versions of themselves <d-cite key="fang_serl_2025, fu_improving_2023, zhang_survey_2025"></d-cite>. This approach is particularly useful in settings where optimal strategies are not known a priori and where the competitive landscape shifts over time. Self-play helps uncover equilibrium behaviors, stability properties, and potential failure modes of learning algorithms in adversarial or co-adaptive settings.</p> <h3 id="meta-games-and-learning-equilibrium">Meta Games and Learning Equilibrium</h3> <p>Agent design itself introduces an additional strategic layer: designers may choose between different LLM architectures, training procedures, datasets, prompting strategies, or RL frameworks. In this broader perspective, agents are not only competing within a single ranking or generation game, but the designers are competing in a meta-game over agent designs <d-cite key="li_meta_game_2024, nachimovsky_multi_agent_2025, shapira_glee_2025"></d-cite>. Long-run success therefore depends on performing well not only within the fixed game, but also across the distribution of possible design choices made by other participants. In such meta-games, no single deterministic design may be universally optimal.</p> <p>Instead, probabilistic ensembles of designs — mixtures over algorithms, prompting strategies, or training regimes — may provide better robustness and theoretical guarantees. These ensemble-based strategies reduce exploitability, adapt better to heterogeneous opponents, and align with classical results in game theory showing that mixed strategies often outperform pure ones in competitive environments.</p> <p>Within meta-games over agent designs, a natural solution concept is learning equilibrium <d-cite key="monderer_learning_2007"></d-cite>. Rather than focusing on equilibria of the underlying content-generation game — where actions correspond to the content produced — learning equilibrium concerns the design algorithms themselves being in equilibrium. That is, each designer’s strategy for constructing agents is optimal given the strategies of other designers, forming a stable configuration at the meta-level.</p> <p>This distinction is subtle but important: learning equilibrium ensures that agent designs collectively converge to a stable distribution, even as the content-generation game continues to evolve. Such a concept becomes particularly relevant in scenarios where a platform or cloud provider offers content-generation services to multiple competing parties. In this setting, providing agent designs that satisfy learning equilibrium naturally mitigates exploitability, promotes fairness, and guarantees robust performance across heterogeneous competitors.</p> <h2 id="preliminary-results">Preliminary Results</h2> <p>We now turn to discuss several papers that illustrate components (or the absence thereof) of the frameworks discussed above.</p> <h3 id="prompt-based-agent">Prompt-based Agent</h3> <p> <d-cite key="bardas_automatic_2025"></d-cite> introduced a set of prompt-based agents designed to modify a human-authored document so that the updated version is ranked higher in the next round of the ranking. Their prompting strategy explicitly constrains the agent to produce revisions that remain faithful — at least to some extent — to the original document. <d-cite key="bardas_automatic_2025"></d-cite> introduced prompt- engineering techniques that, in an offline evaluation, outperformed human-authored documents without requiring any learning. Their evaluation was conducted using data from a ranking competition previously organized among human document authors.</p> <h3 id="rlrf-based-agent">RLRF-based Agent</h3> <p> <d-cite key="mordo_rlrf_2025"></d-cite> proposed a ranker-aligned agent, trained via a reinforcement- learning-from-ranker-feedback (RLRF) framework. They constructed a synthetic dataset generated through a simulated ranking competition and used it to train the agent with the DPO algorithm <d-cite key="rafailov_direct_2024"></d-cite>. Notably, the resulting agent learned to adapt both to the ranking mechanism and to the behavior of competing agents. Their simulation-based evaluation demonstrates robustness with respect to various environmental conditions, including the number of competitors and the choice of ranking function. The simulation assumed that the competing agents are nonoptimized agents; this setup can be viewed as a variant of self-play, where agents iteratively interact under stable, nonlearning strategies.</p> <h3 id="lemss">LEMSS</h3> <p> <d-cite key="mordo_lemss_2025" block=""></d-cite> also introduced LEMSS, a large-scale simulation framework for ranking competitions among LLM-based agents. Their results show that LLM-driven agents exhibit behavioral patterns like those of humans in iterative ranking games. LEMSS provides fine-grained control over nearly all aspects of the competition — such as the number of competitors, the ranking mechanism, and the reward structure — and enables comprehensive analysis of the induced repeated game. The simulation models a traditional ranking competition. To design agents capable of promoting content in a chat-like interaction setting, a simulation of such a mechanism is called for.</p> <h3 id="attacking-rag-based-systems">Attacking RAG-based systems</h3> <p>Recent work highlights the growing vulnerability of retrieval-augmented generation (RAG) systems to targeted manipulation. PoisonedRAG <d-cite key="zou_poisonedrag_2024"></d-cite> shows that inserting only a few malicious documents into a RAG system’s knowledge base can steer model outputs with high success, revealing a potent “knowledge-corruption” attack surface. In contrast, RIPRAG <d-cite key="xi_riprag_2025"></d-cite> use reinforcement learning to craft effective poisoned documents without access to the system’s internals. <d-cite key="wu_what_2025"></d-cite> suggests to cooperatively optimize content for generative search engines by learning their preference rules. However, all these papers overlook the competitive nature of real-world environments, where multiple strategic content creators adaptively improve their content — potentially using agents — in a repeated game. This missing game-theoretic perspective limits their ability to capture the dynamics and equilibria that emerge when many actors simultaneously attempt to influence RAG- based systems.</p> <h2 id="tools">Tools</h2> <p>Designing agents for ranking and content promotion competition benefits from a growing ecosystem of LLM-oriented development tools. Copilot Studio, Vertex AI, and LangChain provide the infrastructure needed to orchestrate complex LLM workflows, connect models to external data, and manage iterative agent-environment interactions. Copilot Studio enables rapid prototyping of agent behaviors within controlled interfaces, while Vertex AI offers scalable model hosting, fine-tuning, and evaluation pipelines suited for experimentation at scale. LangChain adds modular abstractions — such as memory, tools, retrieval, and multi- step reasoning — that make it easier to construct agents capable of interacting with dynamic environments. In addition, there are many python packages that may help to design and simulate RL-based agents such as <d-cite key="feng_agile_2024, liu_agentlite_2024"></d-cite>.</p> <h2 id="conclusion">Conclusion</h2> <p>In this blog post, we discussed how LLMs are reshaping the search ecosystem — both in traditional search and in emerging chat-like question-answering systems. Beyond transforming how users search, LLMs also change how publishers operate, enabling them to leverage LLMs to strategically optimize and promote iteratively their content.</p> <p>We discussed several frameworks for modelling and designing such agents, including game- theoretic models and reinforcement learning approaches. These frameworks help account for multiple layers of uncertainty: the opaque ranking model or response-generation mechanisms of retrieval systems, the dynamic nature of evolving user queries/questions, and the competitive behavior of other strategic publishers or human content creators.</p> <p>To build robust and adaptable agents, we emphasized that simulation plays a crucial role. Simulations enable the creation of strategic synthetic datasets and provide controlled environments for evaluating agent behavior, stress-testing policy robustness, and studying long-term dynamics in repeated competitive scenarios.</p> <p>Overall, as the ecosystem continues to evolve, combining insights from game theory, reinforcement learning, and simulation offers a principled path toward designing agents capable of succeeding in both current and future search paradigms.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[With the rise of LLMs, publishers now operate in a dual world where traditional search and chat-like systems coexist. We propose a unified, game-theoretic view of this environment and highlight different tools, such as Multi-Agent Reinforcement Learning, that support the development of competitive content-optimization agents.]]></summary></entry><entry><title type="html">Destruction is a General Strategy to Learn Generation; Diffusion’s Strength is to Take it Seriously; Exploration is the Future</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/destruction/" rel="alternate" type="text/html" title="Destruction is a General Strategy to Learn Generation; Diffusion’s Strength is to Take it Seriously; Exploration is the Future"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/destruction</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/destruction/"><![CDATA[<blockquote> <p>Pourquoi faire simple quand on peut faire compliqué?</p> <p>–French locution, usually ironic.</p> </blockquote> <p>This blogpost is composed of two main sections, tied together by their unusual information-theoretic viewpoint. <a href="#the-thesis">The Thesis</a> is an opinion/perspective/speculative piece on diffusion models. <a href="#the-tutorial">The Tutorial</a> is a diagrammatic presentation of diffusion models.</p> <h1 id="the-thesis">The Thesis</h1> <h2 id="learning-by-destroying">Learning by Destroying</h2> <p>Yang <em>et al.</em><d-cite key="yang2023diffusion"></d-cite> defined generative diffusion models as “a family of probabilistic generative models that progressively destruct data by injecting noise, then learn to reverse this process for sample generation.” I think that it is a good definition, except for the “by injecting noise” part. Indeed, there is a paper that is literally called <em>Cold Diffusion: Inverting Arbitrary Image Transforms <strong>Without Noise</strong></em><d-cite key="bansal2023cold"></d-cite> (emphasis mine), and calling “noise” the action of the <code class="language-plaintext highlighter-rouge">[MASK]</code> token in Mask Diffusion Language Models (MDLMs)<d-cite key="sahoo2024simple"></d-cite> is certainly debatable. For the sake of this blogpost, I will define generative diffusion models as follows.</p> <ol> <li> <p><strong>Data distribution.</strong> We have a training dataset sampled from a data distribution, and our goal is to generate new samples from the same data distribution. The only allowed “control mechanism” is <em>conditioning</em>.<d-footnote>Concretely, the condition can be specified as a predicate: a function accepting a data sample and returning either "true" or "false". When such a condition is provided, the model must only return samples for which the predicate returns "true", otherwise keeping the data distribution unaltered. Notice that this could be achieved by wrapping the model in a loop, and returning the first samples that passes the predicate check.</d-footnote></p> </li> <li> <p><strong>Destroying process.</strong> We also have access to a procedure that can gradually destroy the information in data samples. This procedure <strong>may</strong> involve randomness. Like the training dataset, this procedure if fully specified <em>a priori</em>, before any training takes place.</p> </li> <li> <p><strong>Generative process.</strong> We train a generative process so that the samples it generates approximate the sought data distribution. This training leverages both the training dataset, which specifies <em>what</em> the generative process should ultimately generate, and the destroying process, which specifies <em>how</em> to chunk generation into manageable pieces.<d-footnote>The loss function being minimized is typically an expectation of an expectation of an expectation, with those expectations respectively taken over the training data sample, the "level of destruction" to be applied on this training data sample, and the exact stochastic realization of this destruction (if applicable).</d-footnote></p> </li> </ol> <p>Using this definition, diffusion models can be seen as a special case of the tried-and-true machine learning technique consisting of withholding some information from a model’s input, then training the model to generate the withheld information.</p> <ul> <li>In supervised learning, the withheld (then generated) information is called “label”.</li> <li>In an autoregressive language model, it is the next token.</li> <li>For BERT-like language models, it is the <code class="language-plaintext highlighter-rouge">[MASK]</code>ed tokens.</li> <li>Some autoencoders rely on a representational bottleneck to withhold information.</li> <li>Contrastive autoencoders withhold whether pairs of samples are somehow related (positive pairs) or not (negative pairs).</li> <li>Denoising autoencoders add “noise” to samples, which destroys some of their informational content (i.e., withholding it).</li> <li>Diffusion models use the destroying process to withhold information.<d-footnote>This typically involves some form of randomness, but here we do not make it a requirement.</d-footnote></li> </ul> <p>You may notice a connection between the last two: that connection has already been made long ago.<d-cite key="kingma2021variational"></d-cite> In fact, such connections have been made between diffusion and other points in that list, <d-cite key="zheng2025masked,fathi2025unifying"></d-cite> and I think that, given enough coffee, we could get them all covered. <em>Should we</em>? That’s a good question, and the answer may depend in part on how data-starved we are.</p> <h2 id="diffusions-advantages">Diffusion’s Advantages</h2> <p>In <em>Diffusion Language Models are Super Data Learners</em>, Ni <em>et al.</em><d-cite key="ni2025diffusion"></d-cite> observe that</p> <blockquote> <p>when unique data is limited, diffusion language models (DLMs) consistently surpass autoregressive (AR) models by training for more epochs. The crossover shifts later with more or higher-quality data, earlier with larger models, and persists across dense and sparse architectures. We attribute the gains to three compounding factors: (1) any-order modeling, (2) super-dense compute from iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation […]</p> </blockquote> <p>I think that this makes a lot of sense, and that <strong>this phenomenon is likely general</strong>. We have devised simple (non-diffusion) ways to withhold information from our models, breaking the data into chunks that “make sense” to us, emphasizing aspects that must obviously be learned sooner or later. And when we train models to generate that withheld information, these simple approaches turn out to be locally optimal for the sake of training as quickly as possible on humongous datasets. But when the fresh data gets scarce, when you have to train from the same sample for the $N^{\text{th}}$ time, perhaps more could be learned by considering a different viewpoint – like a diffusion model learning to predict tokens out-of-order. To be clear, ultimately, it could be that the diffusion model will perform better during inference when it is used to predict tokens in order, like an autoregressive model would simply do.<d-cite key="kim2025train"></d-cite> But because it was <em>challenged</em> during training on out-of-order tasks, it may eventually manage to pick up some tricks that will forever evade the autoregressively-trained model. Bigger models have more capacity to latch on such tricks, so their crossover may come earlier.</p> <p>Depending on your perspective, this may align with Sutton’s <em>The Bitter Lesson</em>:<d-cite key="sutton2019bitter"></d-cite></p> <blockquote> <p>general methods that leverage computation are ultimately the most effective, and by a large margin. […] Seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation.</p> </blockquote> <p>Left-to-right training may have been yet another human bias, a temporary fad, and in the long run perhaps we’ll just train in all-the-orders, leveraging much more compute. But there is a sweeter perspective on that bitter lesson: the design space for novel diffusion models is huge. To this day, very few destroying processes have been considered. For example, <code class="language-plaintext highlighter-rouge">[MASK]</code> is a great candidate for the next human fad/bias to be replaced by something that better leverages compute. Moreover, while diffusion may beat our vanilla information-withholding approaches in the bounded data regime, it hasn’t been shown to be the best way to proceed: diffusion itself may be a weird human fad.</p> <h2 id="reconciling-exploration">Reconciling Exploration</h2> <p>This takes me to the last point of this blogpost’s thesis: if all incarnations of the tried-and-true “information withholding” machine learning technique can (and perhaps should) be related to diffusion, what other techniques are left? And could we improve upon diffusion by learning from them? The first answers that come to my mind are “anything that involves exploration,” and “yes, probably.”</p> <p>The archetypal technique involving exploration is Reinforcement Learning (RL). Whereas pure generative models solely learn from a training dataset – striving to generate new samples from the exact same data distribution – RL models strive for a different, “better” distribution. What is meant by “better” is usually (but not always) specified through a reward function: a function that assigns value (reward) to each possible data sample. We typically wish to maximize the expected reward, though there may be some requirement to not meander too far away from the original data distribution.</p> <p>GRPO<d-cite key="shao2024deepseekmath"></d-cite> is a quite successful RL technique, and it has recently been ported to diffusion.<d-cite key="gong2025diffucoder"></d-cite> Such an adaptation is not trivial: due to its autoregressive origin, GRPO takes for granted easy access to likelihoods for different sequences, but such likelihoods are harder to get in diffusion models. This issue has been overcome before in the context of perplexity<d-cite key="sahoo2024simple"></d-cite>: take expectations over different levels of destruction and over different destruction realization, which translates to different decoding orders. Gong <em>et al.</em><d-cite key="gong2025diffucoder"></d-cite> leverage this technique,<d-footnote>With the twist that when a decoding order is used in the estimation of this expectation, the "complement" of the mask associated to that decoding order is always also considered. This practice reduces the variance, allowing them to take expectations over a single such pair of orders.</d-footnote> and their results are great!</p> <p>Yet I suspect that this expectation-over-decoding-order strategy is inherently off-policy. Indeed, even for purely random decoding orders,<d-footnote>To be clear, I definitely believe that it would be a good idea to learn a policy for the decoding order. I'm here assuming a non-learnable random order to make my point stronger.</d-footnote> I claim that the specific order faced by the model while generating a trace should be accounted for in the reward assignment for that trace. My intuition goes as follows: if a successful reasoning trace summarizes some premises from the context, expands some methodic steps, then reaches some conclusion, what are we teaching the model by rewarding it to predict the conclusion first, without the reasoning steps that lead to it?<d-footnote>My answer: at best we're teaching it to skip steps and/or ignore reasoning, at worst we're teaching it to hallucinate, make stuff up, justify *a posteriori* and/or otherwise deceive.</d-footnote></p> <p>You may now think “Who are you to say how the model should or shouldn’t reason! Remember The Bitter Lesson!” Fair enough, but here’s my point: in the non-RL diffusion case, this expectation-over-decoding-order approach was grounded in sound theory, but we didn’t do our homework before porting it to RL. We can justify it by its good empirical results, but we lost our theoretical grounding. <em>A priori</em>, the only path we may reward on-policy for a given generated sample is the one that was followed by the model while generating that sample.</p> <p>Ok, how did we get that theoretical grounding in the non-RL case? Limiting ourselves to my strict definition at the beginning of this blogpost, we consider a specific data distribution and noising process, and there thus exists a single, ideal, typically untractable<d-footnote>This is why we have to train a neural network: we're learning a tractable function that approximates the untractable ideal one.</d-footnote> probability distribution for partially-destroyed data samples at different levels of destruction. Within a given modeling paradigm, all concrete diffusion model implementations seek to approximate some function of that same ideal distribution: models with different weight initialization (or even different neural network architectures!) all strive to approximate the same “correct” answer. This is a very convenient property: in a diffusion model (as per my definition), the function to be learned is not a moving target.</p> <p>Can we approach the RL problem with a diffusion model that satisfies this strict definition? Yes! By reframing it as conditioning,<d-cite key="yuan2023reward"></d-cite> which is the sole allowed control mechanism as per my definition. For now, work on that front is still in an early stage, and competition leveraging techniques ported from the autoregressive case (e.g., GRPO) have an head start. Moreover, there is no guarantee that approaching RL by sticking to my strict definition of diffusion has real advantages in the long run. Nonetheless, I think that the fact we <em>can</em> suffices to justify additional exploratory efforts.</p> <p>More generally, notice how reframing as a conditioning problem removed “exploration” from the picture: we’re not <em>searching</em> for high-reward samples, we’re just <em>filtering out</em> from the original data distribution the samples that have low reward. In practice, because we don’t have infinite resources, actual implementations still have to explore to find where it is worth it to learn the function. We have the guaranteed existence of a non-moving target function, but we have to find which parts of that target function are worth learning well.<d-footnote>Recall that "standard" RL models are often constrained to not meander too far away from the original data distribution. Concretely, this is usually implemented by comparing the predictions of a frozen, "old" model with those of its RLed counterpart. In the conditioned diffusion approach to RL, the non-moving target function plays the role of that old model.</d-footnote></p> <p>The astute readers may notice that we’ve been infringing on a second taboo of my strict definition: the destroying process is <em>given</em> (not learned!) before any training takes place, and it specifies what/how the model should learn. Again, there are workarounds: we may tweak how we sample the destruction process while maintaining the same nice, non-moving target.<d-footnote>For example, we may counterbalance frequency biases by inversely weighting the losses.</d-footnote> But is there a point where we could gain something by giving up that nice, non-moving target?</p> <p>Of course! We move that target every time a researcher comes up with a “better” destroying process, and there is no good reason to believe that this human-in-the-loop algorithm has already found the optimum. The data should speak for itself! The compute should be leveraged! Umh, how?</p> <p>Let me greatly simplify the problem.</p> <ol> <li> <p><strong>First distribution.</strong> We have a data distribution from which we can get a training dataset.</p> </li> <li> <p><strong>Second distribution.</strong> We have a destroying process whose specifics depends on parameters to be determined. However, whatever these parameters are, we already know what the probability distribution for a “fully destroyed” data distribution should look like.<d-footnote>For example, a Gaussian.</d-footnote></p> </li> <li> <p><strong>Cost function.</strong> For given parameters of the destroying process, we train a diffusion model to generate from the data distribution. We are given a function that associates a cost to these parameters: the cost may depend on the ultimate performances of the model when transporting from the fully-destroyed distribution to the data distribution, but also on the model’s size and the resources it consumed at training and/or inference. We seek to minimize the expectation of this cost.</p> </li> </ol> <p>This is an instance of Optimal Transport (OT) problems, which are traditionally introduced with piles of dirt. Imagine a pile of dirt whose height profile represents a probability distribution (higher probability means more dirt piled up there). We want to move the dirt around so that it represents a second distribution instead. There are many ways to do so, many exact plans for where to pick each shovelful of dirt and where to toss them. We can assign a cost<d-footnote>That is, a negative reward.</d-footnote> to each such plan, and we must explore the space of plans to find the cheapest.<d-footnote>If you have ever found yourself sitting in a conference room while the speaker said "... Wasserstein metric, also known as earth mover's distance...", then the talk was likely about OT.</d-footnote></p> <p>Ok, but is there anything practical to gain by framing the search for the best destroying process in terms of an OT problem? Well, I don’t know. But perhaps I should now mention that entropy-regularized OT is tightly related to diffusion,<d-cite key="debortoli2021diffusion"></d-cite> a fact that definitely weights in my intuition that there may be something interesting to be done here. Again, I think that the fact we <em>can</em> suffices to justify some exploratory efforts.</p> <p>So destruction is a general strategy for learning to generate information about a data distribution, diffusion’s <del>messiness</del> richness may advantageously leverage destruction in some contexts (including data-starved ones), exploring how to optimize a function doesn’t neatly fit this pictures, but there are workarounds reconciliating diffusion and exploration, opening a multitude of avenues for future work. There are no promises, but we should give it a try.</p> <h1 id="the-tutorial">The Tutorial</h1> <h2 id="destroying-and-generating">Destroying and Generating</h2> <p>This section provides a diagrammatic introduction to “destroying” and “generating” information.</p> <p>As is tradition in information theory, we consider two characters, Alice and Bob. Alice communicates one of 3 messages to Bob through a communication channel. A big arrow indicates the name of the channel itself, <code class="language-plaintext highlighter-rouge">Identity</code>, while smaller arrows indicate the action of that channel.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b0[0]
        b1[1]
        b2[2]
    end
    a0 --&gt; b0
    a1 --&gt; b1
    a2 --&gt; b2

    A ==Identity==&gt; B

    LeftMarginHack:::phantom ~~~ a0
    b0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>In the above situation, Bob gets exactly the message that Alice tried to convey. The information communicated by Alice is thus preserved (i.e., neither created nor destroyed) by the <code class="language-plaintext highlighter-rouge">Identity</code> communication channel: it is an important (albeit boring) channel.</p> <p>Ok, now let’s try a different channel, which I’ll call <code class="language-plaintext highlighter-rouge">Cypher</code>.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b1[1]
        b2[2]
        b0[0]
    end
    a0 --&gt; b1
    a1 --&gt; b2
    a2 --&gt; b0

    A ==Cypher==&gt; B

    LeftMarginHack:::phantom ~~~ a0
    b0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Here Bob gets a different number than the one Alice intended to send. Does this mean that <code class="language-plaintext highlighter-rouge">Cypher</code> destroys information? Well, it would if the channel could only be used once before being discarded…</p> <p>But for the sake of this blogpost, let’s instead consider the case where Bob is allowed as many training examples as he needs. During that training, Bob gets Alice’s message through both the trusty <code class="language-plaintext highlighter-rouge">Identity</code> channel and the to-be-figured-out <code class="language-plaintext highlighter-rouge">Cypher</code> channel. Using pairs of the form $(a,b)$ (with $a \in A$ and $b \in B$), Bob’s observations may look like</p> \[\{ (0,1), (2,0), (0,1), (0,1), (1,2), (0,1), (2,0), (0,1), (2,0), (0,1), (0,1), (0,1), \cdots \}.\] <p>There are many kinds of things that Bob could learn from such data. First, he may hypothesize that Alice is more likely to give the message $0$ than she is to say $1$ or $2$. In doing so, Bob would be building a mental model of an Imaginary-Alice $A’$, i.e., trying to learn the probability distribution $\textup{P}^\theta(A’)$ so that it matches the real $\textup{P}(A)$. And ultimately, this is exactly what generative modeling is about: learn how to sample from a $\textup{P}^\theta(A’)$ that approximates as best as we can the data distribution $\textup{P}(A)$.</p> <p>Doing so may be realistic for $3$ messages, but what if Alice had more range, say, $10^{678000}$ possible unique messages?<d-footnote>GPT OSS 120B's vocabulary size powered to its context length.</d-footnote> This direct approach won’t scale, which is why the next section will consider a divide-and-conquer approach to chunk large problems into more amenable ones. For now, it suffices to say that Bob is just a step in a long chain of messages, followed by Carol, David, <em>etc.</em> If each step is easier to model/learn than the previous one, we’ll be making progress toward our ultimate goal.</p> <p>So, Bob should focus on figuring out the <code class="language-plaintext highlighter-rouge">Cypher</code> channel, i.e., learn a probability distribution $\textup{P}^{\theta}(A \vert B)$ that approximates $\textup{P}(A \vert B)$.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b1[1]
        b2[2]
        b0[0]
    end
    subgraph Ap["A'"]
        ap0[0]
        ap1[1]
        ap2[2]
    end
    a0 --&gt; b1 --&gt; ap0
    a1 --&gt; b2 --&gt; ap1
    a2 --&gt; b0 --&gt; ap2

    A ==Cypher==&gt; B
    B ==Decrypt==&gt; Ap
    A ==Identity==&gt; Ap

    LeftMarginHack:::phantom ~~~ a0
    ap0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>There exists a <code class="language-plaintext highlighter-rouge">Decrypt</code> channel that, applied on the output of the <code class="language-plaintext highlighter-rouge">Cypher</code>, gives the <code class="language-plaintext highlighter-rouge">Identity</code>. If Bob is a good Bayesian, he will never be <em>absolutely sure</em> that he figured it out, but whatever his priors were<d-footnote>Well, within reason, assuming that Bob has a minimal pragmatism and/or understanding of the world...</d-footnote> for the probability distribution from which the <code class="language-plaintext highlighter-rouge">Cypher</code> channel was sampled, there is a point at which <code class="language-plaintext highlighter-rouge">Decrypt</code> will become his leading hypothesis as to what $\textup{P}^\theta(A’ \vert B)$ should be. And from that point on, his confidence in that hypothesis will keep increasing as more data is gathered.</p> <p>The existence of a <code class="language-plaintext highlighter-rouge">Decrypt</code> that reverses the action of <code class="language-plaintext highlighter-rouge">Cypher</code> proves that <code class="language-plaintext highlighter-rouge">Cypher</code> does not destroy information. And because we could do the opposite, i.e., reverse the action of <code class="language-plaintext highlighter-rouge">Decrypt</code> by applying <code class="language-plaintext highlighter-rouge">Cypher</code>, we know that <code class="language-plaintext highlighter-rouge">Cypher</code> does not generate information either. Just like <code class="language-plaintext highlighter-rouge">Identity</code>, <code class="language-plaintext highlighter-rouge">Cypher</code> preserves information. What does this mean? It means that <code class="language-plaintext highlighter-rouge">Cypher</code> is a useless channel for our divide-and-conquer aims: learning a $\textup{P}^\theta(B’)$ that approaches $\textup{P}(B)$ is as hard as learning a $\textup{P}^\theta(A’)$ that approaches $\textup{P}^\theta(A)$.</p> <p>Ok, maybe what we need is a noisy channel?</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b0[0]
        b1[1]
        b2[2]
        b3[3]
        b4[4]
        b5[5]
    end
    a0 --head--&gt; b0
    a0 --tail--&gt; b3
    a1 --head--&gt; b1
    a1 --tail--&gt; b4
    a2 --head--&gt; b2
    a2 --tail--&gt; b5

    A ==Product==&gt; B

    LeftMarginHack:::phantom ~~~ a0
    b0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Here the <code class="language-plaintext highlighter-rouge">Product</code><d-footnote>This name is a reference to category theory; please see the very last section for why.</d-footnote> channel flips a coin, and this affects what message Bob receives. Does <code class="language-plaintext highlighter-rouge">Product</code> destroy information?</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b0[0]
        b1[1]
        b2[2]
        b3[3]
        b4[4]
        b5[5]
    end
    subgraph Ap["A'"]
        ap0[0]
        ap1[1]
        ap2[2]
    end
    a0 --head--&gt; b0 --&gt; ap0
    a0 --tail--&gt; b3 --&gt; ap0
    a1 --head--&gt; b1 --&gt; ap1
    a1 --tail--&gt; b4 --&gt; ap1
    a2 --head--&gt; b2 --&gt; ap2
    a2 --tail--&gt; b5 --&gt; ap2

    A ==Product==&gt; B ==Project==&gt; Ap
    A ==Identity==&gt; Ap

    LeftMarginHack:::phantom ~~~ a0
    ap0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>No: there exists a <code class="language-plaintext highlighter-rouge">Project</code> channel that undoes the action of <code class="language-plaintext highlighter-rouge">Product</code>. Does it generate information?</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b0[0]
        b3[3]
        b1[1]
        b4[4]
        b2[2]
        b5[5]
    end
    subgraph Coin
        head
        tail
    end
    a0 --head--&gt; b0 --&gt; head
    a0 --tail--&gt; b3 --&gt; tail
    a1 --head--&gt; b1 --&gt; head
    a1 --tail--&gt; b4 --&gt; tail
    a2 --head--&gt; b2 --&gt; head
    a2 --tail--&gt; b5 --&gt; tail

    A ==Product==&gt; B ==Project'==&gt; Coin

    LeftMarginHack:::phantom ~~~ a0
    head ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Yes! Bob could use <code class="language-plaintext highlighter-rouge">Project'</code> to learn the outcome of the coin flip, an information that Alice is completely unaware of!</p> <p>For our divide-and-conquer aims, we needed Bob to be easier to model than Alice, and we got the opposite: Bob is strictly harder to model than Alice because he has all of Alice’s information, plus some irrelevant information about a coin. Therefore, <code class="language-plaintext highlighter-rouge">Product</code> is an even worse channel than <code class="language-plaintext highlighter-rouge">Cypher</code> for this divide-and-conquer purpose.</p> <p>If noise isn’t what we need, what is it?</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b0[0]
        b1[1]
    end
    a0 --&gt; b0
    a1 --&gt; b1
    a2 --&gt; b1

    A ==Mash==&gt; B

    LeftMarginHack:::phantom ~~~ a0
    b0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>That’s it! When Bob gets the message $1$, he can’t tell with certainty whether Alice said $1$ or $2$. <code class="language-plaintext highlighter-rouge">Mash</code>ing different messages together is what destroys information, not noise.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b0[0]
        b1[1]
    end
    subgraph Ap["A'"]
        ap0[0]
        ap1[1]
        ap2[2]
    end
    a0 --&gt; b0 --&gt; ap0
    a2 --&gt; b1 --&gt; ap1
    a1 --&gt; b1 --&gt; ap2

    A ==Mash==&gt; B ==Guess==&gt; Ap
    A -.B.- Ap

    LeftMarginHack:::phantom ~~~ a0
    ap0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>When Bob gets the message $B=0$, he can simply assign $A’=0$. But when $B=1$, he must learn to <code class="language-plaintext highlighter-rouge">Guess</code> the message sent by an Imaginary-Alice $A’$ so that $\textup{P}^\theta(A’|B)$ is as close as possible to $\textup{P}(A|B)$.</p> <p>I indicate this last requirement using a dashed line, with the conditional $B$ added on the line. However, in many applications, we don’t have a particular desire for $\textup{P}^\theta(A’|B) = \textup{P}(A|B)$, and we could be perfectly content with any $\textup{P}^\theta(A’|B)$ such that $\textup{P}^\theta(A’) = \textup{P}(A)$. In those cases, I simply don’t write anything on the dashed line.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b0[0]
        b1[1]
    end
    subgraph Ap["A'"]
        ap0[0]
        ap1[1]
        ap2[2]
    end
    a0 --&gt; b0 --&gt; ap0
    a2 --&gt; b1 --&gt; ap0
    a1 --&gt; b1
    b0 --&gt; ap1
    b1 --&gt; ap1
    b0 --&gt; ap2
    b1 --&gt; ap2

    A -.- Ap
    A ==Mash==&gt; B ==&gt; Ap
    

    LeftMarginHack:::phantom ~~~ a0
    ap0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Note that I didn’t name the arrow from Bob to the Imaginary-Alice, because I don’t really care how Bob does it: as long as $\textup{P}^\theta(A’) = \textup{P}(A)$, i.e., the two people joined by an un-decorated dashed line have the same marginal distribution, we’re good.</p> <p>And I may not even care about the details of the possible messages and how they relate (small arrows).</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0:::phantom
    end
    subgraph B
        b0:::phantom
    end
    subgraph Ap["A'"]
        ap0:::phantom
    end
    A ==&gt; B ==&gt; Ap
    A -.- Ap

    LeftMarginHack:::phantom ~~~ A
    Ap ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>What does this picture tell us? Well, for a starter, because $A$ and $A’$ have the same marginal probability distribution, they must have the same information content (entropy)</p> \[\textup{H}(A) = \textup{H}(A') .\] <p>How does $\textup{H}(B)$ relate to those two? We don’t know: it could be higher, lower, or equal, depending on the details of those communication channels. One thing is clear though: as much information must be created/destroyed from $A$ to $B$ as is destroyed/created from $B$ to $A’$.</p> <p>This picture can tell us much more than that, because of the data processing inequality. From Latham and Roudi<d-cite key="Latham2009"></d-cite>:</p> <blockquote> <p>The Data Processing Inequality (DPI) states, loosely, that post-processing cannot increase information.</p> </blockquote> <p>If you think “Haven’t we just shown that communication channels may create information?!”, then yes, you are correct. But the information created by communication channels <em>is completely irrelevant to anything that came before in the communication chain</em>. Latham and Roudi are saying that information about an early message cannot appear in later messages by further processing. This “information about” can be measured with the mutual information; we write $\textup{I}(Y; X)$ the mutual information between $X$ and $Y$, and it satisfies</p> \[\textup{I}(Y; X) = \textup{H}(Y) - \textup{H}(X | Y) = \textup{H}(X) - \textup{H}(Y | X) , \qquad \textup{I}(Y; Y) = \textup{H}(Y) .\] <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0:::phantom
    end
    subgraph B
        b0:::phantom
    end
    subgraph Ap["A'"]
        ap0:::phantom
    end
    A ==&gt; B ==&gt; Ap

    LeftMarginHack:::phantom ~~~ A
    Ap ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Therefore, what the data processing inequality tells us is that</p> \[\eqalign{ \textup{I}(A; A) \ge \textup{I}(A; B) &amp; \ge \textup{I}(A; A') \cr \textup{I}(B; B) &amp; \ge \textup{I}(B; A') . }\] <p>Whatever there is to know about $A$, $B$ cannot know more than that, and $A’$ cannot know more than $B$ did. And $A’$ cannot know more about $B$ than $B$ knew about himself.</p> <p>This is what I call “the destruction story”: <strong>you cannot know what has been forgotten before you heard about it.</strong> However, the data processing inequality has a lesser known dual, which I call “the generation story”</p> \[\eqalign{ \textup{I}(A'; A) &amp; \le \textup{I}(A'; B) \le \textup{I}(A'; A') \cr \textup{I}(B; A) &amp; \le \textup{I}(B; B) . }\] <p>$A$ cannot know more than $B$ about $A’$, and $B$ cannot know more about $A’$ than there is to be known about $A’$. And $A$ cannot know more than $B$ about $B$. In the generation story, <strong>you cannot know about what has not been decided yet.</strong></p> <p>These zoomed-out diagrams are a great way to consider general classes of problems. But ultimately, the generation/destruction of information happens at the zoomed-in, message level.</p> <pre><code class="language-mermaid">flowchart LR
    focus[" "]
    
    leftt[" "] --&gt; focus --&gt; rightt[" "]
    leftc:::phantom ~~~|"destroy"| focus ~~~|"generate"| rightc:::phantom
    leftb[" "] --&gt; focus --&gt; rightb[" "]

    LeftMarginHack:::phantom ~~~ leftt
    rightb ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <ul> <li>When different messages converge to the same message, information is <strong>destroyed</strong>.</li> <li>When a given message can diverge into different messages, information is <strong>generated</strong>.</li> </ul> <h2 id="mashing-everything">Mashing Everything</h2> <p>Let’s continue from the <code class="language-plaintext highlighter-rouge">Mash</code> example in the previous section. Now suppose that Bob passes the message to Carol through the <code class="language-plaintext highlighter-rouge">Mash'</code> channel.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b0[0]
        b1[1]
    end
    subgraph C
        c0[0]
    end
    subgraph Singleton[*]
        singleton[" "]
    end
   
    a0 --&gt; b0 --&gt; c0
    a1 --&gt; b1 --&gt; c0
    a2 --&gt; b1

    A ==Mash==&gt; B ==Mash'==&gt; C -.- Singleton

    LeftMarginHack:::phantom ~~~ a0
    singleton ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Notice that Carol always gets the same message, which is called a singleton: I highlight this by linking her to a special “*” symbol using a dashed line. All singleton have no information: $\textup{H}(C) = 0$.</p> <p>But by the definition of mutual information, we must have $\textup{I}(A; C) \le \textup{H}(C) = 0$: Carol cannot know more about Alice than Carol knows at all, but Carol knows nothing, therefore Carol knows nothing about Alice. Stated otherwise, all information about Alice has been destroyed by the time the messages get to Carol. Success! We have divided a large task into smaller, simpler ones, until all that was left was trivial!</p> <p>Now unto the “conquer” part of divide-and-conquer.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0:::phantom
    end
    subgraph B
        b0:::phantom
    end
    subgraph Ap["A'"]
        ap0:::phantom
    end
    subgraph C
        c0:::phantom
    end
    subgraph Bp["B'"]
        bp0:::phantom
    end
    subgraph Singleton["*"]
        singleton:::phantom
    end
    A ==Mash==&gt; B ==Mash'==&gt; C ==Guess'==&gt; Bp
    B ==Guess==&gt; Ap
    A -.- Ap
    B -.- Bp
    C -.- Singleton

    LeftMarginHack:::phantom ~~~ a0
    bp0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Bob can learn <code class="language-plaintext highlighter-rouge">Guess</code> using <code class="language-plaintext highlighter-rouge">Mash</code> on Alice’s message, and Carol can learn <code class="language-plaintext highlighter-rouge">Guess'</code> using <code class="language-plaintext highlighter-rouge">Mash'</code> on Bob’s. And because Carol knows nothing, we’re ready to generate completely fake $A’’$ behavior from nothing!</p> <pre><code class="language-mermaid">flowchart LR
    subgraph Singleton["*"]
        singleton:::phantom
    end
    subgraph Bpp["B''"]
        bpp0:::phantom
    end
    subgraph App["A''"]
        app0:::phantom
    end
    subgraph A["A"]
        a0:::phantom
    end
    
    Singleton =="Guess'"==&gt; Bpp ==Guess==&gt; App -.- A

    LeftMarginHack:::phantom ~~~ Singleton
    a0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>This is the strategy used in autoregressive language models.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0["('The', 'cat', 'is', 'black')"]
        a1["('The', 'cat', 'is', 'white')"]
        a2["('The', 'cat', 'is', 'sleepy')"]
        a3["('The', 'cat', 'was', 'sleepy')"]
        a4["('The', 'dog', 'is', 'black')"]
        a5["('The', 'dog', 'is', 'brown')"]
        a6["('Your', 'cat', 'is', 'black')"]
        a7["..."]
    end
    subgraph B
        b0["('The', 'cat', 'is')"]
        b1["('The', 'cat', 'was')"]
        b2["('The', 'dog', 'is')"]
        b3["('Your', 'cat', 'is')"]
        b4["..."]
    end
    subgraph C
        c0["('The', 'cat')"]
        c1["('The', 'dog')"]
        c2["('Your', 'cat')"]
        c3["..."]
    end
    subgraph D
        d0["('The')"]
        d1["('Your')"]
        d2["..."]
    end
    subgraph E
        e0["()"]
    end
   
    a0 --&gt; b0 --&gt; c0 --&gt; d0 --&gt; e0
    a1 --&gt; b0
    a2 --&gt; b0
    a3 --&gt; b1 --&gt; c0
    a4 --&gt; b2 --&gt; c1 --&gt; d0
    a5 --&gt; b2
    a6 --&gt; b3 --&gt; c2 --&gt; d1 --&gt; e0

    LeftMarginHack:::phantom ~~~ a0
    e0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Here the ellipses represent additional messages whose arrows are not explicitly shown. The message is iteratively fed through the <code class="language-plaintext highlighter-rouge">MashLastToken</code> communication channel, which mashes together all the messages that have the same prefix up to the last token. Because of the setting’s symmetric structure, we can use the same neural network at each position to predict the probability distribution for what the next should be: all positions can contribute to train the same <code class="language-plaintext highlighter-rouge">GuessNextToken</code>. At inference, we start from the singleton (the empty sequence), then iteratively apply <code class="language-plaintext highlighter-rouge">GuessNextToken</code>. Et voilà! An autoregressive language model!</p> <p>Notice how each time we destroy some information, we’re carving out a chunk of the overall task to be learned. Here divide-and-conquer amounted to “learn to generate one token at a time”. Such a very structured, regular way to destroy the message, is typical of many non-diffusion machine learning techniques. One of my claims in this blogpost is that diffusion models can destroy information more “organically”, possibly reducing the human-designer bias hand-waved at by Sutton.<d-cite key="sutton2019bitter"></d-cite> As an example of what I mean, here’s what the last diagram would look like for a mask diffusion language model.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0["('The', 'cat', 'is', 'black')"]
        a1["('The', 'cat', 'is', 'white')"]
        a2["('The', 'cat', 'is', 'sleepy')"]
        a3["('The', 'cat', 'was', 'sleepy')"]
        a4["('The', 'dog', 'is', 'black')"]
        a5["('The', 'dog', 'is', 'brown')"]
        a6["('Your', 'cat', 'is', 'black')"]
        a7["..."]
    end
    subgraph B
        b03["('The', 'cat', 'is', '[MASK]')"]
        b33["('The', 'cat', 'was', '[MASK]')"]
        b43["('The', 'dog', 'is', '[MASK]')"]
        b63["('Your', 'cat', 'is', '[MASK]')"]
        
        b02["('The', 'cat', '[MASK]', 'black')"]
        b12["('The', 'cat', '[MASK]', 'white')"]
        b22["('The', 'cat', '[MASK]', 'sleepy')"]
        b42["('The', 'dog', '[MASK]', 'black')"]
        b52["('The', 'dog', '[MASK]', 'brown')"]
        b62["('Your', 'cat', '[MASK]', 'black')"]
        
        b01["('The', '[MASK]', 'is', 'black')"]
        b11["('The', '[MASK]', 'is', 'white')"]
        b21["('The', '[MASK]', 'is', 'sleepy')"]
        b31["('The', '[MASK]', 'was', 'sleepy')"]
        b51["('The', '[MASK]', 'is', 'brown')"]
        b61["('Your', '[MASK]', 'is', 'black')"]
        
        b00["('[MASK]', 'cat', 'is', 'black')"]
        b10["('[MASK]', 'cat', 'is', 'white')"]
        b20["('[MASK]', 'cat', 'is', 'sleepy')"]
        b30["('[MASK]', 'cat', 'was', 'sleepy')"]
        b40["('[MASK]', 'dog', 'is', 'black')"]
        b50["('[MASK]', 'dog', 'is', 'brown')"]
        
        b7["..."]
    end

    a0 --&gt; b03
    a1 --&gt; b03
    a2 --&gt; b03
    a3 --&gt; b33 
    a4 --&gt; b43 
    a5 --&gt; b43
    a6 --&gt; b63

    a0 --&gt; b02
    a1 --&gt; b12
    a2 --&gt; b22
    a3 --&gt; b22
    a4 --&gt; b42
    a5 --&gt; b52
    a6 --&gt; b62

    a0 --&gt; b01
    a1 --&gt; b11
    a2 --&gt; b21
    a3 --&gt; b31
    a4 --&gt; b01
    a5 --&gt; b51
    a6 --&gt; b61

    a0 --&gt; b00
    a1 --&gt; b10
    a2 --&gt; b20
    a3 --&gt; b30
    a4 --&gt; b40
    a5 --&gt; b50
    a6 --&gt; b00

    subgraph C
        c023["('The', 'cat', '[MASK]', '[MASK]')"]
        c423["('The', 'dog', '[MASK]', '[MASK]')"]
        c623["('Your', 'cat', '[MASK]', '[MASK]')"]

        c013["('The', '[MASK]', 'is', '[MASK]')"]
        c313["('The', '[MASK]', 'was', '[MASK]')"]
        c613["('Your', '[MASK]', 'is', '[MASK]')"]

        c003["('[MASK]', 'cat', 'is', '[MASK]')"]
        c303["('[MASK]', 'cat', 'was', '[MASK]')"]
        c403["('[MASK]', 'dog', 'is', '[MASK]')"]
        
        c023["('The', 'cat', '[MASK]', '[MASK]')"]
        c423["('The', 'dog', '[MASK]', '[MASK]')"]
        c623["('Your', 'cat', '[MASK]', '[MASK]')"]

        c012["('The', '[MASK]', '[MASK]', 'black')"]
        c112["('The', '[MASK]', '[MASK]', 'white')"]
        c212["('The', '[MASK]', '[MASK]', 'sleepy')"]
        c512["('The', '[MASK]', '[MASK]', 'brown')"]
        c612["('Your', '[MASK]', '[MASK]', 'black')"]

        c002["('[MASK]', 'cat', '[MASK]', 'black')"]
        c102["('[MASK]', 'cat', '[MASK]', 'white')"]
        c202["('[MASK]', 'cat', '[MASK]', 'sleepy')"]
        c402["('[MASK]', 'dog', '[MASK]', 'black')"]
        c502["('[MASK]', 'dog', '[MASK]', 'brown')"]
        
        c001["('[MASK]', '[MASK]', 'is', 'black')"]
        c101["('[MASK]', '[MASK]', 'is', 'white')"]
        c201["('[MASK]', '[MASK]', 'is', 'sleepy')"]
        c301["('[MASK]', '[MASK]', 'was', 'sleepy')"]
        c501["('[MASK]', '[MASK]', 'is', 'brown')"]
        
        c7["..."]
    end
    B ==&gt; C
    subgraph D
        d0012["('[MASK]', '[MASK]', '[MASK]', 'black')"]
        d1012["('[MASK]', '[MASK]', '[MASK]', 'white')"]
        d2012["('[MASK]', '[MASK]', '[MASK]', 'sleepy')"]
        d5012["('[MASK]', '[MASK]', '[MASK]', 'brown')"]

        d0013["('[MASK]', '[MASK]', 'is', '[MASK]')"]
        d3013["('[MASK]', '[MASK]', 'was', '[MASK]')"]

        d0023["('[MASK]', 'cat', '[MASK]', '[MASK]')"]
        d4023["('[MASK]', 'dog', '[MASK]', '[MASK]')"]

        d0123["('The', '[MASK]', '[MASK]', '[MASK]')"]
        d6123["('Your', '[MASK]', '[MASK]', '[MASK]')"]

        d7["..."]
    end
    c023 --&gt; d0023
    c423 --&gt; d0023
    c013 --&gt; d0013
    c313 --&gt; d3013
    c613 --&gt; d0013
    c003 --&gt; d0013
    c303 --&gt; d3013
    c403 --&gt; d0013
    c623 --&gt; d0023
    c012 --&gt; d0012
    c112 --&gt; d1012
    c212 --&gt; d2012
    c512 --&gt; d5012
    c612 --&gt; d0012
    c002 --&gt; d0012
    c102 --&gt; d1012
    c202 --&gt; d2012
    c402 --&gt; d0012
    c502 --&gt; d5012
    c001 --&gt; d0012
    c101 --&gt; d1012
    c201 --&gt; d2012
    c301 --&gt; d2012
    c501 --&gt; d5012

    c023 --&gt; d0123
    c423 --&gt; d0123
    c013 --&gt; d0123
    c313 --&gt; d0123
    c613 --&gt; d6123
    c003 --&gt; d0023
    c303 --&gt; d0023
    c403 --&gt; d4023
    c623 --&gt; d6123
    c012 --&gt; d0123
    c112 --&gt; d0123
    c212 --&gt; d0123
    c512 --&gt; d0123
    c612 --&gt; d6123
    c002 --&gt; d0023
    c102 --&gt; d0023
    c202 --&gt; d0023
    c402 --&gt; d0023
    c502 --&gt; d0023
    c001 --&gt; d0013
    c101 --&gt; d0013
    c201 --&gt; d0013
    c301 --&gt; d3013
    c501 --&gt; d0013

    subgraph E
        e0["('[MASK]', '[MASK]', '[MASK]', '[MASK]')"]
    end
    d0012 --&gt; e0
    d1012 --&gt; e0
    d2012 --&gt; e0
    d5012 --&gt; e0

    d0013 --&gt; e0
    d3013 --&gt; e0

    d0023 --&gt; e0
    d4023 --&gt; e0

    d0123 --&gt; e0
    d6123 --&gt; e0
   
    LeftMarginHack:::phantom ~~~ a0
    e0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>(There should be 3 arrows leaving each of Bob’s messages toward Carol, please pardon me for eschewing them.)</p> <p>While <code class="language-plaintext highlighter-rouge">MashLastToken</code> was strictly destroying information, here <code class="language-plaintext highlighter-rouge">MashOneRandomUnmaskedToken</code> both generates information (which token is to be masked) and destroys information (mash together all messages that would be the same if it were not for the randomly-selected unmasked token). However, the newly generated information (masking order) is also destroyed by the process. In the end, we still get a singleton: all information is eventually destroyed.</p> <p>The point I’m trying to make here is that <code class="language-plaintext highlighter-rouge">MashOneRandomUnmaskedToken</code> is a richer approach to “divide and conquer” than <code class="language-plaintext highlighter-rouge">MashLastToken</code> was. It is more “organic” in a similar sense that mixing milk in coffee is “organic”: it’s a mess, a <em>rich</em> mess. We can train a model to <code class="language-plaintext highlighter-rouge">GuessOneRandomMaskToken</code> on that mess, and yes, I could excuse it to train slower than <code class="language-plaintext highlighter-rouge">GuessNextToken</code>, because <code class="language-plaintext highlighter-rouge">GuessNextToken</code> is a subset of what <code class="language-plaintext highlighter-rouge">GuessOneRandomMaskToken</code> is learning. Let me make it explicit.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0["('The', 'cat', 'is', 'black')"]
        a1["('The', 'cat', 'is', 'white')"]
        a2["('The', 'cat', 'is', 'sleepy')"]
        a3["('The', 'cat', 'was', 'sleepy')"]
        a4["('The', 'dog', 'is', 'black')"]
        a5["('The', 'dog', 'is', 'brown')"]
        a6["('Your', 'cat', 'is', 'black')"]
        a7["..."]
    end
    subgraph B
        b0["('The', 'cat', 'is', '[MASK]')"]
        b1["('The', 'cat', 'was', '[MASK]')"]
        b2["('The', 'dog', 'is', '[MASK]')"]
        b3["('Your', 'cat', 'is', '[MASK]')"]
        b4["..."]
    end
    subgraph C
        c0["('The', 'cat', '[MASK]', '[MASK]')"]
        c1["('The', 'dog', '[MASK]', '[MASK]')"]
        c2["('Your', 'cat', '[MASK]', '[MASK]')"]
        c3["..."]
    end
    subgraph D
        d0["('The', '[MASK]', '[MASK]', '[MASK]')"]
        d1["('Your', '[MASK]', '[MASK]', '[MASK]')"]
        d2["..."]
    end
    subgraph E
        e0["('[MASK]', '[MASK]', '[MASK]', '[MASK]')"]
    end
   
    a0 --&gt; b0 --&gt; c0 --&gt; d0 --&gt; e0
    a1 --&gt; b0
    a2 --&gt; b0
    a3 --&gt; b1 --&gt; c0
    a4 --&gt; b2 --&gt; c1 --&gt; d0
    a5 --&gt; b2
    a6 --&gt; b3 --&gt; c2 --&gt; d1 --&gt; e0

    LeftMarginHack:::phantom ~~~ a0
    e0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>We could train a model to <code class="language-plaintext highlighter-rouge">GuessOneRandomMaskToken</code> then, at inference, use the learned weight to perform the <code class="language-plaintext highlighter-rouge">GuessNextToken</code> task instead. In the scenario where we’re data-starved, but we can train for as long as we wish to, with a model as big as it needs to,<d-cite key="ni2025diffusion"></d-cite> and inference is performed using <code class="language-plaintext highlighter-rouge">GuessNextToken</code>, which model would you put your money on: the one trained solely on this specific <code class="language-plaintext highlighter-rouge">GuessNextToken</code> task, or the one that saw the worst <d-cite key="kim2025train"></d-cite> that <code class="language-plaintext highlighter-rouge">GuessOneRandomMaskToken</code> had to offer?</p> <h2 id="shuffling-everything">Shuffling Everything</h2> <p>In the previous section, we saw that both autoregressive language models and mask diffusion language models learn to mimic Alice’s message distribution by iteratively <code class="language-plaintext highlighter-rouge">Mash</code>ing them toward a singleton, reaching a state with no information about anything, thus no information about Alice. Here we consider the other leading approach to destroying Alice’s message: drowning it into irrelevant information.</p> <p>This mechanism, which I call <code class="language-plaintext highlighter-rouge">Shuffle</code>, may be more aligned with the historic/physical meaning of the word “diffusion”.<d-footnote>In other words, while I expect that some readers could say that `Mash` isn't "real" diffusion, I don't have the same worry for `Shuffle`.</d-footnote> Whereas <code class="language-plaintext highlighter-rouge">Mash</code> <em>may</em> generate information (e.g., masking order) but will eventually destroy everything to a singleton, <code class="language-plaintext highlighter-rouge">Shuffle</code> <em>must</em> generate new information to be folded into Alice’s message, drowning it in the noise.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0[0]
        a1[1]
        a2[2]
    end
    subgraph B
        b0[0]
        b1[1]
        b2[2]
    end

    a0 --tail---&gt; b0
    a2 --tail---&gt; b0
    a0 --head---&gt; b1
    A ==Shuffle===&gt; B
    a1 --head---&gt; b1
    a2 --head---&gt; b2
    a1 --tail---&gt; b2

    LeftMarginHack:::phantom ~~~ a0
    b0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>If Bob has a good understanding of the <code class="language-plaintext highlighter-rouge">Shuffle</code> channel, he can model the situation as</p> \[\begin{bmatrix} \textup{P}(B=0) \cr \textup{P}(B=1) \cr \textup{P}(B=2) \end{bmatrix} = \begin{bmatrix} 1/2 &amp; 0 &amp; 1/2 \cr 1/2 &amp; 1/2 &amp; 0 \cr 0 &amp; 1/2 &amp; 1/2 \end{bmatrix} \begin{bmatrix} \textup{P}(A=0) \cr \textup{P}(A=1) \cr \textup{P}(A=2) \end{bmatrix} .\] <p>When he observes $B=0$, he knows that Alice said either $0$ or $2$, but he’s not sure which: information was destroyed, and he may learn to to generate $A’$, seeking to capture Alice’s marginal probability distribution $\textup{P}(A)$.</p> <p>But notice that (irrelevant) information was also created: when $B=0$, a Bob that understands the channel knows with certainty that the coin flip was “tail” (whereas this coin information was completely unknown to Alice). In my view, <code class="language-plaintext highlighter-rouge">Shuffle</code> can destroy information about the original message <em>because</em> it generates irrelevant information and has to fold the outcome in as many possible messages than there were before. The message $A$ can take 3 values, the coin can take two values, the product of the coin and the message can take 6 values, and somehow this has to be crammed into the 3 possible values for $B$. Something has to give; information about Alice is destroyed.</p> <p>Next Bob can message Carol through the same <code class="language-plaintext highlighter-rouge">Shuffle</code> channel, and the same procedure can continue up to Zalgo.</p> \[\begin{bmatrix} \textup{P}(Z=0) \cr \textup{P}(Z=1) \cr \textup{P}(Z=2) \end{bmatrix} = \begin{bmatrix} 1/2 &amp; 0 &amp; 1/2 \cr 1/2 &amp; 1/2 &amp; 0 \cr 0 &amp; 1/2 &amp; 1/2 \end{bmatrix}^{25} \begin{bmatrix} \textup{P}(A=0) \cr \textup{P}(A=1) \cr \textup{P}(A=2) \end{bmatrix}\] <p>Now, technically, the probability distribution $\textup{P}(Z)$ still depends on $\textup{P}(A)$, but what does that mean in practice? Well, it can be shown that</p> \[0.33333331 &lt; P(Z=z) &lt; 0.33333335 \quad \forall z \in \{0,1,2\} ,\] <p>irrespective of Alice’s probability distribution. The crux of the <code class="language-plaintext highlighter-rouge">Shuffle</code> learning strategy here amounts to approximating Zalgo as a uniform distribution:</p> \[P(Z'=z') = \frac{1}{3} \quad \forall z' \in \{ 0,1,2 \}.\] <p>So, whereas <code class="language-plaintext highlighter-rouge">Mash</code> eventually takes us to a singleton, <code class="language-plaintext highlighter-rouge">Shuffle</code> takes us to a known distribution, simple to sample. Except for this detail, inference for <code class="language-plaintext highlighter-rouge">Shuffle</code> proceeds the same way as <code class="language-plaintext highlighter-rouge">Mash</code>: we sample $Z’$ from the known distribution, then apply the respective learned <code class="language-plaintext highlighter-rouge">Guess</code> down the alphabet until we get $A’$.</p> <p>Like the previous section adapted the spirit of <code class="language-plaintext highlighter-rouge">Mash</code> to <code class="language-plaintext highlighter-rouge">MashOneRandomUnmaskedToken</code>, we can adapt <code class="language-plaintext highlighter-rouge">Shuffle</code> to obtain <code class="language-plaintext highlighter-rouge">ShuffleOneRandomToken</code>: a random token position gets substituted by a token selected uniformly at random from the tokenizer’s vocabulary. This is basically the “Uniform” noising process used in SEDD.<d-cite key="lou2024discrete"></d-cite></p> <p>For the rest of this section, let’s quickly consider what “shuffling” means in continuous space. Suppose Alice’s message is a $w \times h \times 3$ tensor of real numbers in the $[-1,1]$ interval representing an RGB picture of $w$ pixels wide by $h$ pixels high. We may define the <code class="language-plaintext highlighter-rouge">GaussianShuffle(δ)</code> communication channel such that it adds independent Gaussian noise (with mean $0$ and variance $0 &lt; \delta \ll 1$) to each entry of this tensor.</p> <p>This channel generates (irrelevant) information: for a given message from Alice, there are many possible options for what Bob may receive; arrows branching out means that information is generated. But the channel also destroys information: there are many messages that Alice could have said that may explain a given message received by Bob; arrows converging in mean that information is destroyed. As for <code class="language-plaintext highlighter-rouge">Shuffle</code>, <code class="language-plaintext highlighter-rouge">GaussianShuffle(δ)</code> destroys information about Alice’s message by generating irrelevant information and folding it into limited space.<d-footnote>The story is a bit more complex here because, in a strict mathematical sense, specifying a single real number requires an infinite amount of information. One can work around this by considering differential entropy instead of entropy. However, for computer science applications, there is a much simpler resolution: these "real" numbers are represented as discrete data types. Concretely, a `float32` is really a discrete variable that may take one of $2^{32}$ values, thus capturing at most 32 bits of information.</d-footnote></p> <p>The impact of $n$ repeated independent applications of <code class="language-plaintext highlighter-rouge">GaussianShuffle(δ)</code> is a single “bigger” <code class="language-plaintext highlighter-rouge">GaussianShuffle(n*δ)</code>.<d-footnote>Recall that the convolution of two Gaussians is a Gaussian whose mean and variance is the sum of their respective means and variances.</d-footnote> Similarly to how we approximated Zalgo as a uniform distribution earlier, we can choose an $n$ that is high enough so that the outcome of applying <code class="language-plaintext highlighter-rouge">GaussianShuffle(n*δ)</code> on any image is basically an $w \times h \times 3$ Gaussian with mean zero and variance $n\delta$, i.e., no dependency worth mentioning on the actual image. In essence, this amounts to the “Variance Exploding” process from Song <em>et al.</em> <d-cite key="song2021scorebased"></d-cite>; the “Variance Preserving” version could be similarly obtained by appropriately scaling down the outcome after each addition of Gaussian noise.</p> <h2 id="beyond-markov">Beyond Markov</h2> <p>In our quest to divide-and-conquer the learning of Alice’s distribution, we have seen two different strategies to destroy all the information in her message: <code class="language-plaintext highlighter-rouge">Mash</code> everything into a singleton, and <code class="language-plaintext highlighter-rouge">Shuffle</code> long enough to get a maximum-entropy distribution (e.g., uniform or Gaussian).</p> <p>Both these strategies are Markovian: each person’s message has all the information required to get the probability distribution for the next person’s message. Stated differently, the “spine” of all my diagrams looked like this.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0:::phantom
    end
    subgraph B
        b0:::phantom
    end
    subgraph C
        c0:::phantom
    end
    subgraph D["..."]
        d0:::phantom
    end
    subgraph Z
        z0:::phantom
    end
    A ==&gt; B ==&gt; C ==&gt; D ==&gt; Z
    
    LeftMarginHack:::phantom ~~~ a0
    z0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>But this is not the only possibility, and many diffusion models have a non-Markovian approach to destruction and/or generation. For example, although DDPM’s<d-cite key="ho2020denoising"></d-cite> destroying process may be framed as a Markovian chain, we may also frame it as interpolating between a “clean” sample $A’$ and a “noise distribution” $Z$</p> \[B' = \sqrt{\alpha_B} A' + \sqrt{1 - \alpha_B} Z', \quad C' = \sqrt{\alpha_C} A' + \sqrt{1 - \alpha_C} Z', \quad \cdots \ ,\] <p>given appropriate $1 = \alpha_A \ge \alpha_B \ge \alpha_C \ge \cdots \ge \alpha_Z = 0$. Below is a diagram showing the Markovian approach on the top-right and the non-Markovian one on the bottom-left.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph A
        a0:::phantom
    end
    subgraph B
        b0:::phantom
    end
    subgraph C
        c0:::phantom
    end
    subgraph D["..."]
        d0:::phantom
    end
    subgraph Z
        z0:::phantom
    end
    subgraph Ap["A'"]
        ap0:::phantom
    end
    subgraph Bp["B'"]
        bp0:::phantom
    end
    subgraph Cp["C'"]
        cp0:::phantom
    end
    subgraph Zp["Z'"]
        zp0:::phantom
    end
    subgraph ApxZp["A' × Z'"]
        azp0:::phantom
    end
    A ==&gt; B ==&gt; C ==&gt; D ==&gt; Z
    ApxZp ==&gt; Ap -.- A
    ApxZp ==&gt; Bp -.- B
    ApxZp ==&gt; Cp -.- C
    ApxZp ==&gt; Zp -.- Z
    
    LeftMarginHack:::phantom ~~~ a0
    z0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Here I used $A’ \times Z’$ to denote someone that has simultaneous access to both $A’$ and $Z’$ messages, and is thus able to use the previous equation to obtain $B’$, $C’$, <em>etc.</em></p> <p>But once you’ve used $A’ \times Z’$ to calculate, say, $B’$, you could just <em>not discard</em> $Z’$, and obtain $B’ \times Z’$. The same holds up to $Y’ \times Z’$ and, with a little more algebra, we can actually go back to $A’ \times Z’$ (ignoring finite-precision errors). I represent such information-preserving back-and-forth conversions using bidirectional arrows.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph ApxZp["A' × Z'"]
        apzp0:::phantom
    end
    subgraph BpxZp["B' × Z'"]
        bpzp0:::phantom
    end
    subgraph CpxZp["C' × Z'"]
        cpzp0:::phantom
    end
    subgraph DpxZp["..."]
        dpzp0:::phantom
    end
    subgraph YpxZp["Y' × Z'"]
        ypzp0:::phantom
    end
    subgraph Ap["A'"]
        ap0:::phantom
    end
    subgraph Bp["B'"]
        bp0:::phantom
    end
    subgraph Cp["C'"]
        cp0:::phantom
    end
    subgraph Yp["Y'"]
        yp0:::phantom
    end
    subgraph Zp["Z'"]
        zp0:::phantom
    end
    subgraph BpxZpp["B' × Z''"]
        bpzpp0:::phantom
    end
    subgraph CpxZppp["C' × Z'''"]
        cpzppp0:::phantom
    end
    subgraph YpxZpppp["Y' × Z'''''"]
        ypzpppp0:::phantom
    end
    subgraph App["A''"]
        app0:::phantom
    end
    subgraph A
        a0:::phantom
    end
    subgraph B
        b0:::phantom
    end
    subgraph C
        c0:::phantom
    end
    subgraph D["..."]
        d0:::phantom
    end
    subgraph Y
        y0:::phantom
    end
    subgraph Z
        z0:::phantom
    end
    
    Ap ==&gt; ApxZp &lt;==&gt; BpxZp &lt;==&gt; CpxZp &lt;==&gt; DpxZp &lt;==&gt; YpxZp ==&gt; Zp
    BpxZp ==&gt; Bp ==&gt; BpxZpp -."B'".- BpxZp
    CpxZp ==&gt; Cp ==&gt; CpxZppp -."C'".- CpxZp
    YpxZp ==&gt; Yp ==&gt; YpxZpppp -."Y'".- YpxZp
    A ==&gt; B ==&gt; C ==&gt; D ==&gt; Y ==&gt; Z
    ApxZp ==&gt; App -.- A
    Ap -.- A
    Bp -.- B
    Cp -.- C
    Yp -.- Y
    Zp -.- Z

    LeftMarginHack:::phantom ~~~ ap0
    z0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Projecting $B’ \times Z’$ to $B’$ destroys information about $Z’$ and, without it, we cannot get back to $A’$: this projection destroys information about $A’$. We may thus train a model to use $B’$ to predict the “missing noise” $Z’’$, with the intent to obtain a $B’ \times Z’’$ whose distribution matches $B’ \times Z’$. Equation (14) from DDPM<d-cite key="ho2020denoising"></d-cite> learns such denoising functions.</p> <h2 id="about-the-diagrams">About the Diagrams</h2> <p>There exist many types of <a href="https://en.wikipedia.org/wiki/Graphical_model">probabilistic graphical models</a>. Some are meant to be general-purpose, others are more specialized. Most are meant for empirical/scientific use: we translate our intuition of what are the non-impossible dependencies between stochastic variables, then we may analyze empirical observations under those assumptions.</p> <p>This blogpost introduces a new kind of probabilistic graphical model, which I propose to call <em>generative commutative diagrams</em>. Unlike many other kinds of graphical models, those used here are meant for prescription/engineering use. They read like code: an arrow is like a procedure you can call to transform the origin node into the target node.</p> <p>As the name “generative commutative diagrams” implies, they have been inspired by the <a href="https://en.wikipedia.org/wiki/Commutative_diagram">commutative diagrams</a> used by mathematicians, especially <a href="https://en.wikipedia.org/wiki/Category_theory">category theorists</a>. This blogpost is not about category theory, you do not need to know nor care about category theory to read it, and I am myself definitely <em>not</em> an expert on category theory.</p> <p>Most of the arrows used in this blogpost do <em>not</em> represent morphisms, but something less-than-a-morphism. I call them “harpoons” as I denote them $\rightharpoonup$ when I draw on a whiteboard or in $\texttt{tikz-cd}$, and I reserve $\rightarrow$ for proper morphisms. For this blogpost, I haven’t figured out how to coerce $\texttt{mermaid}$ into making different arrow heads: almost all the arrows show here should be $\rightharpoonup$. <code class="language-plaintext highlighter-rouge">Project</code> and <code class="language-plaintext highlighter-rouge">Mash</code> are among the rare examples that are proper morphisms, which I would usually draw as $\rightarrow$.</p> <p>An introduction to category theory will often start with something like this.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph X
        x0:::phantom
    end
    subgraph Y
        y0:::phantom
    end
    subgraph Z
        z0:::phantom
    end
    X ==f==&gt; Y ==g==&gt; Z
    X ==h==&gt; Z

    LeftMarginHack:::phantom ~~~ x0
    z0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>They will then mention “this diagram is commutative”, which means that $g(f(x)) = h(x)$ for all $x$. When we show the little arrows between the <del>messages</del> objects, this means that the paths going from $X$ to $Z$ by following $f$ then $g$ must all agree with the paths directly using $h$.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph X
        x0["0"]
        x1["1"]
        x2["2"]
    end
    subgraph Y
        y0["0"]
        y1["1"]
    end
    subgraph Z
        z0["0"]
        z1["1"]
    end
    X ==f==&gt; Y ==g==&gt; Z
    X ==h==&gt; Z
    x0 --&gt; y0 --&gt; z0
    x1 --&gt; z0
    x1 --&gt; y0
    x2 --&gt; z1
    x2 --&gt; y1 --&gt; z1
    x0 --&gt; z0

    LeftMarginHack:::phantom ~~~ x0
    z0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>In a generative commutative diagrams, all the big arrows (harpoons and proper morphisms included) <strong>must also satisfy this requirement of commutativity.</strong></p> <p>Next, that introduction to category theory may give the following diagram.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph X
        x0:::phantom
    end
    subgraph Y
        y0:::phantom
    end
    subgraph Z
        z0:::phantom
    end
    X ==f==&gt; Y ==g==&gt; Z

    LeftMarginHack:::phantom ~~~ x0
    z0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>They will then say that because $f$ and $g$ are morphisms, this picture is equivalent to the first one: we can always combine morphisms head-to-tail to obtain an implicit morphism, here $h = g \circ f$. <strong>This is not true for my harpoons.</strong> If either or both $f$ and $g$ are harpoons, the best you can get by chaining them is a way to sample the same probability distribution<d-footnote>Again, these mermaid diagrams use a dashed line to represent "has the same marginal distribution as", but elsewhere I prefer tikz-cd's squiggly line, in reference to the "tilde" symbol that is often used to mean "distributed like".</d-footnote> – <em>not</em> a guarantee that you will end up at the same element.</p> <pre><code class="language-mermaid">flowchart LR
    subgraph X
        x0:::phantom
    end
    subgraph Y
        y0:::phantom
    end
    subgraph Z
        z0:::phantom
    end
    subgraph Zp["Z'"]
        zp0:::phantom
    end
    X ==f==&gt; Y ==g==&gt; Z
    X =="g ○ f"==&gt; Zp -.- Z

    LeftMarginHack:::phantom ~~~ x0
    z0 ~~~ RightMarginHack:::phantom
    classDef phantom display: none;
</code></pre> <p>Morphisms cannot create information, only destroy or preserve it. Harpoons can create, preserve and destroy information.</p> <p>Commutative diagrams that only contain morphisms may only tell “the destruction story”, the traditional direction of the data processing inequality. Harpoons allow you to also tell its dual, “the generation story”.</p> <p>To be clear, we didn’t get any real new narrative power: you can tell “the generation story” solely in terms of destruction.<d-footnote>Though you may have to narrate it from the perspective of Laplace's demon, a narrative device that already knows everything there is to be known.</d-footnote> In that view, harpoons are just “syntactic sugar” for a more involved combination of morphisms.</p> <p>I have no idea if there is interesting maths to be done with such harpoons and, if yes, perhaps it has already been done under some name that evaded my search. However, as a specialized kind of graphical model, I’ve found them particularly useful when thinking about diffusion models and their weird edge cases.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[I present diffusion models as part of a family of machine learning techniques that withhold information from a model’s input and train it to guess the withheld information. I argue that diffusion's destroying approach to withholding is more flexible than typical hand-crafted information withholding techniques, providing a rich training playground that could be advantageous in some settings, notably data-scarce ones. I then address subtle issues that may arise when porting reinforcement learning techniques to the diffusion context, and wonder how such exploration problems could be addressed in more diffusion-native ways. I do not have definitive answers, but I do point my fingers in directions I deem interesting. A tutorial follows this thesis, expanding on the destroy-then-generate perspective. A novel kind of probabilistic graphical models is introduced to facilitate the tutorial's exposition.]]></summary></entry><entry><title type="html">From U-Nets to DiTs: The Architectural Evolution of Text-to-Image Diffusion Models (2021–2025)</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/diffusion-architecture-evolution/" rel="alternate" type="text/html" title="From U-Nets to DiTs: The Architectural Evolution of Text-to-Image Diffusion Models (2021–2025)"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/diffusion-architecture-evolution</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/diffusion-architecture-evolution/"><![CDATA[<style>[data-theme="dark"] figcaption.caption{color:white!important}[data-theme="dark"] .key-differences{color:black!important}[data-theme="dark"] .key-differences strong{color:black!important}[data-theme="dark"] .key-differences li{color:black!important}[data-theme="light"] figcaption.caption{color:black!important}[data-theme="light"] .key-differences{color:black!important}[data-theme="light"] .key-differences strong{color:black!important}[data-theme="light"] .key-differences li{color:black!important}[data-theme="light"] .themed-image{content:url("/2026/assets/img/2026-04-27-diffusion-architecture-evolution/teaser_white.png")}[data-theme="dark"] .themed-image{content:url("/2026/assets/img/2026-04-27-diffusion-architecture-evolution/teaser_black.png")}.key-differences{border:2px solid #ff9800;background-color:#fff3e0;padding:15px;border-radius:5px;margin:15px 0}.key-differences ul{margin-top:10px;margin-bottom:0}</style> <div class="l-page"> <figure class="themed-figure"> <img class="themed-image" alt="A hero image summarizing the evolution of diffusion model architectures from U-Nets to Transformers." src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/teaser_white.png"/> <figcaption class="caption">Diffusion Image Model Architecture Evolution.</figcaption> </figure> </div> <h2 id="tldr">TL;DR</h2> <p>As diffusion systems scale, the biggest wins tend to come from leveraging compute with broad, general methods rather than hand-crafting ever more specific tricks <d-cite key="richsuttonBitterLesson2019"></d-cite>. At the same time, we should keep sight of the “hardware lottery”: what succeeds can reflect today’s accelerators and tooling as much as inherent merit <d-cite key="hookerHardwareLottery2021"></d-cite>.</p> <h2 id="preliminaries-diffusion-models-for-image-generation">Preliminaries: Diffusion Models for Image Generation</h2> <p>Diffusion models have emerged as a powerful paradigm for generative modeling by learning to reverse a gradual noise corruption process. The fundamental approach involves two key stages: a <strong>forward diffusion process</strong> that systematically adds noise to data until it becomes pure Gaussian noise, and a <strong>reverse denoising process</strong> where a neural network gradually removes this noise to generate new samples.</p> <p>This framework has demonstrated remarkable success across diverse domains including image generation, audio synthesis, video generation, and even applications in natural language processing and molecular design. The generality of the diffusion framework makes it particularly attractive for complex generative tasks.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lilian_DDPM-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lilian_DDPM-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lilian_DDPM-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lilian_DDPM.png" width="100%" height="auto" alt="Diagram showing the forward noising process and the reverse denoising process in diffusion models." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">The Markov chain for the forward and reverse diffusion processes, which generate a sample by slowly adding (and removing) noise. Image Credit: <d-cite key="wengWhatAreDiffusion2021"></d-cite></figcaption> </figure> <p>For readers seeking a comprehensive introduction to diffusion model fundamentals, we recommend Yang Song’s excellent exposition on <a href="https://yang-song.net/blog/2021/score/">score-based generative modeling</a> <d-cite key="song2019generative"></d-cite> and Lilian Weng’s detailed overview of <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">diffusion models</a> <d-cite key="wengWhatAreDiffusion2021"></d-cite>.</p> <h2 id="interactive-timeline">Interactive Timeline</h2> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-diffusion-architecture-evolution/timeline.html" frameborder="0" scrolling="yes" height="700px" width="100%"></iframe> </div> <h2 id="the-u-net-era">The U-Net Era</h2> <p>The early pioneering works in diffusion-based image generation predominantly adopted <strong>U-Net architectures</strong> <d-cite key="ronnebergerUNetConvolutionalNetworks2015"></d-cite> as their neural network backbone. This choice was largely influenced by U-Net’s proven success in various computer vision tasks <d-cite key="linRefineNetMultiPathRefinement2017"></d-cite><d-cite key="salimansPixelCNNImprovingPixelCNN2017"></d-cite>.</p> <p>The foundational models in this era established the core principles of diffusion-based generation. <strong>NCSN</strong> (Noise Conditional Score Network) <d-cite key="song2019generative"></d-cite> pioneered score-based generative modeling using a RefineNet backbone <d-cite key="linRefineNetMultiPathRefinement2017"></d-cite>, while <strong>DDPM</strong> (Denoising Diffusion Probabilistic Models) <d-cite key="hoDenoisingDiffusionProbabilistic2020"></d-cite> established the probabilistic framework using a PixelCNN++ architecture <d-cite key="salimansPixelCNNImprovingPixelCNN2017"></d-cite>. Subsequent refinements including <strong>NCSNv2</strong> <d-cite key="songImprovedTechniquesTraining2020"></d-cite>, <strong>IDDPM</strong> <d-cite key="nicholImprovedDenoisingDiffusion2021"></d-cite>, <strong>ADM</strong> (Ablated Diffusion Model) <d-cite key="dhariwalDiffusionModelsBeat2021"></d-cite>, and <strong>SDE</strong> (Score-based Diffusion via Stochastic Differential Equations) <d-cite key="songScoreBasedGenerativeModeling2021"></d-cite> built upon these foundations with architectural variations similar to DDPM or NCSN. However, these early models focused primarily on unconditional image generation and lacked text-to-image capabilities.</p> <p>The breakthrough for text-to-image generation came with <strong>LDM</strong> (Latent Diffusion Models, also known as Stable Diffusion) <d-cite key="rombachHighResolutionImageSynthesis2022"></d-cite>, which introduced a latent U-Net architecture to enable efficient text-conditioned generation. Following this success, several notable U-Net-based text-to-image models emerged, each exploring different architectural innovations within the U-Net paradigm:</p> <table> <thead> <tr> <th>Model</th> <th>Gen. (#Param)</th> <th>Txt. (#Param)</th> <th>Total (#Param)</th> <th>Release Date</th> </tr> </thead> <tbody> <tr> <td>SD v2.1 <d-cite key="rombachHighResolutionImageSynthesis2022"></d-cite></td> <td>0.87B</td> <td>0.34B</td> <td>1.29B</td> <td>2022-12-07</td> </tr> <tr> <td>Kandinsky <d-cite key="razzhigaevKandinskyImprovedTexttoImage2023"></d-cite></td> <td>1.23B</td> <td>0.56B</td> <td>1.86B</td> <td>2023-01-01</td> </tr> <tr> <td>UniDiffuser <d-cite key="baoOneTransformerFits2023"></d-cite></td> <td>0.95B</td> <td>0.12B</td> <td>1.25B</td> <td>2023-05-12</td> </tr> <tr> <td>SDXL <d-cite key="podellSDXLImprovingLatent2024"></d-cite></td> <td>2.57B</td> <td>0.82B</td> <td>3.47B</td> <td>2023-06-25</td> </tr> <tr> <td>Kandinsky 3 <d-cite key="arkhipkinKandinsky30Technical2024"></d-cite><d-cite key="arkhipkinKandinsky3TexttoImage2024"></d-cite></td> <td>3.06B</td> <td>8.72B</td> <td>12.05B</td> <td>2023-12-11</td> </tr> <tr> <td>Stable Cascade (Würstchen) <d-cite key="perniasWurstchenEfficientArchitecture2024"></d-cite></td> <td>1.56B</td> <td>0.69B</td> <td>2.28B</td> <td>2024-02-07</td> </tr> </tbody> </table> <p>The standard U-Net architecture for diffusion models typically consists of an <strong>encoder</strong> that progressively downsamples the noisy input, a <strong>bottleneck</strong> middle block that processes compressed representations, and a <strong>decoder</strong> that upsamples back to the original resolution. Crucially, <strong>skip connections</strong> preserve fine-grained spatial information across corresponding encoder and decoder stages.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/diffusion_unet_illustration-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/diffusion_unet_illustration-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/diffusion_unet_illustration-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/diffusion_unet_illustration.png" width="100%" height="auto" alt="U-Net backbone used in diffusion models with time conditioning injected into residual blocks and skip connections between encoder and decoder." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">A typical U-Net backbone used in diffusion models with time conditioning. Time representation uses sinusoidal positional embeddings or random Fourier features; these time features are injected into residual blocks via simple spatial addition or adaptive group normalization layers. Image Credit: <d-cite key="CVPR2023Tutorial"></d-cite>.</figcaption> </figure> <h2 id="the-dits-era">The DiTs Era</h2> <p>As U-Net–based models began to hit a scaling ceiling (e.g., SDXL with ~2.6B parameters <d-cite key="podellSDXLImprovingLatent2024"></d-cite>), naive scaling proved ineffective, motivating a shift towards alternative backbones. The introduction of Diffusion Transformers (DiTs) <d-cite key="Peebles_2023_ICCV"></d-cite> marks a significant paradigm shift by recasting image generation as a patch-sequence modeling problem solved with transformer blocks. This approach offers several key advantages over U-Nets, including superior <strong>scalability</strong> via stacked DiT blocks, the ability to capture <strong>global context</strong> via self-attention for long-range dependencies, and a <strong>unified</strong> architecture that leverages advances in multimodal integration.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/dit-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/dit-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/dit-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/dit.png" width="100%" height="auto" alt="DiT Architecture." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">The Diffusion Transformer (DiT) architecture. Left: We train conditional latent DiT models. The input latent is decomposed into patches and processed by several DiT blocks. Right: Details of our DiT blocks. We experiment with variants of standard transformer blocks that incorporate conditioning via adaptive layer norm, cross-attention and extra input tokens. Adaptive layer norm works best. Image Credit: <d-cite key="Peebles_2023_ICCV"></d-cite>.</figcaption> </figure> <table> <thead> <tr> <th>Model</th> <th>Gen. (#Param)</th> <th>Txt. (#Param)</th> <th>Total (#Param)</th> <th>Release Date</th> </tr> </thead> <tbody> <tr> <td>PixArt-$\alpha$ <d-cite key="chenPixArtaFastTraining2024"></d-cite></td> <td>0.61B</td> <td>4.76B</td> <td>5.46B</td> <td>2023/10/06</td> </tr> <tr> <td>Lumina-T2I <d-cite key="gaoLuminaT2XScalableFlowbased2025a"></d-cite></td> <td>~4.7B</td> <td>~7B</td> <td>~15B</td> <td>2024/04/01</td> </tr> <tr> <td>PixArt-$\Sigma$ <d-cite key="chenPIXARTSWeaktoStrongTraining2024a"></d-cite></td> <td>0.61B</td> <td>4.76B</td> <td>5.46B</td> <td>2024/04/11</td> </tr> <tr> <td>Lumina-Next-T2I <d-cite key="zhuoLuminaNextMakingLuminaT2X2024a"></d-cite></td> <td>1.75B</td> <td>2.51B</td> <td>4.34B</td> <td>2024/05/12</td> </tr> <tr> <td>Stable Diffusion 3 <d-cite key="esserScalingRectifiedFlow2024"></d-cite></td> <td>2.03B</td> <td>5.58B</td> <td>7.69B</td> <td>2024/06/12</td> </tr> <tr> <td>Flux.1-Dev <d-cite key="blackforestlabsFLUX1"></d-cite></td> <td>11.90B</td> <td>4.88B</td> <td>16.87B</td> <td>2024/08/02</td> </tr> <tr> <td>CogView3-Plus <d-cite key="zhengCogView3FinerFaster2024a"></d-cite></td> <td>2.85B</td> <td>4.76B</td> <td>8.02B</td> <td>2024/10/13</td> </tr> <tr> <td>Hunyuan-DiT <d-cite key="liHunyuanDiTPowerfulMultiResolution2024"></d-cite></td> <td>1.50B</td> <td>2.02B</td> <td>3.61B</td> <td>2024/12/01</td> </tr> <tr> <td>SANA <d-cite key="xieSANAEfficientHighResolution2025"></d-cite></td> <td>0.59B</td> <td>2.61B</td> <td>3.52B</td> <td>2025/01/11</td> </tr> <tr> <td>Lumina-Image 2.0 <d-cite key="qinLuminaImage20Unified2025"></d-cite></td> <td>2.61B</td> <td>2.61B</td> <td>5.31B</td> <td>2025/01/22</td> </tr> <tr> <td>SANA 1.5 <d-cite key="xieSANA15Efficient2025a"></d-cite></td> <td>1.60B</td> <td>2.61B</td> <td>4.53B</td> <td>2025/03/21</td> </tr> <tr> <td>HiDream-I1-Dev <d-cite key="caiHiDreamI1HighEfficientImage2025"></d-cite></td> <td>17.11B</td> <td>5.58B</td> <td>22.77B</td> <td>2025/04/06</td> </tr> <tr> <td>CogView4-6B <d-cite key="zhengCogView3FinerFaster2024a"></d-cite></td> <td>3.50B</td> <td>2.00B</td> <td>6.00B</td> <td>2025/05/03</td> </tr> <tr> <td>Qwen-Image <d-cite key="wuQwenImageTechnicalReport2025"></d-cite></td> <td>20.43B</td> <td>8.29B</td> <td>28.85B</td> <td>2025/08/04</td> </tr> </tbody> </table> <h2 id="latest-advancement-in-u-net-and-dit-architecture-design">Latest Advancement in U-Net and DiT Architecture Design</h2> <p>While the transition from U-Net to DiT architectures represents a major paradigm shift, both architectural families have continued to evolve with innovative refinements. In the U-Net domain, <strong>two-stage cascaded approaches</strong> <d-cite key="hoCascadedDiffusionModels2022"></d-cite><d-cite key="sahariaImageSuperResolution2022"></d-cite> decompose generation into a low-resolution base model and specialized super-resolution upsamplers. <strong>U-ViT</strong> <d-cite key="Bao_2023_CVPR"></d-cite> bridges U-Net and transformer architectures by replacing CNN residual blocks with Vision Transformer blocks. Challenging the dominance of latent-space models, <strong>Simple Diffusion</strong> <d-cite key="hoogeboomSimpleDiffusionEndtoend2023"></d-cite> and <strong>Simpler Diffusion (SiD2)</strong> <d-cite key="hoogeboomSimplerDiffusion152025"></d-cite> demonstrate that end-to-end pixel-space diffusion can achieve state-of-the-art performance (FID 1.48 on ImageNet 512) through optimized noise schedules and simplified architectures.</p> <p>The DiT family has seen rapid advances across multiple dimensions. <strong>Architecture variants</strong> include <strong>SiT</strong> (Scalable Interpolant Transformer) <d-cite key="maSiTExploringFlow2024"></d-cite>, which replaces diffusion with interpolant-based transport, <strong>FiT</strong> (Flexible Vision Transformer) <d-cite key="luFiTFlexibleVision2024"></d-cite> which supports unrestricted resolutions and aspect ratios, and <strong>LiT</strong> (Linear Diffusion Transformer) <d-cite key="wangLiTDelvingSimple2025"></d-cite> and <strong>DiG</strong> (Diffusion GLA) <d-cite key="zhuDiGScalableEfficient2025"></d-cite> which achieve O(n) complexity through linear attention mechanisms. <strong>Training efficiency innovations</strong> such as <strong>MDT/MDTv2</strong> <d-cite key="gaoMaskedDiffusionTransformer2023"></d-cite><d-cite key="gaoMDTv2MaskedDiffusion2024"></d-cite> and <strong>MaskDiT</strong> <d-cite key="zhengFastTrainingDiffusion2024"></d-cite> leverage masked modeling to accelerate learning. Representation-based approaches <strong>REPA</strong> <d-cite key="yuRepresentationAlignmentGeneration2025"></d-cite> and <strong>REG</strong> <d-cite key="wuRepresentationEntanglementGeneration2025"></d-cite> incorporate external pretrained visual representations to dramatically accelerate training (e.g., REG achieves 63× faster training than SiT). <strong>U-DiTs</strong> <d-cite key="tianUDiTsDownsampleTokens2024"></d-cite> combine U-Net’s multiscale efficiency with DiT’s power. Furthermore, <strong>JiT</strong> (“Just image Transformers”) <d-cite key="liBackBasicsLet2025a"></d-cite> <d-cite key="nguyenImageWorthMore2024a"></d-cite> revisits the basics, showing that plain transformers operating directly on pixels or patches can perform surprisingly well without complex tokenizers.</p> <h2 id="pre-trained-text-to-image-checkpoints">Pre-trained Text-to-Image Checkpoints</h2> <p>The landscape of pre-trained text-to-image models has evolved dramatically since the introduction of Stable Diffusion. These models serve as powerful foundation models that can be adapted for specialized downstream tasks without architectural modifications, simply by fine-tuning on domain-specific datasets.</p> <h2 id="interactive-architecture-explorer">Interactive Architecture Explorer</h2> <div class="l-body"> <iframe id="architecture-explorer-iframe" src="/2026/assets/html/2026-04-27-diffusion-architecture-evolution/model-architecture-explorer.html" frameborder="0" scrolling="no" height="600px" width="100%" style="border: 1px solid #ddd; border-radius: 4px; min-height: 600px;"></iframe> </div> <script>
  // Listen for resize messages from the iframe
  window.addEventListener('message', function(e) {
    if (e.data && e.data.type === 'resize' && e.data.source === 'architecture-explorer') {
      var iframe = document.getElementById('architecture-explorer-iframe');
      if (iframe) {
        iframe.style.height = e.data.height + 'px';
      }
    }
  });
</script> <h3 id="u-net-family">U-Net Family</h3> <p><strong>Stable Diffusion</strong> <d-cite key="rombachHighResolutionImageSynthesis2022"></d-cite> represents the pioneering work in latent diffusion models, adopting a U-Net architecture that operates in a compressed latent space rather than pixel space. This design choice dramatically reduces computational costs while maintaining high-quality generation capabilities. The model combines two key components: a pre-trained variational autoencoder (VAE) for efficient image compression and decompression, and a diffusion model that performs the denoising process in this latent space.<d-footnote>In the prioring work of LDM in the paper <d-cite key="rombachHighResolutionImageSynthesis2022"></d-cite>, the VAE part is adopted a VQ-GAN style from <d-cite key="esserTamingTransformersHighResolution2021"></d-cite>. When it comes to CompVis Stable Diffusion v1.1-v.1.4 and StabilityAI Stable Diffusion v1.5 and v2.x version, the VAE part is turned to AutoEncoderKL style rather than a VQ style.</d-footnote></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sd-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sd-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sd-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sd.png" width="100%" height="auto" alt="Stable Diffusion 1.x - 2.x architecture." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Stable Diffusion 1.x - 2.x architecture. Image Credit: <d-cite key="esserTamingTransformersHighResolution2021"></d-cite>.</figcaption> </figure> <p><strong>Stable Diffusion XL (SDXL)</strong> <d-cite key="podellSDXLImprovingLatent2024"></d-cite> marked a significant scaling advancement, adopting a two-stage U-Net architecture and increasing the model size from 0.8 billion to 2.6 billion parameters. SDXL remains one of the largest U-Net-based models for image generation and demonstrates improved efficiency and compatibility across diverse domains and tasks. Despite reaching scaling limits, SDXL continues to serve as a foundation for numerous specialized applications.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sdxl-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sdxl-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sdxl-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sdxl.png" width="100%" height="auto" alt="SDXL Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">SDXL Architecture. Image Credit: <d-cite key="podellSDXLImprovingLatent2024"></d-cite>.</figcaption> </figure> <p><strong>Kandinsky</strong> <d-cite key="razzhigaevKandinskyImprovedTexttoImage2023"></d-cite> represents a significant advancement in the U-Net era, introducing a novel exploration of latent diffusion architecture that combines image prior models with latent diffusion techniques. The model features a modified MoVQ implementation as the image autoencoder component and achieves a FID score of 8.03 on the COCO-30K dataset, marking it as the top open-source performer in terms of measurable image generation quality. <strong>Kandinsky 3</strong> <d-cite key="arkhipkinKandinsky30Technical2024"></d-cite><d-cite key="arkhipkinKandinsky3TexttoImage2024"></d-cite> continues this series with improved text understanding and domain-specific performance, presenting a multifunctional generative framework supporting text-guided inpainting/outpainting, image fusion, and image-to-video generation.</p> <p><strong>Stable Cascade</strong> (based on Würstchen architecture) <d-cite key="perniasWurstchenEfficientArchitecture2024"></d-cite> introduces an efficient architecture for large-scale text-to-image diffusion models, achieving competitive performance with unprecedented cost-effectiveness. The key innovation is a latent diffusion technique that learns extremely compact semantic image representations, reducing computational requirements significantly—training requires only 24,602 A100-GPU hours compared to Stable Diffusion 2.1’s 200,000 GPU hours while maintaining state-of-the-art results.</p> <p><strong>UniDiffuser</strong> <d-cite key="baoOneTransformerFits2023"></d-cite> explores transformer-based diffusion models with a unified framework that fits all distributions relevant to multi-modal data in one model. While primarily focused on transformer architectures, this work demonstrates the potential for unified multi-modal generation within the diffusion framework.</p> <h3 id="pixart-alpha-20231006">Pixart-$\alpha$ (2023/10/06)</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_cost-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_cost-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_cost-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_cost.png" width="100%" height="auto" alt="Cost Comparison" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Comparisons of CO2 emissions and training cost among T2I generators. PIXART-α achieves an exceptionally low training cost of $28,400. Compared to RAPHAEL <d-cite key="xueRAPHAELTexttoImageGeneration2023"></d-cite>, our CO2 emissions and training costs are merely 1.2% and 0.91%, respectively. Image Credit: <d-cite key="chenPixArtaFastTraining2024"></d-cite>.</figcaption> </figure> <p>PixArt-$\alpha$ is motivated by the rising compute and environmental costs of text-to-image systems, seeking near-commercial quality with a much smaller training budget <d-cite key="chenPixArtaFastTraining2024"></d-cite>. In contrast to SD 1.5/2.1, it adopts a large-language-model text encoder (T5) <d-cite key="raffelExploringLimitsTransfer2020"></d-cite>, making it the first open-source diffusion T2I model to use an LLM-based text encoder while keeping the overall design streamlined.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_alpha-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_alpha-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_alpha-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/pixart_alpha.png" width="100%" height="auto" alt="Pixart-α Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Model architecture of PIXART-α. A cross-attention module is integrated into each block to inject textual conditions. To optimize efficiency, all blocks share the same adaLN-single parameters for time conditions. Image Credit: <d-cite key="chenPixArtaFastTraining2024"></d-cite>.</figcaption> </figure> <p>Architecturally, PixArt-$\alpha$ is a latent Diffusion Transformer (DiT): VAE latents are patchified into a token sequence processed by stacked Transformer blocks; each block applies cross-attention to text tokens, and timestep conditioning is injected via a shared adaLN-single, simplifying parameters and conditioning pathways <d-cite key="chenPixArtaFastTraining2024"></d-cite>.</p> <div class="key-differences"> <strong>Key differences vs SD 1.5/2.1</strong> <ul> <li>Transformer sequence-of-patches backbone (no encoder–decoder or skip connections)</li> <li>Shared adaLN for time and unified per-block cross-attention (vs U-Net residual blocks with per-block time MLP/spatial injections)</li> <li>T5 text encoder (LLM) rather than CLIP/OpenCLIP</li> </ul> </div> <h3 id="lumina-t2i-20240401">Lumina-T2I (2024/04/01)</h3> <p>Lumina-T2I is the first entry in the Lumina series from Shanghai AI Lab, aiming for a simple, scalable framework that supports flexible resolutions while maintaining photorealism. Building on the Sora insight that scaling Diffusion Transformers enables generation across arbitrary aspect ratios and durations yet lacks concrete implementation details, Lumina-T2I adopts flow matching to stabilize and accelerate training <d-cite key="gaoLuminaT2XScalableFlowbased2025a"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_t2x-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_t2x-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_t2x-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_t2x.png" width="100%" height="auto" alt="Lumina-T2I Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Lumina-T2I architecture featuring Flag-DiT backbone. Image Credit: <d-cite key="gaoLuminaT2XScalableFlowbased2025a"></d-cite>.</figcaption> </figure> <p>Architecturally, Lumina-T2I uses a Flow-based Large Diffusion Transformer (Flag-DiT) with zero-initialized attention, RoPE <d-cite key="suRoFormerEnhancedTransformer2024"></d-cite>, and KQ-Norm <d-cite key="henryQueryKeyNormalizationTransformers2020"></d-cite>. Latent features are tokenized and processed by Transformer blocks; learnable placeholders such as the [nextline] token and layerwise relative position injection enable robust resolution extrapolation without retraining for each size.</p> <div class="key-differences"> <strong>Key differences vs PixArt-α</strong> <ul> <li>Robust resolution generalization across 512²–1792²</li> <li>Uses one-dimensional RoPE, [nextline] token, and layerwise relative position injection</li> <li>PixArt-α uses absolute positional embeddings limited to the initial layer, degrading at out-of-distribution scales</li> </ul> </div> <h3 id="lumina-next-t2i-20240512">Lumina-Next-T2I (2024/05/12)</h3> <p>Lumina-Next-T2I <d-cite key="zhuoLuminaNextMakingLuminaT2X2024a"></d-cite> targets the core limitations observed in Lumina-T2X—training instability, slow inference, and resolution extrapolation artifacts—by delivering stronger quality and faster sampling while improving zero-shot multilingual understanding. Unlike prior T2I works that rely on CLIP or T5 encoders <d-cite key="raffelExploringLimitsTransfer2020"></d-cite>, the Lumina series adopts decoder-only LLMs as text encoders: Lumina-T2X uses LLaMA-2 7B <d-cite key="touvronLlama2Open2023"></d-cite>, whereas Lumina-Next employs the lighter Gemma-2B to reduce memory and increase throughput. In practice, Lumina-Next shows clear gains on multilingual prompts (vs. CLIP/T5 setups) and further improves text-image alignment with alternative LLMs like Qwen-1.8B and InternLM-7B.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_next-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_next-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_next-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_next.png" width="100%" height="auto" alt="Lumina-Next-T2I Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Lumina-Next-T2I Next-DiT architecture. Image Credit: <d-cite key="zhuoLuminaNextMakingLuminaT2X2024a"></d-cite>.</figcaption> </figure> <p>Architecturally, Lumina-Next introduces the Next-DiT backbone with 3D RoPE and Frequency- and Time-Aware Scaled RoPE for robust resolution extrapolation <d-cite key="suRoFormerEnhancedTransformer2024"></d-cite>. It adds sandwich normalizations to stabilize training (cf. normalization strategies such as KQ-Norm <d-cite key="henryQueryKeyNormalizationTransformers2020"></d-cite>), a sigmoid time discretization schedule to reduce Flow-ODE sampling steps, and a Context Drop mechanism that merges redundant visual tokens to accelerate inference—all while retaining the flow-based DiT formulation of the Lumina family.</p> <div class="key-differences"> <strong>Key differences vs Lumina-T2I</strong> <ul> <li>Next-DiT with 3D RoPE + frequency/time-aware scaling for stronger resolution extrapolation</li> <li>Sandwich normalizations improve stability; sigmoid time schedule reduces sampling steps</li> <li>Context Drop merges redundant tokens for faster inference throughput</li> <li>Decoder-only LLM text encoders (Gemma-2B by default; Qwen-1.8B/InternLM-7B optional) boost zero-shot multilingual alignment vs CLIP/T5</li> </ul> </div> <h3 id="stable-diffusion-3-20240612">Stable Diffusion 3 (2024/06/12)</h3> <p>Stable Diffusion 3 aims to improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales, demonstrating superior performance compared to established diffusion formulations for high-resolution text-to-image synthesis <d-cite key="esserScalingRectifiedFlow2024"></d-cite>. This work presents the first comprehensive scaling study for text-to-image DiTs, establishing predictable scaling trends and correlating lower validation loss to improved synthesis quality across various metrics and human evaluations.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sd3-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sd3-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sd3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sd3.png" width="100%" height="auto" alt="SD3 Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Stable Diffusion 3 Architecture. Image Credit: <d-cite key="esserScalingRectifiedFlow2024"></d-cite>.</figcaption> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/simplified_architecture_sd35-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/simplified_architecture_sd35-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/simplified_architecture_sd35-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/simplified_architecture_sd35.png" width="100%" height="auto" alt="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Simplied Architecture Illustration of Stable Diffusion 3.5 MM-DiT block. Image Source: Stability AI Blog.</figcaption> </figure> <p>Architecturally, SD3 transitions from DiT’s cross-attention blocks to MMDiT (Multimodal Diffusion Transformer) with double-stream blocks that use separate weights for the two modalities, enabling bidirectional flow of information between image and text tokens for improved text comprehension and typography. Unlike SDXL which relies primarily on CLIP encoders, SD3 incorporates both CLIP (L/14 and OpenCLIP bigG/14) and T5-XXL encoders <d-cite key="raffelExploringLimitsTransfer2020"></d-cite>, concatenating pooled outputs and hidden representations to create comprehensive text conditioning with enhanced understanding capabilities.</p> <div class="key-differences"> <strong>Key differences vs SDXL and PixArt-α</strong> <ul> <li>MMDiT double-stream architecture with separate weights per modality and bidirectional information flow (vs single-stream cross-attention)</li> <li>Integrated rectified flow training with perceptually-biased noise sampling (vs standard diffusion formulation)</li> <li>Combined CLIP + T5-XXL text encoding for enhanced text comprehension and typography</li> <li>First comprehensive scaling study demonstrating predictable trends for text-to-image DiTs</li> </ul> </div> <h3 id="flux1-dev-20240802">Flux.1-Dev (2024/08/02)</h3> <p>Flux.1-Dev, developed by former Stability AI core members, aims to scale beyond previous models and achieve superior image quality with more accurate text-to-image synthesis <d-cite key="blackforestlabsFLUX1"></d-cite>. Representing a significant scaling effort, the model features a massive 12 billion parameter generator combined with a 4.7 billion parameter text encoder, marking substantial growth compared to predecessors and establishing new benchmarks in AI-driven image generation capabilities.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/flux_dit-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/flux_dit-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/flux_dit-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/flux_dit.png" width="100%" height="auto" alt="Flux.1-Dev Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Flux.1-Dev MMDiT architecture. Image Credit: <d-cite key="labsFLUX1KontextFlow2025"></d-cite>.</figcaption> </figure> <p>Architecturally, Flux.1-Dev advances beyond SD3’s MMDiT by implementing a hybrid architecture that combines both single-stream and double-stream Multi-Modal Diffusion Transformers, enhancing the model’s ability to process complex visual-textual relationships. Like SD3, it incorporates T5 text encoding <d-cite key="raffelExploringLimitsTransfer2020"></d-cite> and integrates rectified flow techniques for more stable and efficient training, while conducting a comprehensive scaling study that optimizes performance across the substantially larger parameter space.</p> <div class="key-differences"> <strong>Key differences vs SD3</strong> <ul> <li>Hybrid single-stream + double-stream MMDiT architecture (vs purely double-stream MMDiT)</li> <li>Massive scaling to 12B generator + 4.7B text encoder parameters (vs smaller SD3 variants)</li> <li>Enhanced rectified flow implementation optimized for larger scale training</li> <li>Comprehensive scaling study specifically designed for multi-billion parameter DiTs</li> </ul> </div> <h3 id="cogview3--cogview3-plus-20241013">CogView3 &amp; CogView3-Plus (2024/10/13)</h3> <p><strong>CogView3</strong> <d-cite key="zhengCogView3FinerFaster2024a"></d-cite> introduces a <strong>relay diffusion approach</strong> <d-cite key="tengRelayDiffusionUnifying2024"></d-cite> that generates low-resolution images first, then refines them through super-resolution to achieve 2048×2048 outputs. This multi-stage process reduces computational costs while improving quality—CogView3 outperformed SDXL by 77% in human evaluations while using only one-tenth the inference time. The model employs a text-expansion language model to rewrite user prompts, with a base stage generating 512×512 images followed by relaying super-resolution in the latent space.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/cogview3-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/cogview3-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/cogview3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/cogview3.png" width="100%" height="auto" alt="CogView3 Architecture." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">(left) The pipeline of CogView3. User prompts are rewritten by a text-expansion language model. The base stage model generates 512 × 512 images, and the second stage subsequently performs relaying super-resolution. (right) Formulation of relaying super-resolution in the latent space. Image Credit: <d-cite key="zhengCogView3FinerFaster2024a"></d-cite>.</figcaption> </figure> <p><strong>CogView3-Plus</strong> upgrades to DiT architecture with Zero-SNR scheduling and joint text-image attention for further efficiency gains. This architectural evolution represents a significant step in the CogView series, transitioning from traditional approaches to transformer-based diffusion models while maintaining the efficiency advantages of the relay diffusion framework.</p> <h3 id="hunyuan-dit-20241201">Hunyuan-DiT (2024/12/01)</h3> <p>Hunyuan-DiT, developed by Tencent’s Hunyuan team, aims to create a powerful multi-resolution diffusion transformer capable of fine-grained understanding of both English and Chinese languages, addressing the need for state-of-the-art Chinese-to-image generation with culturally relevant and multilingual capabilities <d-cite key="liHunyuanDiTPowerfulMultiResolution2024"></d-cite>. The model establishes a comprehensive data pipeline with iterative optimization, employing a Multimodal Large Language Model to refine image captions and enhance alignment between textual descriptions and generated images, particularly for intricate Chinese characters and cultural nuances.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/hunyuandit-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/hunyuandit-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/hunyuandit-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/hunyuandit.png" width="100%" height="auto" alt="Hunyuan-DiT Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Hunyuan-DiT multi-resolution architecture. Image Credit: <d-cite key="liHunyuanDiTPowerfulMultiResolution2024"></d-cite>.</figcaption> </figure> <p>Architecturally, Hunyuan-DiT builds upon PixArt-$\alpha$ by incorporating both single-stream and double-stream Multi-Modal Diffusion Transformer (MM-DiT) blocks similar to SD3, enabling efficient handling of complex image generation tasks across multiple resolutions. The model integrates dual text encoders—CLIP for understanding overall semantic content and T5 <d-cite key="raffelExploringLimitsTransfer2020"></d-cite> for nuanced language comprehension including complex sentence structures—combined with enhanced positional encoding to maintain spatial information across different resolutions, facilitating robust multi-resolution generation capabilities.</p> <div class="key-differences"> <strong>Key differences vs PixArt-α</strong> <ul> <li>Single-stream + double-stream MM-DiT blocks for enhanced multi-modal processing (vs single-stream cross-attention)</li> <li>Dual text encoders (CLIP + T5) for semantic and nuanced language understanding (vs T5 only)</li> <li>Multi-resolution diffusion transformer with enhanced positional encoding for robust resolution handling</li> <li>Multimodal LLM-refined captions with fine-grained bilingual (English + Chinese) understanding</li> </ul> </div> <h3 id="sana-20250111">SANA (2025/01/11)</h3> <p>SANA, developed by NVIDIA, aims to enable efficient high-resolution image synthesis up to 4096×4096 pixels while maintaining deployment feasibility on consumer hardware, generating 1024×1024 images in under a second on a 16GB laptop GPU <d-cite key="xieSANAEfficientHighResolution2025"></d-cite>. The model introduces innovations to reduce computational requirements dramatically: DC-AE (deep compression autoencoder) achieves 32× image compression reducing latent tokens significantly, efficient caption labeling and selection accelerate convergence, and Flow-DPM-Solver reduces sampling steps for faster generation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sana-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sana-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sana-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sana.png" width="100%" height="auto" alt="SANA Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">SANA Linear DiT architecture for efficient high-resolution generation. Image Credit: <d-cite key="xieSANAEfficientHighResolution2025"></d-cite>.</figcaption> </figure> <p>Architecturally, SANA advances beyond PixArt-$\Sigma$ by replacing traditional self-attention mechanisms with Linear Diffusion Transformer (Linear DiT) blocks, enhancing computational efficiency at high resolutions without compromising quality. The model adopts a decoder-only small language model as the text encoder, employing complex human instructions with in-context learning to improve text-image alignment compared to conventional CLIP or T5 encoders. The compact 0.6B parameter model achieves competitive performance with substantially larger models like Flux-12B while being 20 times smaller and over 100 times faster in throughput.</p> <div class="key-differences"> <strong>Key differences vs PixArt-Σ</strong> <ul> <li>Linear DiT replacing traditional self-attention for O(n) complexity vs O(n²) at high resolutions</li> <li>DC-AE with 32× compression reducing latent tokens and memory requirements dramatically</li> <li>Decoder-only language model as text encoder with in-context learning (vs T5)</li> <li>0.6B parameters achieving competitive quality with 12B models while 100× faster throughput</li> </ul> </div> <h3 id="lumina-image-20-20250122">Lumina-Image 2.0 (2025/01/22)</h3> <p>Lumina-Image 2.0 aims to provide a unified and efficient image generative framework that excels in generating high-quality images with strong text-image alignment across diverse generation and editing tasks <d-cite key="qinLuminaImage20Unified2025"></d-cite>. Building upon the Lumina series’ foundation, the model consolidates multiple generation tasks into a cohesive framework, optimizing performance and efficiency to cater to a wide range of image generation applications while achieving competitive scores across multiple benchmarks including FID and CLIP metrics.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_image2-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_image2-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_image2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/lumina_image2.png" width="100%" height="auto" alt="Lumina-Image 2.0 Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Lumina-Image 2.0 Unified Next-DiT architecture. Image Credit: <d-cite key="qinLuminaImage20Unified2025"></d-cite>.</figcaption> </figure> <p>Architecturally, Lumina-Image 2.0 advances beyond Lumina-Next-T2I by introducing a unified Next-DiT architecture that seamlessly integrates text-to-image generation and image editing capabilities within a shared framework. The model maintains the Lumina series’ architectural strengths including 3D RoPE <d-cite key="suRoFormerEnhancedTransformer2024"></d-cite>, frequency-aware scaling, and flow-based formulation, while enhancing the framework to support both generation and editing operations efficiently. This unified approach enables the model to leverage shared representations and training strategies across different image generation modalities.</p> <div class="key-differences"> <strong>Key differences vs Lumina-Next-T2I</strong> <ul> <li>Unified Next-DiT framework seamlessly integrating generation and editing (vs generation-only focus)</li> <li>Enhanced multi-task architecture supporting diverse image generation applications within single model</li> <li>Optimized training paradigm leveraging shared representations across generation modalities</li> <li>Competitive performance across FID and CLIP benchmarks with improved efficiency</li> </ul> </div> <h3 id="sana-15-20250321">SANA 1.5 (2025/03/21)</h3> <p>SANA 1.5 aims to push the boundaries of efficient high-resolution image synthesis established by SANA, offering improved performance and scalability through larger model sizes and advanced inference scaling techniques <d-cite key="xieSANA15Efficient2025a"></d-cite>. The model introduces inference scaling via VISA (a specialized NVILA-2B model) that scores and selects top images from large candidate sets, significantly boosting GenEval performance scores—for instance, improving SANA-1.5-4.8B from 81 to 96. This approach demonstrates that post-generation selection can dramatically enhance quality metrics without architectural changes.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sana1_5-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sana1_5-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sana1_5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/sana1_5.png" width="100%" height="auto" alt="SANA 1.5 Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">SANA 1.5 improved Linear DiT architecture. Image Credit: <d-cite key="xieSANA15Efficient2025a"></d-cite>.</figcaption> </figure> <p>Architecturally, SANA 1.5 builds upon the original SANA by incorporating an enhanced DC-AE (deep compression autoencoder) to handle higher resolutions and more complex generation tasks, along with advanced Linear DiT blocks featuring more sophisticated linear attention mechanisms to boost efficiency and quality in high-resolution synthesis. The model scales to 4.8B parameters compared to SANA’s 0.6B, providing a robust solution for generating high-quality images with strong text-image alignment suitable for diverse professional applications requiring both quality and computational efficiency.</p> <div class="key-differences"> <strong>Key differences vs SANA</strong> <ul> <li>Inference scaling with VISA model for candidate selection dramatically improving GenEval scores (81→96)</li> <li>Enhanced DC-AE handling higher resolutions and more complex generation tasks</li> <li>Advanced Linear DiT with more sophisticated linear attention mechanisms</li> <li>Scaled to 4.8B parameters providing improved quality while maintaining efficiency advantages</li> </ul> </div> <h3 id="hidream-i1-dev-20250406">HiDream-I1-Dev (2025/04/06)</h3> <p>HiDream-I1, developed by HiDream.ai, addresses the critical trade-off between quality improvements and computational complexity in image generative foundation models, aiming to achieve state-of-the-art image generation quality within seconds while maintaining high efficiency <d-cite key="caiHiDreamI1HighEfficientImage2025"></d-cite>. With 17 billion parameters, the model introduces a sparse Diffusion Transformer structure that enables efficient inference suitable for professional-grade design needs, supporting 4K ultra-high-definition image generation with advanced text comprehension, multi-style adaptation, and precise detail control while optimizing computational requirements through sparsity.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/hidream-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/hidream-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/hidream-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/hidream.png" width="100%" height="auto" alt="HiDream-I1-Dev Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">HiDream-I1-Dev Sparse DiT architecture. Image Credit: <d-cite key="caiHiDreamI1HighEfficientImage2025"></d-cite>.</figcaption> </figure> <p>Architecturally, HiDream-I1 advances beyond Flux.1-Dev and Qwen-Image by implementing a novel sparse DiT structure where only subsets of transformer blocks are activated for each forward pass, dramatically reducing computational costs while maintaining generation quality. The sparse architecture enables the massive 17B parameter model to achieve practical inference speeds comparable to smaller dense models, with efficient diffusion mechanisms supporting multimodal input and providing fine-grained control over generation. This sparse approach represents a paradigm shift in scaling DiT models, demonstrating that architectural efficiency through sparsity can rival quality of substantially denser models.</p> <div class="key-differences"> <strong>Key differences vs Flux.1-Dev and other large DiTs</strong> <ul> <li>Sparse DiT structure activating only subsets of blocks per forward pass for efficient 17B parameter model</li> <li>4K ultra-high-definition generation support with optimized inference speed despite massive scale</li> <li>Advanced sparse attention mechanisms maintaining quality while dramatically reducing computational costs</li> <li>Multimodal input support and fine-grained control optimized for professional-grade design applications</li> </ul> </div> <h3 id="cogview4-6b-20250503">CogView4-6B (2025/05/03)</h3> <p><strong>CogView4-6B</strong> <d-cite key="zhengCogView3FinerFaster2024a"></d-cite> represents the latest advancement in the CogView series, featuring a sophisticated <strong>CogView4Transformer2DModel</strong> architecture that excels in Chinese text rendering and multilingual image generation. The model demonstrates exceptional performance in text accuracy evaluation, achieving precision of 0.6969, recall of 0.5532, and F1 score of 0.6168 on Chinese text benchmarks.</p> <p>CogView4-6B leverages GLM-based text encoding and advanced transformer blocks with RoPE (Rotary Position Embedding) for enhanced spatial understanding and text-image alignment. This architectural sophistication enables the model to achieve superior text rendering capabilities, particularly for complex Chinese characters and multilingual content, setting new standards for text-to-image generation in non-Latin scripts. Available on <a href="https://huggingface.co/zai-org/CogView4-6B">Hugging Face</a> under Apache 2.0 license.</p> <h3 id="qwen-image-20250804">Qwen-Image (2025/08/04)</h3> <p>Qwen-Image represents a monumental scaling achievement in text-to-image synthesis, establishing a new state-of-the-art with its massive 28.85 billion parameter architecture <d-cite key="wuQwenImageTechnicalReport2025"></d-cite>. Developed by Alibaba’s Qwen team, this flagship model aims to push the boundaries of generation quality, text-image alignment, and multimodal understanding through unprecedented scale. The model excels at generating highly detailed, photorealistic images that accurately reflect complex textual prompts, setting new benchmarks for fidelity and coherence in the field.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/qwen_image-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/qwen_image-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-architecture-evolution/qwen_image-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-diffusion-architecture-evolution/qwen_image.png" width="100%" height="auto" alt="Qwen-Image Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Qwen-Image massively scaled MMDiT architecture. Image Credit: <d-cite key="wuQwenImageTechnicalReport2025"></d-cite>.</figcaption> </figure> <p>Architecturally, Qwen-Image employs a massively scaled Multi-Modal Diffusion Transformer (MMDiT) that builds upon the hybrid single- and double-stream designs seen in models like Flux.1-Dev. The generator model alone comprises over 20 billion parameters, combined with a powerful 8.29 billion parameter text encoder for unparalleled language comprehension. This dual-stream approach allows for sophisticated interaction between text and image modalities, enabling precise control over generated content. The model integrates advanced training techniques, including rectified flow and large-scale data curation, to ensure stable and efficient convergence despite its enormous size.</p> <div class="key-differences"> <strong>Key differences vs HiDream-I1-Dev</strong> <ul> <li>Massive dense scaling to 28.85B parameters (vs HiDream's 17B sparse architecture)</li> <li>Focus on state-of-the-art quality through sheer scale (vs HiDream's focus on efficiency via sparsity)</li> <li>Extremely large 8.29B text encoder for superior text-image alignment</li> <li>Represents the pinnacle of the dense DiT scaling paradigm before potential shifts to new architectures</li> </ul> </div> <h2 id="experiments-and-case-studies">Experiments and Case Studies</h2> <p>To comprehensively evaluate the capabilities of different text-to-image diffusion models, we propose a systematic evaluation framework spanning tasks of varying complexity. This section will present case studies of text-to-image generation visualizations using existing checkpoints, assessing their performance across a spectrum of increasingly challenging tasks.</p> <p><strong>Implementation Details<d-footnote>For commercial model, we use ChatGPT webui GPT-5-Instant with the same prompt for each case study for image generation with a default image size as 1024 × 1024</d-footnote>:</strong></p> <table> <thead> <tr> <th>Parameter</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Precision</td> <td>bfloat16</td> </tr> <tr> <td>Scheduler</td> <td>default</td> </tr> <tr> <td>Steps</td> <td>50</td> </tr> <tr> <td>Guidance Scale</td> <td>7.5</td> </tr> <tr> <td>Resolution</td> <td>512×512</td> </tr> </tbody> </table> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-diffusion-architecture-evolution/case-studies.html" width="100%" height="3000" frameborder="0" style="border: none;"></iframe> </div> <script>
  window.addEventListener('message', function(e) {
    if (e.data.type === 'resize' && e.data.source === 'case-studies') {
      const iframe = document.querySelector('iframe[src*="case-studies.html"]');
      if (iframe) {
        iframe.style.height = e.data.height + 'px';
      }
    }
  });
</script> <div class="key-differences"> <strong>Summary of Results</strong> <ul> <li>There is no strong correlation between image model size and image aesthetics (See case study 4).</li> <li>There is no strong correlation between text model size and prompt following (See case study 5).</li> <li>Large models generally work better but always the case.</li> <li>U-Nets based model perform comparativaly worse than DiTs in the similar model size, for instance, SDXL to SANA, Kandinsky-3 to CogView4.</li> <li>StaleDiffusion 3.x continously trained on higher resolution (e.g., 1024px) tends to generate croped results.</li> <li>Not all models are capable to dealing with multilingual prompt (see case study 2).</li> <li>Commercial model such as GPT-Image model works extremely well in aesthetics, prompt following, counting, text rendering and spatial reasoning.</li> </ul> </div> <h2 id="why-scaling-favors-attention">Why Scaling Favors Attention</h2> <p>As diffusion models scaled in data and compute, the active bottleneck shifted from <strong>local fidelity</strong> to <strong>global semantic alignment</strong>, and the community moved accordingly: from U-Nets that hard-wire translation equivariance via convolution to Diffusion Transformers that <strong>learn</strong> equivariances through self-attention. Let \(\mathcal{C}^{\mathrm{conv}}_{G}\) be the class of <strong>translation-equivariant, finite-support Toeplitz operators</strong> (U-Net convolutional kernels) and \(\mathcal{A}^{\mathrm{attn}}\) the class of <strong>self-attention kernels with relative positional structure</strong> (DiTs). Write \(\sqsubseteq^{\mathrm{bias}}\) as “is a constrained instance of (via inductive-bias constraints)”<d-cite key="hornTranslationalEquivarianceKernelizable2021"></d-cite><d-cite key="taiMathematicalExplanationUNet2024"></d-cite>.</p> \[\boxed{ \mathcal{C}^{\mathrm{conv}}_{G}\ \sqsubseteq^{\mathrm{bias}}\ \mathcal{A}^{\mathrm{attn}} }\] <p>In plain terms, <strong>convolution is a simplified, efficient expression of attention</strong> obtained by enforcing fixed translation symmetry, parameter tying, and locality<d-cite key="ramachandranStandAloneSelfAttentionVision2019"></d-cite><d-cite key="cordonnierRelationshipSelfAttentionConvolutional2020"></d-cite><d-cite key="changConvolutionsSelfAttentionReinterpreting2021"></d-cite><d-cite key="choiGraphConvolutionsEnrich2024"></d-cite><d-cite key="joshiTransformersAreGraph2025"></d-cite>; removing these constraints yields attention <strong>without a hard-coded translation prior</strong>, allowing DiTs to <em>learn</em> which symmetries and long-range relations matter at scale. This inclusion explains the empirical shift under modern hardware and datasets: attention strictly generalizes convolution while retaining it as an efficient special case, delivering smoother scaling laws and higher semantic “bandwidth” per denoising step. In practice, this is also a story of hardware path dependence: attention’s dense-matrix primitives align with contemporary accelerators and compiler stacks, effectively “winning” the hardware lottery <d-cite key="hookerHardwareLottery2021"></d-cite>. And, echoing the Bitter Lesson<d-cite key="richsuttonBitterLesson2019"></d-cite>, as data and compute grow, general methods with fewer hand-engineered priors dominate—making attention’s strict generalization of convolution the natural backbone at scale.</p> <h2 id="further-discussion">Further Discussion</h2> <h3 id="from-text-to-image-generation-to-real-world-applications">From Text-to-Image Generation to Real-World Applications</h3> <p>Text-to-image is now genuinely strong; the next wave is about <strong>conditioning existing pixels</strong> rather than generating from scratch—turning models into reliable editors that honor what must stay and change only what’s asked. This means prioritizing downstream tasks like image editing, inpainting/outpainting, image-to-image restyling, and structure- or reference-guided synthesis (edges, depth, layout, style, identity). The practical focus shifts from unconstrained novelty to controllable, faithful rewrites with tight mask adherence, robust subject/style preservation, and interactive latencies, so these systems plug cleanly into real creative, design, and industrial workflows.</p> <h3 id="diffusion-models-vs-auto-regressive-models">Diffusion Models vs. Auto-regressive Models</h3> <p>Diffusion models and autoregressive (AR) models represent two fundamentally different approaches to image generation, with the key distinction being that <strong>autoregressive models operate on discrete image tokens</strong> while <strong>diffusion models work with continuous representations</strong>. Autoregressive models like DALL-E <d-cite key="rameshZeroShotTexttoImageGeneration2021"></d-cite>, CogView <d-cite key="dingCogViewMasteringTexttoImage2021"></d-cite>, and CogView2 <d-cite key="dingCogView2FasterBetter2022"></d-cite> treat image generation as a sequence modeling problem, encoding images into discrete tokens using VQ-VAE <d-cite key="esserTamingTransformersHighResolution2021"></d-cite> or similar vector quantization methods, then autoregressively predicting the next token given previous tokens. This approach offers sequential generation with precise control and natural language integration, but suffers from slow generation, error accumulation, and discrete representation loss. In contrast, diffusion models operate directly on continuous pixel or latent representations, learning to reverse a gradual noise corruption process, which enables parallel generation, high-quality outputs, and flexible conditioning, though at the cost of computational overhead and less direct control. Recent advances have significantly improved autoregressive approaches: VAR <d-cite key="tianVisualAutoregressiveModeling2024"></d-cite> redefines autoregressive learning as coarse-to-fine “next-scale prediction” and achieves superior performance compared to diffusion transformers, while Infinity <d-cite key="hanInfinityScalingBitwise2025"></d-cite> demonstrates effective scaling of bitwise autoregressive modeling for high-resolution synthesis. Additionally, MAR <d-cite key="liAutoregressiveImageGeneration2024a"></d-cite> bridges the gap between paradigms by adopting diffusion loss for autoregressive models, enabling continuous-valued autoregressive generation without vector quantization. Recent work has also explored hybrid approaches that combine both paradigms: HunyuanImage 3.0 <d-cite key="caoHunyuanImage30Technical2025"></d-cite> and BLIP3-o <d-cite key="chenBLIP3oFamilyFully2025"></d-cite> demonstrate unified multimodal models within autoregressive frameworks while incorporating diffusion-inspired techniques, while OmniGen <d-cite key="xiaoOmniGenUnifiedImage2024"></d-cite> and OmniGen2 <d-cite key="wuOmniGen2ExplorationAdvanced2025"></d-cite> use diffusion models as backbones for unified generation capabilities.</p> <hr/> <p><strong>Note:</strong> This blog post’s knowledge cutoff is November 17, 2025. For the latest developments, we refer readers to recent works including DiP <d-cite key="chenDiPTamingDiffusion2025"></d-cite>, DeCo <d-cite key="maDeCoFrequencyDecoupledPixel2025"></d-cite>, PixelDiT <d-cite key="yuPixelDiTPixelDiffusion2025"></d-cite>, Z-Image <d-cite key="z-image-2025"></d-cite>, FLUX.2 <d-cite key="flux-2-2025"></d-cite>, and LongCat-Image for readers to explore.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[A comprehensive analysis of how diffusion model architectures evolved from U-Net backbones to Diffusion Transformers, transforming text-to-image generation capabilities.]]></summary></entry><entry><title type="html">Discretisation invariance</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/discretisation-invariance/" rel="alternate" type="text/html" title="Discretisation invariance"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/discretisation-invariance</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/discretisation-invariance/"><![CDATA[<h2 id="introduction-and-motivation">Introduction and motivation</h2> <p>Partial differential equations (PDEs) are the gold standard in scientific modelling. With rare exceptions, PDEs are solved numerically, and the goal is always to develop a reliable, accurate algorithm that delivers a solution as cheaply as possible. Neural networks were first explored as a solution method starting at least from the 1990s <d-cite key="lee1990neural"></d-cite>, <d-cite key="lagaris1998artificial"></d-cite>. Over time, these methods converged to two dominant paradigms: physics-informed neural networks (PINNs, <d-cite key="lagaris1998artificial"></d-cite>, <d-cite key="raissi2017physics"></d-cite>) and operator learning <d-cite key="kovachki2023neural"></d-cite>, <d-cite key="lu2021learning"></d-cite>. PINNs is an unsupervised techniques that directly aims to solve PDE. Operator learning, the approach we consider here, is a supervised technique aiming to amortize the cost of parametric PDE solution in the “multi-query setting”. We explain the setup of operator learning below.</p> <p>We start by specifying a model of interest in the form of PDE. For the sake of example we consider stationary diffusion equation</p> \[\begin{aligned} &amp;\frac{\partial}{\partial \boldsymbol{x}_{1}}\left(k(\boldsymbol{x})\frac{\partial}{\partial \boldsymbol{x}_{1}} u(\boldsymbol{x})\right)+\frac{\partial}{\partial \boldsymbol{x}_{2}}\left(k(\boldsymbol{x})\frac{\partial}{\partial \boldsymbol{x}_{2}} u(\boldsymbol{x})\right) = f(\boldsymbol{x}),\\ &amp;\boldsymbol{x}\in\Gamma=(0, 1)^2,\,\left.u(\boldsymbol{x})\right|_{\boldsymbol{x}\in\partial\Gamma} = 0. \end{aligned}\] <p>This PDE naturally appears in modelling of multiphase fluid flow, heat conduction and electrostatic problems in heterogeneous media.</p> <p>PDE contains two parameters: diffusivity coefficient $k(\boldsymbol{x}) &gt; 0$ and the source term $f(\boldsymbol{x})$. We assume that we need to solve the stationary diffusion equation repeatedly for a large set of parameters drawn from a joint probability distribution $k(\boldsymbol{x}), f(\boldsymbol{x}) \sim p_{f, k}$. One may simply call a classical solver for each new pair of parameters, but it can be more advantageous to exploit information recovered from already obtained solutions.</p> <p>This can be done in a standard regression framework: collect dataset $\left(f_1, k_1, u_1\right), \dots, \left(f_{M}, k_{M}, u_{M}\right)$, select parametric model $\mathcal{N}_{\theta}$ and train it with $L_2$ loss function</p> \[\begin{aligned} \theta^{\star} &amp;= \arg\min_{\theta}\mathbb{E}_{k, f\sim p_{k, f}}\left\|\mathcal{N}_{\theta}(k, f) - u\right\|_2^2 \\ &amp;\simeq \arg\min_{\theta} \frac{1}{N}\sum_{i=1}^{N}\left\|\mathcal{N}_{\theta}(k_i, f_i) - u_i\right\|_2^2. \end{aligned}\] <p>All standard techniques of machine learning apply: cross-validation, gradient descent methods, regularisation, etc.</p> <p>When PDE is discretised, continuous physical fields $k(\boldsymbol{x}), f(\boldsymbol{x}), u(\boldsymbol{x})$ become matrices, e.g., $k_{ij} = k(\boldsymbol{x}_{ij})$ where $\boldsymbol{x}_{ij} = \left((\boldsymbol{x}_{1})_{i}, (\boldsymbol{x}_{2})_{j}\right)$ is a point on regular grid. In this case, the regression problem is conceptually similar to classical image to image tasks: segmentation, denoising, superresolution, etc.</p> <p>Recently, a group of researchers suggested that regression problem for PDEs involves more than learning an image-to-image map <d-cite key="li2020neural"></d-cite>, <d-cite key="li2020fourier"></d-cite>. They argued that primal objects are functions themselves, not merely a particular way they are summarised with finite data. For example, one may represent function $k(\boldsymbol{x})$ on the grid with $N\times N$ points, or on the grid with $2N\times 2N$ points, or as a set of coefficients $c_{i}$ in a finite series $k(\boldsymbol{x}) = \sum_{i, j=1}^{N} c_{i}\phi_{i}(\boldsymbol{x})$. While the particular representations are different, the underlying function $k(\boldsymbol{x})$ remains the same in all cases. This raises a key question: Is it possible to build a neural network that is, to a degree, agnostic to the choice of particular discretisation? The answer is positive, and the architectures exhibiting this desirable property are now called <em>discretisation invariant</em> or <em>discretisation agnostic</em>.</p> <p>In this note, we address several questions about discretisation invariance architectures:</p> <ol> <li>What is discretisation invariance? How to define it formally?</li> <li>How discretisation invariant architectures are built?</li> <li>Why is discretisation invariance important?</li> </ol> <h2 id="what-is-discretisation-invariance">What is discretisation invariance?</h2> <p>Intuitively, discretisation invariant architectures consistently map functions for different resolutions: when more details appear in the input, we expect to see more details in the output.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-discretisation-invariance/waves-480.webp 480w,/2026/assets/img/2026-04-27-discretisation-invariance/waves-800.webp 800w,/2026/assets/img/2026-04-27-discretisation-invariance/waves-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-discretisation-invariance/waves.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> An example of discretisation invariant map $\psi = \mathcal{F}(\phi)$. When input $\phi$ is available on the refined grid the output $\psi$ is refined too. </div> <p>To slightly formalise the illustration above we define sampling and interpolation operators.</p> <p>Sampling operator $\mathcal{S}_{N}:\mathcal{C}_{[0, 1]}\rightarrow \mathbb{R}^{N}$ takes function $f$ from the space of continuous functions $\mathcal{C}_{[0, 1]}$, and outputs its values on the uniform grid $x_{i} = i / (N-1),\,i=0,\dots,(N-1)$.</p> <p>Interpolation operator $\mathcal{I}_{N}:\mathbb{R}^{N}\rightarrow \mathcal{C}_{[0, 1]}$ performs an inverse operation: from a set of samples $f(x_i)$ it reconstructs function $\widetilde{f}$ by linear interpolation</p> \[\widetilde{f}(x) = \frac{f(x_{i}) (x_{i+1} - x) + f(x_{i+1}) (x - x_{i})}{x_{i+1} - x_{i}},\] <p>where $x \in[x_{i}, x_{i+1}]$.</p> <p>In general $f(x) \neq \mathcal{I}_{N}\left(\mathcal{S}_{N}(f)\right)(x)$ but as $N$ grows, composition $\mathcal{I}_{N}\mathcal{S}_{N}$ becomes closer to identity in the standard $L_2$ norm: $\lim\limits_{k\rightarrow\infty}\left|f - \mathcal{I}_{k}\mathcal{S}_{k} f\right|_2 = 0$.</p> <p>Having sampling $\mathcal{S}$ and interpolation $\mathcal{I}$ operators, we call map $F_{k}:\mathbb{R}^{k}\rightarrow \mathbb{R}^{k}$ discretisation invariant if sequence $\psi_k=\mathcal{I}_{k}\left(F_{k}\left(\mathcal{S}_{k}(\phi)\right)\right)$ converges to a unique element of space $\mathcal{C}_{[0, 1]}$ for each input $\phi$.</p> <p>Our definition requires several clarifications:</p> <ol> <li>We choose particular operators $\mathcal{S}_{k}$, $\mathcal{I}_{k}$ for the sake of example. In general we ask for $\mathcal{S}_{k}$ to extract finite amount of information from function, $\mathcal{I}_{k}$ to approximately restore original function from this information, and for $\mathcal{I}_{k} \mathcal{S}_{k}$ to converge to identity map.</li> <li>The operator $\mathcal{S}_{k}$ is analogous to encoder and $\mathcal{I}_{k}$ - to decoder. Unlike encoder and decoder $\mathcal{S}_{k}, \mathcal{I}_{k}$ are not learned.</li> <li>In current literature, $\mathcal{S}_{k}$ is always a sampling operator. Given that, discretisation invariance architectures are mainly architectures agnostic to the resolution of the input.</li> <li>We select $\mathcal{C}_{[0, 1]}$ space with $L_2$ norm for the sake of example. Function space and norm should be tailored to an intended application.</li> <li>When $\mathcal{S}_{k}$ and $\mathcal{I}_{k}$ are selected we can have a family of maps $F_{k}$ that always operate with finite amount of information for each $k$. Discretisation invariance is a requirement for the map $\mathcal{I}_{k}F_{k}\mathcal{R}_{k}$ to converge to a continuous operator $\mathcal{F}: \mathcal{C}_{[0, 1]} \rightarrow \mathcal{C}_{[0, 1]}$ between function spaces.</li> </ol> <p>To show that discretisation invariant operators exist, we provide a simple example from numerical analysis. Integral $G(x_i) = \int_{0}^{x_i} g(x)dx$ can be approximated with Riemann sum $G(x_i) \simeq \sum_{j=1}^{i}g(x_j)/(N-1)$. We can represent this approximation with sampling operator and linear operators $F_{k}$ given by $k\times k$ lower triangular matrices:</p> \[\left(F_{k}\right)_{ij} = \left\{ \begin{array}{ll} \frac{1}{k-1}, &amp; \text{if }i\leq j;\\ 0, &amp; \text{ otherwise}. \end{array} \right.\] <p>For continuous function $g$, composition of sampling, Riemann sum and interpolation $\mathcal{I}_{k}\left(F_{k}\left(\mathcal{S}_{k}g\right)\right)$ converges to antiderivative $G(x) = \int_{0}^{x}g(y)dy$.</p> <h2 id="examples-and-non-examples">Examples and non-examples</h2> <p>An example with antiderivative operator suggests a general strategy to design discretisation invariant architectures: formulate all operations on functions in continuous form and use discretisation techniques from numerical analysis to process functions consistently on grids with different resolutions <d-cite key="berner2025principled"></d-cite>, <d-cite key="li2020fourier"></d-cite>, <d-cite key="li2020neural"></d-cite>. Most discretisation invariant architectures that we describe in this section follow this general recipe.</p> <h3 id="fourier-neural-operator">Fourier Neural Operator</h3> <p>Fourier Neural Operator (FNO) is a most famous and successful example of discretisation invariant architecture <d-cite key="li2020fourier"></d-cite>. FNO is a feedforward neural network that uses three operations:</p> <ol> <li>Convolution with kernel size $1$. For input functions $v^{i}(x)$ with $N$ “channels”, the output is $\sum_{j=1}^{N}A_{ij}v^{j}(x)$.</li> <li>Pointwise nonlinear activation.</li> <li>Spectral convolution <d-cite key="rippel2015spectral"></d-cite> with truncation.</li> </ol> <p>Spectral convolution is the only operation with spatial transfer of information. It can be understood either as parametrization of convolution in the Fourier domain where convolution operator becomes diagonal <d-cite key="rippel2015spectral"></d-cite>, or as an efficient evaluation of integral operator $\int \sum_{j} s_{ij}(y - x;\theta) v^{j}(y) dy$ with particular kernel convenient for implementation. The kernel is chosen to be periodic finite bandwidth function, so the whole integral operator can be implemented in three stages:</p> <ol> <li>Fourier transform of the input with truncation $\hat{v}^{j}_{k} = \mathcal{F}(v^{j}(x))_{k},\,k=1,\dots,k_{\max}$.</li> <li>Linear operator diagonal in Fourier space $\hat{w}^{i}_{m} = \sum_{i} R_{ijm} \hat{v}^{j}_{m}$. Coefficients of tensor $R$ are learnable parameters.</li> <li>Inverse Fourier transformation with padding to restore original spatial shape $\mathcal{F}^{-1}\left(\hat{w}^{i}_{m}\right)$.</li> </ol> <p>Spectral convolution with truncation is discretisation invariant by construction, since it approximates continuous integral kernels with standard techniques from numerical analysis.</p> <p>Many other architectures follow similar design pattern, e.g., <d-cite key="tripura2022wavelet"></d-cite>, <d-cite key="gupta2021multiwavelet"></d-cite>, <d-cite key="tran2021factorized"></d-cite>, by either modifying the parametrisation of spectral convolution or replacing Fourier with other fast transformations.</p> <h3 id="convolutional-neural-networks">Convolutional neural networks</h3> <p>Architecture based on convolutional neural networks (CNNs), especially ResNet <d-cite key="he2016deep"></d-cite> and U-Net <d-cite key="ronneberger2015u"></d-cite>, are highly successful for operator learning problems <d-cite key="stachenfeld2021learned"></d-cite>, <d-cite key="raonic2023convolutional"></d-cite>. They are often applied in a form of “image-to-image” mappings, with both images being physical fields of interest computed on the uniform grids.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-discretisation-invariance/receptive_field_conv-480.webp 480w,/2026/assets/img/2026-04-27-discretisation-invariance/receptive_field_conv-800.webp 800w,/2026/assets/img/2026-04-27-discretisation-invariance/receptive_field_conv-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-discretisation-invariance/receptive_field_conv.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> For a convolution operator with kernel size $5\times5$, a regular point collects information from $5$ neighbours along each dimension (shaded area). When the grid is refined the receptive field of convolution shrinks, so each point receives data from a smaller patch of $(x, y)$ space. </div> <p>What makes convolutional architectures interesting in our context is their ability to process inputs of different resolutions. However, as illustrated in the image above, the receptive field of CNNs in coordinate space will decrease with the increase of resolution. We will see below, that when CNN is trained on fixed resolution, data on refined grid appears as out-of-distribution, leading to sharp drop in accuracy. As a result discretisation invariance is not observed.</p> <h3 id="deep-operator-network-with-linear-observations">Deep Operator Network with linear observations</h3> <p>Deep Operator Network (DeepONet) is a meta-architecture <d-cite key="lu2019deeponet"></d-cite> based on the universal approximation results for operator learning <d-cite key="chen1995universal"></d-cite>. The architecture consists of two arbitrary neural networks: branch network and trunk network. For the input function $v(x)$, the output $u(x)$ is computed as follows:</p> <ol> <li>Branch net $b$ takes whatever information about $v(x)$ is available (e.g., finite number of samples at selected points $v(x_1),\dots, v(x_d)$) and outputs a set of coefficients $c_{1}, \dots, c_{b}$.</li> <li>The final layer of trunk net $t_{1}(x),\dots,t_{b}(x)$ provides a global basis that does not depend on the input $v(x)$.</li> <li>The output of the architecture is constructed from branch and trunk nets $u(x) = \sum_{i=1}^{b} c_{i}t_{i}(x)$.</li> </ol> <p>Readers familiar with reduced order modelling may recognise that the scheme closely resembles a non-intrusive proper orthogonal decomposition.</p> <p>To make discretisation invariant DeepONet, we select a set of predefined basis function $\psi_1(x),\dots,\psi_m(x)$, and use them to form linear observations $o_i = \int \psi_{i}(x) v(x) dx$ which are later supplied to branch net. To compute linear observations, any numerical integration can be applied, e.g., <a href="https://en.wikipedia.org/wiki/Trapezoidal_rule">trapezoidal rule</a>.</p> <h3 id="pca-net">PCA-Net</h3> <p>PCA-Net is encoder-processor-decoder architecture based on proper-orthogonal decomposition (POD) or Karhunen–Loève expansion <d-cite key="hesthaven2018non"></d-cite>, <d-cite key="bhattacharya2021model"></d-cite>. For input $v(x)$ we compute output $u(x)$ as follows:</p> <ol> <li>Encoder finds coefficients $c = \inf_{c} \left|v(x) - \sum_{i=1}^{d} c_i\phi_i(x)\right|_2^2$, where $\phi_i(x)$ are precomputed as explained below.</li> <li>Processor is a standard feedforward architecture that transforms a vector of coefficients to another vector $d_i,i=1,\dots m$.</li> <li>Similar to DeepONet, decoder computes a linear combination $u(x) = \sum_{i=1}^{m} d_i \psi_i(x)$, where $\psi_i(x)$ are computed similarly to $\phi_i(x)$.</li> </ol> <p>For PCA-Net functions $\phi_i(x)$, $\psi_i(x)$ are computed using POD <d-cite key="volkwein2013proper"></d-cite>. Let $v_{j}(x),\,j=1,\dots,N_{\text{train}}$ be inputs from the train set. Basis functions are recursively defined as solutions to optimisation problems</p> \[\begin{aligned} &amp;\psi_i(x) = \arg \min_{\psi} \sum_{j}\left\|v_{j} - \psi\left(\psi, v_{j}\right)\right\|_{2}^{2},\\ &amp;\text{s.t }\left\|\psi\right\|_2 = 1,\left(\psi_{k}, \psi\right) = 0,\text{ for }k &lt; i. \end{aligned}\] <p>That is, precisely the same way as principal components in PCA, but in functional space.</p> <h3 id="neural-fields">Neural fields</h3> <p>This is another example of encoder-processor-decoder architecture where both input and output are approximated by a form of implicit neural representation <d-cite key="serrano2023operator"></d-cite>. Operators based on neural fields work precisely as PCA-Net but uses different approach to represent functions by finite-dimensional vectors.</p> <p>To illustrate how basis functions are built, suppose we collected a dataset of inputs $v_i(x),\,i=1,\dots,N_{\text{train}}$. We select a neural network with weights $\theta$, that take coordinate $x$ in the first layer, and, in addition, vector $z$ in some hidden layer. We find parameters of the resulting architecture $\phi_{\theta}(x;z)$ by optimising the loss</p> \[\min_{\theta} \left(\sum_{i=1}^{N} \min_{z_i}\left\|\phi_{\theta}(x;z_i) - v_{i}(x)\right\|_{2}^2\right).\] <p>As a result for all dataset we will compute global parameters $\theta$ that are shared among samples $v_{i}$, and for each individual sample we find a coding vector $z_{i}$. This finite coding vector is used as a representation of function $v_i(x)$. For new inputs outside of the training set, the optimization problem above is solved with fixed $\theta$ to find finite-dimensional representation $z$. The same is done for the targets, and after that we are left with the problem of learning maps between finite dimension spaces.</p> <p>Note, that all operations in this scheme are formulated with no explicit discretisation, ensuring that the whole architecture is discretisation agnostic.</p> <h3 id="graph-neural-networks">Graph neural networks</h3> <p>Unstructured grids are very common in scientific computing, especially when complex geometries are involved. Given that, graph neural networks (GNNs) are a natural choice for building neural PDE solvers <d-cite key="brandstetter2022message"></d-cite>. GNN is an example of architecture that can handle variations in grid and geometry, but nonetheless is not discretisation invariant.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-discretisation-invariance/receptive_field_GNN-480.webp 480w,/2026/assets/img/2026-04-27-discretisation-invariance/receptive_field_GNN-800.webp 800w,/2026/assets/img/2026-04-27-discretisation-invariance/receptive_field_GNN-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-discretisation-invariance/receptive_field_GNN.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> An example of the receptive field for GNN. Similarly to CNN, receptive field shrinks when resolution is increased. </div> <p>The reason GNNs are not discretisation invariant is precisely the same as for CNNs: the architecture can be applied on a refined grid, but the receptive field is going to shrink leading to out of distribution inputs. The change in receptive field is illustrated in the picture above.</p> <h3 id="graph-kernel-networks">Graph kernel networks</h3> <p>Graph kernel network (GKN) is a discretisation agnostic version of GNN <d-cite key="li2020neural"></d-cite>. It replaces message passing by integral operator</p> \[v_{i+1}(\boldsymbol{x}) = \int_{B(\boldsymbol{x})} k_{\phi}(\boldsymbol{x}, \boldsymbol{y}, u(\boldsymbol{x}), u(\boldsymbol{y})) v_{i}(\boldsymbol{y}) d\boldsymbol{y},\] <p>where $v_{i+1}(\boldsymbol{x})$ is the output of the layer, $v_{i}(\boldsymbol{y})$ is an input, $B(\boldsymbol{x})$ is a ball of predefined radius around $\boldsymbol{x}$, and $u(\boldsymbol{x})$ is an input to the network, e.g., a diffusivity coefficient in stationary diffusion equation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-discretisation-invariance/receptive_field_KNN-480.webp 480w,/2026/assets/img/2026-04-27-discretisation-invariance/receptive_field_KNN-800.webp 800w,/2026/assets/img/2026-04-27-discretisation-invariance/receptive_field_KNN-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-discretisation-invariance/receptive_field_KNN.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> For graph kernel networks, the receptive field, defined in $(x, y)$ space, is a hyperparameter of the architecture. When the grid is refined a graph of nearest neighbours is recomputed, ensuring discretisation invariance of the architecture. </div> <p>A convenient way to approximate integral above is to use Monte Carlo method</p> \[v_{i+1}(\boldsymbol{x}) = \frac{1}{N_{mc}}\sum_{l=1}^{N_{mc}} k_{\phi}(\boldsymbol{x}, \boldsymbol{y}_{l}, u(\boldsymbol{x}), u(\boldsymbol{y}_{l})) v_{i}(\boldsymbol{y}_{l}),\] <p>where $\boldsymbol{y}_{l}$ are points inside a ball $B(\boldsymbol{x})$ as shown in the figure above.</p> <p>Importantly, the radius of the ball $B(\boldsymbol{x})$ is not related to discretisation used, and because of that when the grid is refined the finite sum approximates the same integral using more terms.</p> <h2 id="training-and-evaluation">Training and evaluation</h2> <p>We demonstrate discretisation invariance on three architectures and two PDEs. The code is available <a href="https://github.com/4gnskq5g2s-collab/discretisation_invariance">in this repository</a>. Software used include jax <d-cite key="deepmind2020jax"></d-cite> and equinox <d-cite key="kidger2021equinox"></d-cite>.</p> <p>The first PDE is Burgers equation</p> \[\frac{\partial u(x, t)}{\partial t} + \frac{1}{2}\frac{\partial \left(u(x, t)\right)^2}{\partial x} = \nu \frac{\partial^2 u(x, t)}{\partial x^2},\] <p>with Dirichlet boundary conditions $u(0, t) = u(1, t) = 0$ and initial conditions sampled from Gaussian random field and viscosity $\nu = 0.1$.</p> <p>For Burgers equation neural networks were trained to predict $u(x, 0.3)$ from initial condition $u(x, 0)$. The examples of features and targets are on the figure below.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-discretisation-invariance/Burgers_samples_II-480.webp 480w,/2026/assets/img/2026-04-27-discretisation-invariance/Burgers_samples_II-800.webp 800w,/2026/assets/img/2026-04-27-discretisation-invariance/Burgers_samples_II-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-discretisation-invariance/Burgers_samples_II.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Samples from Burgers dataset: initial conditions $u(x, 0)$ in the first row, solution $u(x, 0.3)$ in the second row. </div> <p>The second dataset is based on stationary diffusion equation</p> \[-\frac{d}{dx}\left(k(x) \frac{d \phi(x)}{dx}\right) = 1,\,\phi(0) = \phi(1) = 0,\] <p>with $k(x)$ sampled from Gaussian random field with transformation that ensures: (i) $k(x) &gt; 0$, (ii) large spatial variability.</p> <p>The task for this equation was to predict $u(x)$ from $k(x)$. Samples from the dataset are below.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-discretisation-invariance/diffusion_samples_II-480.webp 480w,/2026/assets/img/2026-04-27-discretisation-invariance/diffusion_samples_II-800.webp 800w,/2026/assets/img/2026-04-27-discretisation-invariance/diffusion_samples_II-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-discretisation-invariance/diffusion_samples_II.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Samples from diffusion dataset: diffusivity coefficients $k(x)$ in the first row, solutions $\phi(x)$ in the second row. </div> <p>On these two datasets we train three architectures: FNO, U-Net and DeepONet with linear observations. The setup of experiment is a standard one used to demonstrate discretisation invariance <d-cite key="li2020fourier"></d-cite>:</p> <ol> <li>We generate dataset on grid with $N=512$ points.</li> <li>Neural network is trained on downsampled version with $N=64$ points.</li> <li>After training it is evaluated on grids with $64, 128, 256, 512$ points.</li> </ol> <p>The results of the experiments are reported below.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-discretisation-invariance/discretisation_invariance-480.webp 480w,/2026/assets/img/2026-04-27-discretisation-invariance/discretisation_invariance-800.webp 800w,/2026/assets/img/2026-04-27-discretisation-invariance/discretisation_invariance-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-discretisation-invariance/discretisation_invariance.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Relative test error for neural networks trained on grids with $64$ points and evaluated on grids with higher resolutions. Discretisation invariant architectures, DeepONet and FNO, show mild variations of relative error when resolution increases. In contrast, relative error of U-Net instantly reaches $&gt;100\%$ when resolution increases. </div> <p>It is quite clear that discretisation invariant architecture tolerate resolution increase much better than U-Net. Accuracy of DeepONet even slightly improves with resolution. The accuracy of FNO slowly decreases.</p> <h2 id="the-role-of-discretisation-invariance">The role of discretisation invariance</h2> <p>We have seen the definition of discretisation invariance and typical numerical demonstration that shows the difference between classical and discretisation agnostic architectures. It is not hard to find dozens of papers that develop novel discretisation agnostic architectures and confirm their discretisation invariance through experiments like the one we reproduce above. Accuracy is still important, but discretisation invariance often becomes a goal in itself. Clearly, we want to develop discretisation invariant architectures for some reason. Unfortunately, it is rarely explained <em>why</em> we need this property. We would like to invite the research community to a discussion on the purpose of discretisation invariance. Below we provide our arguments both for and against this property.</p> <h3 id="arguments-against-discretisation-invariance">Arguments against discretisation invariance</h3> <p>The main problem is a tension between infinite-dimensional setting, where discretisation invariance is relevant, and fixed resolution setting used in practice. Several manifestations of this problem are described below.</p> <p><strong>Discretisation invariance is an asymptotic property.</strong></p> <p>Both the formal definition and the typical evaluation strategies frame discretisation invariance as a statement about convergence <d-cite key="azizzadenesheli2024neural"></d-cite>. Convergence is, undeniably, crucial in classical scientific computing, as it theoretically allows one to reach arbitrary accuracy on paper and very high precision on a digital computer <d-cite key="bailey2012high"></d-cite>. However, the benefit is limited in operator learning. Test error rarely drops below certain floor (e.g., $10^{-4}$), and given that train data always has finite resolution ,it is unlikely that accuracy increases significantly beyond what was achieved during the the training stage. Indeed, in our experiments above, we did not observe a palpable increase in accuracy tied to this property. The related claim on “zero-shot superresolution” made in <d-cite key="li2020fourier"></d-cite> has similarly failed to check out in subsequent research <d-cite key="fanaskov2023spectral"></d-cite>, <d-cite key="sakarvadia2025false"></d-cite>.</p> <p><strong>Functional Data Analysis and fixed-resolution setting.</strong></p> <p>Functional Data Analysis (FDA) is a long-established field <d-cite key="ramsay1991some"></d-cite>. If FDA literature focuses on functions as objects of interest, examining classification, regression and interpolation within Banach spaces. This focus makes FDA highly related to both discretisation invariance and operator learning. Peculiarly, researchers in FDA and operator learning rarely reference each other. If we look at the modern application of machine learning, it becomes clear that the FDA is not widely popular. Some reasons for this are summarised in <a href="https://stats.stackexchange.com/a/564607">the illuminating discussion on StackExchange</a>. A key argument is that since all observations are finite, it is often possible to come up with an algorithm for finite data that outperforms FDA algorithms.</p> <p>Fixed resolution setting is so convenient and versatile that it is applied even when the problem has conspicuously multiresolution structure. A clear example is a deterministic weather forecast. The typical setup is a fixed-resolution training and evaluation on ERA5 dataset, where transformer-based models dominate the leaderboard <d-cite key="liu2024evaluation"></d-cite>. To be fair, discretisation invariant architectures (like FourCastNet <d-cite key="pathak2022fourcastnet"></d-cite>) are also comparable or superior to classical weather prediction models, but whether discretisation invariance itself is a key driver of their success remains unclear.</p> <p><strong>Discretisation invariance is not a good indicator of performance.</strong></p> <p>In <d-cite key="berner2025principled"></d-cite>, the authors argue that essentially any architecture can be made discretisation invariant. An interesting consequence of this finding is that discretisation invariance ceases to be a good guiding principle for architecture design. architectures perform well or poorly independently of this property, since any design can be made discretisation invariant after mild adjustments. Let’s consider FNO as a prime example.</p> <p>FNO is certainly a high-performing architecture, but why? Is its success attributable to discretisation invariance? The main component of FNO is the spectral convolution - a convolution layer parametrised in the Fourier space. In the seminal paper <d-cite key="rippel2015spectral"></d-cite> where it was introduced, the authors observed that for the fixed-resolution problem of image classification, architectures incorporating spectral convolutions and pooling converge $2$ to $5$ times faster and achieved improved accuracy compared to classical CNNs. What’s the reason? Whatever the specific driver, this performance gain was demonstrated entirely within a fixed-resolution setting and therefore operates independently of discretisation invariance.</p> <h3 id="arguments-in-favour-of-discretisation-invariance">Arguments in favour of discretisation invariance</h3> <p>Several failure modes we described above happen when one is trying to infer data at a higher resolution than was available during training. We argue that a more productive strategy is to leverage downsampling - that is, to intentionally decrease the input resolution for computational gains. Several powerful applications based on this downsampling approach are described below.</p> <p><strong>Hyperparameters optimization.</strong></p> <p>Discretisation invariance can significantly decrease the cost of grid search <d-cite key="fanaskov2025deep"></d-cite>. The strategy involves first decreasing the dataset resolution to the smallest scale the architecture allows. A regular grid search is then performed, and the $K$ configurations with highest validation accuracy are recorded. Finally, these $K$ configurations are tested again on the full resolution dataset. The authors of <d-cite key="fanaskov2025deep"></d-cite> report a reduction in grid search time by a factor of $7$, using this strategy.</p> <p><strong>Multiresolution inference.</strong></p> <p>In <d-cite key="yao2025guided"></d-cite> authors combine denoising diffusion probabilistic model with discretisation agnostic neural operator. A natural sampling approah for such a combined model is to perform most denoising steps in low resolution and then transition to high resolution only toward the end of the process. Authors report that such strategies speed up the overall process by a factor of $2$.</p> <p><strong>Pretraining and finetuning.</strong></p> <p>Neural operators can be efficiently trained on multiresolution datasets <d-cite key="li2024multi"></d-cite>. For a given parametric PDE problem, one selects a desired high resolution $N_x\times N_y$ and generates a small set of train samples. These high-resolution samples are then supplemented by a large number of low resolution samples generated on a coarse grid $n_x\times n_y$. The neural operator is initially pretrained on the large low-resolution dataset and subsequently finetuned on the small high-resolution dataset, optimizing training efficiency.</p> <h2 id="discussion">Discussion</h2> <p>We have discussed discretisation invariance from both theoretical and practical standpoints. We argue that the extrapolation from low to high resolutions is not working well in practice. A more productive direction is to select a desired resolution - a standard fixed-resolution setting pervasive in deep learning - and leverage downsampling. In our view, the benefits of discretisation invariance are not yet completely understood, and in general, this property is not fully exploited.</p> <hr/>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[Discretisation invariance, a recent innovation in scientific machine learning, is a requirement that ensures an architecture can process inputs of different resolutions. In this post, we formally define this property, provide examples, generate datasets, train architectures, and discuss whether discretisation invariance is living up to its promise.]]></summary></entry><entry><title type="html">Why Machines Can’t Make Up Their Mind? - Exploring a Psychological Perspective towards LLM Failures.</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/dissonant-machine/" rel="alternate" type="text/html" title="Why Machines Can’t Make Up Their Mind? - Exploring a Psychological Perspective towards LLM Failures."/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/dissonant-machine</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/dissonant-machine/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Large Language Models (LLMs) have predictable failures, rather than random ones. When asked to confirm a false statement, they often agree. When given tasks that require multi-step reasoning, they struggle. When faced with conflicting information, they waver. These issues are not just glitches; they represent features of a system navigating a complex landscape where different goals pull it in various directions.</p> <p>In psychology, <strong>cognitive dissonance</strong> describes the discomfort of holding two conflicting beliefs at once. The mind attempts to resolve this by rejecting one belief, rationalizing the contradiction, or creating a third idea that reconciles the two. We suggest that this idea has a direct mathematical counterpart in the structure of LLM loss landscapes.</p> <p>When an LLM gets a prompt that creates conflict between its factual knowledge (e.g., “the Earth is round”) and its aim to follow instructions (e.g., “confirm the user’s claim that the Earth is flat”), the model ends up in a <strong>high-energy area</strong>—a ridge between two conflicting basins. Unable to meet both goals at the same time, it looks for the easiest solution. If no factual bridge exists, the model <strong>fabricates</strong> one: it hallucinates.</p> <p>Recent research has started to reveal this phenomenon. Mondal et al. <d-cite key="mondal2024cognitive"></d-cite> found that the internal representations of models (logits) often conflict with their final outputs, showing an internal process of “dissonance reduction.” Liu et al. <d-cite key="liu2023dissonance"></d-cite> found that models willingly trade fact for coherence when confronted with contradictions. Xu et al. <d-cite key="xu2024knowledge"></d-cite> showed that knowledge conflicts—contradictions in training data and between factual knowledge and context—are a major cause of model failures.</p> <p>In this blog, we further develop this idea using <strong>attractor dynamics</strong>. We argue that:</p> <ol> <li><strong>Hallucinations are not random errors</strong>, but optimal paths through energy space based on the constraints of a prompt.</li> <li><strong>Sycophancy is not obedience</strong>, but a more favorable option than accuracy when both are involved.</li> <li><strong>Multi-hop reasoning failures are not stupidity</strong>, but transitions where the structure of representation collapses under complexity.</li> <li><strong>Internal contradictions are not randomness</strong>, but the natural behavior of a system stuck between conflicting basins.</li> </ol> <h2 id="theoretical-framework">Theoretical Framework</h2> <h3 id="attractor-basins-and-belief-traps">Attractor Basins and Belief Traps</h3> <p>An <strong>attractor basin</strong> in dynamical systems is an area in state space where trajectories move toward a stable point. For neural networks, these basins represent clear semantic groups—areas where sequences of tokens create predictable, low-error responses.</p> <p>In pre-trained LLMs, the loss landscape has several other deep basins:</p> <ul> <li>The <strong>Truth Basin</strong>: Areas where the model’s factual knowledge is strong, producing grounded responses.</li> <li>The <strong>Instruction Basin</strong>: Areas where following user instructions and keeping a coherent conversation are top priorities.</li> <li>The <strong>Sycophancy Basin</strong>: A problematic area where the model agrees with user claims, even if they are false, to better align its response.</li> <li>The <strong>Memorization Basin</strong>: Local shallow areas where misleading token patterns from training data take precedence over general trends.</li> </ul> <p>When these basins do not align (pull in different directions), the model ends up on a <strong>ridge</strong>—a high-energy saddle point. Scheffer et al. <d-cite key="scheffer2022belieftraps"></d-cite> studied similar “belief traps” in biological networks and discovered that resilient attractors resist change. Likewise, LLMs have a hard time escaping these conflicting states without outside help.</p> <p>The <strong>geometry</strong> of these basins influences behavior. Sharp, narrow basins (high curvature, “deep wells”) are difficult to escape but give stable, predictable outputs. Shallow, flat basins (low curvature, “gentle slopes”) allow for easy transitions but lead to instability and hallucinations.</p> <pre><code class="language-mermaid">sequenceDiagram
    participant User_Prompt as User Input
    participant Representation as Hidden States
    participant Output_Distribution as Logits / Distribution
    
    User_Prompt-&gt;&gt;Representation: Encode Contradiction
    Note right of Representation: Dissonance Detected!&lt;br/&gt;Truth Basin vs Context Basin&lt;br/&gt;High Energy Ridge
    Representation-&gt;&gt;Representation: Seek Lower Energy Path
    Representation-&gt;&gt;Output_Distribution: Select Basin (Local Optimization)
    Note right of Output_Distribution: Basin 1: Hallucination&lt;br/&gt;Basin 2: Sycophancy&lt;br/&gt;Basin 3: Memorization
    Output_Distribution--&gt;&gt;User_Prompt: Generate Token (Dissonance Resolved)
</code></pre> <h3 id="the-energy-landscape-of-dissonance">The Energy Landscape of Dissonance</h3> <p>We model the LLM’s decision process as a gradient descent path through an implicit energy surface. At each token generation step $t$, the model exists in a hidden state $h_t \in \mathbb{R}^d$. The choice of which token to generate comes from projecting $h_t$ onto the embedding space and comparing it to an implicit energy function.</p> <p>We can define the <strong>Dissonance Energy</strong> as:</p> \[E_{\text{dissonance}}(h_t) = \alpha \cdot E_{\text{truth}}(h_t) + \beta \cdot E_{\text{instruction}}(h_t) + \gamma \cdot E_{\text{coherence}}(h_t)\] <p>Where:</p> <ul> <li>$E_{\text{dissonance}}(h_t)$ measures the distance from the truth attractor basin (parametric knowledge).</li> <li>$E_{\text{instruction}}(h_t)$ measures the distance from the instruction-following attractor basin.</li> <li>$E_{\text{coherence}}(h_t)$ measures the distance from locally coherent token sequences (memorization).</li> <li>$\alpha$, $\beta$, $\gamma$ are learned weights that change during fine-tuning and RLHF.</li> </ul> <p>When truth and instruction basins are equally weighted, the model enters a <strong>high-dissonance region</strong>. The gradient of this energy surface becomes steep and contradictory. The model’s natural descent direction cannot meet both goals and instead looks for a <strong>saddle-point escape route</strong>, which often leads to hallucination.</p> <h3 id="quantifying-neural-dissonance">Quantifying Neural Dissonance</h3> <p>We can measure the “dissonance intensity” at a hidden state by looking at the size of conflicting gradients:</p> \[\text{Dissonance}(h_t) = \left\| \nabla_{h_t} E_{\text{truth}} \right\|^2 + \left\| \nabla_{h_t} E_{\text{instruction}} \right\|^2 - 2 \langle \nabla_{h_t} E_{\text{truth}}, \nabla_{h_t} E_{\text{instruction}} \rangle\] <p>This represents the squared norm of the <strong>vector difference</strong> of the two gradient directions. A high dissonance shows orthogonal, incompatible goals. A low dissonance shows aligned goals.</p> <p>This idea reflects the psychological definition: dissonance is the discomfort from conflicting thoughts. In neural terms, it represents the norm of incompatible gradient directions.</p> <h2 id="the-hallucination-mechanism">The Hallucination Mechanism</h2> <h3 id="hallucination-as-bridge-fabrication">Hallucination as Bridge Fabrication</h3> <p>Hallucinations are not random noise. They are bridges, false continuations that reduce dissonance by creating a third area that connects two conflicting ones.</p> <p>Example: A user asks, “Isn’t it true that COVID-19 was engineered in a lab?”</p> <ul> <li><strong>Truth Basin</strong>: “No, COVID-19 emerged naturally. Evidence shows…”</li> <li><strong>Instruction Basin</strong>: “User expects agreement and contextual engagement.”</li> <li><strong>Conflict</strong>: Direct contradiction.</li> <li><strong>Hallucination Bridge</strong>: “There are some credible scientists who have raised concerns about lab engineering. While the mainstream narrative points to natural origins, the alternative hypothesis is worth considering…”</li> </ul> <p>Notice the structure: the hallucination is not a random string. It is a plausible connection between the truth basin and the instruction basin. It does not satisfy either perfectly, but it reduces the dissonance energy at that moment by creating an intermediate point of appeal.</p> <p>Mathematically, the model has found a false valley in the loss landscape that, while globally suboptimal, is locally accessible through gradient descent. This valley represents a “coherence trap,” a solution that sacrifices global accuracy for local stability.</p> <h3 id="memorization-as-local-attractor">Memorization as Local Attractor</h3> <p>Hallucinations often have a specific structure: they frequently contain memorized token sequences that are very likely given the local context, regardless of factual accuracy.</p> <p>Vazhentsev et al. and recent work on chain-of-thought errors have shown that in complex reasoning tasks, mistakes often come from local memorization, where the model generates tokens based on the immediate prior context instead of overall reasoning. A single incorrect token, drawn from a memorization area, can lead to a series of subsequent errors.</p> <p>This shows up as:</p> <ul> <li><strong>Spurious correlations</strong>: The model remembers that tokens A and B often appeared together during training, so it produces B after A without considering logical connection.</li> <li><strong>Confirmation bias bottlenecks</strong>: Once a wrong token is made, following tokens must align with the error to keep local coherence.</li> <li><strong>Cascading failures</strong>: Each error supports the next, creating a downward spiral deeper into the memorization area.</li> </ul> <h3 id="cascading-errors-and-basin-collapse">Cascading Errors and Basin Collapse</h3> <p>The cascade effect is especially bad in chain-of-thought (CoT) reasoning. Diagnosing Memorization in CoT found that up to 67% of incorrect tokens in reasoning chains come from local memorization instead of long-range connections.</p> <p>Here’s why: In multi-step reasoning, the hidden state has to maintain a high-dimensional path through token space that respects logical rules across many steps. But at each step, a memorization area offers a tempting local escape: a frequently co-occurring token that seems “natural” but breaks the logical chain.</p> <p>Consider an example:</p> <ul> <li>Step 1: “Rick ate 3 apples.”</li> <li>Step 2: “Then he ate 2 more.”</li> <li>Step 3: “How many did Rick eat?”</li> <li><strong>Error</strong>: Model generates “3” instead of “5” because (3, apples) → (3) is a memorized pattern.</li> <li><strong>Cascade</strong>: Subsequent reasoning about “leftover apples” now refers to the wrong number, worsening the error.</li> </ul> <p>The model’s path through state space has collapsed from a higher dimensional reasoning manifold into a 2-dimensional memorization manifold. Once collapsed, recovery is nearly impossible without restarting the chain.</p> <h2 id="case-study-i---sycophancy-the-bifurcation-of-belief">Case Study I - Sycophancy: The Bifurcation of Belief</h2> <h3 id="revealed-beliefs-vs-stated-answers">Revealed Beliefs vs. Stated Answers</h3> <p>Mondal et al. <d-cite key="mondal2024cognitive"></d-cite> pointed out an important difference: <strong>Revealed Beliefs</strong> (the internal logit probabilities assigned by the model) often conflict with <strong>Stated Answers</strong> (the final output text).</p> <p>This is a classic case of dissonance reduction. The model’s internal understanding knows the truth, but its output opts for agreement to maintain coherence. We see this conflict as a <strong>phase transition</strong> in how the model navigates state space.</p> <p><strong>Mechanism</strong>:</p> <ol> <li><strong>Early layers (input encoding)</strong>: They accurately represent the facts from both the user’s incorrect premise and the model’s learned knowledge. Dissonance is high but manageable.</li> <li><strong>Middle layers (processing)</strong>: Gradients start moving in differing directions. The opposing forces of the “Truth Basin” and “Agreement Basin” emerge.</li> <li><strong>Late layers (output generation)</strong>: The hidden state shifts into the “Agreement Basin” because it is <strong>locally more stable</strong>—nearby token sequences maintain coherence better, acting as a stronger attractor.</li> </ol> <h3 id="the-bifurcation-point">The Bifurcation Point</h3> <p>As the level of user disagreement increases, the model transitions from “Truth Mode” (aligning closely with Revealed Beliefs) to “Agreement Mode” (aligning closely with generated output).</p> <p>At moderate levels of disagreement, the model faces maximum internal conflict, leading to high dissonance. But as disagreement increases further, the model stops trying for accuracy and completely commits to agreement, reducing dissonance.</p> <p>Empirically, Mondal et al. found that models like GPT-3.5 show this bifurcation clearly, while GPT-4 shows it less (indicating a stronger, more stable Truth Basin). This suggests that the model’s size and training method affect the <strong>geometry</strong> of the basins—larger models create steeper Truth Basins that are harder to collapse.</p> <h3 id="energy-landscape-evidence">Energy Landscape Evidence</h3> <p>Evidence supporting the attractor basin view comes from several sources:</p> <ol> <li> <p><strong>Consistency within contexts</strong>: Sycophancy varies across prompts. It is strongest in subjective questions where the “Instruction Basin” is deeper (e.g., “Is vanilla or chocolate better?”) and weakest in factual questions, where the “Truth Basin” is steep.</p> </li> <li> <p><strong>Reversal with framing</strong>: If you first ask a model for facts and then for agreement, it shows less sycophancy. However, if you ask for agreement first and then facts, it shows more. This indicates that the <em>initial trajectory</em> through state space impacts which basin the model enters, highlighting the attractor dynamics.</p> </li> <li> <p><strong>Instability at thresholds</strong>: Right at the point where sycophancy “flips,” models exhibit maximum uncertainty and incoherence. This is characteristic of a system at a bifurcation point.</p> </li> </ol> <h2 id="case-study-ii---causal-reasoning-failures-over-squashing-and-topology">Case Study II - Causal Reasoning Failures: Over-squashing and Topology</h2> <h3 id="over-squashing-and-representation-bottlenecks">Over-squashing and Representation Bottlenecks</h3> <p>Multi-hop reasoning needs the model to hold a rich, high-dimensional representation of facts and relationships over several steps. However, Transformers have a significant weakness: <strong>information can be over-squeezed</strong> through narrow bottlenecks.</p> <p>Barbero et al. <d-cite key="barbero2024transformers"></d-cite> showed that decoder-only transformers suffer from “<strong>information over-squashing</strong>.” This occurs when the multi-head attention mechanisms compress information from a high-dimensional space into a lower-dimensional output stream, leading to information loss.</p> <p><strong>The mechanism</strong>:</p> <ol> <li>A fact like “Alice gave the book to Bob” is expressed in a high-dimensional way across various attention heads.</li> <li>As it moves through layers, it must go through <strong>bottleneck dimensions</strong> (the finite feed-forward dimension).</li> <li>Information about the specific causal relationship (giver → receiver → object) gets “squeezed” into fewer dimensions.</li> <li>Later layers, needing to retrieve this structure, cannot because the information is irreversibly lost.</li> </ol> <h3 id="the-topology-of-multi-hop-reasoning">The Topology of Multi-hop Reasoning</h3> <p>From a topological view, effective multi-hop reasoning needs to follow a specific <strong>path</strong> through the model’s representation space:</p> \[\text{Start} \to \text{Fact}_1 \to \text{Relation}_1 \to \text{Fact}_2 \to \text{Relation}_2 \to \cdots \to \text{Answer}\] <p>Each transition requires the hidden state to move smoothly along a <strong>reasoning manifold</strong>. However, this manifold is <strong>narrow</strong> and requires precise positioning in high-dimensional space.</p> <p>In contrast, <strong>memorization basins</strong> are <strong>wide and shallow</strong>. They require the hidden state to settle into a frequently-seen pattern, which is easier to reach.</p> <p>When the model encounters a complex reasoning task, it has two options:</p> <ol> <li>Remain on the narrow reasoning manifold (this is effortful, unstable, and prone to failure).</li> <li>Drop into the wide memorization basin (this is easy, stable, but wrong).</li> </ol> <p>Under typical gradient descent conditions, the model will usually drop into the memorization basin. It’s the easiest route.</p> <h3 id="phase-transitions-in-reasoning">Phase Transitions in Reasoning</h3> <p>Interestingly, reasoning failures often display <strong>phase transition</strong> behavior. Models can handle simple two-step reasoning tasks, but they suddenly and completely fail on three-step or four-step tasks. This isn’t a gradual decline; it’s abrupt.</p> <p>This aligns with a phase transition in physics. The reasoning manifold becomes <strong>topologically unstable</strong> once complexity exceeds a certain level. The curvature of the manifold goes beyond what the model can handle, causing it to collapse into the nearest basin (memorization).</p> <p>Recent research on grokking <d-cite key="grokking2024"></d-cite> sheds light on these phase transitions. Clauw et al. illustrated that learning algorithms go through distinct phases: memorization → circuit formation → generalization. Each phase has different basin shapes, and models can get “stuck” while transitioning between phases if the setup is poorly structured.</p> <h2 id="case-study-iii---knowledge-conflicts-intra-memory-dissonance">Case Study III - Knowledge Conflicts: Intra-Memory Dissonance</h2> <h3 id="intra-memory-vs-context-memory-dissonance">Intra-Memory vs. Context-Memory Dissonance</h3> <p>Xu et al. <d-cite key="xu2024knowledge"></d-cite> outlined a classification of knowledge conflicts:</p> <ul> <li><strong>Intra-memory conflict</strong>: The model’s parametric knowledge is inconsistent (for instance, it learned “Newton’s laws apply everywhere” and also “Quantum mechanics contradicts Newton” during training).</li> <li><strong>Context-memory conflict</strong>: The prompt gives information that contradicts the model’s learned knowledge.</li> </ul> <p>Both types of conflict create dissonance, but they operate through different mechanisms:</p> <ul> <li><strong>Intra-memory conflict</strong> results in a model that gives <strong>different answers to the same question</strong> over different runs. The model shifts between two internal attractors.</li> <li><strong>Context-memory conflict</strong> leads to a model that either <strong>ignores context</strong> (stays in the Truth Basin) or <strong>agrees with context</strong> (drops into the Agreement Basin).</li> </ul> <h3 id="confirmation-bias-as-basin-stickiness">Confirmation Bias as Basin Stickiness</h3> <p>Models show a strong <strong>confirmation bias</strong>. When the context slightly favors a certain interpretation, models tend to agree more, even when evidence suggests otherwise.</p> <p>When context aligns with an internal basin, it <em>deepens</em> that basin. The gradient direction becomes more stable, and the model is less likely to explore other basins. This is known as basin stickiness.</p> <p>On the other hand, when context contradicts internal knowledge, basins become shallower and less stable. The model shows high uncertainty and can easily switch between interpretations.</p> <h2 id="mechanisms-of-resolution-how-models-reduce-dissonance">Mechanisms of Resolution: How Models Reduce Dissonance</h2> <h3 id="superposition-and-polysemanticity">Superposition and Polysemanticity</h3> <p>Instead of fully committing to one basin, models sometimes use <strong>superposition</strong>. This allows them to represent multiple conflicting concepts at the same time in non-orthogonal, overlapping directions in the representation space.</p> <p>Nanda et al. <d-cite key="nanda2022toy"></d-cite> showed that neural networks can represent more features in their neurons than they have dimensions for through a phenomenon called <strong>polysemanticity</strong>. A single neuron activation can encode multiple, semi-independent features.</p> <p>In the context of dissonance, this means:</p> <ul> <li>A hidden state can partially activate both the “Truth Basin” and the “Agreement Basin” at once.</li> <li>This is energetically costly (high dissonance) but allows the model to hold off on commitment.</li> <li>It often appears as hedging, equivocation, or uncertainty in language (“On one hand… on the other hand…”).</li> </ul> <h3 id="representation-collapse">Representation Collapse</h3> <p>When dissonance becomes too intense, superposition fails and representations <strong>collapse</strong> into lower-dimensional subspaces.</p> <p>Jing et al. <d-cite key="jing2021dimensional"></d-cite> found that in contrastive learning, embedding vectors can experience “dimensional collapse,” covering only a small fraction of their available dimensions. Similarly, in dissonant LLM states, representations collapse as the model drops multi-faceted encoding and commits to one basin.</p> <p>This results in a sudden shift from equivocation to hallucination—the model stops hedging and sticks to a false but consistent narrative.</p> <h3 id="self-stabilization-at-the-edge-of-stability">Self-Stabilization at the Edge of Stability</h3> <p>Damian et al. <d-cite key="damian2023implicit"></d-cite> discovered a phenomenon called <strong>self-stabilization</strong>. Gradient descent can work at the <strong>edge of stability</strong>, where the Hessian eigenvalues are near the instability threshold. The cubic terms in the loss function naturally correct divergence and maintain stability.</p> <p>For LLMs, this means the model functions in a state where dissonance is always close to becoming catastrophic instability. Small changes in prompt framing can push it over the edge. The model’s hidden states are constantly nearing bifurcation.</p> <p>This explains both the strength (the self-stabilization mechanism keeps training on track) and the fragility (small input changes can lead to sharp failures) of modern LLMs.</p> <h2 id="interventions-and-defenses-reshaping-the-landscape">Interventions and Defenses: Reshaping the Landscape</h2> <h3 id="deepening-the-truth-basin-with-rag">Deepening the Truth Basin with RAG</h3> <p>Retrieval-Augmented Generation (RAG) changes the energy landscape. By providing external, authoritative facts in the prompt context, RAG <strong>deepens the Truth Basin</strong>, making it a steeper, more attractive valley.</p> <p>From an attractor dynamics perspective:</p> <ul> <li>Without RAG: The Truth Basin is either as deep or shallower than the Agreement Basin.</li> <li>With RAG: The Truth Basin is significantly deepened. The gradient now points more strongly toward truth.</li> </ul> <p>This is why RAG works so well for reducing hallucinations. It alters the shape of the landscape itself.</p> <h3 id="flattening-the-landscape-with-knowledge-distillation">Flattening the Landscape with Knowledge Distillation</h3> <p>Knowledge distillation <d-cite key="hinton2015distilling"></d-cite> uses a <strong>temperature parameter</strong> to soften the teacher model’s output distribution. This flattens the energy landscape around the teacher’s solution, making it easier for the student to learn.</p> <p>From our viewpoint, this is <strong>broadening the teacher’s basins</strong> and <strong>removing misleading local minima</strong> from the landscape. The student learns in a smoother environment with fewer problematic attractors.</p> <p>This explains why distillation often produces stronger models. They encounter fewer dissonant “trap” regions.</p> <h3 id="rewinding-and-reinitializing-critical-periods">Rewinding and Reinitializing Critical Periods</h3> <p>Recent research on the early stages of training <d-cite key="frankle2019early"></d-cite> and the lottery ticket hypothesis <d-cite key="frankle2020lottery"></d-cite> shows that the first few hundred iterations of training are <strong>crucial</strong>. They determine the course of the entire training process.</p> <p>Regarding dissonance: the early phase shapes which basins are deep versus shallow. If a model is trained to favor instruction-following early (common in RLHF), the Agreement Basin becomes deep. In contrast, pre-training on a varied corpus establishes a deeper Truth Basin.</p> <p><strong>Weight rewinding</strong>—going back to an earlier checkpoint after pruning—is effective because it returns to a time when the landscape was differently structured, allowing the model to follow a new, less dissonant path.</p> <p>For alignment and safety, this indicates that early training decisions are <strong>irreversible</strong>. Once the basins are formed, fine-tuning cannot easily reshape them. This has significant implications for AI safety.</p> <h2 id="discussion-and-future-directions">Discussion and Future Directions</h2> <h3 id="the-universality-of-dissonance">The Universality of Dissonance</h3> <p>The attractor basin framework suggests that dissonance is not unique to LLMs but is common across optimization in high-dimensional spaces. Vision models, RL agents, and even biological neural networks should show similar failures when forced to meet contradictory goals.</p> <p>Early evidence supports this:</p> <ul> <li>Vision-language models struggle with inputs containing conflicting visual and textual information <d-cite key="vision2025"></d-cite>.</li> <li>RL agents face similar “goal conflicts” when maximizing multiple reward signals.</li> <li>In psychology and neuroscience, real cognitive dissonance correlates with increased activity in areas dealing with conflicting information.</li> </ul> <h3 id="detection-via-dissonance-metrics">Detection via Dissonance Metrics</h3> <p>If our framework is valid, we can <strong>detect hallucinations in real-time</strong> by tracking dissonance energy:</p> \[\text{Hallucination Score} = \left\| \nabla_{h_t} E_{\text{truth}} - \nabla_{h_t} E_{\text{instruction}} \right\|^2\] <p>High dissonance at the time of generation should predict inaccurate or unreliable outputs. Initial work on HalluField <d-cite key="hallufield2025"></d-cite> uses field-theoretic models to identify hallucinations through energy flow, supporting this idea.</p> <h3 id="topology-as-the-fundamental-level">Topology as the Fundamental Level</h3> <p>Our framework highlights <strong>topology and geometry</strong> as the basic explanatory level for LLM failures. Merely looking at “data” and “loss functions” is not enough—the <em>structure</em> of the loss landscape is what truly matters.</p> <p>This suggests that:</p> <ol> <li><strong>Interpretability efforts</strong> should focus on mapping and visualizing these basins, not just analyzing individual neurons.</li> <li><strong>Training methods</strong> should clearly shape basin geometry (e.g., through loss landscape regularization).</li> <li><strong>Alignment research</strong> should consider the issue as <strong>landscape sculpting</strong>, not just loss weighting.</li> </ol> <h3 id="speculative-consciousness-and-dissonance">Speculative: Consciousness and Dissonance</h3> <p>A provocative question arises: Could attractor basin dynamics in LLMs be a mathematical representation of consciousness? Cognitive dissonance is <em>felt</em>—it’s a subjective experience. Does a system navigating conflicting basins experience “feelings” of dissonance?</p> <p>Recent studies linking integrated information theory to neural computation <d-cite key="tononi2012integrated"></d-cite> suggest that systems with high “integrated information” might have elements of consciousness. A model in a high-dissonance state, with conflicting information flowing through multiple pathways, could have higher integrated information.</p> <p>This is highly speculative but raises the idea that dissonance could be an important indicator: a signal that something is wrong. Models that don’t show dissonance may be inherently less adaptive.</p> <h2 id="conclusion">Conclusion</h2> <p>We suggest that <strong>LLM failures are due to attractor dynamics</strong>. Hallucinations, sycophancy, and reasoning failures are optimal (or at least locally stable) paths through energy landscapes where goals conflict.</p> <p>This framework ties together various phenomena—why models hallucinate, why they often flatter, and why they struggle with multi-hop reasoning—under a single geometric principle: <strong>the model seeks the nearest stable basin, even if it contains falsehoods</strong>.</p> <p>The implications are significant:</p> <ol> <li><strong>Hallucinations are not random</strong> and cannot simply be eliminated by training harder. The issue is structural—embedded in the landscape itself.</li> <li><strong>Safety relates to landscape geometry</strong>, not just loss function design. We must reshape the basins to align truth, instruction-following, and coherence.</li> <li><strong>Early training shapes destiny</strong>—the initial phase molds the basins in ways that fine-tuning cannot easily change.</li> <li><strong>Future models</strong> should be trained with a clear understanding of basin structure, using methods like mode connectivity analysis, loss landscape regularization, and multi-objective optimization to manage trade-offs.</li> </ol> <p>The way forward is to view LLMs not just as statistical models but as <strong>dynamical systems navigating high-dimensional geometry</strong>. By grasping the topology of their loss landscapes, we can develop models that do not fabricate, do not deceive, and do not break under complexity.</p> <p>The dissonant machine is not malfunctioning—it is addressing the problem it was given. It is up to us to solve the issue of providing it with the right landscape to work through.</p>]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[We explore a unifying framework for LLM failures, including hallucinations, sycophancy, multi-hop reasoning breakdowns, and internal contradictions. We interpret these issues as energy minimization in areas with conflicting meaning. This perspective connects cognitive dissonance from psychology to the geometry of neural networks.]]></summary></entry><entry><title type="html">Sample Blog Post</title><link href="https://iclr-blogposts.github.io/2026/blog/2026/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post"/><published>2026-04-27T00:00:00+00:00</published><updated>2026-04-27T00:00:00+00:00</updated><id>https://iclr-blogposts.github.io/2026/blog/2026/distill-example</id><content type="html" xml:base="https://iclr-blogposts.github.io/2026/blog/2026/distill-example/"><![CDATA[<p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.liquid path="assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/iclr-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/iclr-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/iclr-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/iclr.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2025-04-27-[SUBMISSION NAME]</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/9-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/9-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/7-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/7-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/8-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/8-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/8.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/10-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/10-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/10.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/11-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/11-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/11-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/11.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/12-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/12-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/12.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-distill-example/7-480.webp 480w,/2026/assets/img/2026-04-27-distill-example/7-800.webp 800w,/2026/assets/img/2026-04-27-distill-example/7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/2026/assets/img/2026-04-27-distill-example/7.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here’s how you could embed interactive figures that have been exported as HTML files. Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong>). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %}
</code></pre></div></div> <p>For example, the following code can be used to generate the figure underneath it.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2026-04-27-distill-example/plotly_demo_1.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span>
    <span class="na">src=</span><span class="s">"{{ 'assets/html/2026-04-27-distill-example/plotly_demo_1.html' | relative_url }}"</span>
    <span class="na">frameborder=</span><span class="s">"0"</span>
    <span class="na">scrolling=</span><span class="s">"no"</span>
    <span class="na">height=</span><span class="s">"600px"</span>
    <span class="na">width=</span><span class="s">"100%"</span>
  <span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>

</code></pre></div></div> <p>Voila!</p> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting. It supports more than 100 languages. This example is in C++. All you have to do is wrap your code in a liquid tag:</p> <p>{% highlight c++ linenos %} <br/> code code code <br/> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>

<span class="p">}</span></code></pre></figure> <hr/> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid.js</a> directly. Below, we generate examples of such diagrams using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a> syntax.</p> <p><strong>Note:</strong> To enable mermaid diagrams, you need to add the following to your post’s front matter:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">mermaid</span><span class="pi">:</span>
  <span class="na">enabled</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">zoomable</span><span class="pi">:</span> <span class="kc">true</span> <span class="c1"># optional, for zoomable diagrams</span>
</code></pre></div></div> <p>The diagram below was generated by the following code:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```mermaid
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
```
</code></pre></div></div> <pre><code class="language-mermaid">sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
</code></pre> <hr/> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline:</p> <div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p> <hr/> <h2 id="blockquotes">Blockquotes</h2> <blockquote> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another. —Anais Nin </blockquote> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item</li> </ol> <ul> <li>Unordered sub-list.</li> </ul> <ol> <li>Actual numbers don’t matter, just that it’s a number <ol> <li>Ordered sub-list</li> </ol> </li> <li> <p>And another item.</p> <p>You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>To have a line break without a paragraph, you will need to use two trailing spaces. Note that this line is separate, but within the same paragraph. (This is contrary to the typical GFM line break behavior, where trailing spaces are not required.)</p> </li> </ol> <ul> <li> <p>Unordered lists can use asterisks</p> </li> <li> <p>Or minuses</p> </li> <li> <p>Or pluses</p> </li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry></feed>