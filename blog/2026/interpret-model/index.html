<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> How To Open the Black Box: Modern Models for Mechanistic Interpretability | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Understanding how transformers represent and transform internal features is a core challenge in mechanistic interpretability. Traditional tools like attention maps and probing reveal only partial structure, often blurred by polysemanticity and superposition. New model-based methods offer more principled insight: Sparse Autoencoders extract sparse, interpretable features from dense activations; Semi-Nonnegative Matrix Factorization uncovers how neuron groups themselves encode concepts; Cross-Layer Transcoders track how these representations evolve across depth; and Weight-Sparse Transformers encourage inherently modular computation through architectural sparsity. Together, these approaches provide complementary pathways for opening the black box and understanding the circuits that underpin transformer behavior."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/interpret-model/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "How To Open the Black Box&#58 Modern Models for Mechanistic Interpretability",
            "description": "Understanding how transformers represent and transform internal features is a core challenge in mechanistic interpretability. Traditional tools like attention maps and probing reveal only partial structure, often blurred by polysemanticity and superposition. New model-based methods offer more principled insight&#58 Sparse Autoencoders extract sparse, interpretable features from dense activations; Semi-Nonnegative Matrix Factorization uncovers how neuron groups themselves encode concepts; Cross-Layer Transcoders track how these representations evolve across depth; and Weight-Sparse Transformers encourage inherently modular computation through architectural sparsity. Together, these approaches provide complementary pathways for opening the black box and understanding the circuits that underpin transformer behavior.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>How To Open the Black Box: Modern Models for Mechanistic Interpretability</h1> <p>Understanding how transformers represent and transform internal features is a core challenge in mechanistic interpretability. Traditional tools like attention maps and probing reveal only partial structure, often blurred by polysemanticity and superposition. New model-based methods offer more principled insight: Sparse Autoencoders extract sparse, interpretable features from dense activations; Semi-Nonnegative Matrix Factorization uncovers how neuron groups themselves encode concepts; Cross-Layer Transcoders track how these representations evolve across depth; and Weight-Sparse Transformers encourage inherently modular computation through architectural sparsity. Together, these approaches provide complementary pathways for opening the black box and understanding the circuits that underpin transformer behavior.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#traditional-interpretability-analysis">Traditional Interpretability Analysis</a> </div> <ul> <li> <a href="#attention-analysis">Attention Analysis</a> </li> <li> <a href="#probing">Probing</a> </li> <li> <a href="#why-traditional-methods-fall-short">Why Traditional Methods Fall Short?</a> </li> </ul> <div> <a href="#sparse-autoencoder">Sparse Autoencoder</a> </div> <ul> <li> <a href="#framework-overview">Framework Overview</a> </li> <li> <a href="#layers-activation">Layers &amp; Activation</a> </li> <li> <a href="#sae-evaluation">SAE Evaluation</a> </li> <li> <a href="#feature-evaluation">Feature Evaluation</a> </li> </ul> <div> <a href="#semi-nonnegative-matrix-factorization">Semi-Nonnegative Matrix Factorization</a> </div> <ul> <li> <a href="#method">Method</a> </li> <li> <a href="#discussions">Discussions</a> </li> </ul> <div> <a href="#cross-layer-transcoder">Cross-Layer Transcoder</a> </div> <ul> <li> <a href="#architecture">Architecture</a> </li> <li> <a href="#discussions">Discussions</a> </li> </ul> <div> <a href="#weight-sparse-transformer">Weight-Sparse Transformer</a> </div> <ul> <li> <a href="#architecture">Architecture</a> </li> <li> <a href="#discussions">Discussions</a> </li> </ul> <div> <a href="#final-remarks">Final Remarks</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>As modern transformer models grow in scale and capability, understanding how they make decisions has become both more important and more difficult. We can observe the inputs we give them and the outputs they produce, but the computations inside each layer unfold within a dense, high-dimensional space where many interacting components influence one another. From the outside, these models appear to compute through a swirl of entangled activations, offering few clues about why a particular answer emerged. This opacity makes it hard to trust models, hard to diagnose their failures, and hard to improve them in principled ways.</p> <p>Mechanistic interpretability (MI) aims to bridge this gap by treating neural networks not as black boxes but as algorithms whose internal structure we can study, decompose, and eventually understand. MI is guided by a few core questions <d-cite key="rai2025practicalreviewmechanisticinterpretability, sharkey2025openproblemsmechanisticinterpretability"></d-cite>:</p> <ol> <li><em>What features do models represent internally?</em></li> <li><em>How are these features combined into computational pathways or “circuits”?</em></li> <li><em>And to what extent are these mechanisms universal across architectures and scales?</em></li> </ol> <p>The first of these problem, i.e. understanding what information the model represents, is the natural entry point. Before we can trace circuits or explain behaviors, we must identify the elementary pieces of computation: the internal features encoded in the residual stream. Early interpretability work approached this challenge through traditional tools such as attention visualizations, probing classifiers, and attribution methods. While these techniques provide useful glimpses into model behavior, they often expose only fragments of the underlying structure, obscured by polysemanticity and the superposition of many concepts within the same neurons.</p> <p>This has led to a shift toward more model-based approaches that directly extract or induce interpretable structure from transformer activations. In this post, we explore four such methods: Sparse Autoencoders, which learn sparse latent features that disentangle the residual stream; Semi-Nonnegative Matrix Factorization, which decomposes MLP activations into neuron-grounded building blocks; Cross-Layer Transcoders, which connect these features across depth to reveal how computation unfolds layer by layer; and Weight-Sparse Transformers, which aim to build interpretability into the architecture itself by encouraging models to develop modular, monosemantic features during training. Together, these approaches form a growing toolkit for understanding the internal representations and circuits that govern transformer behavior.</p> <h2 id="traditional-interpretability-analysis">Traditional Interpretability Analysis</h2> <h3 id="attention-analysis">Attention Analysis</h3> <p>Attention analysis examines the attention weights ($\alpha_{ij} = \text{softmax}_j(\frac{q_i\cdot k_j}{\sqrt{d_k}})$) inside a transformer to visualize which tokens the model attends to at each position. Different attention heads often specialize in distinct linguistic patterns, such as syntax, coreference, or semantic relations. However, attention weights only show where information flows, not which features are being computed. They reveal connections between tokens, but they cannot tell us what internal concepts the model is using or how those concepts are represented.</p> <div style="display: flex; justify-content: center; margin: 20px 0;"> <table id="tab:attention-example" style="border-collapse: collapse; width: auto;"> <caption style="caption-side: top; padding: 8px; font-weight: bold; text-align: center;">Table 1. The Famous Attention Weights Example for Query Tokens</caption> <thead> <tr> <th style="border-bottom: 1px solid #333; padding: 6px 12px;">Query</th> <th style="border-bottom: 1px solid #333; padding: 6px 12px;">The</th> <th style="border-bottom: 1px solid #333; padding: 6px 12px;">cat</th> <th style="border-bottom: 1px solid #333; padding: 6px 12px;">sat</th> <th style="border-bottom: 1px solid #333; padding: 6px 12px;">on</th> <th style="border-bottom: 1px solid #333; padding: 6px 12px;">the</th> <th style="border-bottom: 1px solid #333; padding: 6px 12px;">mat</th> </tr> </thead> <tbody> <tr> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px;"><strong>"The"</strong></td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">1.0</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.0</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.0</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.0</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.0</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.0</td> </tr> <tr> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px;"><strong>"cat"</strong></td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.3</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.7</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.0</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.0</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.0</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.0</td> </tr> <tr> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px;"><strong>"sat"</strong></td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.1</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.7</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.2</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.0</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.0</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.0</td> </tr> <tr> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px;"><strong>"on"</strong></td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.05</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.1</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.6</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.25</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.0</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.0</td> </tr> <tr> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px;"><strong>"the"</strong></td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.1</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.1</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.2</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.3</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.3</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.0</td> </tr> <tr> <td style="padding: 6px 12px;"><strong>"mat"</strong></td> <td style="padding: 6px 12px; text-align: center;">0.0</td> <td style="padding: 6px 12px; text-align: center;">0.2</td> <td style="padding: 6px 12px; text-align: center;">0.1</td> <td style="padding: 6px 12px; text-align: center;">0.3</td> <td style="padding: 6px 12px; text-align: center;">0.2</td> <td style="padding: 6px 12px; text-align: center;">0.2</td> </tr> </tbody> </table> </div> <p>The famous attention pattern example shown in <a href="#tab:attention-example">Table 1</a> has a predominantly causal structure where each token attends primarily to itself and preceding tokens, with “sat” focusing heavily on “cat” (0.7) to capture the subject-verb relationship, and later tokens like “mat” distributing attention more broadly across the sequence.</p> <p>However, attention analysis has significant limitations. High attention weights indicate correlation rather than causation, they show where the model looked but not whether that information actually influenced the output. Moreover, attention patterns become increasingly difficult to interpret in deeper layers where residual connections allow information to bypass attention mechanisms entirely.</p> <h3 id="probing">Probing</h3> <p>Probing evaluates what information is present in a model’s hidden states by freezing the model and training a simple classifier on top of its internal representations. If the probe succeeds, it indicates that the information is linearly accessible in that layer. Probing is therefore useful for discovering whether the model has learned to encode liguistic knowledge like part-of-speech tags, entities, sentiment, or syntactic structure.</p> <p>However, a successful probe does not mean the model actually uses that information for its predictions, overly powerful probes may read out information that the model never makes use of, giving a misleading sense of interpretability.</p> <h3 id="why-traditional-methods-fall-short">Why Traditional Methods Fall Short?</h3> <ol> <li> <strong>Polysemanticity</strong> <br> Polysemanticity is like having a single light switch that controls three different rooms. It refers to the fact that a single neuron often responds to several unrelated concepts at once. Instead of representing one clean aspect, the neuron activates for a mixture of patterns. For example, the same neuron might correspond to a fruit (“apple”), a technology company (“Apple”), and even for unrelated acronyms like “doctor”.<br> <ul> <li> <strong>Attention analysis</strong> only shows that the neuron or head activated, not <em>which meaning</em> triggered it.</li> <li> <strong>Probing</strong> can confirm the presence of certain information, but not how multiple concepts are bundled together inside the same neuron.</li> </ul> </li> <li> <strong>Superposition</strong> <br>Superposition means the representation is more like blended colors on a palette than neatly separated channels. It describes how transformers store more features than the neurons they have by overlapping multiple concepts in the same hidden space. Instead of assigning each feature its own neuron, the model encodes features as distinct patterns across many neurons. Different features reuse the same dimensions but with different activation signatures. <br>For example, <a href="#tab:superposition-example">Table 2</a> shows an example where four neurons might simultaneously encode three different features, each represented by its own pattern across those same four coordinates. <ul> <li> <strong>Attention analysis</strong> reveals correlations but can’t separate the mixed signals from overlapping features.</li> <li> <strong>Probing</strong> detects that the model encodes a concept, but it also can’t disentangle how many features are intertwined inside the same space.</li> </ul> </li> </ol> <div style="display: flex; justify-content: center; margin: 20px 0;"> <table id="tab:superposition-example" style="border-collapse: collapse; width: auto;"> <caption style="caption-side: top; padding: 8px; font-weight: bold; text-align: center;">Table 2. Feature Superposition: A Toy Example</caption> <thead> <tr> <th style="border-bottom: 1px solid #333; padding: 6px 12px;">Feature</th> <th style="border-bottom: 1px solid #333; padding: 6px 12px;">N1</th> <th style="border-bottom: 1px solid #333; padding: 6px 12px;">N2</th> <th style="border-bottom: 1px solid #333; padding: 6px 12px;">N3</th> <th style="border-bottom: 1px solid #333; padding: 6px 12px;">N4</th> </tr> </thead> <tbody> <tr> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px;"><strong>Feature A</strong></td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.1</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.2</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.3</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.4</td> </tr> <tr> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px;"><strong>Feature B</strong></td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.5</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.2</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.1</td> <td style="border-bottom: 1px solid #ddd; padding: 6px 12px; text-align: center;">0.2</td> </tr> <tr> <td style="padding: 6px 12px;"><strong>Feature C</strong></td> <td style="padding: 6px 12px; text-align: center;">0.2</td> <td style="padding: 6px 12px; text-align: center;">0.2</td> <td style="padding: 6px 12px; text-align: center;">0.3</td> <td style="padding: 6px 12px; text-align: center;">0.3</td> </tr> </tbody> </table> </div> <p>While attention patterns and probing results help us peek inside a model, they only scratch the surface. They show that information exists, but not how it is organized within the dense, superposed activation space. These limitations highlight the need for alternative interpretability methods for feature extraction!</p> <h2 id="sparse-autoencoder">Sparse Autoencoder</h2> <p>Sparse Autoencoders (SAEs) are designed to solve one of the central challenges in mechanistic interpretability: the dense and superposed nature of transformer activations. Instead of working directly in the model’s tangled representation space, SAEs learn a new basis where features become sparse, separated, and often far more interpretable. This makes them a powerful tool for uncovering the building blocks of a model’s internal computation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-interpret-model/sae_framework-480.webp 480w,/2026/assets/img/2026-04-27-interpret-model/sae_framework-800.webp 800w,/2026/assets/img/2026-04-27-interpret-model/sae_framework-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-interpret-model/sae_framework.jpg" class="img-fluid" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 1. The Framework of Sparse Autoencoder (SAE)<d-cite key="shu2025surveysparseautoencodersinterpreting"></d-cite></figcaption> </figure> <h3 id="framework-overview">Framework Overview</h3> <p>An overview of the SAE framwork is shown in <a href="#Fig:sae-framework">Figure 1</a>. We decompose the SAE framework into four main components: input representation, encoding, decoding, and training with the loss function.</p> <h4 id="input">Input</h4> <p>For a specific layer $l$ in the model we want to interpret (e.g. a Transformer), we denote the hidden representation of token $x_n$ as $z_n^{(l)}$. Each vector $z_n^{(l)}$ is treated as a single input for the SAE.</p> <p><em>Note:</em> The SAE takes one token’s representation at a time, not an entire sequence. However, this vector already encodes rich contextual information. By the time a token $x_n$ reaches layer $l$ in a Transformer, its representation already contains information about the all of the previous tokens, thanks to self-attention mechanism and positional encoding. In other words, the sequence context comes from the Transformer, while the SAE merely learns to decompose the resulting representation.</p> <h4 id="encode">Encode</h4> <p>The input vector is transformed into a sparse activation $h(z)$ by:</p> \[h(z) = \sigma(W_{\text{enc}}z+b_{\text{enc}}),\] <p>where $\sigma$ is a sparsity-encouraging activation function (e.g., ReLU, Top-K, JumpReLU). The encoder maps the original $d$-dimensional activation into an overcomplete latent space of size $m$, where $m \gg d$. In practice, $m$ is often chosen to be $4\times$ to $8\times$ larger than the original dimension so that the model can represent many more features than the Transformer’s native space allows.</p> <h4 id="decode">Decode</h4> <p>Once the sparse activation $h(z)$ is computed, the SAE reconstructs the original representation through a linear decoding step:</p> \[\hat{z}= h(z)\cdot W_{\text{dec}}+b_{\text{dec}},\] <p>The decoder combines the active features in $h(z)$ to approximate the original input vector $z$. Each row of the decoder matrix $W_{\text{dec}}$ corresponds to a feature vector, namely a direction in activation space representing a distinct learned concept. The sparse activations in $h(z)$ forces to select a small subset of these feature vectors and combine them to approximate the original input vector $z$. Because only a few latent units are active for any given token, the reconstruction is built from a small, interpretable set of feature vectors, which is what gives SAEs their power for mechanistic analysis.</p> <h4 id="loss-function">Loss Function</h4> <p>Training an SAE is all about balancing two goals:</p> <ol> <li> <em>Reconstruct the original activation well.</em> The SAE should be able to take a hidden representation from the transformer, break it into interpretable features, and then put it back together again. If the reconstruction is bad, the features aren’t capturing the right structure.</li> <li> <em>Use as few features as possible.</em> We want each input to activate only a small number of latent units so those activations are easy to interpret. If everything activates all the time, the resulting features won’t mean anything.</li> </ol> <p>Hence, SAEs are trained with a loss that balances two objectives:</p> <ul> <li> <strong>Reconsruction loss</strong> to accurately reconstruct the original activation, and</li> <li> <strong>Sparsity loss</strong> to enforce sparsity in the latent activations.</li> </ul> <p>The combined loss can be written as:</p> \[\mathcal{L}(z) \;=\; \underbrace{\|z - \hat{z}\|_2^2}_{\text{Reconstruction loss}} \;+\; \underbrace{\lambda \|h(z)\|_1}_{\text{Sparsity loss}}.\] <p>Averaging over a dataset $\mathcal{X}$ gives:</p> \[\mathcal{L}(\mathcal{X}) = \frac{1}{|\mathcal{X}|} \sum_{z \in \mathcal{X}} \left( \|z - \hat{z}\|_2^2 + \lambda \|h(z)\|_1 \right).\] <p>The reconstruction loss is typically the mean squared error between the reconstructed vector $\hat{z}$ and the original input vector $z$, encouraging the decoder’s feature vectors to span the space of real activations.</p> <p>The sparsity loss is usually an $L_1$ penalty on the latent activation $h(z)$, which encourages most latent units to stay inactive while allowing a few meaningful ones to activate.</p> <p><em>Note:</em> The ideal sparsity measure is the $L_0$ norm that counts non-zero entries, but it is non-differentiable. The $L_1$ norm serves as its closest convex relaxation, making it practical for gradient-based optimization.</p> <h4 id="sae-vs-vae">SAE vs. VAE</h4> <p>Since SAEs are a type of autoencoder, it’s natural to compare them to Variational Autoencoders (VAEs)—one of the most widely used autoencoder variants in deep learning. Although both architectures share the same high-level structure (an encoder, a latent space, and a decoder), they are designed for very different goals and impose different assumptions on what the latent space should look like. A detailed comparison is provided in <a href="tab:sae-vae-comparison">Table 3</a>.</p> <div class="table" id="tab:sae-vae-comparison"> <table> <caption style="caption-side: top; padding: 8px; font-weight: bold; text-align: center;">Table 3. Comparison of Sparse Autoencoders and Variational Autoencoders</caption> <thead> <tr> <th>Model</th> <th>SAE (Sparse Autoencoder)</th> <th>VAE (Variational Autoencoder)</th> </tr> </thead> <tbody> <tr> <td><strong>Goal</strong></td> <td>Learn sparse, interpretable features</td> <td>Learn continuous latent representations</td> </tr> <tr> <td><strong>Latent Space</strong></td> <td> <strong>Overcomplete</strong> ($m \gg d$)</td> <td> <strong>Compressed</strong> ($m \ll d$)</td> </tr> <tr> <td><strong>Constraint</strong></td> <td> <strong>Sparsity</strong> (most activations = 0)</td> <td> <strong>Probabilistic</strong> (latent distributions)</td> </tr> <tr> <td><strong>Architecture</strong></td> <td>Deterministic encoder/decoder</td> <td>Probabilistic encoder, deterministic decoder</td> </tr> <tr> <td><strong>Loss Function</strong></td> <td>Reconstruction + $L_1$ sparsity</td> <td>Reconstruction + KL divergence</td> </tr> <tr> <td><strong>Use Case</strong></td> <td> <strong>Interpretability</strong> of neural networks</td> <td> <strong>Generation</strong> and representation learning</td> </tr> </tbody> </table> </div> <p>In summary, although SAEs and VAEs both belong to the autoencoder family, they operate in almost opposite regimes. VAEs <strong>compress</strong> information to learn smooth, generative latent spaces, while SAEs <strong>expand</strong> the latent space and enforce sparsity to uncover disentangled, human-readable features—making them especially well-suited for mechanistic interpretability.</p> <h3 id="layers--activation">Layers &amp; Activation</h3> <p>Sparse Autoencoders are intentionally designed to be shallow. In most interpretability work, an SAE consists of a single linear/ReLU encoder and a single linear decoder, sometimes with a small number of additional layers (but rarely more than 2–3). This stands in sharp contrast to the deep, nonlinear architecture of transformer models.</p> <p><em>Why SAE has so few layers?</em></p> <ul> <li> <strong>Interpretability</strong>: Every extra layer introduces more mixing and entanglement, making feature meanings harder to trace.</li> <li> <strong>Simplicity</strong>: Linear or single-nonlinearity encoders keep the learned features easy to inspect.</li> <li> <strong>Sparsity</strong>: Shallow networks respond more directly to $L_1$-induced sparsity and are less prone to hiding patterns behind multiple nonlinearities.</li> </ul> <p>In short: <em>SAEs stay shallow so their learned features remain clean and interpretable.</em></p> <h4 id="activation">Activation</h4> <p>Transformers and other deep networks use smooth nonlinearities such as GeLU, SiLU, Tanh, or Sigmoid. These functions are excellent for training large models - yet terrible for learning sparse, interpretable features.</p> <ul> <li> <strong>GeLU:</strong> $\text{GeLU}(x)=x\cdot \Phi(x)$, where $\Phi(x)$ is the Gaussian CDF.</li> <li> <strong>SiLU/Swish:</strong> $\text{SiLU}(x) = x\cdot \sigma(x)$, where $\sigma(x)$ is the logistic sigmoid.</li> </ul> <p>These smooth activations rarely produce exact zeros; instead, they create soft, continuous outputs where nearly everything has a nonzero value - but SAEs need zeros, since sparsity is the entire idea!</p> <p><strong>ReLU</strong> ($\text{ReLU}(x) = \max(0, x)$) is the most common activation used in SAEs because it naturally encourages sparsity. Its hard cutoff at zero produces many exact zeros, making it much easier to determine when a particular feature is present or absent. This behavior aligns well with the interpretability goal: each active latent dimension can be treated as a clear, discrete signal.</p> <p>That said, many modern SAE designs rely on even stronger sparsity mechanisms.</p> <p>One example is the <strong>Top-K</strong> activation, which keeps only the K largest activations and sets all others to zero. This enforces a fixed level of sparsity, as exactly K “most relevant” features selected by the model are active per input. It avoids threshold tuning entirely, since K is a direct and intuitive hyperparameter. Top-K has become popular in interpretability work because it produces consistently clean, discrete feature usage across all samples.</p> <p>Another sparsity-oriented activation used in SAEs is <strong>JumpReLU</strong>, which introduces a learnable activation threshold. Instead of activating as soon as the input becomes positive, JumpReLU only responds when the input exceeds a learned value $\theta$:</p> \[\text{JumpReLU}(x) = \max(0,\, x - \theta).\] <p>This allows the model to determine how strong a signal must be before a feature should be “turned on” during training. The result follows a flexible but still highly sparse activation pattern. Many inputs fall below the learned threshold and produce exact zeros, while only sufficiently strong signals activate a feature. JumpReLU therefore becomes more adaptive than ReLU, but still far more interpretable than smooth activations like GeLU or SiLU.</p> <h3 id="sae-evaluation">SAE Evaluation</h3> <p><a href="tab:sae-eval">Table 4</a> summarizes the key metrics used to evaluate SAEs across several models<d-cite key="shu2025surveysparseautoencodersinterpreting"></d-cite>. These metrics fall into two broad categories: structural metrics, which assess whether the SAE is a faithful surrogate for the original activations, and functional metrics, which assess whether the learned features behave as clean, interpretable units. Together, they provide a more complete picture of SAE quality than reconstruction loss alone.</p> <h4 id="structural-metrics">Structural Metrics</h4> <p>Before assessing the interpretability of individual features, we must first determine whether an SAE is a reliable approximation of the original layer. Structural metrics answer this question by measuring both sparsity and reconstruction fidelity.</p> <p>$L_0$ sparsity counts how many latents fire on a typical token. Lower values indicate a cleaner, more selective representation. However, overly aggressive sparsity can degrade reconstruction quality. Metrics such as MSE, cross-entropy loss, KL divergence, and explained variance quantify how well the SAE preserves the geometry and predictive behavior of the original activations. If a language model maintains similar next-token predictions when its hidden states are replaced by SAE reconstructions, the SAE sits on a good sparsity–fidelity frontier: sparse enough to be interpretable, but faithful enough not to distort the model’s behavior.</p> <h4 id="functional-metrics">Functional Metrics</h4> <p>Reconstruction quality alone does not guarantee interpretability. An SAE can produce low error while still learning “bad” features, e.g., latents that collapse multiple concepts or absorb unrelated signals. Functional metrics capture these failure modes.</p> <p><strong>Absorption</strong> measures how frequently the “correct” latent vector fails to activate and is replaced by an unrelated but correlated feature. Mean absorption tracks partial failures, while full absorption captures cases where none of the appropriate latents are activated. Low absorption indicates that concepts are represented consistently rather than being swallowed by a few dominant features.</p> <p><strong>Spurious Correlation Removal</strong> (SCR) tests whether the SAE isolates spurious features that contribute to shortcut behavior. By identifying and ablating latents most associated with a known spurious attribute, SCR quantifies how much debiasing occurs when removing the top 5, 50, or 500 such features. High SCR scores indicate that the SAE has cleanly separated true signal from superficial correlations.</p> <p>Finally, <strong>Sparse Probing</strong> compares concept probes trained on SAE latents to probes trained on the model’s dense activations. When a probe using only a small number of SAE features matches or exceeds the dense baseline, it suggests that the SAE has discovered disentangled, concept-aligned representations. Conversely, poor sparse-probe performance shows that the SAE’s features, despite having good structural scores, are not semantically meaningful.</p> <div class="table" id="tab:sae-eval"> <table> <caption style="caption-side: top; padding: 8px; font-weight: bold; text-align: center;"> Table 4. Metrics for Evaluating SAEs </caption> <thead> <tr> <th>Category</th> <th>Metric</th> <th>What it Measures</th> <th>Interpretability Intuition</th> </tr> </thead> <tbody> <tr> <td rowspan="5"><strong>Structural</strong></td> <td><strong>L0 Sparsity</strong></td> <td>Average number of active latents per token.</td> <td>How sparse the SAE actually is; lower L0 means fewer features fire on each input.</td> </tr> <tr> <td><strong>MSE</strong></td> <td>Mean squared error between original activations and SAE reconstructions.</td> <td>Basic reconstruction fidelity: does the SAE preserve the underlying representation?</td> </tr> <tr> <td><strong>Cross-Entropy Loss</strong></td> <td>Next-token loss when the model is run on reconstructed activations instead of originals.</td> <td>Checks whether reconstruction is “good enough” for the language modeling task.</td> </tr> <tr> <td><strong>KL Divergence</strong></td> <td>KL between the original and SAE-reconstructed next-token distributions.</td> <td>Measures how much the SAE changes the model’s predictive distribution.</td> </tr> <tr> <td><strong>Explained Variance</strong></td> <td>Fraction of variance in activations captured by the SAE reconstruction.</td> <td>High variance explained means the SAE captures most of the geometry of the layer.</td> </tr> <tr> <td rowspan="2"><strong>Absorption</strong></td> <td><strong>Mean Absorption</strong></td> <td>Fraction of cases where the “correct” feature fails to activate and a similar latent fires instead.</td> <td>Lower is better; high values indicate that meaningful concepts are getting swallowed by unrelated latents.</td> </tr> <tr> <td><strong>Full Absorption</strong></td> <td>Stricter version where none of the correct latents activate and the concept is fully absorbed elsewhere.</td> <td>Detects severe failures where a concept is entirely misrepresented.</td> </tr> <tr> <td rowspan="3"><strong>Spurious Correlation Removal (SCR)</strong></td> <td><strong>Top-5 SCR</strong></td> <td>Debiasing performance when ablating the 5 most spurious latents.</td> <td>Tests if a few well-chosen features can remove shortcut correlations.</td> </tr> <tr> <td><strong>Top-50 SCR</strong></td> <td>Same as above, but ablating the top 50 latents.</td> <td>Shows how much debiasing we gain with a modestly larger intervention.</td> </tr> <tr> <td><strong>Top-500 SCR</strong></td> <td>SCR when removing the top 500 spurious latents.</td> <td>Upper bound on how well the SAE separates true signal from spurious features.</td> </tr> <tr> <td rowspan="2"><strong>Sparse Probing</strong></td> <td><strong>LLM Probe</strong></td> <td>Probe accuracy using the original dense LLM activations.</td> <td>Baseline: how well concepts can be decoded from the unmodified model.</td> </tr> <tr> <td><strong>SAE Probe</strong></td> <td>Probe accuracy using a small number of SAE latents.</td> <td>Higher than or close to the LLM probe suggests clean, concept-aligned SAE features.</td> </tr> </tbody> </table> </div> <p>Taken together, these structural and functional metrics tell us whether an SAE is a faithful and useful surrogate for the original model. However, they do not yet tell us what the individual features mean. Once we know that an SAE reconstructs well, maintains sparsity, and avoids major failure modes like absorption or spurious entanglement, we can shift our focus to the interpretability of the features themselves.</p> <h3 id="feature-evaluation">Feature Evaluation</h3> <p>Natually, the next question is: <strong>are the latent features actually correspond to meaningful concepts in the model?</strong> Evaluation generally falls into two categories: input-based (what activates a feature) and output-based (what the feature does when changed).</p> <h4 id="input-based-evaluation">Input-based Evaluation</h4> <p>Input-based analysis examines the inputs or hidden states that cause a feature to activate. Common methods include:</p> <ul> <li>Top activating examples: Inspect tokens or contexts where a feature is strongest. If they cluster around a clear linguistic pattern like plural nouns, numbers, or closing brackets, the feature is likely to be meaningful.</li> <li>Sparsity/selectivity measurements: Good features activate rarely and consistently for the same type of input.</li> </ul> <p>These evaluations aims to answer the question: <em>“What concept is this feature detecting?”</em></p> <h4 id="output-based-evaluation">Output-based Evaluation</h4> <p>Output-based evaluation checks whether a feature plays a <strong>causal</strong> role in the model’s behavior:</p> <ul> <li>Activation patching: Replace or modify a feature’s activation during the forward pass (often using activations from a different run) to test whether that feature is necessary or sufficient for a behavior.</li> <li>Feature-direction interventions: Add or subtract the feature’s decoder vector in Transformer’s residual stream to examine whether that direction corresponds to a meaningful, causal concept.</li> </ul> <p>These evaluations aim to address the question: <em>“Does this feature actually matter for the model’s computation?”</em></p> <h2 id="semi-nonnegative-matrix-factorization">Semi-Nonnegative Matrix Factorization</h2> <p>SAEs have become the dominant tool for feature discovery in MI, largely because they provide a flexible, scalable way to learn disentangled directions in activation space. But SAEs also reveal an important limitation: they learn features from scratch, without reference to the model’s underlying mechanisms. In particular, SAEs trained on the residual stream often struggle to produce features that cleanly correspond to the computations inside the model’s MLP layers. This motivates the next question: <em>What if instead of learning new features, we directly decompose the model’s own MLP activations to reveal how neuron groups compose concepts?</em></p> <p>The recent Semi-Nonnegative Matrix Factorization (SNMF)<d-cite key="shafran2025decomposingmlpactivationsinterpretable"></d-cite> approach offers exactly this perspective. It bypasses the autoencoder architecture entirely and treats MLP activations themselves as the object to factorize, yielding features that are sparse combinations of real neurons, with coefficients that directly reveal which inputs activate which features.</p> <h3 id="method">Method</h3> <p>The core idea behind SNMF is simple but powerful: instead of training a full encoder–decoder network like an SAE, directly factorize the MLP activation matrix into interpretable building blocks. This approach rests on the assumption that the MLP’s output to the residual stream can be expressed as a linear combination of underlying features. Because SNMF operates entirely on collected activations, it is a fully unsupervised, training-free method, since it does not modify the original model or require gradient-based optimization.</p> <p>For a chosen MLP layer, SNMF gathers neuron activations across a sequence of n tokens, forming a matrix $A \in \mathbb{R}^{d_a \times n}$. The goal is to decompose this matrix as:</p> \[A \approx ZY,\] <p>where</p> <ul> <li>\(Z \in \mathbb{R}^{d_a \times k}\) contains the MLP features, each column representing a sparse linear combination of neurons (a co-activation pattern), and</li> <li>\(Y \in \mathbb{R}^{k \times n}_{\ge 0}\) is a nonnegative coefficient matrix indicating how strongly each feature contributes to each token’s activation vector.</li> </ul> <p>The intuition is that neuron activations should combine additively and sparsely to produce higher-level concepts. Enforcing nonnegativity in $Y$ ensures these combinations remain parts-based: features cannot “subtract” from one another, making the representation more interpretable.</p> <p>The factorization alternates between two update steps:</p> <ol> <li>Multiplicative updates for $Y$, which preserve nonnegativity: $Y \leftarrow Y \odot \frac{Z^\top A}{Z^\top ZY + \epsilon},$</li> <li>Closed-form ridge-regression updates for $Z$: $Z = A Y^\top (YY^\top + \lambda I)^{-1}.$</li> </ol> <p>After each pair of updates, SNMF applies winner-take-all sparsification to the columns of $Z$, keeping only the largest-magnitude entries and setting the rest to zero. This encourages each feature to rely on a small, coherent subset of neurons.</p> <p>Each SNMF feature can then be mapped back into the residual stream via the MLP’s output projection matrix, making it directly comparable to SAE features and suitable for causal interventions such as steering or ablations. The result is a set of interpretable, neuron-grounded features that reveal how the MLP layer internally organizes semantic structure.</p> <h3 id="discussions">Discussions</h3> <p>SNMF offers an appealing alternative to SAEs by grounding features directly in the model’s own neurons rather than learning new latent directions. Because of the assumption that each feature is a sparse combination of real MLP neurons and each token’s activation is expressed as a nonnegative mixture of these features, the resulting representations are often more transparent. They reveal how groups of neurons cooperate to encode meaningful concepts, and the nonnegativity constraint in $Y$ provides a clean way to examine which tokens most strongly activate each feature. This gives SNMF a natural interpretability advantage: instead of discovering artificial directions in activation space, it exposes structure that already exists within the network.</p> <p>However, despite its conceptual clarity, SNMF faces significant limitations when applied to modern language models. The method requires collecting large matrices of MLP activations and repeatedly factorizing them, which becomes computationally expensive as model width grows. Unlike SAEs, which can be trained incrementally using minibatches, SNMF depends on having access to large dense activation datasets at once, making memory and storage major bottlenecks. Its results also depend strongly on the dataset used to extract activations: if the chosen text distribution does not sufficiently cover the model’s behaviors, important features may be omitted or fragmented. Furthermore, because SNMF restricts features to be linear combinations of neurons, it may fail to capture structure that emerges only at the level of directions or higher-dimensional subspaces; some concepts that SAEs can isolate simply cannot be expressed as sparse neuron combinations.</p> <p>The optimization procedure itself can also be brittle. Multiplicative updates and sparsification thresholds may lead to unstable or inconsistent features, and small changes in hyperparameters can yield noticeably different decompositions. Finally, SNMF lacks the natural overcompleteness of SAEs: choosing too few features underfits the representation, while choosing too many risks redundancy or noise. In practice, these constraints make SNMF more suitable as a diagnostic tool for understanding local MLP structure rather than a scalable alternative to SAEs for whole-model feature extraction.</p> <p>Even with these limitations, SNMF remains a valuable complement to SAE-based approaches. It provides insight into how the model’s neurons themselves are organized and how co-activation patterns contribute to semantic representation. As a bridge between raw neuron-level analysis and learned feature extraction methods, SNMF helps illuminate the internal structure of MLP layers and sets the stage for deeper analyses of how features evolve across depth.</p> <h2 id="cross-layer-transcoder">Cross-Layer Transcoder</h2> <p>While SAEs and SNMF both uncover meaningful feature structure within a single layer, they offer only a static view of the model. Neither approach tells us how these features transform as they propagate forward through the network. In practice, transformer representations change dramatically from layer to layer—features can split into multiple subfeatures, merge into broader abstractions, fade out, or invert their meaning entirely. These dynamics are often invisible if we inspect each layer independently. To move from “feature discovery” to understanding computation as a multi-step process, we need a method that aligns feature spaces across depth and traces how information flows between layers. This is the motivation behind the Cross-Layer Transcoder(CLT)<d-cite key="dunefsky2024transcodersinterpretablellmfeature"></d-cite>: a small learned model that maps the activation features of one layer into the feature basis of another. Where SAEs focus on extracting sparse, interpretable features from a single layer, CLTs let us track how those features transform as the representation flows forward through the network.</p> <h3 id="architecture">Architecture</h3> <p>As is shown in <a href="fig:clt">Figure 2</a>, CLT is designed to mirror how a transformer updates its residual stream across depth. It consists of a collection of “features” arranged into the same number of layers as the underlying transformer. Each layer of the CLT reads from the transformer’s residual stream at that depth and contributes to reconstructing the MLP outputs of that layer and all subsequent layers.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-interpret-model/clt-480.webp 480w,/2026/assets/img/2026-04-27-interpret-model/clt-800.webp 800w,/2026/assets/img/2026-04-27-interpret-model/clt-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-interpret-model/clt.jpg" class="img-fluid" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 2. Replacement Model Constructed by Cross-Layer Transcoder (CLT)<d-cite key="dunefsky2024transcodersinterpretablellmfeature"></d-cite></figcaption> </figure> <p>At layer $\ell$, the CLT first encodes the transformer’s residual activation $x_\ell$ using a learned linear map followed by a sparsifying nonlinearity (typically <strong>JumpReLU</strong>):</p> \[a_\ell = \text{JumpReLU}(W^{(\ell)}_{\text{enc}} x_\ell),\] <p>where $a_\ell$ is the vector of CLT feature activations for that layer. These features are “cross-layer” because they are not restricted to influencing only one layer. Instead, each feature at layer $\ell$ can help reconstruct the MLP outputs $y_{\ell’,\, \ell’ \ge \ell}$, via separate decoder weights for each downstream layer:</p> \[\hat{y}_{\ell} = \sum_{\ell' = 1}^{\ell} W^{(\ell')\to \ell}_{\text{dec}}\, a_{\ell'}.\] <p>In other words, the transcoder at layer $\ell$ has a shared encoder but multiple decoders, each responsible for predicting the MLP output of a different downstream layer. This design makes each feature a stable, reusable computational unit: the encoder determines what the feature detects at layer $\ell$, while the decoders determine where and how that feature influences the rest of the model. Thus, the MLP output at a given layer is reconstructed jointly from all features in that layer and all earlier layers.</p> <p>All cross-layer transcoders are trained jointly as a single end-to-end model. Although each layer $\ell$ has its own encoder and its own set of decoders targeting every downstream layer, the entire stack of encoders and decoders is optimized together. During training, the CLT reads the transformer’s residual stream at each layer to produce feature activations, and the combined contributions of all earlier features are used to reconstruct the MLP output at every layer. The loss sums <strong>reconstruction</strong> errors across all layers, together with a <strong>sparsity penalty</strong> that encourages selective activations and minimal decoder weights. Because gradients flow through all layers at once, features learned at early depths become reusable building blocks for predicting later MLP outputs, allowing the CLT to capture the model’s computation as a coherent, multi-step transformation rather than a collection of isolated layer-wise mappings.</p> <p>In practice, CLTs can have as much as hundreds of thousands to tens of millions of features across layers, but still remain far smaller than the transformer models they interpret. They are trained on recorded activations from the base model, using standard optimization methods. Once trained, a CLT provides a structured, layer-aligned view of how representations evolve, merge, and transform as they propagate forward through the transformer.</p> <h3 id="discussions-1">Discussions</h3> <p>Despite their conceptual appeal, CLTs are difficult to deploy at scale, as each layer must have its own encoder and multiple decoder matrices aimed at every subsequent layer. For modern LLMs with dozens of layers and thousands of channels, the total parameter count quickly explodes, even though the CLT is only modeling the MLP pathways. Training such a model requires storing the full residual stream and MLP outputs for vast numbers of tokens, making data collection and GPU memory usage significant bottlenecks.</p> <p>Even after training, CLTs face interpretability challenges. Because reconstruction quality matters for every layer, early-layer features must support many downstream predictions, causing heavy coupling between layers. This often makes learned features diffuse rather than crisp, especially in deeper models where errors compound. Moreover, CLTs capture correlations in activations, not causal mechanisms; they approximate how the model tends to update its representations, but cannot guarantee that the learned mapping reflects the true internal computation. As depth increases, nonlinear or context-specific transformations are also harder to approximate with a simple transcoder.</p> <p>Yet CLTs remain a valuable research tool when used appropriately. They provide a structured way to trace how representations flow across layers, offering a layer-aligned view of model computation that complements within-layer methods like SAEs.</p> <h2 id="weight-sparse-transformer">Weight-Sparse Transformer</h2> <p>SAE, SNMF and CLT provide powerful post-hoc tools for disentangling features inside large language models, but they don’t change the model itself. A complementary line of work asks a more radical question: <em>What if we trained the model from scratch to be interpretable?</em></p> <p>This motivates Weight-Sparse Transformers (WSTs) <d-cite key="gao2025weightsparsetransformersinterpretablecircuits"></d-cite>, models in which most parameters are encouraged to become exactly zero during training. Instead of discovering meaningful features after the fact, WSTs reshape the model so that features are simpler, more local, and more monosemantic, thus can potentially make feature-extraction tools like SAEs and transcoders more faithful and informative.</p> <h3 id="architecture-1">Architecture</h3> <p>WSTs preserve the layout of standard transformers, i.e. attention blocks, MLP blocks, and residual connections. However, they differ in one fundamental respect: every major weight matrix is kept sparse throughout training. Instead of learning dense projections, the model is forced to operate with only a small subset of nonzero weights, which in turn encourages each attention head and MLP neuron to interact with only a limited number of residual channels.</p> <p>WSTs rely on a simple but powerful mechanism to enforce sparsity: deterministic magnitude pruning. After each optimizer update, the model applies an <strong>AbsTopK</strong> operation to <em>every</em> weight matrix, retaining only the weights with the largest absolute values and setting all others to zero. Because this pruning happens during every training step, the model adapts early on to the sparse connectivity pattern and learns to route computation through a stable, minimal set of connections. The global sparsity level is gradually increased over the course of training, guiding the transformer from a nearly dense regime to a highly sparse one. By the end of training, many rows and columns of the attention and MLP matrices consist entirely of zeros, and the remaining structure reveals a compact computational graph that the transformer actually uses.</p> <p>The effects of this sparse architecture ripple through the entire model. In the attention layers, the Q, K, V, and output projection matrices contain many zero entries, so each head reads from and writes to only a few chosen channels. Attention patterns become localized, reducing entanglement and making the function of each head easier to understand. The same dynamic appears in the MLP blocks, where each neuron receives input from only a handful of residual coordinates and produces updates to only a few output dimensions. Rather than acting as diffuse nonlinear mixers, MLP neurons become simple feature detectors with clear roles.</p> <p>Because both attention and MLP updates are sparse, each layer modifies only a small portion of the residual stream. Dense transformers rewrite nearly the entire hidden vector at every layer, mixing information in hard-to-decipher ways. In contrast, WSTs update only the coordinates that matter, producing cleaner, more modular computation. The resulting circuits are far smaller and easier to trace, enabling a level of mechanistic clarity that dense models rarely exhibit.</p> <h3 id="discussions-2">Discussions</h3> <p>While WSTs offer clean, interpretable circuits, they come with notable trade-offs. Continuous magnitude pruning makes training less stable and less efficient, and the resulting models generally underperform dense transformers at the same scale. Because sparsity forces the network into a rigid connectivity pattern, WSTs struggle to match the flexibility and capacity of dense architectures.</p> <p>These limitations make WSTs valuable primarily as scientific probes rather than practical LLM replacements. They are excellent for studying modular computation, but unlikely to scale to state-of-the-art performance. Future work may focus on hybrid models that blend sparse and dense components, or on training objectives that encourage structure without imposing hard constraints.</p> <p>This perspective naturally bridges back to SAEs. SAEs provide a post-hoc interpretability layer for dense models, recovering disentangled features without altering the architecture, while WSTs build interpretability directly into the model by enforcing sparse computation. Together, they illustrate two complementary strategies: discover structure inside dense models and induce structure during training—a promising direction for future research.</p> <h2 id="final-remarks">Final Remarks</h2> <p>Mechanistic interpretability is steadily moving beyond one-off visualizations and heuristic reasoning toward something more systematic—a science built on feature extraction, circuit reconstruction, and architecture-level constraints. Sparse Autoencoders have shown that dense layers contain rich, recoverable structure; Semi-Nonnegative Matrix Factorization reveals how those structures relate to the model’s own neurons; Cross-Layer Transcoders expose how representations transform as they propagate through depth; and Weight-Sparse Transformers explore how we might build models whose internal computations are disentangled from the start. Each technique brings its own strengths and limitations, but together they form an increasingly coherent toolkit for understanding the computations inside modern transformers.</p> <p>Interpretability remains a profound challenge: models continue to grow, tasks become more complex, and no single method will decode every aspect of their internal reasoning. Yet the shift toward feature-level and circuit-level approaches marks real progress. By treating neural networks not as opaque function approximators but as systems composed of reusable computational primitives, we are beginning to see how high-level behaviors emerge from low-level structure. The work ahead will involve hybrid techniques, new representational abstractions, and architectures designed with interpretability in mind. But the direction is clear. Opening the black box is no longer merely a philosophical aspiration—it is becoming a practical engineering discipline, and its foundations are starting to solidify.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-interpret-model.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/vis-llm-latent-geometry/">Visualizing LLM Latent Space Geometry Through Dimensionality Reduction</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/symbolic-connect/">From Dense Monoliths to Modular Minds: The Rise of Symbolic Routing in LLMs</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/subject-invariant-eeg/">The Decoupling Hypothesis: Attempting Subject-Invariant EEG Representation Learning via Auxiliary Injection</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/style-representations/">Artistic Style and the Play of Neural Style Representations</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>