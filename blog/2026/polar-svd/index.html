<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Faster SVD via Accelerated Newton-Schulz Iteration | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Traditional SVD algorithms rely heavily on QR factorizations, which scale poorly on GPUs. We show how the recently proposed Chebyshev-Accelerated Newton-Schulz (CANS) iteration can replace them and produce an SVD routine that is faster across a range of matrix types and precisions."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/polar-svd/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.def{background:#fcf5fb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px;border-radius:8px;padding:8px 12px}.def p{color:white;text-align:left;margin:12px 0;font-size:16px}html[data-theme="dark"] .def{background:#404040;border:1px solid rgba(255,255,255,0.25);box-shadow:0 0 4px rgba(0,0,0,0.8)}html[data-theme="dark"] .def p{color:#f5ebff}.small-fig{width:70%;height:auto;margin-left:auto;margin-right:auto;display:block}.medium-fig{width:80%;height:auto;margin-left:auto;margin-right:auto;display:block}.def summary{cursor:pointer;padding:6px 12px;font-weight:600;list-style:none}.def summary::-webkit-details-marker{font-size:1.1em}.def[open]{box-shadow:0 2px 8px rgba(0,0,0,0.12)}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Faster SVD via Accelerated Newton-Schulz Iteration",
            "description": "Traditional SVD algorithms rely heavily on QR factorizations, which scale poorly on GPUs. We show how the recently proposed Chebyshev-Accelerated Newton-Schulz (CANS) iteration can replace them and produce an SVD routine that is faster across a range of matrix types and precisions.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Faster SVD via Accelerated Newton-Schulz Iteration</h1> <p>Traditional SVD algorithms rely heavily on QR factorizations, which scale poorly on GPUs. We show how the recently proposed Chebyshev-Accelerated Newton-Schulz (CANS) iteration can replace them and produce an SVD routine that is faster across a range of matrix types and precisions.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#algorithm">Algorithm</a> </div> <div> <a href="#experiments">Experiments</a> </div> <ul> <li> <a href="#algorithm-accuracy">Algorithm Accuracy</a> </li> <li> <a href="#low-rank-matrices-analyzes">Low-Rank Matrices Analyzes</a> </li> </ul> <div> <a href="#svd-via-polar-decomposition">SVD via Polar Decomposition</a> </div> <div> <a href="#how-to-compute-polar-decomposition">How to Compute Polar Decomposition?</a> </div> <ul> <li> <a href="#rational-functions-qdwh">Rational Functions (QDWH)</a> </li> <li> <a href="#polynomial-functions-newton-schulz-iteration">Polynomial Functions (Newton-Schulz Iteration)</a> </li> <li> <a href="#accelerated-newton-schulz-iteration">Accelerated Newton-Schulz Iteration</a> </li> </ul> <div> <a href="#why-matrix-multiplications-instead-of-qr">Why Matrix Multiplications Instead of QR?</a> </div> <div> <a href="#detailed-algorithm-description">Detailed Algorithm Description</a> </div> <div> <a href="#related-work">Related Work</a> </div> <ul> <li> <a href="#cuda-qr-gesvd">CUDA QR (gesvd)</a> </li> <li> <a href="#cuda-jacobi-gesvdj">CUDA Jacobi (gesvdj)</a> </li> <li> <a href="#cuda-polar-gesvdp">CUDA Polar (gesvdp)</a> </li> </ul> <div> <a href="#discussion">Discussion</a> </div> <div> <a href="#appendix">Appendix</a> </div> </nav> </d-contents> <p>In recent years, the polar decomposition has attracted considerable attention, with the Muon optimizer <d-cite key="jordan2024muon"></d-cite> making a major contribution to its renewed popularity. Riding this wave of interest, several accelerated methods for computing the polar decomposition have emerged: CANS by Grishina et al. <d-cite key="grishina2025accelerating"></d-cite> and Polar-Express by Amsel et al. <d-cite key="amsel2025polar"></d-cite>, which solely rely on matrix multiplications. Combined with advances in GPU-optimized matrix-multiplication kernels and the introduction of TF-32 and BF-16 precisions, these developments have made polar decomposition remarkably efficient on modern hardware.</p> <p>In this blogpost, we show that these ideas can be used to accelerate the computation of the singular value decomposition (SVD) via the polar decomposition on GPUs, yielding speedups of up to 2× compared with existing implementations.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-polar-svd/square.webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-polar-svd/square.webp" class="img-fluid rounded z-depth-2 only-light medium-fig" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-polar-svd/square_dark.webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-polar-svd/square_dark.webp" class="img-fluid rounded z-depth-2 only-dark medium-fig" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Algorithm comparison on a random square matrix on an NVIDIA B200 GPU. <strong>Note:</strong> the algorithms have different accuracy, see Tables 1 and 2 for a more comprehensive analysis. </div> <p>Because the CANS iteration relies heavily on matrix multiplications, it is advantageous to use the lower-precision TensorFloat-32 format to speed up computation. Consequently, CANS-SVD can be executed in two modes: (1) full FP-32 precision for a more accurate decomposition, or (2) TF-32 precision for a faster but less accurate result. Other SVD implementations typically do not offer this level of precision flexibility.</p> <h2 id="algorithm">Algorithm</h2> <p>Our method builds on the approach of computing the SVD via polar decomposition proposed in <d-cite key="nakatsukasa2013stable"></d-cite>, but replaces the original polar factor computation with the modern techniques of <d-cite key="grishina2025accelerating"></d-cite>. The resulting algorithm can be implemented in just a few lines of code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">cans_svd</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">matrix</span><span class="p">):</span>
    <span class="n">_A</span> <span class="o">=</span> <span class="nf">cans_preprocessing</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>     <span class="c1"># preproccess matrix A
</span>    <span class="n">W</span> <span class="o">=</span> <span class="nf">ns_base_iteration</span><span class="p">(</span><span class="n">_A</span><span class="p">)</span>      <span class="c1"># compute the polar factor of A
</span>    <span class="n">H</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">A</span>                    <span class="c1"># obtain the H - symmetric matrix
</span>    <span class="n">V</span><span class="p">,</span> <span class="n">S</span> <span class="o">=</span> <span class="nf">eigh</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>                 <span class="c1"># eigendecomposition of symmetric matrix
</span>    <span class="n">U</span> <span class="o">=</span> <span class="n">W</span> <span class="o">@</span> <span class="n">V</span>
    <span class="n">U</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">qr</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>                   <span class="c1"># compute QR of U to fix orthogonality
</span>    <span class="k">return</span> <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span><span class="p">.</span><span class="n">T</span>
</code></pre></div></div> <details><summary>What is SVD?</summary> <p>For a matrix $A \in \mathbb{R}^{m \times n}$, the singular value decomposition defined as</p> \[A = U \Sigma V^{\top},\] <p>where $U \in \mathbb{R}^{m \times m}$ and $V \in \mathbb{R}^{n \times n}$ are orthogonal matrices, and $\Sigma \in \mathbb{R}^{m \times n}$ is a diagonal matrix whose diagonal entries are the non-negative singular values $\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_{\min(m, n)} \ge 0$.</p> </details> <details><summary>What is polar decomposition?</summary> <p>Any matrix $A \in \mathbb{R}^{m \times n}$ with $m \ge n$ admits a polar decomposition</p> \[A = WH,\] <p>where $W \in \mathbb{R}^{m \times n}$ has orthonormal columns and $H \in \mathbb{R}^{n \times n}$ is symmetric positive semidefinite. The polar factor $W$ can be viewed as the closest matrix with orthonormal columns to $A$ in the Frobenius norm.</p> </details> <p>Here, <code class="language-plaintext highlighter-rouge">cans_preprocessing</code> refers to preprocessing method introduced in <d-cite key="grishina2025accelerating"></d-cite>. And <code class="language-plaintext highlighter-rouge">ns_base_iteration</code> refers to iteration based on Newton-Schulz method, which efficiently compute the polar factor <d-cite key="grishina2025accelerating,amsel2025polar,chen2014stable"></d-cite>. Both steps can be executed in TF-32. Often, the bottleneck in computing the SVD via polar decomposition is the computation of the polar factor. Indeed the <code class="language-plaintext highlighter-rouge">eigh</code> and <code class="language-plaintext highlighter-rouge">qr</code> steps typically are more fast, therefore, we focus on accelerating the polar factor computation. A ready-to-use JAX implementation can be found <a href="https://github.com/anonymous-502475/polar-svd" rel="external nofollow noopener" target="_blank">here</a>.</p> <h2 id="experiments">Experiments</h2> <p>We compare our algorithm with standard GPU-based SVD implementations from NVIDIA’s <a href="https://docs.nvidia.com/cuda/cusolver/index.html" rel="external nofollow noopener" target="_blank">cuSOLVER</a> library and observe the following results:</p> <ul> <li>On the NVIDIA B200 GPU, the TF-32 variant of CANS-SVD achieves up to a 2× speedup over the CUDA Polar-based SVD and up to a 10× speedup over the CUDA QR- and Jacobi-based SVD.</li> <li>The CUDA Polar-based SVD implementation (<code class="language-plaintext highlighter-rouge">driver=svdp</code>) exhibits numerical instability on ill-conditioned or singular matrices, limiting its practical applicability.</li> <li>The ability to use TF-32 precision is a unique advantage of CANS-SVD, whereas cuSOLVER algorithms rely on more complex procedures (such as QR and Givens rotations) that can only be computed in FP-32.</li> <li>The FP-32 variant of CANS-SVD consistently provides high accuracy across all test scenarios, while the TF-32 variant maintains a similar orthogonality error but exhibits higher reconstruction error. This gives rise to a trade-off between accuracy and precision.</li> </ul> <h3 id="algorithm-accuracy">Algorithm Accuracy</h3> <p>To assess numerical stability, we generate random matrices $A \in \mathbb{R}^{4096 \times 4096}$ with varying condition numbers $c = ||A||_2 ||A^{-1}||_2$, where $||\cdot||_2$ is the spectral norm. The condition number is a key factor influencing stability and accuracy of numerical linear algebra algorithms <d-cite key="higham2002accuracy"></d-cite>. We report relative reconstruction and orthogonality errors:</p> \[\texttt{err}_{\texttt{rec}} = \frac{|| A - U \Sigma V^\top ||_F}{||A||_F}, \quad \texttt{err}_{\texttt{ort}} = \max\left\{\frac{||U^\top U - I||_F}{||I||_F}, \frac{||V^\top V - I||_F}{||I||_F}\right\}.\] <h4 id="table-1-relative-reconstruction-error-for-different-condition-number-values">Table 1. Relative <u>Reconstruction error</u> for different condition number values.</h4> <table> <thead> <tr> <th>Method</th> <th>$c=1.1$</th> <th>$c=10$</th> <th>$c=10^2$</th> <th>$c=10^4$</th> </tr> </thead> <tbody> <tr> <td>CUDA POLAR</td> <td>$5.4\cdot10^{-6}$</td> <td>$4.3\cdot10^{-6}$</td> <td>$5.4\cdot10^{-6}$</td> <td><span style="color:red"> $7.6\cdot10$</span></td> </tr> <tr> <td>CUDA QR</td> <td>$1.8\cdot10^{-5}$</td> <td>$1.6\cdot10^{-5}$</td> <td>$2.7\cdot10^{-5}$</td> <td>$1.7\cdot10^{-5}$</td> </tr> <tr> <td>CUDA JACOBI</td> <td>$1.9\cdot10^{-3}$</td> <td>$9.8\cdot10^{-4}$</td> <td>$8.4\cdot10^{-4}$</td> <td>$6.0\cdot10^{-4}$</td> </tr> <tr> <td>CANS-SVD (TF-32)</td> <td>$6.5\cdot10^{-4}$</td> <td>$8.6\cdot10^{-4}$</td> <td>$7.1\cdot10^{-4}$</td> <td>$7.8\cdot10^{-4}$</td> </tr> <tr> <td>CANS-SVD (FP-32)</td> <td>$3.9\cdot10^{-6}$</td> <td>$4.7\cdot10^{-6}$</td> <td>$4.1\cdot10^{-6}$</td> <td>$4.4\cdot10^{-6}$</td> </tr> </tbody> </table> <h4 id="table-2-relative-orthogonality-error-for-different-condition-number-values">Table 2. Relative <u>Orthogonality error</u> for different condition number values.</h4> <table> <thead> <tr> <th>Method</th> <th>$c=1.1$</th> <th>$c=10$</th> <th>$c=10^2$</th> <th>$c=10^4$</th> </tr> </thead> <tbody> <tr> <td>CUDA POLAR</td> <td>$5.5\cdot10^{-6}$</td> <td>$4.1\cdot10^{-6}$</td> <td>$4.0\cdot10^{-6}$</td> <td>$8.1\cdot10^{-6}$</td> </tr> <tr> <td>CUDA QR</td> <td>$1.4\cdot10^{-5}$</td> <td>$8.9\cdot10^{-6}$</td> <td>$1.3\cdot10^{-5}$</td> <td>$1.4\cdot10^{-5}$</td> </tr> <tr> <td>CUDA JACOBI</td> <td>$2.9\cdot10^{-3}$</td> <td>$2.1\cdot10^{-3}$</td> <td>$2.1\cdot10^{-3}$</td> <td>$2.3\cdot10^{-3}$</td> </tr> <tr> <td>CANS-SVD (TF-32)</td> <td>$3.1\cdot10^{-6}$</td> <td>$3.1\cdot10^{-6}$</td> <td>$3.0\cdot10^{-6}$</td> <td>$2.7\cdot10^{-6}$</td> </tr> <tr> <td>CANS-SVD (FP-32)</td> <td>$3.1\cdot10^{-6}$</td> <td>$3.1\cdot10^{-6}$</td> <td>$3.0\cdot10^{-6}$</td> <td>$2.7\cdot10^{-6}$</td> </tr> </tbody> </table> <p>In the table, values highlighted in <span style="color:red">red</span> correspond to cases where the algorithm failed to compute the SVD.</p> <h3 id="low-rank-matrices-analyzes">Low-Rank Matrices Analyzes</h3> <p>A substantial part of our evaluation focuses on the behavior of the algorithms on low-rank matrices. For iterative schemes, convergence can be significantly harder to achieve when the input matrix is not full rank. To illustrate this phenomenon, the figure below compares performance on randomly generated low-rank matrices.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-polar-svd/low_rank.webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-polar-svd/low_rank.webp" class="img-fluid rounded z-depth-1 only-light medium-fig" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-polar-svd/low_rank_dark.webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-polar-svd/low_rank_dark.webp" class="img-fluid rounded z-depth-1 only-dark medium-fig" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Algorithm comparison on a random square low-rank matrix on NVIDIA B200 GPU (rank is 10× smaller than the matrix size). We do not report CUDA POLAR algorithm in this figure, as the method failed to compute the SVD. See Table 3 below for details. </div> <p>Since orthogonality error remains relatively stable across ranks, we focus on reconstruction error for random matrices $A \in \mathbb{R}^{4096 \times 4096}$ with varying rank.</p> <h4 id="table-3-relative-reconstruction-error-for-different-ranks">Table 3. Relative <u>Reconstruction error</u> for different ranks.</h4> <table> <thead> <tr> <th>Method</th> <th>$r=16$</th> <th>$r=256$</th> <th>$r=4095$</th> <th>$r=4096$</th> </tr> </thead> <tbody> <tr> <td>CUDA POLAR</td> <td><span style="color:red"> $7.2 \cdot10$</span></td> <td><span style="color:red"> $7.6\cdot10$</span></td> <td><span style="color:red"> $7.6\cdot10$</span></td> <td>$2.4\cdot10^{-6}$</td> </tr> <tr> <td>CUDA QR</td> <td>$4.3\cdot10^{-7}$</td> <td>$8.7\cdot10^{-7}$</td> <td>$1.7\cdot10^{-5}$</td> <td>$1.6\cdot10^{-5}$</td> </tr> <tr> <td>CUDA JACOBI</td> <td>$6.9\cdot10^{-5}$</td> <td>$3.8\cdot10^{-4}$</td> <td>$2.6\cdot10^{-3}$</td> <td>$2.6\cdot10^{-3}$</td> </tr> <tr> <td>CANS-SVD (TF-32)</td> <td>$5.1\cdot10^{-4}$</td> <td>$6.4\cdot10^{-4}$</td> <td>$8.5\cdot10^{-4}$</td> <td>$7.6\cdot10^{-4}$</td> </tr> <tr> <td>CANS-SVD (FP-32)</td> <td>$4.1\cdot10^{-5}$</td> <td>$3.4\cdot10^{-6}$</td> <td>$8.5\cdot10^{-6}$</td> <td>$2.8\cdot10^{-6}$</td> </tr> </tbody> </table> <p>As shown above, the CUDA POLAR algorithm failed to compute the SVD even when the rank of the matrix was at least one less than its size.</p> <h2 id="svd-via-polar-decomposition">SVD via Polar Decomposition</h2> <p>Let us discuss in detail how to compute the SVD using the polar decomposition. There is a fundamental relation that connects these two decompositions:</p> \[A = \underbrace{U V}_{W}{}^\top \;\underbrace{V \Sigma V}_{H}{}^\top,\] <p>where $A = U \Sigma V^\top$ is the SVD of the matrix $A$. It is straightforward to check that $W$ has orthonormal columns and that $H$ is symmetric positive semidefinite. Thus, given the SVD, one can easily obtain the corresponding polar decomposition.</p> <p>The main idea of computing the SVD via the polar decomposition is to first compute the polar factor $W$ using an iterative method and then compute the eigenvalue decomposition of</p> \[H = W^\top A.\] <p>Since $H$ is symmetric, its eigendecomposition can be computed using well-established algorithms <d-cite key="nakatsukasa2013stable"></d-cite>. This approach was developed and further discussed by Nakatsukasa and Higham in <d-cite key="nakatsukasa2013stable,higham2015faster"></d-cite>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-polar-svd/meme.webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-polar-svd/meme.webp" class="img-fluid rounded z-depth-1 small-fig" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Although the polar decomposition is a classical and well-established construct in matrix analysis, and is used in applications such as Muon, it is far less widely known than the SVD or QR decomposition. </div> <h2 id="how-to-compute-polar-decomposition">How to Compute Polar Decomposition?</h2> <p>The practical approach to computing the polar decomposition is to obtain the polar factor using iterative methods. These iterative methods update a sequence of matrices by transforming their singular values in a controlled manner. At each step, a carefully chosen function $f$ is applied to the singular values of the current iterate, and repeated composition of $f$ is designed to drive all singular values toward $1$.</p> \[X_{k + 1} = f(X_k) = f(U \Sigma_k V^\top) = U f(\Sigma_k) V^\top, \quad X_0 = U \Sigma_0 V^\top\] <p>Typically, the function $f$ is chosen from polynomials or rational functions. It is crucial to select $f$ so that it acts on a matrix through its singular values, admitting the relation $f(U \Sigma V^\top) = U f(\Sigma) V^\top$. Once all singular values approach $1$, the iterate converges to the product of the left and right singular vectors, which yields the <a href="#svd-via-polar-decomposition">polar factor</a>:</p> \[X_k \to U V^\top = W, \quad \text{when } f(\Sigma_k) \to I.\] <p>The choice of $f$ strongly influences convergence speed: the rate at which the iterated composition $f(f(\dots f(x)))$ drives singular values toward $1$ determines how quickly the method approaches the polar factor. Below, we investigate rational functions (QDWH) and polynomial functions (Newton-Schulz iteration).</p> <h3 id="rational-functions-qdwh">Rational Functions (QDWH)</h3> <p>There are several iterative methods that are based on rational functions available, including Newton iteration <d-cite key="kenney1992scaling"></d-cite> and Halley’s <d-cite key="gander1985halley,gander1990algorithms"></d-cite> iteration:</p> \[X_{k + 1} = X_k(3I + X_k^\top X_k) (I + 3X_k^\top X_k)^{-1}, \quad X_0 = A.\] <p>However, these methods are not practical because they require matrix inversion. To address this, in 2010 Nakatsukasa et al. <d-cite key="nakatsukasa2010optimizing"></d-cite> introduced the QR-based dynamically weighted Halley iteration (QDWH), demonstrating that Halley’s iteration can be implemented without matrix inversion using QR decomposition. Existing stable methods for computing QR decomposition make QDWH preferable for practical usage. Moreover, QDWH algorithm is used in the CUDA implementation of Polar-based SVD <d-cite key="nakatsukasa2013stable"></d-cite>. Although the QDWH algorithm is highly stable in practice, modern GPU hardware is not optimized for fast QR decomposition. Instead, most optimization effort goes into basic operations such as matrix multiplications.</p> <h3 id="polynomial-functions-newton-schulz-iteration">Polynomial Functions (Newton-Schulz Iteration)</h3> <p>We next consider the Newton-Schulz iteration <d-cite key="bjorck1971iterative,kovarik1970some"></d-cite>:</p> \[X_{k + 1} = \frac{3}{2} X_k - \frac{1}{2} X_k X_k^\top X_k, \quad X_0 = A.\] <p>This iteration relies solely on matrix multiplications, which can make it faster in practice than the QDWH algorithm, even though QDWH requires fewer iterations. Newton–Schulz has become increasingly popular in applications, for example, it is used as a core component of the Muon optimizer <d-cite key="jordan2024muon"></d-cite>, where a slightly modified polynomial is employed.</p> <p>Notably, this method converges if $|| X_0 ||_2 &lt; \sqrt{3}$. Therefore, the initial matrix should be normalized before the iteration begins. Ideally, one would divide the matrix by its spectral norm so that $|| X_0 ||_2 = 1$, but computing the spectral norm is expensive. In our experiments, we use a normalization method based on the $1$-norm and $\infty$-norm from QDWH implementation in <a href="https://github.com/jax-ml/jax/blob/23db456a8acd01a04ed5a9f87f8265cc21703926/jax/_src/tpu/linalg/qdwh.py#L132" rel="external nofollow noopener" target="_blank">JAX</a>:</p> \[X_0 = A / \sqrt{\| A \|_1 \| A \|_{\infty}}, \quad \text{since } \| A \|_2 \leq \sqrt{\| A \|_1 \| A \|_{\infty}}.\] <p>The $1$-norm and $\infty$-norm are straightforward to compute, as they correspond to the maximum $\ell_1$ norm of the columns and rows of the matrix, respectively.</p> <h3 id="accelerated-newton-schulz-iteration">Accelerated Newton-Schulz Iteration</h3> <p>Recent studies by Grishina et al. <d-cite key="grishina2025accelerating"></d-cite> and Amsel et al. <d-cite key="amsel2025polar"></d-cite> introduce accelerated Newton-Schulz iteration modification based on another polynomial design. These methods named CANS and Polar-Express, respectively. They provide new optimal strategies to find coefficients for the polynomial.</p> <p>These methods differ slightly. First, CANS performs preprocessing of the matrix before starting the iteration. Second, in Polar-Express, for numerical stability, all coefficients are divided by $1.01$ except in the last iteration, which affects the accuracy.</p> <p>write about preprocessing</p> <h2 id="why-matrix-multiplications-instead-of-qr">Why Matrix Multiplications Instead of QR?</h2> <p>In the previous discussion, we argued that matrix multiplications are far more optimized on modern GPUs than QR decompositions. We now support this claim by comparing the execution time of QR decompositions and matrix multiplications (MM) on various GPUs.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-polar-svd/qr_mm_ratio.webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-polar-svd/qr_mm_ratio.webp" class="img-fluid rounded z-depth-2 only-light" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-polar-svd/qr_mm_ratio_dark.webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-polar-svd/qr_mm_ratio_dark.webp" class="img-fluid rounded z-depth-2 only-dark" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Comparison illustrating how the number of matrix multiplications that can be executed within the runtime of a single QR decomposition varies with matrix size for a square matrix. Matrix multiplications were performed in TF-32 precision in the left figure and in FP-32 precision in the right figure. </div> <p>As shown in the figure above, for medium-sized matrices, matrix multiplication is significantly faster than QR decomposition, with the gap particularly large in TF-32 precision, since most GPU libraries do not support QR in TF-32. Notably, the gap between the cost of matrix multiplication and QR widens even further on newer GPU architectures, where MM performance improves much more rapidly than QR. Therefore, whenever possible, it is preferable to perform several matrix multiplications rather than a small number of QR decompositions.</p> <h2 id="algorithm-description">Algorithm Description</h2> <p>In this section, we discuss the <a href="#algorithm">CANS-SVD algorithm</a> and the details of its implementation. The procedure begins by preprocessing the input matrix and then computing its polar decomposition using the CANS iteration from <d-cite key="grishina2025accelerating"></d-cite>. CANS preprocessing is a crucial component of the algorithm, as it significantly accelerates the convergence of the CANS iteration. Importantly, this acceleration effect persists even when as <code class="language-plaintext highlighter-rouge">ns_base_iteration</code> is used an alternative iterative method for computing the polar factor (such as Polar-Express).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">_A</span> <span class="o">=</span> <span class="nf">cans_preprocessing</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>     <span class="c1"># preprocess matrix A
</span>    <span class="n">W</span> <span class="o">=</span> <span class="nf">ns_base_iteration</span><span class="p">(</span><span class="n">_A</span><span class="p">)</span>      <span class="c1"># compute the polar factor of A
</span>    <span class="n">H</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">A</span>                    <span class="c1"># obtain H – the symmetric matrix
</span></code></pre></div></div> <p>After obtaining the polar factor, we compute the symmetric eigendecomposition of the matrix $H$ using standard algorithms available in numerical linear algebra libraries, such as those provided in JAX.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">V</span><span class="p">,</span> <span class="n">S</span> <span class="o">=</span> <span class="nf">eigh</span><span class="p">(</span><span class="n">H</span><span class="p">)</span> <span class="c1"># eigendecomposition of symmetric matrix
</span>    <span class="n">U</span> <span class="o">=</span> <span class="n">W</span> <span class="o">@</span> <span class="n">V</span>
</code></pre></div></div> <p>Matrix $A$ can then be decomposed as <code class="language-plaintext highlighter-rouge">A = U @ diag(S) @ V.T</code>. However, if $A$ is singular, both QDWH- and NS-based methods converge to a singular matrix $W$, and the resulting matrix $U$ will not be orthogonal.</p> <p>To address this issue, existing CUDA implementations employ certain engineering workarounds, adding a small perturbation to matrices, as described in the <a href="https://docs.nvidia.com/cuda/cusolver/index.html#cusolverdnxgesvdp" rel="external nofollow noopener" target="_blank">cuSOLVER documentation</a>. Since such perturbations may affect the accuracy of the singular values, we do not use them. Instead, following <d-cite key="nakatsukasa2013stable"></d-cite>, we perform a QR decomposition of the matrix of left singular vectors to restore orthogonality, which provides an effective solution for handling singular matrices:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">U</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">qr</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>  <span class="c1"># compute QR of U to fix orthogonality
</span></code></pre></div></div> <h2 id="related-work">Related Work</h2> <h3 id="cuda-qr-gesvd">CUDA QR (<code class="language-plaintext highlighter-rouge">gesvd</code>)</h3> <p>QR-based algorithms reduce a matrix to bidiagonal form via Householder reflections (or Givens rotations) and then iteratively apply implicit QR steps to compute singular values. These methods are robust and numerically stable, forming the backbone of LAPACK’s SVD routines <d-cite key="demmel1990accurate"></d-cite>.</p> <p>However, despite their robustness, QR-based SVD algorithms are poorly parallelizable: Householder (or Givens) bidiagonalization involves long dependency chains. This makes them GPU-unfriendly and significantly less efficient on modern massively parallel architectures <d-cite key="struski2024efficient"></d-cite>.</p> <h3 id="cuda-jacobi-gesvdj">CUDA Jacobi (<code class="language-plaintext highlighter-rouge">gesvdj</code>)</h3> <p>Jacobi-based SVD applies a sequence of plane rotations (Givens rotations) to eliminate the off-diagonal entries of $A^\top A$. At each step, the algorithm picks a pair of columns, computes a $2 \times 2$ rotation that makes them orthogonal, and updates the matrix. Repeating these pairwise orthogonalizations drives the matrix toward a diagonal form.</p> <p>Because many independent column pairs can be processed simultaneously, the method is highly parallel and well suited for GPUs. The parallelism of Jacobi method gives GPU better performance on small and medium size matrices than QR-based method (<a href="https://docs.nvidia.com/cuda/cusolver/index.html" rel="external nofollow noopener" target="_blank">cuSOLVER</a>).</p> <h3 id="cuda-polar-gesvdp">CUDA Polar (<code class="language-plaintext highlighter-rouge">gesvdp</code>)</h3> <h2 id="discussion">Discussion</h2> <p>The blog post shows how an SVD algorithm based on polar decomposition can be accelerated by replacing the QDWH method (which relies on QR factorizations) with the CANS iteration (which uses only matrix multiplications). However, we do not address the symmetric eigenvalue decomposition, which is also traditionally computed via a sequence of QR iterations and which we have not discussed above. It is therefore likely that this task could also be accelerated by replacing those iterations with faster matrix-multiplication–based methods. We leave this as a direction for future research.</p> <h2 id="appendix">Appendix</h2> <p>For the first part of our experiments, we consider square random matrices $A \in \mathbb{R}^{n \times n}$ with condition number $c = 10$. Formally, we generate two random matrices $G_1$ and $G_2$ with i.i.d. entries drawn from the standard Gaussian distribution. We then obtain matrices $U$ and $V$ as the Q factors from the QR decompositions of $G_1$ and $G_2$, respectively, and construct</p> \[A = U \Sigma V^\top,\] <p>where $\Sigma$ is a diagonal matrix with entries $\sigma_i = c^{(n - i) / (n - 1)}$. This procedure allows us to generate random matrices with prescribed condition numbers.</p> <p>We generate random matrices with a fixed rank $r$ using a similar scheme, except that</p> \[\sigma_i = \begin{cases} 1, &amp; i \le r, \\ 0, &amp; \text{otherwise}, \end{cases}\] <p>so that the resulting matrix has exactly rank $r$.</p> <p>For the CANS algorithm <d-cite key="grishina2025accelerating"></d-cite>, we use the following hyperparameters:</p> <table> <thead> <tr> <th> </th> <th>TF-32 version</th> <th>FP-32 version</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">degree</code></td> <td>$3$</td> <td>$3$</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">preprocess_iters</code></td> <td>$2$</td> <td>$2$</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">delta</code></td> <td>$0.99$</td> <td>$0.99$</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">max_iter</code></td> <td>$50$</td> <td>$50$</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">tolerance</code></td> <td>$10^{-3}$</td> <td>$10^{-5}$</td> </tr> </tbody> </table> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-polar-svd.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-vlms-waste-their-vision/">Why vlms waste their vision</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>