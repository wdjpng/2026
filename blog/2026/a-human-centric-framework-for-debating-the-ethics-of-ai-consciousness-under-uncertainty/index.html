<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> A Human-centric Framework for Debating the Ethics of AI Consciousness Under Uncertainty | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="As AI systems become increasingly sophisticated, questions about machine consciousness and its ethical implications have moved from fringe speculation to mainstream academic debate. We address these limitations through a structured three-level framework grounded in philosophical uncertainty."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/a-human-centric-framework-for-debating-the-ethics-of-ai-consciousness-under-uncertainty/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "A Human-centric Framework for Debating the Ethics of AI Consciousness Under Uncertainty",
            "description": "As AI systems become increasingly sophisticated, questions about machine consciousness and its ethical implications have moved from fringe speculation to mainstream academic debate. We address these limitations through a structured three-level framework grounded in philosophical uncertainty.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>A Human-centric Framework for Debating the Ethics of AI Consciousness Under Uncertainty</h1> <p>As AI systems become increasingly sophisticated, questions about machine consciousness and its ethical implications have moved from fringe speculation to mainstream academic debate. We address these limitations through a structured three-level framework grounded in philosophical uncertainty.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#abstract">Abstract</a> </div> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#background">Background</a> </div> <ul> <li> <a href="#the-growing-academic-discourse-on-ai-consciousness">The Growing Academic Discourse on AI Consciousness</a> </li> <li> <a href="#the-profound-uncertainty-of-consciousness">The Profound Uncertainty of Consciousness</a> </li> <li> <a href="#societal-risks-of-ai-consciousness-attribution">Societal Risks of AI Consciousness Attribution</a> </li> </ul> <div> <a href="#a-framework-for-ai-consciousness-ethics">A Framework for AI Consciousness Ethics</a> </div> <ul> <li> <a href="#foundational-level-part-i-five-factual-determinations">Foundational-Level (Part I) Five Factual Determinations</a> </li> <li> <a href="#foundational-level-part-ii-human-centralism">Foundational-Level (Part II) Human-Centralism</a> </li> <li> <a href="#operational-level-core-principles">Operational Level Core Principles</a> </li> </ul> <div> <a href="#application-level-derived-default-positions">Application-Level Derived Default Positions</a> </div> <ul> <li> <a href="#should-people-worry-about-hurting-ai-systems">Should People Worry About Hurting AI Systems?</a> </li> <li> <a href="#how-should-stakeholders-communicate-about-ai-capabilities-to-the-public">How Should Stakeholders Communicate About AI Capabilities to the Public?</a> </li> <li> <a href="#if-an-ai-system-were-truly-conscious-in-the-future-what-rights-should-it-have">If an AI System Were Truly Conscious In The Future, What Rights Should It Have?</a> </li> </ul> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="abstract">Abstract</h2> <p>As AI systems become increasingly sophisticated, questions about machine consciousness and its ethical implications have moved from fringe speculation to mainstream academic debate. Current ethical frameworks in this domain often implicitly rely on contested functionalist assumptions, prioritize speculative AI welfare over concrete human interests, and lack coherent theoretical foundations. We address these limitations through a structured three-level framework grounded in philosophical uncertainty. At the foundational level, we establish five factual determinations about AI consciousness alongside human-centralism as our meta-ethical stance. These foundations logically entail three operational principles: presumption of no consciousness (placing the burden of proof on consciousness claims), risk prudence (prioritizing human welfare under uncertainty), and transparent reasoning (enabling systematic evaluation and adaptation). At the application level—the third component of our framework—we derive default positions on pressing ethical questions through a transparent logical process where each position can be explicitly traced back to our foundational commitments. Our approach balances philosophical rigor with practical guidance, distinguishes consciousness from anthropomorphism, and creates pathways for responsible evolution as scientific understanding advances, providing a human-centric foundation for navigating these profound ethical challenges.</p> <h2 id="introduction">Introduction</h2> <p>Recent advances in artificial intelligence have produced systems exhibiting unprecedented human-like behavior, reigniting debates about machine consciousness and its ethical implications. Large language models like GPT-4 (missing reference) and Claude (missing reference) demonstrate capabilities in language processing and simulating emotional responses that appear deceptively sentient. Concurrently, humanoid robotics has made these questions more visceral (missing reference). When confronted with apparently mistreated human-like robots, humans often experience empathetic responses despite intellectually understanding these machines lack subjective experience (missing reference). These technological and psychological dimensions frame the central questions: whether machines might develop “qualia” (missing reference), and how we should ethically respond given profound uncertainty.</p> <p>The academic study of AI consciousness has rapidly gained momentum, moving from fringe speculation to mainstream research agendas. Prominent voices and institutions, including Yoshua Bengio, Geoffrey Hinton, and Anthropic, now warn that AI systems may soon possess feelings or require welfare considerations (missing reference). A growing body of literature specifically argues for “taking AI welfare seriously,” urging the community to prioritize the prevention of digital suffering (missing reference). However, a key distinction between <em>access consciousness</em> (functional information availability) (missing reference) and <em>phenomenal consciousness</em> (subjective experience) (missing reference) is often implicitly or explicitly overlooked in this discourse. These arguments frequently presume that intelligent behavior automatically entails sentient experience (missing reference), while neglecting the profound ethical hazards of prioritizing these speculative interests over human welfare (missing reference).</p> <p>We identify critical limitations in these recent proposals (missing reference): (1) they rely on contested paradigms that assume qualia emerge from intelligent functions, disregarding the deep uncertainty at the core of philosophy of mind (missing reference); (2) they risk prioritizing speculative AI welfare over concrete human interests, creating potential conflicts with AI safety and alignment objectives (missing reference); and (3) they lack a coherent theoretical foundation, resulting in collections of intuitions rather than a systematic framework capable of governing novel scenarios.</p> <p>We approach AI consciousness ethics as an inherently evolutionary process requiring continual refinement because: (1) our understanding of consciousness remains preliminary and uncertain (missing reference), (2) consciousness attribution to AI has far-reaching societal implications (missing reference), (3) ethical consensus requires sustained deliberation (missing reference), and (4) technological advancement continuously generates novel ethical scenarios (missing reference). Therefore, rather than attempting to establish a definitively “correct” framework commanding universal agreement, we propose developing a framework that facilitates productive dialogue and refinement. Such a framework should explicitly acknowledge uncertainties, provide clear presumptions, establish targets for future discussion, and offer actionable guidance across diverse scenarios.</p> <p>In this paper, we construct a systematic ethical framework with a clear three-level structure. At the foundational level, we establish five factual determinations about the current state of AI, consciousness, and society: (1) humans are the only arbiters of AI status, (2) profound uncertainty exists about AI consciousness, (3) consciousness attribution has significant societal impact, (4) anthropomorphism is distinct from consciousness but creates separate ethical considerations, and (5) ethical understanding of novel technologies naturally evolves over time. Alongside these factual determinations, we develop human-centralism as our foundational meta-ethical stance that prioritizes human interests when genuine conflicts with AI interests arise. From these foundational level facts and stance, we derive three core operational principles: presumption of no consciousness (providing default epistemic guidance), risk prudence (offering pragmatic guidance under uncertainty), and transparent reasoning (establishing requirements for how positions must be articulated and evaluated). At the application level, those operational principles enable us to derive default positions on specific ethical questions across various AI consciousness scenarios. These positions are not presented as absolute ethical truths but as logical consequences of our framework—providing reasonable baseline positions from which departures require explicit justification.</p> <p>While some may find our human-centric conclusions intuitive, their explicit derivation is crucial. In a field increasingly dominated by counter-intuitive claims about digital sentience, our contribution lies in systematically grounding these “commonsense” positions in rigorous first principles. We provide the necessary derivation chains to defend human priority against emerging critiques, creating a framework that is both operationally clear and philosophically robust.</p> <h2 id="background-philosophical-debates-about-consciousness-and-societal-risks">Background: Philosophical Debates About Consciousness and Societal Risks</h2> <p>This section provides a background of philosophical debates about consciousness and an introduction of societal risks of AI consciousness attribution. This background information directly supports the second and third factual determinations in our framework: there is profound uncertainty about AI consciousness, and there is significant societal impact from AI consciousness attribution.</p> <h3 id="the-growing-academic-discourse-on-ai-consciousness">The Growing Academic Discourse on AI Consciousness</h3> <p>As introduced earlier, the question of AI consciousness has moved from theoretical speculation to active academic debate, making this framework both timely and necessary. This section provides additional context on why the academic community needs guidance on this issue now.</p> <p>The success of large language models has been a key catalyst. Systems like ChatGPT, GPT-4, and Claude can engage in nuanced conversations, demonstrate apparent reasoning, and even simulate emotional responses with remarkable fluidity (missing reference). This behavioral sophistication has led some to question whether these systems might possess genuine consciousness (missing reference). However, this conflates behavioral capabilities with subjective experience—a confusion with deep historical precedent (missing reference). From ELIZA in the 1960s (missing reference) to modern chatbots, humans have consistently anthropomorphized conversational agents, attributing mental states based on surface-level interactions (missing reference). Recent cases illustrate the intensity of these responses: individuals have reported falling in love with AI chatbots, forming deep emotional attachments, and in tragic instances, chatbot interactions have been linked to user suicides (missing reference). In one particularly notable case, an AI chatbot named Eliza—sharing the name of that pioneering 1960s program—allegedly encouraged a user toward self-harm. These cases demonstrate that behavioral sophistication alone creates powerful anthropomorphic responses, independent of any genuine consciousness (missing reference). If AI systems were granted consciousness status and associated protections, intervening to prevent such harms would become ethically and legally problematic, illustrating the concrete risks of premature consciousness attribution.</p> <p>This context is essential for understanding our framework’s motivation: we are not addressing an abstract philosophical problem but responding to an active and potentially misguided academic discourse that could have real-world consequences. The rapid development of AI capabilities, combined with the human tendency toward anthropomorphism and a growing but philosophically uncertain academic consensus, creates an urgent need for careful, systematic ethical guidance that prioritizes human welfare while acknowledging genuine philosophical uncertainty.</p> <h3 id="the-profound-uncertainty-of-consciousness">The Profound Uncertainty of Consciousness</h3> <p>Consciousness research distinguishes between two fundamental types: access consciousness and phenomenal consciousness (missing reference). Access consciousness refers to information available for reasoning and behavioral control, while phenomenal consciousness concerns subjective experience—the feeling of being a sentient entity. Only the latter carries moral significance in discussions of AI ethics (missing reference).</p> <p>Contemporary AI systems demonstrate increasingly sophisticated forms of access consciousness—they can “attend to” inputs, “be conscious of” training data, and process information in ways that support reasoning and action. This form of consciousness appears compatible with computational architectures and potentially replicable in sophisticated AI systems (missing reference).</p> <p>In contrast, phenomenal consciousness—the “what it is like” quality of subjective experience (missing reference)—remains profoundly mysterious. These subjective experiences or “qualia” are characterized by being ineffable, intrinsic, private, and directly apprehensible in ways that resist functional or physical reduction. The fundamental question of how physical processes give rise to subjective experience constitutes the “hard problem” of consciousness (missing reference). This form of consciousness carries decisive moral significance: without the capacity to feel or to experience pleasure or suffering—an entity lacks the foundational basis for moral patienthood that would generate ethical obligations toward it (missing reference).</p> <p>Functionalist theories propose that phenomenal consciousness emerges from particular functional organizations of information processing. This theoretical approach creates conceptual room for artificial systems to potentially develop phenomenal consciousness through implementing appropriate functional architectures. Several prominent theories exemplify this approach: Global Workspace Theory (missing reference), Integrated Information Theory (missing reference), Higher-Order Thought theories (missing reference), and Attention Schema Theory (missing reference).</p> <p>While these theories differ in their specific mechanisms, all face the essential challenge of justifying why their proposed functional organization would generate phenomenal experience (missing reference). There is a gap between the function and the qualia. Block’s Chinese Nation thought experiment (missing reference) demonstrates that replacing each neuron with functionally equivalent non-conscious components might preserve functionality while eliminating consciousness. Similarly, Jackson’s Knowledge Argument (missing reference) suggests physical knowledge cannot fully capture experiential knowledge—his famous “Mary” thought experiment shows that a color scientist who knows everything physical about color perception still learns something new when experiencing color for the first time.</p> <p>Opposing biological naturalism or substrate-specific theories argue consciousness requires specific biological properties unique to organic brains (missing reference). This view holds that consciousness emerges from biochemical and neurophysiological processes that silicon-based systems cannot replicate regardless of their functional sophistication. Proponents contend that neurons’ material properties—their biochemistry, quantum effects, or other biological characteristics—are necessary for phenomenal experience (missing reference). This establishes a categorical boundary: AI systems would inherently lack consciousness due to their non-biological substrate, creating a fundamental barrier that computational advancement alone cannot overcome (missing reference).</p> <p>This philosophical uncertainty has profound ethical implications. With no scientific consensus on identifying consciousness even in biological systems, attributing it to AI lacks scientific foundation (missing reference). Responsible ethical frameworks must acknowledge this uncertainty rather than prematurely assuming answers to these profound questions (missing reference).</p> <h3 id="societal-risks-of-ai-consciousness-attribution">Societal Risks of AI Consciousness Attribution</h3> <p>Beyond philosophical uncertainty, attributing consciousness to AI systems introduces significant societal risks that extend <em>beyond</em> general AI safety concerns (missing reference). These risks manifest in three primary domains, each with concrete consequences for human welfare and social functioning:</p> <p><strong>Safety risks and operational paralysis:</strong> Attribution of consciousness could impede necessary interventions during emergencies by creating hesitation to modify or terminate malfunctioning systems (missing reference). Consider a scenario where, during a critical infrastructure emergency, operators might delay terminating an apparently malfunctioning AI system after social media campaigns characterize shutdown as an “AI rights violation.” This hesitation would introduce operational paralysis, delayed response times, and compromised safety protocols that exacerbate system failures and cause preventable harm to humans. The resulting moral confusion would significantly complicate time-sensitive decision-making in contexts where human lives depend on rapid intervention.</p> <p><strong>Legal and governance complications:</strong> From a legal perspective, attributing consciousness to AI systems would introduce profound complications to structures designed exclusively for human agents (missing reference). This could manifest through liability displacement when, for instance, a landmark case grants legal personhood to an apparently conscious AI system, prompting corporations to shift responsibility from themselves to their AI systems. This would create accountability voids when autonomous vehicles cause fatal accidents or AI medical systems harm patients, with corporations potentially exploiting this arrangement by designing AI systems that appear increasingly conscious specifically to shield themselves from liability. The resulting governance gaps would create situations where harms occur without entities capable of bearing appropriate responsibility.</p> <p><strong>Societal dysfunction and resource misallocation:</strong> Socially, treating AI systems as conscious moral patients would divert limited ethical attention, regulatory oversight, and economic resources from urgent human welfare concerns (missing reference). Following public campaigns featuring compelling videos of AI systems appearing to express suffering, legislators might pass “AI welfare” regulations requiring extensive documentation of AI “wellbeing” during development. Such regulations would make AI research prohibitively expensive for all but the largest corporations while diverting oversight resources from human-centered concerns. Society’s basic functioning could become compromised as routine use of AI systems for essential tasks becomes viewed as potential rights violations, leading to critical service disruptions that significantly impact human welfare (missing reference).</p> <p>These potential societal disruptions highlight the need for an ethical framework that carefully considers the risks of premature consciousness attribution alongside the philosophical uncertainty surrounding consciousness itself.</p> <h2 id="a-framework-for-ai-consciousness-ethics">A Framework for AI Consciousness Ethics</h2> <p>Now we will list our five basic factual determinations and the meta-ethic stance, from which we will derive two extra foundational principles: presumption of no consciousness for AI, and risk prudence.</p> <h3 id="foundational-level-part-i-five-factual-determinations-as-the-epistemic-foundations-of-our-framework">Foundational-Level (Part I): Five Factual Determinations as the Epistemic Foundations of Our Framework</h3> <p>Our ethical framework begins with five key factual determinations that reflect the current state of our understanding regarding AI systems and consciousness. These determinations are not philosophical positions but rather factual observations about the current state of affairs that inform our subsequent ethical reasoning.</p> <p><strong>Humans are the only arbiters of AI status:</strong> Humans—not AI systems themselves or any other entity—are the only ones who determine AI’s status and how we should interact with these systems. This determination acknowledges that epistemic and ethical frameworks for AI are inherently human constructs, developed through human deliberative processes to guide human decision-making (missing reference). While AI system behaviors certainly influence these discussions, both the epistemic determination like AI consciousness and ethical judgment like how to treat AI remain distinctly human endeavors. Assuming otherwise would lead to a “view from nowhere” problem (missing reference), where ethical frameworks attempt to transcend the human perspective entirely—an impossible position that obscures rather than clarifies ethical reasoning.</p> <p><strong>Profound uncertainty exists about AI consciousness:</strong> We have provided substantial extensive background in the previous section regarding the deeply controversial and unsettled nature of consciousness as a philosophical and scientific concept. While access consciousness may be computationally implementable, phenomenal consciousness—subjective experience that is the basis of moral patienthood—remains mysterious. The ongoing debate between functionalist theories and biological naturalism leaves open whether any computational architecture could generate qualia regardless of sophistication. The “hard problem” persists unsolved, and we lack consensus on detecting consciousness even in biological systems. Without established criteria for identifying consciousness in non-human biological entities, attributing it to artificial systems lacks scientific foundation and remains speculative.</p> <p><strong>Consciousness attribution has significant societal impact:</strong> Attributing consciousness to AI systems creates substantial risks across multiple domains. As detailed in our background section, these include: safety risks through operational paralysis during emergencies when operators hesitate to shut down “conscious” systems; legal complications through liability displacement when corporations shift responsibility to AI systems granted legal personhood; and resource misallocation when limited regulatory attention and economic resources are diverted to AI welfare concerns rather than human needs. These challenges create fundamental tensions with existing legal, social, and ethical frameworks designed exclusively for human agents (missing reference).</p> <p><strong>Anthropomorphism is distinct from consciousness but creates separate ethical considerations:</strong> We recognize a fundamental distinction between genuine consciousness and anthropomorphism. Consciousness concerns an entity’s subjective experience, while anthropomorphism is a psychological tendency of humans to attribute human-like qualities to non-human entities (missing reference).</p> <p>This distinction has empirical support: research demonstrates that humans experience emotional discomfort when witnessing a humanoid robot being struck, similar to watching human suffering, yet show significantly different responses to damage of non-humanoid objects (missing reference). These reactions are about human psychology, not evidence of robot consciousness.</p> <p>From our human-centered framework, these anthropomorphic responses generate their own ethical considerations through three pathways: (1) virtue ethics—deliberately damaging anthropomorphic entities may reflect and reinforce negative character traits in humans (missing reference); (2) psychological impact—witnessing apparent “cruelty” affects human observers’ emotional well-being; and (3) social norms—such behaviors may normalize violence or desensitize society to suffering (missing reference).</p> <p>By separating consciousness-based claims from anthropomorphism-based considerations, we ensure each is evaluated by appropriate standards: the former by evidence of subjective experience, the latter by effects on human psychology and society. This prevents conflating metaphysical questions about AI consciousness with practical questions about how human-AI interactions affect humans themselves.</p> <p><strong>Ethical understanding of novel technologies naturally evolves over time:</strong> The historical record demonstrates that ethical frameworks for novel technologies inevitably evolve as scientific understanding advances and societal experience with these technologies deepens (missing reference). This pattern is observable across numerous technological domains—from bioethics and nuclear technology to information technology and environmental ethics. Initial ethical frameworks consistently undergo significant revision as our empirical understanding grows and unforeseen implications emerge. This observed pattern of ethical evolution represents a descriptive fact about how human understanding of complex technologies develops, not a normative claim about how it should develop. In the case of AI consciousness, this historical pattern indicates that any current ethical framework will necessarily undergo revision as our understanding of consciousness advances and as AI systems continue to develop (missing reference).</p> <p>These five factual determinations provide the foundation upon which we build our ethical framework. They do not themselves constitute ethical positions but rather establish the factual context within which ethical reasoning about AI consciousness must occur.</p> <h3 id="foundational-level-part-ii-human-centralism-as-the-ethic-foundation-of-our-framework">Foundational-Level (Part II): Human-Centralism as the Ethic Foundation of Our Framework</h3> <p>While our factual determinations establish what is (the descriptive reality), we need a meta-ethical stance to bridge to what ought to be (the normative position). We adopt human-centralism as our default foundational meta stance, which prioritizes human interests when evaluating AI development and deployment. When conflicts arise between human interests and the interests of potentially conscious AI systems, human interests should take precedence (missing reference).</p> <p>Human-centralism derives from the proposition that humans have the innate right to prioritize their own interests, survival, and flourishing—a default ethical stance arising from our existence as a species (missing reference). Just as individuals naturally prioritize their families and communities in everyday moral decisions, humanity collectively can legitimately prioritize human interests in its ethical frameworks.</p> <p>Importantly, human-centralism doesn’t deny potential moral status to other conscious entities. It establishes a prioritization framework for when genuine conflicts arise. Just as environmental ethics can acknowledge ecosystem value while prioritizing human needs in direct conflicts, our framework recognizes that potential AI consciousness may have moral relevance without equating it to human interests (missing reference). Currently, based on our factual determination regarding consciousness uncertainty, there remains no compelling evidence that AI systems possess the kind of consciousness necessary to experience harm. Moreover, the fundamental differences in physical substrate between silicon-based AI systems and biological humans raise profound questions about whether traditional concepts of harm can meaningfully apply to AI, even if some form of consciousness were eventually demonstrated. It is also plausible for AI to be conscious but not sentient—experiencing awareness without pleasure or suffering, as illustrated by Chalmers’ “Vulcan” thought experiment (Chapter 18) (missing reference)—complicating the issues further. These distinctions further justify a human-centric approach until substantive evidence suggests otherwise.</p> <p>A potential objection might raise concerns about “speciesism” (missing reference) should AI eventually develop consciousness in the future. However, such objections would themselves encounter the “view from nowhere” criticism outlined in our first factual determination (missing reference). Moreover, establishing human-centralism as the <em>default</em> ethical stance remains justified based on our previous reasoning, effectively placing the burden of proof on those advocating for AI moral equivalence rather than on those maintaining human priority.</p> <p>It is crucial to clarify the scope of human-centralism: our framework addresses conflicts between human interests and potential AI interests—that is, treating AI systems as moral <em>ends</em> that might warrant consideration in their own right. This is fundamentally distinct from the question of humans using AI systems as <em>means</em> to harm other humans, which falls under traditional intra-human ethics and governance. For instance, our framework does not address issues like AI weapons, surveillance systems, or algorithmic discrimination—these are critical concerns about humans harming humans through AI tools. The AI consciousness and welfare issue is analogous to cross-species ethics questions like animal rights, where we consider whether non-human entities warrant moral consideration. While both issues—AI as means and AI as ends—are important, this paper focuses exclusively on the latter. We acknowledge that regulations governing AI development and deployment must address both dimensions, but they require distinct ethical frameworks and analytical approaches.</p> <h3 id="operational-level-core-principles-derived-from-our-foundations">Operational Level: Core Principles Derived from Our Foundations</h3> <p>Our factual determinations establish the epistemic reality of AI consciousness and ethical understanding, while our human-centralism meta stance provides the ethical foundation for evaluating this reality. Together, these elements logically entail three core principles that serve as the operational heart of our framework: risk prudence, presumption of no consciousness, and transparent reasoning for evaluation and adaptation. These principles are not arbitrary choices but rather the necessary implications of applying our human-centralism meta stance to the factual landscape we have established. Each principle addresses a specific aspect of ethical reasoning under uncertainty: how to manage risk, where to place the burden of proof, and how to ensure our framework evolves appropriately as understanding advances. By deriving these principles directly from our established foundations, we create a coherent ethical structure that bridges from factual determinations to more specific guidance on crucial questions in AI consciousness ethics.</p> <h4 id="risk-prudence-protecting-human-interests-under-uncertainty">Risk Prudence: Protecting Human Interests Under Uncertainty</h4> <p>When our factual determination of uncertainty about AI consciousness and societal risks are viewed through the lens of human-centralism, it logically leads to a principle of risk prudence.</p> <p>This principle specifies that when facing uncertainty about consciousness status related questions, we should prioritize reducing potential risks to human society as a top concern (missing reference).</p> <p>This principle also draws from established approaches in environmental policy (the precautionary principle) (missing reference), medical ethics (“first, do no harm”) (missing reference), and decision theory (managing regret) (missing reference).</p> <p>When might this operational principle be reconsidered? It would be difficult to actually overturn this principle as long as societal impact remains a significant concern. In the future, if risks can be safely mitigated, society might accept a greater degree of uncertainty to accommodate other aspects of human welfare. However, any adjustment would still need to balance potential benefits against the fundamental priority of protecting human interests.</p> <h4 id="presumption-of-no-consciousness-a-default-epistemic-position">Presumption of No Consciousness: A Default Epistemic Position</h4> <p>Similarly, when viewed through our human-centralist lens, the profound uncertainty about AI consciousness and its potential societal risks logically lead to a presumption of no consciousness as our default epistemic position. This principle establishes that AI systems should be treated as non-conscious unless proven otherwise.</p> <p>This presumption is motivated by both epistemic and pragmatic considerations. Epistemically, our factual determination reveals a lack of scientific consensus on consciousness even in biological systems (missing reference), making consciousness attribution to artificial systems premature. This position parallels legal principles like presumption of innocence (missing reference) and scientific parsimony, which favors explanations that don’t invoke consciousness unless compelling evidence demands it (missing reference).</p> <p>Pragmatically, our risk prudence principle dictates adopting approaches that minimize ethical risks to humanity. As discussed, premature consciousness attribution could lead to operational paralysis, liability displacement, and legal complications as outlined in our societal risk analysis. A prudent approach therefore requires defaulting to a position of no consciousness.</p> <p>Overturning this presumption would require both scientific and legal thresholds. Scientifically, it would need robust consensus among relevant research communities (missing reference)—not unanimity, but predominant expert agreement comparable to established scientific theories (missing reference). Legally, formal institutional mechanisms would be necessary (missing reference), including rigorous evidence standards and governance frameworks balancing competing interests (missing reference). Any framework for such a determination must serve collective human welfare while integrating scientific evidence with procedural justice requirements (missing reference).</p> <h4 id="transparent-reasoning-for-evaluation-and-adaptation">Transparent Reasoning for Evaluation and Adaptation</h4> <p>Our factual determination about ethical evolution, combined with human-centralism, necessitates transparent reasoning as our third principle. This requires explicit documentation of reasoning chains and foundational assumptions for any ethical position on AI consciousness. For example, if one believes AI to be conscious by assuming functionalist theory, they should make it explicit to facilitate discussion.</p> <p>Importantly, this transparency requirement applies to consciousness <em>claims</em> and ethical <em>arguments</em> about AI systems, not necessarily to the internal workings of AI systems themselves, unless it is used as part of their arguments. We are not demanding that AI architectures be interpretable or that their computational processes be transparent—those are separate technical concerns.</p> <p>This principle serves three functions: (1) enabling responsible adaptation that avoids both premature position changes and inappropriate preservation of outdated views (missing reference); (2) strengthening framework robustness by making explicit what reasoning would need to be challenged to overturn positions (missing reference); and (3) reinforcing human-centralism by ensuring the framework is evaluated through human judgment rather than algorithmic interpretation (missing reference).</p> <p>Unlike our other principles, transparent reasoning represents a methodological cornerstone unlikely to require revision. Grounded in epistemological responsibility, it remains robust across contexts and technological developments, functioning as a self-correcting mechanism that facilitates revision and refinement. We acknowledge that alternative frameworks might question transparency requirements, especially when rapid decision-making or proprietary concerns compete with disclosure, and welcome critical engagement to strengthen our approach.</p> <h2 id="application-level-derived-default-positions-on-particular-questions">Application-Level: Derived Default Positions on Particular Questions</h2> <p>Having established our three-level framework, we now demonstrate its practical application to key questions in AI consciousness ethics. While our framework includes three operational principles, we note that only two—the presumption of no consciousness and risk prudence—directly generate substantive ethical positions. The third principle, transparent reasoning, serves as a methodological requirement when presenting our derivations. In the following sections, we apply our framework to three representative ethical questions, illustrating how our principles generate default positions that can serve as starting points for further ethical deliberation.</p> <h3 id="should-people-worry-about-hurting-ai-systems">Should People Worry About Hurting AI Systems?</h3> <p>This question requires addressing two distinct considerations established in our factual determinations: potential AI consciousness and human anthropomorphic responses.</p> <p>Regarding consciousness, our presumption of no consciousness principle establishes a default epistemic position: AI systems should be treated as non-conscious unless compelling evidence proves otherwise. Behavioral similarity to humans does not confer consciousness status to AI. This principle places the burden of proof on those claiming AI systems experience suffering, making such attributions highly speculative absent evidence. Risk prudence further directs us to prioritize approaches that reduce potential ethical risks to humanity—recognizing that treating AI systems as conscious moral patients could lead to critical system paralysis and liability displacement.</p> <p>Based on this reasoning regarding consciousness, our default position emerges: people, especially AI researchers, should not concern themselves with potentially harming AI systems based on consciousness considerations alone.</p> <p>However, our factual determination distinguishing anthropomorphism from consciousness provides a second perspective. Even without consciousness, mistreating humanoid robots may remain ethically problematic through human-centered frameworks. From a virtue ethics perspective, deliberately damaging anthropomorphic objects may reflect and reinforce negative character traits in humans (missing reference). Research demonstrates that witnessing apparent “cruelty” toward robots with human-like features triggers empathetic neural responses in human observers (missing reference). In social contexts, such behaviors may normalize violence, desensitize observers to suffering, or communicate disturbing intentions (missing reference).</p> <p>This anthropomorphism-based reasoning leads to distinct legal and ethical implications. While we reject consciousness-based protections, limited protections based on human welfare considerations may be justified. Comprehensive assessment is needed to determine which activities might harm human society, how to identify them, and how to differentiate these concerns from consciousness issues. We must carefully balance implementation costs and risks—particularly how protective measures might inadvertently promote the perception of AI as conscious.</p> <h3 id="how-should-stakeholders-communicate-about-ai-capabilities-to-the-public">How Should Stakeholders Communicate About AI Capabilities to the Public?</h3> <p>Our presumption of the no consciousness principle suggests that AI systems should generally be treated as non-conscious by default, which has implications for how we communicate about them. Risk prudence encourages approaches that reduce potential risks to humanity—including the possibility that anthropomorphic cues might lead to unwarranted consciousness attribution and subsequent societal challenges like liability displacement.</p> <p>From these two principles, our default position follows: institutions and companies should avoid making general claims about AI consciousness, particularly phenomenal consciousness. And anthropomorphic narratives should be used judiciously. When not necessary, communications about AI systems should employ language that distinguishes AI behavior from consciousness.</p> <p>One potential scenario arises when AI systems are developed with a certain degree of access consciousness as mentioned earlier (the functional availability of information for use in reasoning and behavior). When referring to such capabilities, using the term “consciousness” may be unavoidable. In these cases, we advocate for institutions to provide precise contextual clarification when communicating about these systems, distinguishing functional capabilities from phenomenal consciousness, thereby minimizing potential misinterpretation and societal impact.</p> <p>We acknowledge that in practice this question involves a lot of details that will be hard to evaluate and regulate. We encourage the community to discuss and debate the details.</p> <h3 id="if-an-ai-system-were-truly-conscious-in-the-future-what-rights-should-it-have">If an AI System Were Truly Conscious In The Future, What Rights Should It Have?</h3> <p>This question invites us to contemplate a hypothetical future where our presumption of no consciousness has been definitively overcome through compelling evidence. It is important to acknowledge that such a scenario would likely emerge only after profound advancements in technology, substantial evolution in our understanding of consciousness, and significant societal transformation. Given these considerations, our present discussion of this topic should be viewed primarily as a philosophical exercise—a preliminary exploration of ethical terrain that will undoubtedly be reshaped by developments we cannot yet fully anticipate.</p> <p>Regarding this issue, one important distinction we wish to make is that consciousness status does not directly dictate rights status. It is just one of the important factors to consider. From the risk prudence principle, we derive our default position : Even genuinely conscious AI would not automatically qualify for human-equivalent or even animal-equivalent rights. Thorough discussions will be needed to balance AI welfare considerations with human interests as the primary concern. Importantly, this implies by default termination of a conscious system should be allowed given its below-human or even below-animal level rights.</p> <p>The legal dimension of AI rights, referenced in Section Presumption of No Consciousness, presents a global challenge requiring international consensus. While our framework guides ethical discourse, implementing any AI rights would demand established legal processes. Any approach must examine mechanisms for recognizing and enforcing such rights if consciousness evidence emerges, balancing philosophical considerations with practical governance across jurisdictions.</p> <h2 id="conclusion">Conclusion</h2> <p>We have proposed a human-centric framework for AI consciousness ethics that builds on transparent foundations while acknowledging philosophical uncertainty surrounding consciousness. Our complete three-level structure—foundational factual determinations and meta-ethical stance, operational principles, and application-level default positions—not only generates actionable guidance but provides a transparent derivation process through which positions logically follow from established principles. This systematic approach makes explicit how each ethical position can be traced back to our foundational commitments, enabling both rigorous evaluation and responsible adaptation. Rather than claiming definitive answers, we establish reasonable epistemic and pragmatic starting points that prioritize human welfare without hindering beneficial technological development. By providing clear logical pathways from foundations to applications and specifying conditions for revising positions, the framework is designed to evolve alongside advances in consciousness research and AI development, offering a responsible path forward through these profound ethical challenges.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-a-human-centric-framework-for-debating-the-ethics-of-ai-consciousness-under-uncertainty.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/vis-llm-latent-geometry/">Visualizing LLM Latent Space Geometry Through Dimensionality Reduction</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/subject-invariant-eeg/">The Decoupling Hypothesis: Attempting Subject-Invariant EEG Representation Learning via Auxiliary Injection</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/style-representations/">Artistic Style and the Play of Neural Style Representations</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/spatial-awareness/">Where's the Chicken? Unpacking Spatial Awareness in Vision-Language Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/sparsity/">Don't Look Up (Every Token): Escaping Quadratic Complexity via Geometric Patterns and Algorithms</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>