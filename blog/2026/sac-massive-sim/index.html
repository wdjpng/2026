<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Getting SAC to Work on a Massive Parallel Simulator: An RL Journey With Off-Policy Algorithms | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="This post details how to get the Soft-Actor Critic (SAC) and other off-policy reinforcement learning algorithms to work on massively parallel simulators (e.g., Isaac Sim with thousands of robots simulated in parallel). In addition to tuning SAC for speed, the post also explores why SAC fails where PPO succeeds, highlighting a common problem in task design that many codebases share."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/sac-massive-sim/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Getting SAC to Work on a Massive Parallel Simulator: An RL Journey With Off-Policy Algorithms",
            "description": "This post details how to get the Soft-Actor Critic (SAC) and other off-policy reinforcement learning algorithms to work on massively parallel simulators (e.g., Isaac Sim with thousands of robots simulated in parallel). In addition to tuning SAC for speed, the post also explores why SAC fails where PPO succeeds, highlighting a common problem in task design that many codebases share.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Getting SAC to Work on a Massive Parallel Simulator: An RL Journey With Off-Policy Algorithms</h1> <p>This post details how to get the Soft-Actor Critic (SAC) and other off-policy reinforcement learning algorithms to work on massively parallel simulators (e.g., Isaac Sim with thousands of robots simulated in parallel). In addition to tuning SAC for speed, the post also explores why SAC fails where PPO succeeds, highlighting a common problem in task design that many codebases share.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#a-suspicious-trend-ppo-ppo-ppo">A Suspicious Trend: PPO, PPO, PPO, ...</a> </div> <div> <a href="#why-it-matters-fine-tuning-on-real-robots">Why It Matters? - Fine Tuning on Real Robots</a> </div> <div> <a href="#the-path-of-least-resistance-hypothesis">(The Path of Least Resistance) Hypothesis</a> </div> <div> <a href="#the-hunt-begins">The Hunt Begins</a> </div> <div> <a href="#ppo-gaussian-distribution">PPO Gaussian Distribution</a> </div> <div> <a href="#sac-squashed-gaussian">SAC Squashed Gaussian</a> </div> <div> <a href="#quick-fix">Quick Fix</a> </div> <div> <a href="#transition-what-does-that-mean-for-the-rl-community">Transition: What Does That Mean for the RL Community?</a> </div> <div> <a href="#tuning-for-speed-part-ii">Tuning for Speed (Part II)</a> </div> <div> <a href="#defining-proper-action-bound-extracting-the-limits-with-ppo">Defining Proper Action Bound - Extracting the Limits with PPO</a> </div> <div> <a href="#need-for-speed-or-how-i-learned-to-stop-worrying-about-sample-efficiency">Need for Speed or: How I Learned to Stop Worrying About Sample Efficiency</a> </div> <div> <a href="#does-it-work-more-environments">Does it work? - More Environments</a> </div> <div> <a href="#solving-harder-environments">Solving Harder Environments</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <p>Spoiler alert: <a href="#appendix-affected-paperscode">quite a few papers/code</a> are affected by the problem described below.</p> <p>This post is divided into two main parts. The first part analyzes why SAC does not work out of the box in Isaac Sim environments (until the <a href="#quick-fix">quick fix section</a>). The <a href="#tuning-for-speed-part-ii">second part</a> discusses how to tune SAC for speed and make it perform as good as PPO.</p> <h2 id="a-suspicious-trend-ppo-ppo-ppo-">A Suspicious Trend: PPO, PPO, PPO, …</h2> <p>The story begins a few months ago when I saw another paper using the same recipe for learning locomotion: train a PPO<d-cite key="schulman2017proximal"></d-cite> agent in simulation using thousands of environments in parallel and domain randomization, then deploy it on the real robot. This recipe has become the standard since 2021, when ETH Zurich and NVIDIA showed that it was possible to learn locomotion in minutes on a single workstation<d-cite key="rudin2022learning"></d-cite>. The codebase and the simulator (called Isaac Gym<d-cite key="makoviychuk2021isaac"></d-cite> at that time) that were published became the basis for much follow-up work<d-footnote>Like the <a href="https://www.youtube.com/watch?v=7_LW7u-nk6Q" rel="external nofollow noopener" target="_blank">BD-1 Disney robot</a></d-footnote>.</p> <p>As an RL researcher interested in learning directly on real robots, I was curious and suspicious about one aspect of this trend: why is no one trying an algorithm other than PPO<d-footnote>I was not the only one asking why SAC doesn't work: <a href="https://forums.developer.nvidia.com/t/poor-performance-of-soft-actor-critic-sac-in-omniverseisaacgym/266970" rel="external nofollow noopener" target="_blank">nvidia forum</a> <a href="https://www.reddit.com/r/reinforcementlearning/comments/lcx0cm/scaling_up_sac_with_parallel_environments/" rel="external nofollow noopener" target="_blank">reddit1</a> <a href="https://www.reddit.com/r/reinforcementlearning/comments/12h1faq/isaac_gym_with_offpolicy_algorithms" rel="external nofollow noopener" target="_blank">reddit2</a></d-footnote>? PPO benefits from fast and parallel environments<d-cite key="berner2019dota"></d-cite>, but PPO is not the only deep reinforcement learning (DRL) algorithm for continuous control tasks, and there are alternatives like SAC<d-cite key="haarnoja2018soft"></d-cite> or TQC<d-cite key="kuznetsov2020tqc"></d-cite> that can lead to better performance<d-cite key="huang2023openrlbenchmark"></d-cite>.</p> <p>So I decided to investigate why practitioners do not use these off-policy algorithms, and maybe why they don’t work with massively parallel simulators.</p> <h2 id="why-it-matters---fine-tuning-on-real-robots">Why It Matters? - Fine Tuning on Real Robots</h2> <p>If we could make SAC work with these simulators, then it would be possible to train in simulation and fine-tune on the real robot using the same algorithm (PPO is too sample-inefficient to train on a single robot).</p> <p>By using other algorithms, it might also be possible to get better performance. Finally, it is always good to better understand what works or not and why. As researchers, we tend to publish only positive results, but a lot of valuable insights are lost in our unpublished failures.</p> <h2 id="the-path-of-least-resistance-hypothesis">(The Path of Least Resistance) Hypothesis</h2> <p>Before digging any further, I had some hypotheses as to why PPO was the only algorithm used:</p> <ul> <li>PPO is fast to train (in terms of computation time) and was tuned for the massively parallel environment.</li> <li>As researchers, we tend to take the path of least resistance and build on proven solutions (the original training code is open source, and the simulator is freely available) to get new, interesting results<d-footnote>Yes, we tend to be lazy.</d-footnote>.</li> <li>Some peculiarities in the environment design may favor PPO over other algorithms. In other words, the massively parallel environments might be optimized for PPO.</li> <li>SAC/TQC and derivatives are tuned for sample efficiency, not fast wall clock time. In the case of massively parallel simulation, what matters is how long it takes to train, not how many samples are used. They probably need to be tuned/adjusted for this new setting.</li> </ul> <p>Note: During my journey, I will be using <a href="https://github.com/DLR-RM/stable-baselines3" rel="external nofollow noopener" target="_blank">Stable-Baselines3</a><d-cite key="raffin2021sb3"></d-cite> and its fast Jax version <a href="https://github.com/araffin/sbx" rel="external nofollow noopener" target="_blank">SBX</a>.</p> <h2 id="the-hunt-begins">The Hunt Begins</h2> <p>There are now many massively parallel simulators available (Isaac Sim, Brax, MJX, Genesis, …), here, I chose to focus on Isaac Sim because it was one of the first and is probably the most influential one.</p> <figure> <video src="https://b2drop.eudat.eu/public.php/dav/files/z5LFrzLNfrPMd9o/ppo_trained.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> <div class="caption"> A PPO agent trained on the <code>Isaac-Velocity-Flat-Unitree-A1-v0</code> locomotion task. <br> Green arrow is the desired velocity, blue arrow represents the current velocity </div> <p>As with any RL problem, starting simple is the key to success <d-footnote>Also known as <a href="https://en.wikipedia.org/wiki/John_Gall_(author)#Gall's_law" rel="external nofollow noopener" target="_blank">Gall's law</a></d-footnote>. Therefore, I decided to focus on the <code class="language-plaintext highlighter-rouge">Isaac-Velocity-Flat-Unitree-A1-v0</code> locomotion task first, because it is simple but representative. The goal is to learn a policy that can move the Unitree A1 quadruped in any direction on flat ground, following a commanded velocity (the same way you would control a robot with a joystick). The agent receives information about its current task as input (joint positions, velocities, desired velocity, …) and outputs desired joint positions (12D vector, three joints per leg). The robot is rewarded for following the correct desired velocity (linear and angular) and for other secondary tasks (feet air time, smooth control, …). An episode ends when the robot falls over and is timed out after 1000 steps<d-footnote>The control loop runs at <a href="https://github.com/isaac-sim/IsaacLab/blob/f1a4975eb7bae8509082a8ff02fd775810a73531/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/velocity_env_cfg.py#L302" rel="external nofollow noopener" target="_blank">50 Hz</a>, so after 20 seconds</d-footnote>.</p> <p>To begin, I did some sanity checks. I ran PPO with the <a href="https://github.com/isaac-sim/IsaacLab/blob/f52aa9802780e897c184684d1cbc2025fafcef4a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/a1/agents/sb3_ppo_cfg.yaml" rel="external nofollow noopener" target="_blank">tuned hyperparameters</a> found in the repository, and it was able to quickly solve the task. In 5 minutes, it gets an average episode return of ~30 (above an episode return of 15, the task is almost solved). Then I tried SAC and TQC, with default hyperparameters (and observation normalization), and, as expected, it didn’t work. No matter how long it was training, there was no sign of improvement.</p> <p>Looking at the simulation GUI, something struck me: the robots were making very large random movements. Something was wrong.</p> <figure> <video src="https://b2drop.eudat.eu/public.php/dav/files/z5LFrzLNfrPMd9o/limits_train.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> <div class="caption"> SAC out of the box on Isaac Sim during training. </div> <p>Because of the very large movements, my suspicion was towards what action the robot is allowed to take. Looking at the code, the RL agent commands a (scaled) <a href="https://github.com/isaac-sim/IsaacLab/blob/f1a4975eb7bae8509082a8ff02fd775810a73531/source/isaaclab/isaaclab/envs/mdp/actions/joint_actions.py#L134" rel="external nofollow noopener" target="_blank">delta</a> with respect to a default <a href="https://github.com/isaac-sim/IsaacLab/blob/f1a4975eb7bae8509082a8ff02fd775810a73531/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/velocity_env_cfg.py#L112" rel="external nofollow noopener" target="_blank">joint position</a>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Note desired_joint_pos is of dimension 12 (3 joints per leg)
</span><span class="n">desired_joint_pos</span> <span class="o">=</span> <span class="n">default_joint_pos</span> <span class="o">+</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">action</span>
</code></pre></div></div> <p>Then, let’s look at the action space itself (I’m using <code class="language-plaintext highlighter-rouge">ipdb</code> to have an interactive debugger):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">ipdb</span><span class="p">;</span> <span class="n">ipdb</span><span class="p">.</span><span class="nf">set_trace</span><span class="p">()</span>
<span class="o">&gt;&gt;</span> <span class="n">vec_env</span><span class="p">.</span><span class="n">action_space</span>
<span class="nc">Box</span><span class="p">(</span><span class="o">-</span><span class="mf">100.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">,</span> <span class="p">(</span><span class="mi">12</span><span class="p">,),</span> <span class="n">float32</span><span class="p">)</span>
</code></pre></div></div> <p>Ah ah! The action space defines continuous actions of dimension 12 (nothing wrong here), but the limits \([-100, 100]\) are surprisingly large, e.g., it allows a delta of +/- 1432 deg!! in joint angle when <a href="https://github.com/isaac-sim/IsaacLab/blob/f1a4975eb7bae8509082a8ff02fd775810a73531/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/a1/rough_env_cfg.py#L30" rel="external nofollow noopener" target="_blank">scale=0.25</a>, like for the Unitree A1 robot. To understand why normalizing the action space <a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html" rel="external nofollow noopener" target="_blank">matters</a> (usually a bounded space in \([-1, 1]\)), we need to dig deeper into how PPO works.</p> <h2 id="ppo-gaussian-distribution">PPO Gaussian Distribution</h2> <p>Like many RL algorithms, PPO relies on a probability distribution to select actions<d-cite key="shengyi2022the37implementation"></d-cite>. During training, at each timestep, it samples an action \(a_t \sim N(\mu_\theta(s_t), \sigma^2)\) from a Gaussian distribution in the case of continuous actions<d-footnote>This is not true for the PPO implementation in Brax which uses a squashed Gaussian like SAC.</d-footnote>. The mean of the Gaussian \(\mu_\theta(s_t)\) is the output of the actor neural network (with parameters \(\theta\)) and the standard deviation is a <a href="https://github.com/DLR-RM/stable-baselines3/blob/55d6f18dbd880c62d40a276349b8bac7ebf453cd/stable_baselines3/common/distributions.py#L150" rel="external nofollow noopener" target="_blank">learnable parameter</a> \(\sigma\), usually <a href="https://github.com/leggedrobotics/rsl_rl/blob/f80d4750fbdfb62cfdb0c362b7063450f427cf35/rsl_rl/modules/actor_critic.py#L26" rel="external nofollow noopener" target="_blank">initialized</a> with \(\sigma_0 = 1.0\).</p> <p>This means that at the beginning of training, most of the sampled actions will be in \([-3, 3]\) (from the <a href="https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule" rel="external nofollow noopener" target="_blank">Three Sigma Rule</a>):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/gaussian.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/gaussian.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> The initial Gaussian distribution used by PPO for sampling actions. </div> <p>Back to our original topic, because of the way \(\sigma\) is initialized, if the action space has large bounds (upper/lower bounds » 1), PPO will almost never sample actions near the limits. In practice, the actions taken by PPO will be far from them. Now, let’s compare the initial PPO action distribution with the Unitree A1 action space:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/gaussian_large_bounds.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/gaussian_large_bounds.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> The same initial Gaussian distribution, but with the perspective of the Unitree A1 action space $$[-100, 100]$$ </div> <p>For reference, we can plot the action distribution of PPO after training<d-footnote>The code to record and plot action distribution is in the <a href="#appendix-plot-action-distribution">Appendix</a></d-footnote>: </p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/dist_actions_trained_ppo.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/dist_actions_trained_ppo.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Distribution of actions for PPO after training (on 64 000 steps). </div> <p>The min/max values per dimension:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;</span> <span class="n">actions</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mf">3.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.</span> <span class="p">,</span> <span class="o">-</span><span class="mf">3.9</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.8</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.9</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.7</span><span class="p">])</span>
<span class="o">&gt;&gt;</span> <span class="n">actions</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">array</span><span class="p">([</span> <span class="mf">3.2</span><span class="p">,</span>  <span class="mf">2.8</span><span class="p">,</span>  <span class="mf">2.7</span><span class="p">,</span>  <span class="mf">2.8</span><span class="p">,</span>  <span class="mf">2.9</span><span class="p">,</span>  <span class="mf">2.7</span><span class="p">,</span>  <span class="mf">3.2</span><span class="p">,</span>  <span class="mf">2.9</span><span class="p">,</span>  <span class="mf">7.2</span><span class="p">,</span>  <span class="mf">5.7</span><span class="p">,</span>  <span class="mf">5.</span> <span class="p">,</span>  <span class="mf">5.8</span><span class="p">])</span>

</code></pre></div></div> <p>Again, most of the actions are centered around zero (which makes sense, since it corresponds to the quadruped initial position, which is usually chosen to be stable), and there are almost no actions outside \([-5, 5]\) (less than 0.1%): PPO uses less than 5% of the action space!</p> <p>Now that we know that we need less than 5% of the action space to solve the task, let’s see why this might explain why SAC doesn’t work in this case<d-footnote>Action spaces that are too small are also problematic. See <a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html" rel="external nofollow noopener" target="_blank">SB3 RL Tips and Tricks</a>.</d-footnote>.</p> <h2 id="sac-squashed-gaussian">SAC Squashed Gaussian</h2> <p>SAC and other off-policy algorithms for continuous actions (such as DDPG, TD3, or TQC) have an additional transformation at the end of the actor network. In SAC, actions are sampled from an unbounded Gaussian distribution and then passed through a <a href="https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html" rel="external nofollow noopener" target="_blank">\(tanh()\)</a> function to squash them to the range \([-1, 1]\). SAC then linearly rescales the sampled action to match the action space definition, i.e. it transforms the action from \([-1, 1]\) to \([\text{low}, \text{high}]\)<d-footnote>Rescale from [-1, 1] to [low, high] using <code>action = low + (0.5 * (scaled_action + 1.0) * (high - low))</code>.</d-footnote>.</p> <p>What does this mean? Assuming we start with a standard deviation similar to PPO, this is what the sampled action distribution looks like after squashing<d-footnote>Common PPO implementations clip the actions to fit the desired boundaries, which has the effect of oversampling actions at the boundaries when the limits are smaller than ~4.</d-footnote>:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/squashed_vs_gaussian.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/squashed_vs_gaussian.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> The equivalent initial squashed Gaussian distribution. </div> <p>And after rescaling to the environment limits (with PPO distribution to put it in perspective):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/squashed_rescaled.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/squashed_rescaled.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> The same initial squashed Gaussian distribution but rescaled to the Unitree A1 action space $$[-100, 100]$$ </div> <p>As you can see, these are two completely different initial distributions at the beginning of training! The fact that the actions are rescaled to fit the action space boundaries explains the very large movements seen during training. Also, it explains why it was impossible for SAC to learn anything useful.</p> <h2 id="quick-fix">Quick Fix</h2> <p>When I discovered that the action limits were way too large, my first reflex was to re-train SAC, but with only 3% of the action space, to more or less match the effective action space of PPO. Although it didn’t reach PPO performance, there was finally some sign of life (an average episodic return slightly positive after a while).</p> <p>Next, I tried to use a neural network similar to the one used by PPO for this task and reduce SAC exploration by having a smaller entropy coefficient<d-footnote>The entropy coeff is the coeff that does the trade-off between RL objective and entropy maximization.</d-footnote> at the beginning of training. Bingo! SAC finally learned to solve the task!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/learning_curve.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/learning_curve.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Learning curve on the Unitree A1 task using 1024 envs. </div> <figure> <video src="https://b2drop.eudat.eu/public.php/dav/files/z5LFrzLNfrPMd9o/sac_trained_cut_1.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> <div class="caption"> Trained SAC agent after the quick fix. </div> <p>SAC Hyperparameters (the ones not specified are <a href="https://github.com/araffin/sbx/blob/8238fccc19048340870e4869813835b8fb02e577/sbx/sac/sac.py#L54-L64" rel="external nofollow noopener" target="_blank">SB3 defaults</a>):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sac_hyperparams</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span>
    <span class="n">policy_kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="c1"># Similar to PPO network tuned for Unitree A1 task
</span>        <span class="sh">"</span><span class="s">activation_fn</span><span class="sh">"</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">elu</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">net_arch</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
    <span class="p">},</span>
    <span class="c1"># When using 2048 envs, gradient_steps=512 corresponds
</span>    <span class="c1"># to an update-to-data ratio of 1/4
</span>    <span class="n">gradient_steps</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">ent_coef</span><span class="o">=</span><span class="sh">"</span><span class="s">auto_0.006</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div> <h2 id="transition-what-does-that-mean-for-the-rl-community">Transition: What Does That Mean for the RL Community?</h2> <p>When I discovered the large action limits problem, I was curious to see how widespread it was in the community. After a quick search, it turns out that a lot of papers/code are affected<d-footnote>A notable exception are Brax-based environments because their PPO implementation uses a squashed Gaussian, so the boundaries of the environments had to be properly defined.</d-footnote> by this large boundary problem (see a non-exhaustive <a href="#appendix-affected-paperscode">list of affected papers/code below</a>).</p> <p>Although the initial choice of bounds may be a conscious and convenient one (no need to specify the real bounds, PPO will figure it out), it seems to have worked a bit by accident for those who built on top of it, and probably discouraged practitioners from trying other algorithms.</p> <p>My recommendation would be to always have properly defined action bounds. If they are not known in advance, you can <a href="#appendix-plot-action-distribution">plot the action distribution</a> and adjust the limits when iterating on the environment design <d-footnote>More on that very soon ;)</d-footnote>.</p> <h2 id="tuning-for-speed-part-ii">Tuning for Speed (Part II)</h2> <p>Although SAC can now solve the locomotion task on flat ground, it takes more time to train, is not consistent, and the performance is slightly below PPO’s. In addition, SAC’s learned gaits are not as pleasing as PPO’s, for example, SAC agents tend to keep one leg up in the air…</p> <!--[Part II](../tune-sac-isaac-sim/) explores these aspects (and more environments), reviews SAC design decisions (for example, try to remove the squashed Gaussian), and tunes it for speed, but for now, let's see what this means for the RL community.--> <p>The second part of this post explores these aspects<d-footnote>I also present the ideas that didn't work and could use help (open problems) at the end of this post.</d-footnote>, as well as more complex environments. It also details how to automatically tune SAC for speed (i.e., minimize wall clock time), to learn as fast as PPO.</p> <h2 id="defining-proper-action-bound---extracting-the-limits-with-ppo">Defining Proper Action Bound - Extracting the Limits with PPO</h2> <p>First, let’s define the action space more precisely. Correctly defining the boundaries of the action space is important for both the convergence speed and the final performance. A larger action space gives the agent more flexibility, which can lead to better performance, but slower learning. Conversely, a smaller action space can accelerate learning, though it may result in suboptimal solutions.</p> <p>Thus, rather than simply restricting the action space to a small percentage of the original, I <a href="#appendix-plot-action-distribution">recorded</a> the actions taken by a trained PPO agent and took the 2.5th and 97.5th percentiles for the new limits. In other words, the new action space contains 95% of the actions commanded by a trained PPO agent<d-footnote>I repeat the same process for any new environment where those boundaries would not work</d-footnote>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># np.percentile(actions, 2.5, axis=0)
</span><span class="n">low</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7</span><span class="p">])</span>
<span class="c1"># np.percentile(actions, 97.5, axis=0)
</span><span class="n">high</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">3.8</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">1.9</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">])</span>
</code></pre></div></div> <h2 id="need-for-speed-or-how-i-learned-to-stop-worrying-about-sample-efficiency">Need for Speed or: How I Learned to Stop Worrying About Sample Efficiency</h2> <p>The second aspect I can improve is the hyperparameters of the SAC algorithm. The default hyperparameters of the SAC algorithm are optimized for sample efficiency. While this is ideal for learning directly on a single real robot<d-cite key="haarnoja2018learning"></d-cite>, it is suboptimal for training thousands of robots in simulation.</p> <p><a href="#quick-fix">Previously</a>, I quickly tuned SAC by hand to get it up and running. This was sufficient for obtaining initial results, but it would be very time-consuming to continue tuning manually to reach PPO’s performance level. That’s why I turned to automatic hyperparameter <a href="https://github.com/optuna/optuna" rel="external nofollow noopener" target="_blank">optimization</a>.</p> <h3 id="new-objective-learn-as-fast-as-possible">New Objective: Learn as Fast as Possible</h3> <p>Since I’m using a massively parallel simulator, I no longer care about how many samples are needed to learn something but how quickly it can learn, regardless of the number of samples used. In practice, this translates to an objective function that looks like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">trial</span><span class="p">:</span> <span class="n">optuna</span><span class="p">.</span><span class="n">Trial</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Optimize for best performance after 5 minutes of training.</span><span class="sh">"""</span>
    <span class="c1"># Sample hyperparameters
</span>    <span class="n">hyperparams</span> <span class="o">=</span> <span class="nf">sample_sac_params</span><span class="p">(</span><span class="n">trial</span><span class="p">)</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="n">sbx</span><span class="p">.</span><span class="nc">SAC</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="o">**</span><span class="n">hyperparams</span><span class="p">)</span>
    <span class="c1"># Callback to exit the training loop after 5 minutes
</span>    <span class="n">callback</span> <span class="o">=</span> <span class="nc">TimeoutCallback</span><span class="p">(</span><span class="n">timeout</span><span class="o">=</span><span class="mi">60</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span>
    <span class="c1"># Train with a max budget of 50_000_000 timesteps
</span>    <span class="n">agent</span><span class="p">.</span><span class="nf">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="nf">int</span><span class="p">(</span><span class="mf">5e7</span><span class="p">),</span> <span class="n">callback</span><span class="o">=</span><span class="n">callback</span><span class="p">)</span>
    <span class="c1"># Log the number of interactions with the environments
</span>    <span class="n">trial</span><span class="p">.</span><span class="nf">set_user_attr</span><span class="p">(</span><span class="sh">"</span><span class="s">num_timesteps</span><span class="sh">"</span><span class="p">,</span> <span class="n">agent</span><span class="p">.</span><span class="n">num_timesteps</span><span class="p">)</span>
    <span class="c1"># Evaluate the trained agent
</span>    <span class="n">env</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">args_cli</span><span class="p">.</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">mean_reward</span><span class="p">,</span> <span class="n">std_reward</span> <span class="o">=</span> <span class="nf">evaluate_policy</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">n_eval_episodes</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mean_reward</span>
</code></pre></div></div> <p>The agent is evaluated after five minutes of training, regardless of how many interactions with the environment were needed (the <code class="language-plaintext highlighter-rouge">TimeoutCallback</code> forces the agent to exit the training loop).</p> <h3 id="sac-hyperparameters-sampler">SAC Hyperparameters Sampler</h3> <p>Similar to PPO, many hyperparameters can be tuned for SAC. After some trial and error, I came up with the following sampling function (I’ve included comments that explain the meaning of each parameter):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample_sac_params</span><span class="p">(</span><span class="n">trial</span><span class="p">:</span> <span class="n">optuna</span><span class="p">.</span><span class="n">Trial</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
    <span class="c1"># Discount factor
</span>    <span class="n">gamma</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_float</span><span class="p">(</span><span class="sh">"</span><span class="s">gamma</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">,</span> <span class="mf">0.995</span><span class="p">)</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_float</span><span class="p">(</span><span class="sh">"</span><span class="s">learning_rate</span><span class="sh">"</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="mf">0.002</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="c1"># Initial exploration rate (entropy coefficient in the SAC loss)
</span>    <span class="n">ent_coef_init</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_float</span><span class="p">(</span><span class="sh">"</span><span class="s">ent_coef_init</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="c1"># From 2^7=128 to 2^12 = 4096, the mini-batch size
</span>    <span class="n">batch_size_pow</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_int</span><span class="p">(</span><span class="sh">"</span><span class="s">batch_size_pow</span><span class="sh">"</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="c1"># How big should should the actor and critic networks be
</span>    <span class="c1"># net_arch = trial.suggest_categorical("net_arch", ["default", "simba", "large"])
</span>    <span class="c1"># I'm using integers to be able to use CMA-ES,
</span>    <span class="c1"># "default" is [256, 256], "large" is [512, 256, 128]
</span>    <span class="n">net_arch_complexity</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_int</span><span class="p">(</span><span class="sh">"</span><span class="s">net_arch_complexity</span><span class="sh">"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="c1"># From 1 to 8 (how often should we update the networks, every train_freq steps in the env)
</span>    <span class="n">train_freq_pow</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_int</span><span class="p">(</span><span class="sh">"</span><span class="s">train_freq_pow</span><span class="sh">"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="c1"># From 1 to 1024 (how many gradient steps by step in the environment)
</span>    <span class="n">gradient_steps_pow</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_int</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient_steps_pow</span><span class="sh">"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="c1"># From 1 to 32 (the policy delay parameter, similar to TD3 update)
</span>    <span class="n">policy_delay_pow</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_int</span><span class="p">(</span><span class="sh">"</span><span class="s">policy_delay_pow</span><span class="sh">"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="c1"># Polyak coeff (soft update of the target network)
</span>    <span class="n">tau</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_float</span><span class="p">(</span><span class="sh">"</span><span class="s">tau</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># Display true values
</span>    <span class="n">trial</span><span class="p">.</span><span class="nf">set_user_attr</span><span class="p">(</span><span class="sh">"</span><span class="s">batch_size</span><span class="sh">"</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="n">batch_size_pow</span><span class="p">)</span>
    <span class="n">trial</span><span class="p">.</span><span class="nf">set_user_attr</span><span class="p">(</span><span class="sh">"</span><span class="s">gradient_steps</span><span class="sh">"</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="n">gradient_steps_pow</span><span class="p">)</span>
    <span class="n">trial</span><span class="p">.</span><span class="nf">set_user_attr</span><span class="p">(</span><span class="sh">"</span><span class="s">policy_delay</span><span class="sh">"</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="n">policy_delay_pow</span><span class="p">)</span>
    <span class="n">trial</span><span class="p">.</span><span class="nf">set_user_attr</span><span class="p">(</span><span class="sh">"</span><span class="s">train_freq</span><span class="sh">"</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="n">train_freq_pow</span><span class="p">)</span>
    <span class="c1"># Note: to_hyperparams() does the convertions between sampled value and expected value
</span>    <span class="c1"># Ex: converts batch_size_pow to batch_size
</span>    <span class="c1"># This is useful when replaying trials
</span>    <span class="k">return</span> <span class="nf">to_hyperparams</span><span class="p">({</span>
        <span class="sh">"</span><span class="s">train_freq_pow</span><span class="sh">"</span><span class="p">:</span> <span class="n">train_freq_pow</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">gradient_steps_pow</span><span class="sh">"</span><span class="p">:</span> <span class="n">gradient_steps_pow</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">batch_size_pow</span><span class="sh">"</span><span class="p">:</span> <span class="n">batch_size_pow</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">tau</span><span class="sh">"</span><span class="p">:</span> <span class="n">tau</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">gamma</span><span class="sh">"</span><span class="p">:</span> <span class="n">gamma</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">learning_rate</span><span class="sh">"</span><span class="p">:</span> <span class="n">learning_rate</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">policy_delay_pow</span><span class="sh">"</span><span class="p">:</span> <span class="n">policy_delay_pow</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">ent_coef_init</span><span class="sh">"</span><span class="p">:</span> <span class="n">ent_coef_init</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">net_arch_complexity</span><span class="sh">"</span><span class="p">:</span> <span class="n">net_arch_complexity</span><span class="p">,</span>
    <span class="p">})</span>
</code></pre></div></div> <h3 id="replay-ratio">Replay Ratio</h3> <p>A metric that will be useful to understand the tuned hyperparameters is the replay ratio. The replay ratio (also known as update-to-data ratio or UTD ratio) measures the number of gradient updates performed per environment interaction or experience collected. This ratio represents how often an agent updates its parameters relative to how much new experience it gathers. For SAC, it is defined as <code class="language-plaintext highlighter-rouge">replay_ratio = gradient_steps / (num_envs * train_freq)</code>.</p> <p>In a classic setting, the replay ratio is usually greater than one when optimizing for sample efficiency. That means that SAC does at least one gradient step per interaction with the environment. However, since collecting new data is cheap in the current setting, the replay ratio tends to be lower than 1/4 (one gradient step for every four steps in the environment).</p> <h3 id="optimization-result---tuned-hyperparameters">Optimization Result - Tuned Hyperparameters</h3> <p>To optimize the hyperparameters, I used Optuna’s CMA-ES sampler<d-cite key="takuya2019optuna"></d-cite> for 100 trials<d-footnote>Here, I only optimized for the Unitree A1 flat task due to limited computation power. It would be interesting to tune SAC directly for the "Rough" variant, including `n_steps` and gSDE train frequency as hyperparameters.</d-footnote> (taking about 10 hours with a population size of 10 individuals). Afterward, I retrained the best trials to filter out any lucky seeds<d-cite key="raffin2022learning"></d-cite>, i.e., to find hyperparameters that work consistently across different runs.</p> <p>This is what the optimization history looks like. Many sets of hyperparameters were successful:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/optuna_sac-480.webp 480w,/2026/assets/img/2026-04-27-sac-massive-sim/optuna_sac-800.webp 800w,/2026/assets/img/2026-04-27-sac-massive-sim/optuna_sac-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/optuna_sac.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Hyperparameter optimization history </div> <p>These are the tuned hyperparameters of SAC found by the CMA-ES sampler while optimizing for speed:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">batch_size</span><span class="pi">:</span> <span class="m">512</span>
<span class="na">buffer_size</span><span class="pi">:</span> <span class="s">2_000_000</span>
<span class="na">ent_coef</span><span class="pi">:</span> <span class="s">auto_0.009471776840423638</span>
<span class="na">gamma</span><span class="pi">:</span> <span class="m">0.983100250213744</span>
<span class="na">gradient_steps</span><span class="pi">:</span> <span class="m">32</span>
<span class="na">learning_rate</span><span class="pi">:</span> <span class="m">0.00044689099625712413</span>
<span class="na">learning_starts</span><span class="pi">:</span> <span class="m">2000</span>
<span class="na">policy</span><span class="pi">:</span> <span class="s">MlpPolicy</span>
<span class="na">policy_delay</span><span class="pi">:</span> <span class="m">8</span>
<span class="na">policy_kwargs</span><span class="pi">:</span>
  <span class="na">net_arch</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">512</span><span class="pi">,</span> <span class="nv">256</span><span class="pi">,</span> <span class="nv">128</span><span class="pi">]</span>
  <span class="na">activation_fn</span><span class="pi">:</span> <span class="kt">!!python/name:isaaclab_rl.sb3.elu</span> <span class="s1">'</span><span class="s">'</span>
  <span class="na">optimizer_class</span><span class="pi">:</span> <span class="kt">!!python/name:optax._src.alias.adamw</span> <span class="s1">'</span><span class="s">'</span>
  <span class="na">layer_norm</span><span class="pi">:</span> <span class="kc">true</span>
<span class="na">tau</span><span class="pi">:</span> <span class="m">0.0023055560568780655</span>
<span class="na">train_freq</span><span class="pi">:</span> <span class="m">1</span>
</code></pre></div></div> <p>Compared to the default hyperparameters of SAC, there are some notable changes:</p> <ul> <li>The network architecture is much larger (<code class="language-plaintext highlighter-rouge">[512, 256, 128]</code> vs. <code class="language-plaintext highlighter-rouge">[256, 256]</code>), but similar to that used by <a href="https://github.com/isaac-sim/IsaacLab/blob/f52aa9802780e897c184684d1cbc2025fafcef4a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/a1/agents/rsl_rl_ppo_cfg.py#L21" rel="external nofollow noopener" target="_blank">PPO in Isaac Sim</a>.</li> <li>The lower replay ratio (RR ≈ 0.03 for 1024 environments, or three gradient steps every 100 steps in an environment) and higher policy delay (update the actor once every eight critic updates) make it faster, as less time is taken for gradient updates.</li> <li>The discount factor is lower than the default value of 0.99, which favors shorter-term rewards.</li> </ul> <p>Here is the result in video and the associated learning curves<d-footnote>The results are plotted for only five independent runs (random seeds). This is usually insufficient for RL due to the stochasticity of the results. However, in this case, the results tend to be consistent between runs (limited variability). I observed this during the many runs I did while debugging (and writing this blog post), so the trend is likely correct, even with a limited number of seeds.</d-footnote>:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/learning_curve_unitree.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/learning_curve_unitree.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Learning curve on the Unitree A1 task using 1024 envs. </div> <figure> <video src="https://b2drop.eudat.eu/public.php/dav/files/ATn25xMbccroaiQ/sac_unitree_a1_tuned.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> <div class="caption"> Trained SAC agent after automatic tuning. </div> <p>With these tuned hyperparameters, SAC learns faster, achieves higher performance, and the learned gaits look better (no more feet in the air!). What more could you ask for?</p> <h2 id="does-it-work---more-environments">Does it work? - More Environments</h2> <p>After it successfully learned in the flat Unitree A1 environment, I tested the same hyperparameters (with the same recipe<d-footnote>I updated the limits for each family of robots. The PPO percentiles technique worked nicely.</d-footnote>) on the GO1, GO2, Anymal-B, and Anymal-C environments, as well as the flat <a href="https://github.com/louislelay/disney_bdx_rl_isaaclab" rel="external nofollow noopener" target="_blank">Disney BD-X</a> environment, and … it worked!</p> <figure> <video src="https://b2drop.eudat.eu/public.php/dav/files/ATn25xMbccroaiQ/isaac_part_two.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> <div class="caption"> Trained SAC agent in different environments, using the same tuned hyperparameters. </div> <p>Then, I trained SAC on the “rough” locomotion environments, which are harder environments where the robot has to learn to navigate steps and uneven, accidented terrain (with additional randomization). And … it worked partially.</p> <h2 id="solving-harder-environments">Solving Harder Environments</h2> <h3 id="identifying-the-problem-why-it-doesnt-work">Identifying the problem: Why it doesn’t work?</h3> <p>In the “Rough” environment, the SAC-trained agent exhibits inconsistent behavior. For example, one time the robot successfully climbs down the pyramid steps without falling; at other times, however, it does nothing. Additionally, no matter how long it is trained, SAC does not seem to be able to learn to solve the “inverted pyramid”, which is probably one of the most challenging tasks:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/inverted_pyramid-480.webp 480w,/2026/assets/img/2026-04-27-sac-massive-sim/inverted_pyramid-800.webp 800w,/2026/assets/img/2026-04-27-sac-massive-sim/inverted_pyramid-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/inverted_pyramid.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> The inverted pyramid task. </div> <p>I isolated this task by training SAC only on the inverted pyramid. Upon further inspection, it appeared to be an exploration problem; that is, SAC never experiences successful stepping when executing random movements. This reminded me of SAC failing on the <a href="https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/" rel="external nofollow noopener" target="_blank">mountain car problem</a> because the exploration was inconsistent (the default high-frequency noise is usually a bad default<d-cite key="raffin2021gsde"></d-cite> for robots).</p> <h3 id="improving-exploration-and-performance">Improving Exploration and Performance</h3> <p>To test this hypothesis, I simplified the problem by <a href="https://github.com/isaac-sim/IsaacLab/blob/f52aa9802780e897c184684d1cbc2025fafcef4a/source/isaaclab/isaaclab/terrains/config/rough.py#L32" rel="external nofollow noopener" target="_blank">lowering the step</a> of the inverted pyramid. I also used a more consistent exploration scheme: generalized State-Dependent Exploration (gSDE)<d-cite key="raffin2021gsde"></d-cite>. </p> <p>In its simplest form, gSDE repeats the noise vector for \(n\)-steps, instead of sampling it at every timestep. In other words, instead of selecting actions following \(a_t = \mu_\theta(s_t) + \epsilon_t\)<d-footnote>$$\mu_\theta(s_t)$$ is the actor network output, which represents the mean of the Gaussian distribution.</d-footnote> and sampling \(\epsilon_t \sim N(0, \sigma^2)\) at every step during exploration, gSDE samples \(\epsilon \sim N(0, \sigma^2)\) once and keeps \(\epsilon\) constant for \(n\)-steps. The robot could finally learn to partially solve this task with this improved exploration. </p> <figure> <video src="https://b2drop.eudat.eu/public.php/dav/files/ATn25xMbccroaiQ/sac_rough_anymal_c.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> <div class="caption"> Trained SAC agent with gSDE and n-step return in the "Rough" Anymal-C environment. </div> <p>There was still a big gap in final performance between SAC and PPO. To close the gap, I drew inspiration from the recent FastTD3<d-cite key="seo2025fasttd3"></d-cite> paper and implemented n-step returns. Using <code class="language-plaintext highlighter-rouge">n_steps=3</code> allowed SAC to finally solve the hardest task<d-footnote>Although there is still a slight performance gap between SAC and PPO, after reading the FastTD3 paper and conducting my own experiments, I believe that the environment rewards were tuned for PPO to encourage a desired behavior. In other words, I suspect that the weighting of the reward terms was adjusted for PPO. To achieve similar performance, SAC probably needs different weights. However, this is beyond the scope of this already lengthy blog post.</d-footnote>!</p> <p>In summary, here are the additional manual changes I made to the hyperparameters of SAC compared to those optimized automatically:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Note: we must use train_freq &gt; 1 to enable gSDE</span>
<span class="c1"># which resamples the noise every n steps (here every 10 steps)</span>
<span class="na">train_freq</span><span class="pi">:</span> <span class="m">10</span>
<span class="c1"># Scaling the gradient steps accordingly, to keep the same replay ratio:</span>
<span class="c1"># 32 * train_freq = 320</span>
<span class="na">gradient_steps</span><span class="pi">:</span> <span class="m">320</span>
<span class="na">use_sde</span><span class="pi">:</span> <span class="s">True</span>
<span class="c1"># N-step return</span>
<span class="na">n_steps</span><span class="pi">:</span> <span class="m">3</span>
</code></pre></div></div> <p>And here are the associated learning curves (plotting the current curriculum level on the y-axis<d-footnote>I'm plotting the current state of the terrain curriculum (the higher the number, the harder the task/terrain) as the reward magnitude doesn't tell the whole story for the "Rough" task.</d-footnote>):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/learning_curve_rough.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/learning_curve_rough.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Learning curve on the Anymal-C "Rough" task using 1024 envs (except for PPO). </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sac-massive-sim/learning_curve_rough_efficiency.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-sac-massive-sim/learning_curve_rough_efficiency.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Learning curve in term of sample-effiency on the Anymal-C "Rough" task using 1024 envs (except for PPO). </div> <p>In those plots, you can see the effect of gSDE and the use of n-step returns. SAC is also much more sample-efficient than PPO.</p> <h2 id="conclusion">Conclusion</h2> <p>This concludes the long journey I started a few months ago to make SAC work on a massively parallel simulator. During this adventure, I addressed a common issue that prevents SAC-like algorithms from working in these environments: the use of an unbounded action space.</p> <p>In the end, with a proper action space and tuned hyperparameters, SAC is now competitive with PPO<d-footnote>Although there is still a slight performance gap between SAC and PPO, after reading the FastTD3 paper and conducting my own experiments, I believe that the environment rewards were tuned for PPO to encourage a desired behavior. In other words, I suspect that the weighting of the reward terms was adjusted for PPO. To achieve similar performance, SAC probably needs different weights. However, this is beyond the scope of this already lengthy blog post.</d-footnote> in terms of training time (while being much more sample-efficient) on a large collection of locomotion environments. I hope my voyage encourages others to use SAC in their experiments and unlock fine-tuning on real robots after pretraining in simulation.</p> <h2 id="appendix-affected-paperscode">Appendix: Affected Papers/Code</h2> <p>Please find here a non-exhaustive list of papers/code affected by the large bound problem: </p> <ul> <li><a href="https://github.com/isaac-sim/IsaacLab/blob/c4bec8fe01c2fd83a0a25da184494b37b3e3eb61/source/isaaclab_rl/isaaclab_rl/sb3.py#L154" rel="external nofollow noopener" target="_blank">IsaacLab</a></li> <li><a href="https://github.com/leggedrobotics/legged_gym/blob/17847702f90d8227cd31cce9c920aa53a739a09a/legged_gym/envs/base/legged_robot_config.py#L164" rel="external nofollow noopener" target="_blank">Learning to Walk in Minutes</a></li> <li><a href="https://github.com/nico-bohlinger/one_policy_to_run_them_all/blob/d9d166c348496c9665dd3ebabc20efb6d8077161/one_policy_to_run_them_all/environments/unitree_a1/environment.py#L140" rel="external nofollow noopener" target="_blank">One Policy to Run Them All</a></li> <li><a href="https://github.com/Argo-Robot/quadrupeds_locomotion/blob/45eec904e72ff6bafe1d5378322962003aeff88d/src/go2_train.py#L104" rel="external nofollow noopener" target="_blank">Genesis env</a></li> <li><a href="https://github.com/LeCAR-Lab/ASAP/blob/c78664b6d2574f62bd2287e4b54b4f8c2a0a47a5/humanoidverse/config/robot/g1/g1_29dof_anneal_23dof.yaml#L161" rel="external nofollow noopener" target="_blank">ASAP Humanoid</a></li> <li><a href="https://github.com/LeCAR-Lab/ABS/blob/9b95329ffb823c15dead02be620ff96938e4d0a3/training/legged_gym/legged_gym/envs/base/legged_robot_config.py#L169" rel="external nofollow noopener" target="_blank">Agile But Robust</a></li> <li><a href="https://github.com/Improbable-AI/rapid-locomotion-rl/blob/f5143ef940e934849c00284e34caf164d6ce7b6e/mini_gym/envs/base/legged_robot_config.py#L209" rel="external nofollow noopener" target="_blank">Rapid Locomotion</a></li> <li><a href="https://github.com/MarkFzp/Deep-Whole-Body-Control/blob/8159e4ed8695b2d3f62a40d2ab8d88205ac5021a/legged_gym/legged_gym/envs/widowGo1/widowGo1_config.py#L114" rel="external nofollow noopener" target="_blank">Deep Whole Body Control</a></li> <li><a href="https://github.com/ZiwenZhuang/parkour/blob/789e83c40b95fdd49fda7c1725c8c573df42d2a9/legged_gym/legged_gym/envs/base/legged_robot_config.py#L169" rel="external nofollow noopener" target="_blank">Robot Parkour Learning</a></li> </ul> <p>You can probably find many more by looking at <a href="https://scholar.google.com/scholar?cites=8503164023891275626&amp;as_sdt=2005&amp;sciodt=0,5" rel="external nofollow noopener" target="_blank">works that cite the ETH paper</a>.</p> <ul> <li>Seems to be fixed in <a href="https://github.com/chengxuxin/extreme-parkour/blob/d2ffe27ba59a3229fad22a9fc94c38010bb1f519/legged_gym/legged_gym/envs/base/legged_robot_config.py#L120" rel="external nofollow noopener" target="_blank">Extreme Parkour</a> (clip action 1.2)</li> <li>Almost fixed in <a href="https://github.com/Improbable-AI/walk-these-ways/blob/0e7236bdc81ce855cbe3d70345a7899452bdeb1c/scripts/train.py#L200" rel="external nofollow noopener" target="_blank">Walk this way</a> (clip action 10)</li> </ul> <h2 id="appendix-note-on-unbounded-action-spaces">Appendix: Note on Unbounded Action Spaces</h2> <p>While discussing this blog post with a fellow researcher, they raised another point that could explain why people might choose an unbounded action space.</p> <p>In short, policies can learn to produce actions outside the joint limits to trick the underlying <a href="https://en.wikipedia.org/wiki/Proportional%E2%80%93integral%E2%80%93derivative_controller" rel="external nofollow noopener" target="_blank">PD controller</a> into outputting desired torques. For example, when recovering from a strong push, what matters is not to accurately track a desired position, but to quickly move the joints in the right direction. This makes training almost invariant to the chosen PD gains.</p> <h2 id="appendix-what-i-tried-that-didnt-work">Appendix: What I Tried That Didn’t Work</h2> <p>While preparing this blog post, I tried many things to achieve PPO performance and learn good policies in minimal time. Many of the things I tried didn’t work, but they are probably worth investigating further. I hope you can learn from my failures, too.</p> <h3 id="using-an-unbounded-gaussian-distribution">Using an Unbounded Gaussian Distribution</h3> <p>One approach I tried was to make SAC look more like PPO. In part one, PPO could handle an unbounded action space because it used a (non-squashed) Gaussian distribution (vs. a squashed one for SAC). However, replacing SAC’s squashed Normal distribution with an unbounded Gaussian distribution led to additional problems.</p> <p>Without layer normalization in the critic, it quickly diverges (leading to Inf/NaN). It seems that, encouraged by the entropy bonus, the actor pushes toward very large action values. It also appears that this variant requires specific tuning (and that state-dependent std may need to be replaced with state-independent std, as is done for PPO).</p> <p>If you manage to reliably make SAC work with an unbounded Gaussian distribution, please reach out!</p> <h3 id="kl-divergence-adaptive-learning-rate">KL Divergence Adaptive Learning Rate</h3> <p>One component of PPO that allows for better performance is the learning rate schedule (although it is not critical, it eases hyperparameter tuning). It automatically adjusts the learning rate to maintain a constant KL divergence between two updates, ensuring that the new policy remains close to the previous one (and ensuring that the learning rate is large enough, too). It should be possible to do something similar with SAC. However, when I tried to approximate the KL divergence using either the log probability or the extracted Gaussian parameters (mean and standard deviation), it didn’t work. The KL divergence values were too large and inconsistent. SAC would probably need a trust region mechanism as well.</p> <p>Again, if you find a way to make it work, please reach out!</p> <h3 id="en-vrac---other-things-i-tried">En Vrac - Other Things I Tried</h3> <ul> <li>penalty to be away from action bounds (hard to tune)</li> <li>action space schedule (start with a small action space, make it bigger over time, tricky to schedule, and didn’t improve performance)</li> <li>linear schedule (<code class="language-plaintext highlighter-rouge">learning_rate = LinearSchedule(start=5e-4, end=1e-5, end_fraction=0.15)</code>), it helped for convergence when using <code class="language-plaintext highlighter-rouge">n_steps=1</code> and <code class="language-plaintext highlighter-rouge">use_sde=False</code>, but was not needed at the end</li> </ul> <h2 id="appendix-plot-action-distribution">Appendix: Plot Action Distribution</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="n">gymnasium</span> <span class="kn">import</span> <span class="n">spaces</span>
<span class="kn">from</span> <span class="n">stable_baselines3</span> <span class="kn">import</span> <span class="n">PPO</span>
<span class="kn">from</span> <span class="n">stable_baselines3.common.env_util</span> <span class="kn">import</span> <span class="n">make_vec_env</span>
<span class="kn">from</span> <span class="n">stable_baselines3.common.vec_env</span> <span class="kn">import</span> <span class="n">VecEnvWrapper</span>

<span class="n">sns</span><span class="p">.</span><span class="nf">set_theme</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">PlotActionVecEnvWrapper</span><span class="p">(</span><span class="n">VecEnvWrapper</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    VecEnv wrapper for plotting the taken actions.
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">venv</span><span class="p">,</span> <span class="n">plot_freq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="n">venv</span><span class="p">)</span>
        <span class="c1"># Action buffer
</span>        <span class="k">assert</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">action_space</span><span class="p">,</span> <span class="n">spaces</span><span class="p">.</span><span class="n">Box</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_actions</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">actions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">plot_freq</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_envs</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_actions</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">plot_freq</span> <span class="o">=</span> <span class="n">plot_freq</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">venv</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">step_wait</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">obs</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">venv</span><span class="p">.</span><span class="nf">step_wait</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">obs</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">dones</span><span class="p">,</span> <span class="n">infos</span>

    <span class="k">def</span> <span class="nf">step_async</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">actions</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">actions</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">n_steps</span> <span class="o">%</span> <span class="n">self</span><span class="p">.</span><span class="n">plot_freq</span><span class="p">]</span> <span class="o">=</span> <span class="n">actions</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_steps</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">n_steps</span> <span class="o">%</span> <span class="n">self</span><span class="p">.</span><span class="n">plot_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">plot</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">venv</span><span class="p">.</span><span class="nf">step_async</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># Flatten the env dimension
</span>        <span class="n">actions</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">actions</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_actions</span><span class="p">)</span>
        <span class="n">n_steps</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">num_envs</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">n_steps</span>
        <span class="c1"># Create a figure with subplots for each action dimension
</span>        <span class="n">n_rows</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_actions</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">n_cols</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_actions</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">n_rows</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
        <span class="n">fig</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span>
            <span class="sa">f</span><span class="sh">"</span><span class="s">Distribution of Actions per Dimension after </span><span class="si">{</span><span class="n">n_steps</span><span class="si">}</span><span class="s"> steps</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span>
        <span class="p">)</span>

        <span class="c1"># Flatten the axes array for easy iteration
</span>        <span class="k">if</span> <span class="n">n_rows</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Special case, n_actions == 1
</span>            <span class="n">axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">axes</span><span class="p">]</span>

        <span class="c1"># Plot the distribution for each action dimension
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_actions</span><span class="p">):</span>
            <span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">actions</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">stat</span><span class="o">=</span><span class="sh">"</span><span class="s">density</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Action Dimension </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Action Value</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Density</span><span class="sh">"</span><span class="p">)</span>

        <span class="c1"># Adjust the layout and display the plot
</span>        <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>


<span class="n">vec_env</span> <span class="o">=</span> <span class="nf">make_vec_env</span><span class="p">(</span><span class="sh">"</span><span class="s">Pendulum-v1</span><span class="sh">"</span><span class="p">,</span> <span class="n">n_envs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">wrapped_env</span> <span class="o">=</span> <span class="nc">PlotActionVecEnvWrapper</span><span class="p">(</span><span class="n">vec_env</span><span class="p">,</span> <span class="n">plot_freq</span><span class="o">=</span><span class="mi">5_000</span><span class="p">)</span>

<span class="c1"># from sbx import PPO
# from sbx import SAC
# policy_kwargs = dict(log_std_init=-0.5)
</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">PPO</span><span class="p">(</span><span class="sh">"</span><span class="s">MlpPolicy</span><span class="sh">"</span><span class="p">,</span> <span class="n">wrapped_env</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">learn</span><span class="p">(</span><span class="n">total_timesteps</span><span class="o">=</span><span class="mi">1_000_000</span><span class="p">)</span>
</code></pre></div></div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-sac-massive-sim.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/using-large-language-models-to-simulate-and-predict-human-decision-making/">Using Large Language Models to Simulate and Predict Human Decision-Making</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/useful-calibrated-uncertainties/">What (and What Not) are Calibrated Uncertainties Actually Useful for?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/unlearning-or-untraining/">Is your algorithm Unlearning or Untraining?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/tracing-principles-behind-modern-diffusion-models/">Tracing the Principles Behind Modern Diffusion Models</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>