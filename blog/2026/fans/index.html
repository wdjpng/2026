<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> FANS - Frequency-Adaptive Noise Shaping for Diffusion Models | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Diffusion models have achieved remarkable success in generative modeling, yet they often struggle with spectral bias,the tendency to prioritize low-frequency patterns while inadequately learning high-frequency details. This limitation stems from the uniform noise scheduling employed during training, which allocates equal denoising capacity across all frequencies regardless of the dataset's spectral characteristics. We introduce Frequency-Adaptive Noise Shaping (FANS), a principled framework that addresses this fundamental limitation by dynamically shaping noise distributions according to dataset-specific frequency importance. FANS operates on a simple insight - different datasets exhibit distinct spectral signatures, and noise scheduling should reflect these differences. The framework integrates seamlessly with existing diffusion architectures through a simple modification to the noise sampling procedure during training and inference.We validate FANS on synthetic datasets with controlled spectral properties as well as real world data (CIFAR10, CelebA, Texture, MultimodalUniverse) where we demonstrate consistent improvements over vanilla DDPM baselines. Our experiments reveal that FANS particularly excels on high-frequency-rich datasets, producing sharper, more detailed samples while maintaining comparable performance for standard natural image datasets like CIFAR10 and CelebA."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/fans/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "FANS - Frequency-Adaptive Noise Shaping for Diffusion Models",
            "description": "Diffusion models have achieved remarkable success in generative modeling, yet they often struggle with spectral bias,the tendency to prioritize low-frequency patterns while inadequately learning high-frequency details. This limitation stems from the uniform noise scheduling employed during training, which allocates equal denoising capacity across all frequencies regardless of the dataset's spectral characteristics. We introduce Frequency-Adaptive Noise Shaping (FANS), a principled framework that addresses this fundamental limitation by dynamically shaping noise distributions according to dataset-specific frequency importance. FANS operates on a simple insight - different datasets exhibit distinct spectral signatures, and noise scheduling should reflect these differences. The framework integrates seamlessly with existing diffusion architectures through a simple modification to the noise sampling procedure during training and inference.We validate FANS on synthetic datasets with controlled spectral properties as well as real world data (CIFAR10, CelebA, Texture, MultimodalUniverse) where we demonstrate consistent improvements over vanilla DDPM baselines. Our experiments reveal that FANS particularly excels on high-frequency-rich datasets, producing sharper, more detailed samples while maintaining comparable performance for standard natural image datasets like CIFAR10 and CelebA.",
            "published": "November 30, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</h1> <p>Diffusion models have achieved remarkable success in generative modeling, yet they often struggle with spectral bias,the tendency to prioritize low-frequency patterns while inadequately learning high-frequency details. This limitation stems from the uniform noise scheduling employed during training, which allocates equal denoising capacity across all frequencies regardless of the dataset's spectral characteristics. We introduce Frequency-Adaptive Noise Shaping (FANS), a principled framework that addresses this fundamental limitation by dynamically shaping noise distributions according to dataset-specific frequency importance. FANS operates on a simple insight - different datasets exhibit distinct spectral signatures, and noise scheduling should reflect these differences. The framework integrates seamlessly with existing diffusion architectures through a simple modification to the noise sampling procedure during training and inference.We validate FANS on synthetic datasets with controlled spectral properties as well as real world data (CIFAR10, CelebA, Texture, MultimodalUniverse) where we demonstrate consistent improvements over vanilla DDPM baselines. Our experiments reveal that FANS particularly excels on high-frequency-rich datasets, producing sharper, more detailed samples while maintaining comparable performance for standard natural image datasets like CIFAR10 and CelebA.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#motivations">Motivations</a> </div> <div> <a href="#quick-overview">Quick Overview</a> </div> <div> <a href="#fans">FANS</a> </div> <div> <a href="#synthetic-data">Synthetic Data</a> </div> <div> <a href="#results">Results</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="motivations">Motivations</h2> <p>Diffusion models (DDPM) have achived state of the art performances on data modalities like Stable diffusion for images, Sora for videos, RFdiffusion for protiens and Mattergen for materials <d-cite key="Rombach2021HighResolutionIS,Qin2024WorldSimBenchTV,josephundefined,Zeni2023MatterGenAG"></d-cite>. They work by learning to reverse a gradual noising process. In a <strong>Forward Pass</strong> the data modality is progressively corrupted with gaussian noise until it becomes complete noise. In the <strong>Reverse process</strong> the model learns a denoiser which start from a gaussian noise and denoise this step-by-step to get an apprxomately clean sample. While this framework has proven to be quite effective, the existing works treats the noise as spatially uniform gaussian noise \(\mathcal{N}(\mu, \sigma^2)\).</p> <p>The modalities mentioned above have one thing in common. They exibits <strong>power law</strong> in their Fouries representation. That is, the low-frequency components have orders of magnitude higher variance than high-frequency components <d-cite key="vanHateren1992"></d-cite>. To interpret the fourier component through visual representation, we can say low frequencies capture the global structure while the high frequency encodes the finer details. On viewing diffusion though the fouries lens of the data, it has an important implications: The DDPM forward process noises high-frequency components substantially earlier and faster as compared to the low frequency components. This can be attributed to the fact that Gaussian noise is applied irrespective of the data’s spectral content as we show later.</p> <p>Since the introduced Gaussian noise is agnostic to the data’s spectral characteristics, the forward process thereby imposes a hierarchy of frequencies during generation. As the higher frequencies are noised earler and faster in the forward process, and DDPM learns to reverse this forward process: during backward process the higher frequencies are generated later conditioned on the forward process as we see in Figure 3. Previous works has also observed this phenomenon of imposed hierarchy of frequencies during generation<d-cite key="falck2025fourierspaceperspectivediffusion"></d-cite></p> <p>Images aren’t spectrally flat. Natural images concentrate most of their power in lower frequencies following characteristic power-law distributions, while images from domains like Astronomy, Texture Design have varied concentration of pwer across the frequency bands. This raises a fundamental question: if datasets have inherent frequency biases and the denoising trajectory naturally progresses from coarse-to-fine structure, why do we apply the same uniform white noise at every timestep and across all frequencies? How does this same uniform white noise across all frequencies perform across images form varied domain? Our research asks whether we can explicitly shape the noise spectrum to (i) match each dataset’s actual frequency distribution and (ii) implement a principled time-frequency annealing schedule—shifting from low to high frequencies as denoising progresses—to improve sample quality and training stability without modifying the underlying UNet architecture or DDPM objective.</p> <h2 id="quick-overview">Quick Overview</h2> <p>Let’s start with a quick overview of the two frameworks we will be working with: Diffusion models and Frequency domain of Images.</p> <h3 id="diffusion-models">Diffusion Models</h3> <p>Diffusion models generate data by learning to reverse a gradual noising process. The core idea is elegantly simple: start with real data and progressively corrupt it by adding Gaussian noise over many steps until it becomes pure random noise. This forward process requires no learning and follows a predefined schedule. A neural network then learns to invert this process, starting from random noise and iteratively denoising it to recover clean, structured data. This reverse process is governed by stochastic differential equations (SDEs) that describe how probability distributions evolve over time, with the model learning to approximate the score function which guides the denoising steps.</p> <h3 id="forward-process">Forward Process</h3> <p>It defines a markov chain that progressively add gaussian noise to the data sample \(\mathbf{x}_0 \sim q(\mathbf{x}_0)\) over \(T\) timesteps. Each timestep adds a slight noise to the data resulting in a increasingly noisy samples \(\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_T\), where $\mathbf{x}_T$ approximates an isotropic Gaussian distribution. The forward process is formally defined as :</p> \[q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \, \mathbf{x}_{t-1}, \beta_t \mathbf{I}),\] <p>where $\beta_t \in (0,1)$ controls the amount of noise added to the data.</p> <p>By applying the reparameterization reccursively we obtain the closed form experssion</p> \[q(\mathbf{x}_t | \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \, \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I}),\] <p>where \(\alpha_t = 1 - \beta_t\) and \(\bar{\alpha}_t = \prod_{s=1}^t \alpha_s\).</p> <p>Thus the noisy data at any given time \(t\) in the forward proccess</p> \[\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{(1 - \bar{\alpha}_t)}\epsilon\] <p>where \(\epsilon \in \mathcal{N}(0,\mathbf{I})\).</p> <p>A useful notation in this is the log signal-to-noise ratio \(\lambda_t = log(\bar{\alpha}_t / (1 - \bar{\alpha}_t))\) which increases monotonically from 0 ( clean data) to 1 (noise)</p> <h3 id="reverse-process">Reverse Process</h3> <p>The Reverse process tries to invert the forward process, transforming Gaussian noise \(\mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\) back into a data sample resembling the data distribution \(q(\mathbf{x}_0)\). Since the true reverse transitions \(q(\mathbf{x}_{t-1} | \mathbf{x}_t)\) are intractable, a parameterized model \(p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t)\) is trained to approximate them.</p> <p>The reverse process is modeled as:</p> \[p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t)),\] <p>where \(\boldsymbol{\mu}_\theta\) and \(\boldsymbol{\Sigma}_\theta\) are outputs of a neural network conditioned on \(\mathbf{x}_t\) and the timestep \(t\). The model learns to predict the mean of the denoised sample at each step.</p> <p>Using the forward process derivation, the true mean of \(q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0)\) can be expressed as:</p> \[\boldsymbol{\mu}_q(\mathbf{x}_t, \mathbf{x}_0) = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon} \right),\] <p>where \(\boldsymbol{\epsilon}\) denotes the Gaussian noise added at timestep \(t\).</p> <p>On way of learning is during training the model is optimized to predict this noise directly using a loss of the form:</p> \[L(\theta) = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}} \left[ \left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta( \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}, t ) \right\|^2 \right],\] <p>which corresponds to a reweighted variational bound on the data likelihood.</p> <h3 id="fouries-domain">Fouries Domain</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/image_fft-480.webp 480w,/2026/assets/img/2026-11-25-fans/image_fft-800.webp 800w,/2026/assets/img/2026-11-25-fans/image_fft-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-11-25-fans/image_fft.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/power_law-480.webp 480w,/2026/assets/img/2026-11-25-fans/power_law-800.webp 800w,/2026/assets/img/2026-11-25-fans/power_law-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-11-25-fans/power_law.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure2: First two image shows an image in spatial, in frequency domain. The plot is the log-log plot of Power spectral density and Frequency of the image. </div> <p>Natural Images have rich structures in the frequency domain that is obscured in the pixel space. Understanding this frequency domain perspective is crucial for our investigation.</p> <p>Let’s say we have an image \(x \in \mathbf{R}_{H \times W\times C}\). Its pixels are coeffcient of a standard basis \(\mathcal{B} = \{ \mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_n \} \subset \mathbb{R}^n\). In the spatial domain, the image is defined as a linear combination of standard basis vectors, where each pixel represents a local real-valued intensity. When we apply the Discrete Fourier Transform(DFT) to this image, it performs unitary change of basis giving us the frequency domain equivalent of the image. The dimension of the fourier and the pixel image remains the same but in fourier domain the coefficients of the basis becomes complex valued.</p> <p>For each channel of an image the DFT can be formulated as</p> \[F(u, v) = \sum_{x=0}^{H-1} \sum_{y=0}^{W-1} f(x, y) e^{-j 2\pi \left( \frac{ux}{H} + \frac{vy}{W} \right)}\] <p>where \((u,v)\) aer spatial frequency indices. The radial frequency \(f = \sqrt{u^2 + v^2}\) measure the distance from the DC ( zero frequency component)</p> <p>Now, let’s sort the Fourier coeeficient \(f(u,v)\) from low to high frequency. To do so, we start from the center of our Fourier Space and walk in spirals. That is, sort them using the Manhatten distancen of the indices \((uv,)\) from the center (0,0) of the Fourier representation. From the Figure 2 It’s very evident that signal variance decreases rapidly with increasing frquency. <d-cite key="dielman"></d-cite> shows that similar trend is visible in other doains (Videos, Audio, Proteins) as well.</p> <p><strong>Power Spectral Density(PSD)</strong>: This quantifies how the signal enery is distributed across frequencies. For an image with Fourier Transform \(F(u,v)\), PSD is defined as :</p> \[P(u,v) = |F(u,v)|^2\] <p>representing the power at each frequency component. To get one dimensional spectral profile we compute the radially-average PSD by integrating over annular regions at constant radial frequency \(f\) (simply put we compute average power in ring-shaped zones moving outward from the center). Formally put</p> \[P(f) = \frac{1}{|B_f|} \sum{}_{(u,v) \in B_f} |F(u,v)|^2\] <p>Where \(B_f\) is the radial frequency band defined as \(B_f = {(u,v): f \leq \sqrt{u^2+v^2} \le f+ \delta f}\) and \(\|B_f\|\) is the number of frequencies in that band.</p> <p>The PSD reveals how much each frequency contributes to the overall signal. For a dataset of images, we compute per-image PSDs and average them to obtain a dataset-level spectral profile. This aggregate PSD characterizes the typical frequency distribution of the data and captures domain-specific properties</p> <p>Real-world image datasets exhibit characteristic frequency distributions. The power spectral density of natural images typically follows a power-law decay:</p> \[P \propto f^{-\alpha}\] <p>Emperical studies on natural image statistics shave shown that \(\alpha\) typically resides in the interval \([1,3]\) <d-cite key="field,physrevLett"> </d-cite>. While Standard photgraphic images generally converges towards \(\alpha \approx 2\) <d-cite key="vanHateren1992"></d-cite>, For few domain specific datasets like Astronomy images, we observe this trend \(\alpha \approx 1\).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/universe_image_fft-480.webp 480w,/2026/assets/img/2026-11-25-fans/universe_image_fft-800.webp 800w,/2026/assets/img/2026-11-25-fans/universe_image_fft-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-11-25-fans/universe_image_fft.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/average_frequency_psd_loglog-480.webp 480w,/2026/assets/img/2026-11-25-fans/average_frequency_psd_loglog-800.webp 800w,/2026/assets/img/2026-11-25-fans/average_frequency_psd_loglog-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-11-25-fans/average_frequency_psd_loglog.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure: In the First Figure (Top Right), We can see the fourier representation of an astronomy image varies a lot as compared to the fourier representation of the natural image as shown above. </div> <p>To quantify dataset frequency importance, we divide the spectrum into \(B\) radial frequency bands and compute the normalized band power \(\pi_b\)​ for each band \(B\):</p> \[\pi_b = \frac{1}{N}\sum{N}_{i=1} \frac{\sum{}_{f \in B_i}P_i(f)}{\sum{}_{f} P_i(f)}\] <p>where \(N\) is the number of images in the dataset and \(P_i(f)\) is the PSD of image \(i\). These band powers \({\pi_b}^B_{b=1}\)​ form a probability distribution over frequency bands, representing how the dataset’s signal energy is distributed across the spectrum.</p> <h3 id="diffusion-in-frequency-domain">Diffusion in Frequency Domain</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/heatmap-480.webp 480w,/2026/assets/img/2026-11-25-fans/heatmap-800.webp 800w,/2026/assets/img/2026-11-25-fans/heatmap-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-11-25-fans/heatmap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 3: Comparing the SNR (dB scale) for DDPM we see that High frequencies are corrupted substantially faster (SNR changes more per time increment) than low frequencies.The SNR is computed using monte carlo estimate of the SNR equation discussed below </div> <p>As we are trying to investigate how the DDPMs inductive bias in the forward process<d-cite key="falck2025fourierspaceperspectivediffusion"></d-cite> affects datasets for varying spectral power density, we can view the DDPM forward process under a change of basis to fourier space <d-cite key="dielman,gerdes2024gudgenerationunifieddiffusion"></d-cite>. This is acomplished by applying the Fourier transform \(\mathbf{F}\) to the variable \(x_t\)</p> \[y_t : \mathbf{F}\mathbf{x}_t = \mathbf{F}\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \mathbf{F}\sqrt{(1 - \bar{\alpha}_t)}\epsilon\] <p>Here, \(y_t\) is our fourier-transformed intermediate step at time step \(t\) of the forward process.</p> <p>Taking the quantification of Signal-to-noise Ration from <d-cite key="falck2025fourierspaceperspectivediffusion"></d-cite>, where SNR of \((x_t)_i\) is the signal to noise ration of frquescy \(i\) at timestep \(t\). Formally put</p> \[SNR((x_t)_i) = \frac{\bar{\alpha_t}\varsigma_i}{1-\bar{\alpha_t}}\] <p>Where \(\varsigma_i = Var((x_0)_i)\) represents the signal variance of requency \(i\).</p> <p>Tying these all together, we see in Figure 3, that standard DDPM corrupts high-frequencies faster than low frequencies. This bias is carry forwarded to the reverse process as well.</p> <p>Thus we want to study an alternate noising schedule that respects the datasets spectral signature. To put simply a Frequency Adaptive Noise Scheduler ( FANS ).</p> <h2 id="fans">FANS</h2> <p>In the previous sections we saw the inductive bias of DDPM towards high frequency components both in forward and reverse process. We want to investigate if instead of isotropic gaussian noise scheduler (irrespective of the frequency distribution of the dataset), we use a scheduler that is adaptive to the intrinsic frequency characteristic of the dataset by constructing spectrally-shaped noise. Our key insight is that different frequency bands contribute unequally to perceptual quality and should be treated accordingly during both training and generation.</p> <p>This proposed approach, operates through three complementary mechanism :</p> <ol> <li> <strong>Dataset importance profiling</strong>: We analyze the spectraal distribution of the data to compute frequency band importance weight \(g_b\) that quantifies the relative contribution of each band of the overall data.</li> <li> <strong>Time-Frequency Scheduling</strong>: We introduce a temporal ramping function \(\phi(t)\) that smoothly transitions noise allocation from low to high frequencies as the diffusion process evolves, enabling coarse-to-fine generation.</li> <li> <strong>Variance-Compensated Weighting</strong>: We apply inverse-power weighting to band importances, ensuring that underrepresented high-frequency bands receive compensatory emphasis during training.</li> </ol> <p>We can now formalize these mechanism:</p> <h3 id="radial-frequency-band-decomposition">Radial Frequency Band Decomposition</h3> <p>Recalling from previous discussion, let’s \(x_0 \in \mathbb{R}^{H \times W \times C}\) denote a clean image, and let \(\mathcal{F}\) denote the real-valued FFT used in the implementation. To characterise the dataset’s spectral structure, we partition the \(B\) radial frequency bands \(\{B_b\}_{b=1}^B\) in the discrete \(r\)FFT layout (shape \(H \times W/2 + 1\)) using linear radial boundaries. For each band \(b\) we define :</p> <ul> <li> <strong>Band mask</strong> : \(B_b \in \{0,1\}^{H \times W/2 + 1}\) indicating membership.</li> <li> <table> <tbody> <tr> <td> <strong>Band size</strong> :</td> <td>\(B_b\)</td> <td>denotes the nuber of frequency coefficients in band \(b\).</td> </tr> </tbody> </table> </li> </ul> <p>Bands are constructed using radial frequency \(F(u,v) = \sqrt{u^2 + v^2}\) with edges uniformly spaced between a small positive \(f_{min}\) (to exclude DC component) and \(f_{mac}\) ( Nyquist frequency = 0.5)</p> <p><strong>Intuition</strong> : Excluding the DC component (zero frequency) is critical because it represents the global mean intensity, which dominates the spectrum but carries minimal perceptual information. Including DC in band 0 would artificially inflate its importance \(g_0\) and distort the learned weighting.</p> <h3 id="dataset-importance-profiling">Dataset Importance Profiling</h3> <p>For each image \(x\) in the training set, we compute the <strong>normalized band power</strong> \(\pi_b(x)\) as ;</p> \[\pi_b(x) = \frac{\sum{}_{k \in B_b} |F(x - \bar{x})(k)|^2 }{\sum^{B-1}_{b^\prime = 0 }\sum{}_{k \in B_{b^\prime}} |F(x - \bar{x})(k)|^2 }\] <p>wher \(\bar{x}\) is the per image mean ( removing DC ). This gives the fraction of total power residing in band \(b\) for image \(x\).</p> <p>We then compute the dataset-level band distribution by averaging over \(N\) samples:</p> \[\bar{\pi} = \frac{1}{N} \sum^{N}_{i=1} \pi_b(x_i)\] <p>Note that \(\sum^{B-1}_{b=0} \bar{\pi_b} = 1\) by construction.</p> <p>Finally, we compute importance weights via inverse-power scaling rule :</p> \[g_b = \frac{(\bar{\pi_b} + \epsilon )^{-\alpha}}{\frac{1}{B}\sum^{B-1}_{b^\prime=0}(\bar{\pi_{b^\prime}} + \epsilon)^{-\alpha}}\] <p>wher \(\alpha\) controls the strength of variance compensation. The standardization ensures \(g_b\) has zero mean and unit variance, provising a stable range for the softmax reweighting.</p> <p><strong>Intuition</strong>: Bands with low power \(\bar{\pi_b}\)(e.g., high frequencies) receive higher importance \(g_b\) ​, compensating for their underrepresentation in the data. This prevents the model from neglecting high-frequency reconstruction.</p> <h3 id="time-dependent-soft-band-weighting-check-once">Time-Dependent Soft Band Weighting [check once]</h3> <p>At each timestep \(t∈[0,1]\)t \in [0,1]\(, we compute soft band weights\)w_b(t)$$ via a temperature-scaled softmax with time-frequency ramping:</p> \[w_b(t) = \frac{exp(\beta.g_b - \gamma.\phi(t).\lambda_b)}{\sum^{B-1}_{b^\prime = 0}exp(\beta.g_b^\prime - \gamma.\phi(t).\lambda_b^\prime)}\] <p>where:</p> <ul> <li>\(\lambda_b = b/(B-1)\) is the normalized band index \(\lambda_0 = 0, \lambda_{B-1}=1\)</li> <li>\(\pi(t) \in [0,1]\) is a temporal ramp.</li> <li>\(\beta,\gamma \ge 0\) are the invese temperature hyperparameter.</li> </ul> <p>Early in the diffusion process, the term \(\beta. g_b\) dominates, emphasizing frequencies according to dataset statistics. As \(t\) increases, the ramp \(\phi(t)\) gradually increases the influence of $\gamma\lambda_b$, making the spectrum increasingly uniform.</p> <p><strong>Stabilization: White Noise Guardrail</strong> To ensure stable training during the earliest timesteps, we introduce a white noise mixing schedule:</p> \[w_b^{\text{mix}}(t) = \begin{cases} \frac{1}{B} &amp; \text{if } t &lt; t_{\text{knee}} \\ (1 - \alpha_{\text{mix}}(t)) \cdot \frac{1}{B} + \alpha_{\text{mix}}(t) \cdot w_b(t) &amp; \text{if } t \geq t_{\text{knee}} \end{cases}\] <p>where \(\alpha_{\text{mix}}(t) = \frac{t - t_{\text{knee}}}{1 - t_{\text{knee}}}\)​​ is a linear blend coefficient and \(t_{\text{knee}} = 0.15\) by default.</p> <p><strong>Intuition</strong>: At very small \(t\), the noised samples \(x_t \approx \beta_t \epsilon\) are nearly pure noise. Enforcing strong spectral shaping here can destabilize training because the model has insufficient signal to learn meaningful structure. By using uniform weights early, we ensure the model first learns to denoise white noise (as in standard DDPM), then gradually transitions to FANS-shaped noise.</p> <h3 id="fans-noise-generation">FANS Noise Generation</h3> <p>Tying the above mechanisms together to generate the FANS-Noise we get.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/fans_training-480.webp 480w,/2026/assets/img/2026-11-25-fans/fans_training-800.webp 800w,/2026/assets/img/2026-11-25-fans/fans_training-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-11-25-fans/fans_training.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Given a sample \(x \in \mathcal{R}^{N \times C \times H \times W}\) and normalised time \(t \in [0,1]\) FANS-Noise \(\epsilon_{FANS}\) is generated as :</p> <p>Step 1. <strong>Compute band weights:</strong> \(\{w_b(t)\}^{B-1}_{b=0}\) as discussed above. Step 2. <strong>Allocate spectral power</strong>:</p> <p>The total power available in Fourier space must account for the Parseval Relation (It states that total power is time domain must be equal to the total power in the fourier domain). For the forward process, the noise component has pixel-space variance \(\beta_t^2\). We set the \(\sigma_t = 1\), giving us</p> <p>\(P_{total} = N_p.\sigma^2_t = H.W.1 = H.W\).</p> <p>Why we go for \(N_p\)​ (not \(N_p^2\)​): The power spectral density relates to the sum of squared Fourier coefficients, not their squared sum. For an \(H \times W\) image, the rFFT produces \(H \times (W/2 + 1)\) complex coefficients. By Parseval’s theorem</p> \[\sum_{i=1}^{H \cdot W} x_i^2 = \frac{1}{H \cdot W} \sum_{k} |F(x)(k)|^2\] <p>where the factor \(1/(H \cdot W)\) comes from the DFT normalization convention.</p> <p>We then allocate power to each band according to the learned weights:</p> \[P_b(t) = w_b(t) \cdot P_{\text{total}} = w_b(t) \cdot H \cdot W\] <p>The per-frequency variance within band \(b\) is obtained by distributing \(P_b\)​ uniformly across its members:</p> \[\Sigma_b(t) = \frac{P_b(t)}{|\mathcal{B}_b| + \epsilon}\] <p>where \(\epsilon = 10^{-6}\) prevents division by zero as a numerical safeguard.</p> <p>Step 3. <strong>Draw Complex Gaussian Noise in Fourier Space</strong>:</p> <p>We generate base noise \(Z \in \mathbb{C}^{N \times C \times H \times (W/2+1)}\) by sampling independent real and imaginary components:</p> \[Z_{\text{real}} \sim \mathcal{N}(0, I), \quad Z_{\text{imag}} \sim \mathcal{N}(0, I)\] \[Z(n, c, h, w) = \frac{1}{\sqrt{2}}(Z_{\text{real}}(n,c,h,w) + i \cdot Z_{\text{imag}}(n,c,h,w))\] <p>The \(1/\sqrt{2}\)​ factor ensures</p> \[\mathbb{E}[|Z(k)|^2] = \mathbb{E}[Z_{\text{real}}^2 + Z_{\text{imag}}^2]/2 = 1\] <p>Step 4: <strong>Construct Spectral Variance Mask</strong></p> <p>We build a spatial variance map \(\Sigma_t \in \mathbb{R}^{N \times C \times H \times (W/2+1)}\)that specifies the desired variance at each frequency:</p> \[\Sigma_t(n,c,h,w) = \sum_{b=0}^{B-1} \Sigma_b(t) \cdot \mathbb{1}_{(h,w) \in \mathcal{B}_b}\] <p>where \(\mathbb{1}_{(h,w) \in \mathcal{B}_b}\)​​ is the indicator function for band membership.</p> <p>Step 5: <strong>Apply Frequency-Dependent Scaling</strong> The shaped noise in Fourier space is obtained by element-wise multiplication:</p> \[E_{\text{shaped}}(n,c,h,w) = \sqrt{\Sigma_t(n,c,h,w) + \epsilon_{\text{safe}}} \cdot Z(n,c,h,w)\] <p>where \(\epsilon_{\text{safe}} = 10^{-12}\) ensures numerical stability when \(\Sigma_t \approx 0\).</p> <p>Why square root? We are scaling the amplitude of Fourier coefficients. Since power is amplitude squared, to achieve variance \(\Sigma_t(k)\), we need amplitude \(\sqrt{\Sigma_t(k)}\)​. This follows from:</p> \[\mathbb{E}[|E_{\text{shaped}}(k)|^2] = \mathbb{E}[|\sqrt{\Sigma_t(k)} \cdot Z(k)|^2] = \Sigma_t(k) \cdot \mathbb{E}[|Z(k)|^2] = \Sigma_t(k)\] <p>Step 6: <strong>Inverse Fourier Transform to Pixel Space</strong> We apply the inverse real FFT to recover a real-valued noise image:</p> \[\epsilon_{\text{FANS}}^{\text{raw}} = \text{irfft2d}(E_{\text{shaped}}, s=(H, W))\] <p>where \(s=(H, W)\) specifies the desired output shape and irfft2d is the 2D inverse real FFT.</p> <p>Step 7: <strong>Enforce Unit Variance via Normalization</strong> While the Fourier-space construction theoretically preserves variance, discretization effects, numerical precision, and band edge artifacts can cause the pixel-space variance to deviate from unity. To ensure exact compatibility with the forward process \(x_t = \alpha_t z + \beta_t \epsilon\) where \(\mathbb{E}[\epsilon \epsilon^T] = I\), we enforce unit variance per sample and per channel:</p> \[\mu(n,c) = \frac{1}{H \cdot W} \sum_{h,w} \epsilon_{\text{FANS}}^{\text{raw}}(n,c,h,w)\] \[\sigma^2(n,c) = \frac{1}{H \cdot W} \sum_{h,w} (\epsilon_{\text{FANS}}^{\text{raw}}(n,c,h,w) - \mu(n,c))^2\] \[\epsilon_{\text{FANS}}(n,c,h,w) = \frac{\epsilon_{\text{FANS}}^{\text{raw}}(n,c,h,w) - \mu(n,c)}{\sqrt{\sigma^2(n,c) + \epsilon_{\text{safe}}}}\] <p>Critical importance: This normalization is not optional. Without it, we observed training instabilities where the effective noise magnitude drifted over time, breaking the assumptions of the forward SDE. The normalization ensures:</p> <ul> <li>Zero mean: \(\mathbb{E}[\epsilon_{\text{FANS}}] = 0\) exactly (not just approximately)</li> <li>Unit variance: \(\text{Var}(\epsilon_{\text{FANS}}) = I\) exactly</li> <li> <table> <tbody> <tr> <td>Consistency: \(x_t = \alpha_t z + \beta_t \epsilon_{\text{FANS}}\)​ has the correct conditional distribution $$p_t(x</td> <td>z)$$</td> </tr> </tbody> </table> </li> </ul> <p>Why per-channel normalization? Color channels may have different effective powers after Fourier shaping due to:</p> <ul> <li>Boundary effects in rFFT (asymmetric handling of Nyquist)</li> <li>Floating-point rounding errors accumulated differently per channel</li> <li>Non-uniform distribution of salient features across RGB</li> </ul> <p>Normalizing each channel independently ensures that the model sees noise with identical statistics in all color channels, preventing the network from learning color-dependent denoising biases.</p> <p>Step 8: <strong>Return Shaped Noise</strong> The final output \(\epsilon_{\text{FANS}} \in \mathbb{R}^{N \times C \times H \times W}\) satisfies:</p> <ol> <li>\(\mathbb{E}[\epsilon_{\text{FANS}}] = 0\)(zero mean)</li> <li>\(\mathbb{E}[\epsilon_{\text{FANS}} \epsilon_{\text{FANS}}^T] = I\) (unit covariance)</li> <li>Frequency band \(b\) contains fraction \(\approx w_b(t)\) of total power</li> </ol> <p>Low frequencies dominate at small \(t\), high frequencies at large \(t\)</p> <p>This noise can now be used in the forward process:</p> \[x_t = \alpha_t z + \beta_t \epsilon_{\text{FANS}}\] <p>and in the loss computation:</p> \[\mathcal{L} = \|u_\theta^t(x_t) - (\dot{\alpha}_t z + \dot{\beta}_t \epsilon_{\text{FANS}})\|^2\] <h3 id="sampling-inference">Sampling/ Inference</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/fans_sampling-480.webp 480w,/2026/assets/img/2026-11-25-fans/fans_sampling-800.webp 800w,/2026/assets/img/2026-11-25-fans/fans_sampling-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-11-25-fans/fans_sampling.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>This algorithm shows the sampling method we used for our proposed approach.</p> <p>Now that we have the mechanism to get spectral aware shape, we need to see how performs against a baseline. To test this we simulate experiments where high frequency and fine grained information is of primary interest. For this simulation we do a synthetic study. We designed two synthetic datasets PLTB and EGM each constructed to emphasize a distinct spectral profile. All three datasets are generated programmatically as 512×512 images. We discuss these synhtetic datasets in details in the following section</p> <p>We use this synthetic data because controlled synthetic data provides the ability to vary the distribution of Fourier energy across bands in a principled way and isolate the effect of FANS.</p> <h2 id="synthetic-data">Synthetic Data</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/synth_sample-480.webp 480w,/2026/assets/img/2026-11-25-fans/synth_sample-800.webp 800w,/2026/assets/img/2026-11-25-fans/synth_sample-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-11-25-fans/synth_sample.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="pltb-power-law-texture-bank">PLTB: Power-Law Texture Bank</h3> <p><strong>Motivation:</strong> PLTB targets the high-frequency regime. In this the entire image is defined through its radial power spectrum. We can see the distribution of spectral mass across bands can be observed in the figure.</p> <p><strong>Genertation Procedure:</strong> Each PLTB sample is generated by:</p> <ol> <li>Sampling an i.i.d. complex Gaussian field \(Z(k)\) on the half-spectrum</li> <li>Applying an amplitude mask:</li> </ol> \[A(k) \propto (|k| + \epsilon )^{\alpha/2}\] <p>with slope \(\alpha = 1\)</p> <ol> <li>Multiplying \(Z(k)\) by \(A(k)\) and applying an inverse FFT to obtain the spatial image.</li> <li>Finally, Normalizing brightness and contrast per-sample.</li> </ol> <p>This enables us to isolate testure learning capability of dissusion models. Models with insufficient high-frequency produce visibly smoother samples. A sample is given in figure above</p> <h3 id="egm-edgesgratings-mixture">EGM: Edges–Gratings Mixture</h3> <p><strong>Motivation:</strong> EGM introduces structured high-frequency content—oriented gratings, checkerboards, and sharp edges—that resemble real-image conditions where fine geometric detail matters. This allows discrete orientation content and mixed spatial primitives, providing a more realistic stress test of whether a model can faithfully reproduce high frequency geometry rather than only stochastic texture.</p> <p><strong>Generation Procedure:</strong> Each EGM image is a mixture of:</p> <ol> <li>Sinusoidal gratings with random frequency (0.04 − 0.22 cyc/px), orientation, amplitude, and phase.</li> <li>Checkerboard patterns, producing orthogonal high-frequency peaks.</li> <li>Sparse straight-line segments, introducing broadband edge energy.</li> </ol> <p>Components are randomly combined, and the resulting image is normalized to unit variance. EGM reflects scenarios common in natural images edges, periodic patterns, corner-like junctions—while still enabling controlled frequency manipulation.</p> <p>The controlled spectral mass distribution across bands of these synthetic datasets allows us to benchmark how FANS perform compared to a standard DDPM when the spectral signature of an image deviates from that of a natural image.</p> <p>To ensure that adapting to the spectral signature of an image doesn’t affect the performance in natural image, we benchmark FANS against standard DDPM in datasets like CIFAR10 and CelebA.</p> <p>To show that the synthetic dataset we generated are not entirely an imaginary usecase, we analyse the spectral signature of two real world datasets:</p> <ul> <li>Multimodal Universe: An astronomy based Dataset</li> <li>Texture dataset: A dataset consist of textures.</li> </ul> <h2 id="results">Results.</h2> <p>When we compare this Frequency Aware method against standard DDPM, we found some interesting results.</p> <p><strong>Slope Estimation:</strong></p> <p>A key advantage of FANS over standard DDPM models lies in its ability to accurately capture the spectral characteristics of the data distribution. To quantify this improvement, we evaluate both methods on their capacity to estimate the power-law decay exponent (slope) of the data’s power spectral density.</p> <table> <thead> <tr> <th>Dataset</th> <th style="text-align: center">Original Slope</th> <th style="text-align: right">FANS</th> <th style="text-align: right">DDPM</th> </tr> </thead> <tbody> <tr> <td>PLTB</td> <td style="text-align: center">1.002</td> <td style="text-align: right">1.394</td> <td style="text-align: right">3.566</td> </tr> <tr> <td>EGM</td> <td style="text-align: center">0.989</td> <td style="text-align: right">1.121</td> <td style="text-align: right">2.566</td> </tr> <tr> <td>Multimodal Universe</td> <td style="text-align: center">1.229</td> <td style="text-align: right">1.454</td> <td style="text-align: right">3.515</td> </tr> <tr> <td>Texture Data</td> <td style="text-align: center">1.121</td> <td style="text-align: right">1.618</td> <td style="text-align: right">2.178</td> </tr> <tr> <td>CIFAR10</td> <td style="text-align: center">2.848</td> <td style="text-align: right">2.512</td> <td style="text-align: right">2.733</td> </tr> </tbody> </table> <p>we compute the mean absolute error (MAE) between the estimated and ground-truth spectral slopes across all datasets in Table above. FANS achieves a substantially lower MAE (0.3165) compared to DDPM (1.5197). This difference is stable across datasets spanning texture-rich synthetic domains (PLTB, EGM) and real natural image statistics (CIFAR-10).</p> <p>To quantify statistical reliability, we treat each dataset entry as an independent paired comparison between FANS and DDPM errors. A paired t-test on the absolute errors yields a significant difference (t = −4.92, p &lt; 0.01). Instead, they reflect a systematic reduction in spectral-slope distortion.</p> <p><strong>Spectral Band Analysis:</strong> As FANS is designed to schedule noise based on the spectral property of the dataset, we perform a spectral analysis between the dataset and the images generated by the baseline and FANS.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/bar_plot-480.webp 480w,/2026/assets/img/2026-11-25-fans/bar_plot-800.webp 800w,/2026/assets/img/2026-11-25-fans/bar_plot-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-11-25-fans/bar_plot.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Figure: Shows the PSD over the frequency bands of three datasets (a) Multimodal Universe Dataset, (b) PLTB Dataset ad (c) EGM Dataset. It also shows how the PSD of the real datasets (<span style="color:green">green</span>) and generated images are distributed over the frequency bands for both the FANS (<span style="color:blue">Blue</span>) and Standard DDPM ( Baseline ) model (<span style="color:red">Red</span>)</p> <p>The plot shows the Power Spectral Density per Frequency Band for two synthetic dataset (PLTB and EGM) and a real world dataset ( Multimidal Universe). It shows that both the method could capture the spectral signature of the dataset, and understand how the PSD is distributed across the frequency bands. However the baseline ( Standard DDPM ) tends to concentrate the PSD within few frequency bands, while FANS ** distribute the PSD as per the dataset characteristics** and are more in agreement with the dataset spectral characteristic.</p> <p><strong>Metrics.</strong> To quantify spectral fidelity we use: (1) Jensen-Shannon (JS) divergence between generated and real PSD distributions (lower is better), (2) per-band correlation.</p> <p>Using Jensen-Shannon divergence (JSD), we can see that the PSD distribution across frequency bands of FANS are much closer to the actual dataset as compared to the baseline on all the three datasets.</p> <table> <thead> <tr> <th>Dataset</th> <th style="text-align: center">JSD(FANS)</th> <th style="text-align: right">JSD(Baseline)</th> </tr> </thead> <tbody> <tr> <td>EGM</td> <td style="text-align: center"><strong>0.0308</strong></td> <td style="text-align: right">0.1276</td> </tr> <tr> <td>PLTB</td> <td style="text-align: center"><strong>0.0103</strong></td> <td style="text-align: right">0.0804</td> </tr> <tr> <td>Universe</td> <td style="text-align: center"><strong>0.0088</strong></td> <td style="text-align: right">0.1041</td> </tr> </tbody> </table> <p>Across the datasets FANS shows a stronger correlation bandwise compared to the basseline. This shows the correlation between the bands of the real dataset and the samples generated. FANS has much higher bandwise correlation across both the synthetic and real dataset.</p> <table> <thead> <tr> <th>Dataset</th> <th style="text-align: center">Correlation(FANS)</th> <th style="text-align: right">Correlation(Baseline)</th> </tr> </thead> <tbody> <tr> <td>EGM</td> <td style="text-align: center"><strong>0.911</strong></td> <td style="text-align: right">0.672</td> </tr> <tr> <td>PLTB</td> <td style="text-align: center"><strong>0..968</strong></td> <td style="text-align: right">0.522</td> </tr> <tr> <td>Universe</td> <td style="text-align: center"><strong>0.877</strong></td> <td style="text-align: right">0.532</td> </tr> </tbody> </table> <p><strong>Qualitative Analysis:</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/ptlb_sample-480.webp 480w,/2026/assets/img/2026-11-25-fans/ptlb_sample-800.webp 800w,/2026/assets/img/2026-11-25-fans/ptlb_sample-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-11-25-fans/ptlb_sample.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/egm_sample-480.webp 480w,/2026/assets/img/2026-11-25-fans/egm_sample-800.webp 800w,/2026/assets/img/2026-11-25-fans/egm_sample-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-11-25-fans/egm_sample.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>On PLTB, standard DDPM exhibits catastrophic failure, generating samples visually indistinguishable from Gaussian noise, while FANS produces recognizable structures matching the data distribution. Quantitative analysis shows DDPM samples have near-zero correlation with real data features.</p> <p>For EGM, both methods generate stable images, but FANS preserves fine-grained textures absent in DDPM outputs. Specifically, the characteristic cross-hatched patterns present in training data are preserved by FANS but smoothed out by DDPM. This suggests FANS better captures high-frequency components critical for texture fidelity.</p> <p>To maintain the validity of our experiment all the hyperparameter setting for both the baseline and FANS training and sampling are kept identical. All experiments were performed with T = 1000 sampling steps</p> <p>To show that, the performance gains translates to real world setting as well, we show the performance comparison on real world datasets.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/Universe_sample-480.webp 480w,/2026/assets/img/2026-11-25-fans/Universe_sample-800.webp 800w,/2026/assets/img/2026-11-25-fans/Universe_sample-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-11-25-fans/Universe_sample.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Figure: Qualitative samples for Multimodal Universe Dataset</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-11-25-fans/texture_sample-480.webp 480w,/2026/assets/img/2026-11-25-fans/texture_sample-800.webp 800w,/2026/assets/img/2026-11-25-fans/texture_sample-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-11-25-fans/texture_sample.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Figure: Qualitative samples for Texture Dataset</p> <p>From the Figure we can see that FANS could capture the intricate details of the Multimodal Universe Dataset (FID: 10.003) while Baseline method couldn’t capture such intricacies (FID: 24.012). Similar observation can be made for the texture dataset. The baseline model intriduces a lot of atrifacts in the attempt to capture the texture details, while FANS could easilyt capture the intricate details of the dataset.</p> <p>To ensure that FANS is not only adapting to these high frequency dominated datasets, we capre the FID score of FANS with stand DDPM ( Baseline ) models on CIFAR10 and CelebA datasets.</p> <table> <thead> <tr> <th style="text-align: left">Schedule</th> <th style="text-align: left">CIFAR10 (50)</th> <th style="text-align: left">CIFAR10 (100)</th> <th style="text-align: left">CIFAR10 (200)</th> <th style="text-align: left">CIFAR10 (1000)</th> <th style="text-align: left">CelebA (50)</th> <th style="text-align: left">CelebA (100)</th> <th style="text-align: left">CelebA (200)</th> <th style="text-align: left">CelebA (1000)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>DDPM</strong></td> <td style="text-align: left">17.36</td> <td style="text-align: left">17.10</td> <td style="text-align: left">16.82</td> <td style="text-align: left">16.07</td> <td style="text-align: left">11.01</td> <td style="text-align: left">8.27</td> <td style="text-align: left">8.11</td> <td style="text-align: left">8.26</td> </tr> <tr> <td style="text-align: left"><strong>FANS</strong></td> <td style="text-align: left">16.08</td> <td style="text-align: left">16.11</td> <td style="text-align: left">15.04</td> <td style="text-align: left">14.19</td> <td style="text-align: left">13.18</td> <td style="text-align: left">12.10</td> <td style="text-align: left">10.15</td> <td style="text-align: left">10.10</td> </tr> </tbody> </table> <h2 id="conclusion">Conclusion</h2> <p>In this blog we aimed to analyse and present a principled approach to approach to addressing spectral bias in diffusion models through dynamic, dataset-aware noise scheduling. By leveraging the spectral characteristics of training data to construct frequency-dependent noise distributions, FANS enables models to allocate denoising capacity more efficiently across the frequency spectrum. Through rigorous experiments on synthetic datasets with known spectral characteristics (PLTB and EGM), we demonstrate that FANS consistently improves sample quality compared to vanilla DDPM baselines, particularly for datasets with pronounced high-frequency content. The method’s ability to learn dataset-specific frequency priorities and dynamically adjust noise shaping over time represents a meaningful step toward more adaptive and efficient diffusion training.</p> <p>However, our work also reveals important limitations and directions for future investigation. In the current implementation the computational overhead of spectral profiling and per-sample noise generation, while manageable, adds complexity to the training pipeline.</p> <p>Future work should focus on several key areas: extending FANS to high-resolution natural images and validating its benefits on large-scale datasets like ImageNet, exploring integration with modern architectures like diffusion transformers, and investigating the interplay between FANS and other recent advances such as flow matching and consistency models. Additionally, theoretical analysis of FANS’s convergence properties and its relationship to other forms of adaptive noise scheduling would strengthen the mathematical foundations of the approach.</p> <p>Despite these challenges, FANS demonstrates that incorporating dataset-specific spectral information into the noise generation process can meaningfully improve diffusion model training. As the field continues to push toward higher-resolution, higher-fidelity generation, methods that efficiently allocate model capacity across frequency bands will become increasingly important. We hope this work inspires further exploration of adaptive, data-driven approaches to noise scheduling in generative models.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-11-25-fans.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-long-context/">Text-as-Image, A Visual Encoding Approach for Long-Context Understanding</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/using-large-language-models-to-simulate-and-predict-human-decision-making/">Using Large Language Models to Simulate and Predict Human Decision-Making</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/useful-calibrated-uncertainties/">What (and What Not) are Calibrated Uncertainties Actually Useful for?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/unlearning-or-untraining/">Is your algorithm Unlearning or Untraining?</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>