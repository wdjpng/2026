<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Probabilistic Circuits for Uncertainty Quantification | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Deep learning models struggle with epistemic uncertainty quantification, often exhibiting blind confidence on out-of-distribution data. This work reviews on Probabilistic Circuits (PCs) as a versatile framework for rigorous, tractable reasoning. PCs model the joint probability distribution and by enforcing structural constraints, specifically smoothness, decomposability, and determinism, they allow for the exact computation of marginals, conditionals, and moments in polynomial time without retraining. We discuss on the suitability of PCs for Uncertainty Quantification, describing their advantages and highlighting their PCs for tractable UQ in high-dimensional problems."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/probabilistic-circuits-for-uncertainty-quantification/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Probabilistic Circuits for Uncertainty Quantification",
            "description": "Deep learning models struggle with epistemic uncertainty quantification, often exhibiting blind confidence on out-of-distribution data. This work reviews on Probabilistic Circuits (PCs) as a versatile framework for rigorous, tractable reasoning. PCs model the joint probability distribution and by enforcing structural constraints, specifically smoothness, decomposability, and determinism, they allow for the exact computation of marginals, conditionals, and moments in polynomial time without retraining. We discuss on the suitability of PCs for Uncertainty Quantification, describing their advantages and highlighting their PCs for tractable UQ in high-dimensional problems.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Probabilistic Circuits for Uncertainty Quantification</h1> <p>Deep learning models struggle with epistemic uncertainty quantification, often exhibiting blind confidence on out-of-distribution data. This work reviews on Probabilistic Circuits (PCs) as a versatile framework for rigorous, tractable reasoning. PCs model the joint probability distribution and by enforcing structural constraints, specifically smoothness, decomposability, and determinism, they allow for the exact computation of marginals, conditionals, and moments in polynomial time without retraining. We discuss on the suitability of PCs for Uncertainty Quantification, describing their advantages and highlighting their PCs for tractable UQ in high-dimensional problems.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#epistemic-uncertainty-in-today-s-machine-learning">Epistemic Uncertainty in Today's Machine Learning</a> </div> <div> <a href="#the-grammar-of-tractability">The Grammar of Tractability</a> </div> <div> <a href="#connecting-probabilistic-circuits-to-uncertainty-quantification">Connecting Probabilistic Circuits to Uncertainty Quantification</a> </div> <div> <a href="#scaling-circuit-architectures-to-high-dimensions">Scaling Circuit Architectures to High Dimensions</a> </div> <div> <a href="#applications-of-probabilistic-circuits-for-uq">Applications of Probabilistic Circuits for UQ</a> </div> <ul> <li> <a href="#probabilistic-flow-circuits">Probabilistic Flow Circuits</a> </li> <li> <a href="#multi-token-prediction-with-probabilistic-circuits">Multi-Token Prediction with Probabilistic Circuits</a> </li> <li> <a href="#spn-guided-latent-space-manipulation">SPN-Guided Latent Space Manipulation</a> </li> </ul> <div> <a href="#a-personal-note">A Personal Note</a> </div> </nav> </d-contents> <h2 id="epistemic-uncertainty-in-todays-machine-learning">Epistemic Uncertainty in Today’s Machine Learning</h2> <p>The trajectory of artificial intelligence over the last decade has been defined by a relentless pursuit of predictive accuracy, driven largely by the scaling of deep neural networks. From Large Language Models (LLMs) to generative diffusion systems, the capacity of these models to approximate complex functions is undeniable. However, as these systems migrate from controlled academic benchmarks to high-stakes deployment in healthcare, autonomous navigation, and climate modeling, a critical deficiency has emerged: the inability to reliably quantify what the model does not know.</p> <p>Standard deep learning architectures, despite their expressiveness, often suffer from ‘‘blind confidence’’. They act as black-box function approximators that map inputs to outputs without maintaining a rigorous representation of the underlying joint probability distribution. Consequently, when presented with out-of-distribution (OOD) data, such as a rare physiological anomaly in a patient or an unprecedented weather pattern, these models frequently yield predictions with confusingly high confidence. This phenomenon represents a challenge of epistemic uncertainty quantification (UQ).</p> <p>Uncertainty is generally categorized into two distinct forms: aleatoric uncertainty, which is irreducible and stems from the inherent stochasticity of the data generation process (e.g., sensor noise), and epistemic uncertainty, which is reducible and arises from a lack of knowledge about the model parameters or the true structure of the data<d-cite key="kimpton_challenges_2025,sullivan_introduction_2015"></d-cite>. In the context of scientific engineering and safety-critical AI, distinguishing between these two is paramount. A self-driving car must distinguish between the ‘‘noise’’ of a rainy sensor (aleatoric) and an object it has never been trained to recognize (epistemic).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/illustration_epistemic_vs_aleatoric_uncertainty-480.webp 480w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/illustration_epistemic_vs_aleatoric_uncertainty-800.webp 800w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/illustration_epistemic_vs_aleatoric_uncertainty-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/illustration_epistemic_vs_aleatoric_uncertainty.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Illustration of the difference between aleatoric and epistemic uncertainty. While aleatoric uncertainty arises from inherent data noise, epistemic uncertainty can be reduced by acquiring more samples. This is illustrated by the narrowing confidence interval around the samples. </div> <p>While methods such as Bayesian Neural Networks (BNNs) and Monte Carlo (MC) Dropout have attempted to retrofit uncertainty estimates onto deep networks, they often rely on approximate inference techniques that introduce their own variance and computational overhead<d-cite key="ventola_probabilistic_2023,gal_dropout_2016"></d-cite>. They provide approximations of the posterior, not exact evaluations.</p> <p>This report posits that Probabilistic Circuits offer a transformative solution to this epistemic challenge. Unlike standard neural networks, PCs are designed not merely to predict, but to represent the joint probability distribution of the data as a computational graph. Crucially, they do so while guaranteeing tractability. Through strict structural properties, i.e.,smoothness, decomposability, and determinism, PCs enable the exact computation of marginals, conditionals, and moments in polynomial time<d-cite key="choi_probabilistic_2020a,peharz_probabilistic_2023"></d-cite>. This capability fundamentally alters the UQ landscape, moving from approximate guesses of uncertainty to rigorous, mathematically guaranteed derivations.</p> <p>The following sections explore the <strong>theoretical</strong> mechanics that enable tractability, the architectural revolutions that have allowed PCs to <strong>scale to high-dimensional data</strong>, and the <strong>application</strong> of PCs to complex problems.</p> <p>PCs are no longer just a theoretical curiosity but a valuable component of the next generation of trustworthy AI.</p> <h2 id="the-grammar-of-tractability">The Grammar of Tractability</h2> <p>To understand the unique value of PCs for UQ, one must first appreciate the “grammar’’ of their construction. A PC is not simply a neural network with probabilistic outputs; it is a Directed Acyclic Graph (DAG) that encodes a probability distribution function (PDF) or probability mass function (PMF) through a hierarchy of specific computational units<d-cite key="choi_probabilistic_2020a,peharz_probabilistic_2023"></d-cite>.</p> <h3 id="the-computational-graph">The Computational Graph</h3> <p>At the fundamental level, a PC represents a joint distribution $P(\mathbf{X})$ over a set of random variables $\mathbf{X}$. The graph is composed of three primary types of nodes, each serving a distinct probabilistic function. <strong>Input Units</strong> (Leaves) are the building blocks of the circuit, representing simple, tractable distributions over a single variable or a small subset of variables. Common choices include Gaussian distributions for continuous data, Bernoulli or Categorical distributions for discrete data, or even piecewise polynomials. <strong>Sum Units</strong> ($\oplus$) compute a weighted sum of their children’s outputs. In the probabilistic interpretation, a sum node represents a mixture model<d-cite key="choi_probabilistic_2020a,peharz_probabilistic_2023"></d-cite>. It introduces a latent variable $Z$ that selects which branch of the mixture is active, thereby allowing the circuit to model multimodality and complex dependencies. <strong>Product Units</strong> ($\otimes$) compute the product of their children’s outputs. Probabilistically, product nodes represent factorizations, encoding independence assumptions between subsets of variables<d-cite key="choi_probabilistic_2020a,peharz_probabilistic_2023"></d-cite>. The value computed at the root of the PC for a given input configuration $\mathbf{x}$ corresponds to the (possibly unnormalized) probability density $P(\mathbf{x})$.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_example_pc_not_tractable-480.webp 480w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_example_pc_not_tractable-800.webp 800w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_example_pc_not_tractable-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_example_pc_not_tractable.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Example of a Probabilistic Circuit that is not tractable. The variables at the leaves indicate the random variables. This PC lacks the necessary structural properties to guarantee tractable inference, as the mixture (red) does not fulfill the smoothness property. Illustration taken from<d-cite key="choi_probabilistic_2020a"></d-cite>. </div> <h3 id="structural-constraints-for-tractability">Structural Constraints for Tractability</h3> <p>The distinguishing feature of PCs is the imposition of structural constraints on the graph topology. In generic graphical models like Bayesian Networks or Markov Random Fields, inference is often #P-hard, requiring exponential time in the worst case. PCs circumvent this by enforcing properties that ensure integrals and maximizations commute with the sum and product operations.</p> <p>However, tractability is no ‘‘universal property’’, as Choi et al. explain<d-cite key="choi_probabilistic_2020a"></d-cite>. It is merely dependent on the query and the model, adhering to structural properties. Choi et al. define tractability as follows<d-cite key="choi_probabilistic_2020a"></d-cite>: <em>A class of queries $\mathcal{Q}$ is tractable for a class of models $\mathcal{M}$ if any query $q \in \mathcal{Q}$ on model $m \in \mathcal{M}$ can be computed in time polynomial in the size of the model, i.e., $O(\text{poly}(|m|))$. Thus, $\mathcal{M}$ is a tractable representation for $\mathcal{Q}$.</em></p> <p>The tractability of different probabilistic queries relies on the structural properties of the PC, as summarized in the table below<d-cite key="choi_probabilistic_2020a,peharz_probabilistic_2023,sidheekh_building_2024,zhang_restructuring_2025"></d-cite>:</p> <table> <thead> <tr> <th style="text-align: left">Structural Property</th> <th style="text-align: left">Enabled Query</th> <th style="text-align: center">Mathematical Operation</th> <th style="text-align: left">UQ Application</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Smoothness</strong></td> <td style="text-align: left">Evidence</td> <td style="text-align: center">$P(e)$</td> <td style="text-align: left">Anomaly Detection, OOD Detection</td> </tr> <tr> <td style="text-align: left"> <strong>Decomposability</strong> (+ Smoothness)</td> <td style="text-align: left">Marginals</td> <td style="text-align: center">$\int P(x_{\text{obs}}, x_{\text{miss}}) \, dx_{\text{miss}}$</td> <td style="text-align: left">Missing Data Imputation, Partial Evidence</td> </tr> <tr> <td style="text-align: left"><strong>Decomposability + Smoothness</strong></td> <td style="text-align: left">Conditionals</td> <td style="text-align: center">$P(q \mid e) = \frac{P(q,e)}{P(e)}$</td> <td style="text-align: left">Counterfactuals, Conditional Forecasting</td> </tr> <tr> <td style="text-align: left"><strong>Determinism</strong></td> <td style="text-align: left">MAP Inference</td> <td style="text-align: center">$\underset{x}{\mathrm{argmax}} \, P(x)$</td> <td style="text-align: left">Image Reconstruction, Most Likely Explanation</td> </tr> <tr> <td style="text-align: left"><strong>Structured Decomposability</strong></td> <td style="text-align: left">Circuit Multiplication</td> <td style="text-align: center">$P(x) \cdot Q(x)$</td> <td style="text-align: left">KL Divergence, Bayesian Updating, Ensemble Merging</td> </tr> </tbody> </table> <h4 id="smoothness">Smoothness</h4> <p>A sum node is defined as smooth (or complete) if all of its children define distributions over the exact same set of variables, known as the scope<d-cite key="choi_probabilistic_2020a,peharz_probabilistic_2023,sidheekh_building_2024"></d-cite>.</p> <p>Smoothness ensures that the sum node represents a valid mixture distribution where the weights sum to unity (or a normalizing constant). If a sum node were non-smooth, meaning one child covered variables ${X_1, X_2}$ and another covered only ${X_1}$, the resulting function would not integrate to a consistent value, as the missing variable $X_2$ in the second branch is unaccounted for. Smoothness guarantees that when we perform marginalization (integrating out a variable), the integral distributes linearly over the sum. This property is what allows PCs to handle missing data naturally: the probability mass of the missing variables integrates to 1 in every branch of a smooth sum node, effectively vanishing from the computation without disrupting the validity of the distribution over the observed variables<d-cite key="choi_probabilistic_2020a,peharz_probabilistic_2023"></d-cite>.</p> <h4 id="decomposability">Decomposability</h4> <p>A product node is decomposable if its children define distributions over disjoint sets of variables<d-cite key="choi_probabilistic_2020a,peharz_probabilistic_2023,sidheekh_building_2024"></d-cite>.</p> <p>Decomposability is the structural encoding of conditional independence. It allows high-dimensional integrals to break down into products of lower-dimensional integrals. Mathematically, if a function $f(\mathbf{x})$ decomposes into $g(\mathbf{y})h(\mathbf{z})$ where $\mathbf{y}$ and $\mathbf{z}$ are disjoint, then the integral $\int f(\mathbf{x}) d\mathbf{x} = (\int g(\mathbf{y}) d\mathbf{y}) (\int h(\mathbf{z}) d\mathbf{z})$. Without decomposability, the integral would require evaluating the full high-dimensional joint space, which is computationally intractable. This property ensures that marginal inference in a PC is linear in the size of the circuit, providing a distinct advantage over Normalizing Flows or VAEs where marginals are often intractable<d-cite key="martires_probabilistic_2024"></d-cite>.</p> <h4 id="determinism">Determinism</h4> <p>A sum node is deterministic if, for any complete input configuration, at most one of its children evaluates to a non-zero value<d-cite key="choi_probabilistic_2020a,peharz_probabilistic_2023"></d-cite>.</p> <p>While smoothness and decomposability are sufficient for marginal inference, determinism unlocks tractable Maximum A Posteriori (MAP) inference. The MAP query asks for the most probable configuration of variables given evidence: $\text{argmax}_{\mathbf{x}} P(\mathbf{x} | \mathbf{e})$. In general, maximizing a sum of functions (a mixture) is hard because the mode could lie anywhere between the modes of the components. However, if the sum is deterministic, only one component is non-zero for any $\mathbf{x}$. Consequently, the maximum of the sum becomes the maximum of the non-zero component: $\max \sum f_i(\mathbf{x}) = \sum \max f_i(\mathbf{x})$. This allows the $\max$ operator to push down through sum nodes just as integrals push down through smooth sum nodes. This property is critical for tasks like image inpainting or finding the most likely explanation for a medical diagnosis.</p> <h3 id="structured-decomposability-and-vtrees">Structured Decomposability and Vtrees</h3> <p>Recent research has emphasized a more rigorous constraint known as structured decomposability. A PC is structured-decomposable if the decomposition of variables at every product node follows a hierarchical tree structure over the variables, known as a vtree<d-cite key="zhang_restructuring_2025"></d-cite>.</p> <p>A vtree is a static binary tree where leaves correspond to random variables. A structured PC respects this vtree if every product node in the circuit corresponds to a node in the vtree, partitioning the variables exactly as the vtree does. This property is profound because it enables operations beyond simple inference, such as the efficient multiplication of two circuits. If two PCs respect the same vtree, their product (which represents the product of their densities) remains a structured PC. This algebra of circuits allows for computing KL divergences, merging expert models, and performing Bayesian updates in polynomial time<d-cite key="choi_probabilistic_2020a,zhang_restructuring_2025"></d-cite>.</p> <h2 id="connecting-probabilistic-circuits-to-uncertainty-quantification">Connecting Probabilistic Circuits to Uncertainty Quantification</h2> <p>The theoretical properties of PCs translate directly into capabilities that solve fundamental challenges in UQ. While deep learning models often struggle to distinguish between low-probability events and model errors, PCs provide exact probabilistic metrics.</p> <h3 id="arbitrary-conditioning">Arbitrary Conditioning</h3> <p>The ability to compute exact conditional probabilities $P(\mathbf{Q} \mid \mathbf{E})$ for any disjoint subsets of query variables $\mathbf{Q}$ and evidence variables $\mathbf{E}$ is perhaps the most significant operational advantage of PCs.</p> <p>In standard deep learning, UQ is typically tied to a specific prediction task. A model is trained to predict $Y$ given $X$. If the user suddenly needs to know the probability of a specific input feature $X_1$ given the output $Y$ (an inverse query), the model cannot provide it without retraining or complex inversion techniques. PCs, by modeling the full joint distribution, are agnostic to the direction of inference.</p> <p>This capability is particularly vital for handling missing data. In real-world scientific engineering, sensor failure is common. When a standard neural network encounters missing inputs, it usually requires imputation, guessing the missing values, before processing. This imputation introduces a point estimate that ignores the uncertainty of the missing value. A PC, conversely, handles missing data by integrating out the missing variables analytically<d-cite key="choi_probabilistic_2020a,peharz_probabilistic_2023,sidheekh_building_2024"></d-cite>. The resulting marginal distribution over the observed variables reflects the true uncertainty: the probability density becomes ‘‘flatter’’ or more diffuse, accurately capturing the loss of information. This is not an approximation but the belief given partial evidence.</p> <h3 id="tractable-dropout-inference">Tractable Dropout Inference</h3> <p>Recent work has bridged the gap between the popular UQ technique of MC Dropout and the rigorous world of PCs. MC Dropout estimates uncertainty in neural networks by randomly dropping units during inference and measuring the variance of the predictions. While effective, it is computationally expensive (requiring multiple forward passes) and yields only an empirical approximation.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_tdi-480.webp 480w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_tdi-800.webp 800w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_tdi-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_tdi.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Comparisson of forward passes for a standard PC (a), a PC with MC Dropout sampling (b), and a PC using TDI via variance propagation (c). MCD requires multiple forward passes, each sampling one instantiation of a possible subgraph. In contrast, TDI analytically propagates variances through the graph in a single pass. Illustration taken from<d-cite key="ventola_probabilistic_2023"></d-cite>. </div> <p>Ventola et al. have introduced Tractable Dropout Inference (TDI) for PCs<d-cite key="ventola_probabilistic_2023"></d-cite>. Because PCs track the propagation of moments exactly, it is possible to derive the analytical moments of the output distribution under the dropout noise model in a single forward pass. Instead of sampling dropout masks, TDI propagates the first and second moments (mean and variance) through the sum and product nodes.</p> <p>For a sum node, the mean is the weighted sum of children’s means. The variance computation involves the variances of children plus a term accounting for the variance of the gating weights themselves (if they are stochastic). For product nodes, due to independence (decomposability), the mean is the product of means, and the variance follows standard variance-of-product rules.</p> <p>This allows PCs to provide ‘‘dropout-based’’ uncertainty estimates that are theoretically sound and computationally efficient, eliminating the sampling noise inherent in standard MC Dropout. This technique has been shown to significantly improve the robustness of PCs to distribution shifts and OOD data<d-cite key="ventola_probabilistic_2023"></d-cite>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_tdi_id_vs_ood-480.webp 480w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_tdi_id_vs_ood-800.webp 800w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_tdi_id_vs_ood-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_tdi_id_vs_ood.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> In-Distributino (ID) vs. OOD detection precision difference for PCs (dashed) and PCs with TDI (solid) across different thresholds on different tasks. TDI improves both absolute performance and the ID/OOD trade-off. Without TDI, PCs perform poorly and peak at counterintuitively low thresholds. Illustration obtained from<d-cite key="ventola_probabilistic_2023"></d-cite>. </div> <h3 id="sequential-uncertainty-in-time-series">Sequential Uncertainty in Time Series</h3> <p>In temporal domains, uncertainty accumulates over time. A forecasting model should become less confident the further it predicts into the future. Standard Recurrent Neural Networks (RNNs) often fail to capture this diverging uncertainty.</p> <p>Integrating PCs with recurrent architectures, such as in Recurrent Conditional Whittle Networks (RECOWN), provides a mechanism to quantify this temporal uncertainty<d-cite key="thoma_recowns_2021"></d-cite>. These models use a PC, i.e. a Conditional Whittle SPN, to model the distribution of the spectral coefficients of the time series.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_recown_architecture-480.webp 480w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_recown_architecture-800.webp 800w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_recown_architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_recown_architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The figure shows the architecture of the RECOWN model. The weights of the Conditional Whittle SPN (CDSPN) are determined by a neural network using a short time fourier transform (STFT) on the context $X$. The same STFT is fed to the gated recurrent units of an RNN, which predicts the Fourier coefficients $D^{\prime i}$. The CWSPN computes the conditional Whittle log-likelihood based on such Fourier coefficients. Illustration obtained from<d-cite key="thoma_recowns_2021"></d-cite>. </div> <p>This integration allows for the computation of the Log-Likelihood Ratio Score (LLRS), a dynamic uncertainty metric. When the model encounters a sequence that deviates from the learned temporal dynamics, such as a sudden frequency shift in power grid data, the conditional likelihood computed by the PC drops sharply. This score allows operators to distinguish between a ‘‘hard to predict’’ stochastic sequence (high aleatoric uncertainty) and a ‘‘structurally novel’’ sequence (high epistemic uncertainty), enabling trustworthy anomaly detection in time-series data<d-cite key="thoma_recowns_2021"></d-cite>.</p> <h3 id="probabilistic-calibrated-circuits">Probabilistic Calibrated Circuits</h3> <p>While PCs offer exact and efficient inference, a critical problem of calibration arises when applying them to UQ. The standard training objective for PCs is to approximate the joint distribution $P(X,Y)$ by minimizing the negative log-likelihood, $\mathcal{L}_{\text{NLL}} = - \mathbb{E}_{\text{data}} [\log P_{\text{PC}}(x, y)]$. However, accurate UQ, in the various forms discussed earlier, requires well-calibrated conditional distributions, which are computed only as a secondary step in PCs. Since the training objective prioritizes the joint likelihood, it does not guarantee that the conditionals are calibrated. This misalignment can lead to systematic errors in uncertainty estimate. The model might capture the global density well but fail to accurately reflect the confidence in specific predictions.</p> <p>The authors’ current work addresses this miscalibration by answering two key questions: how can systematic calibration error be quantified, and how can it be mitigated? In response, they introduce Probabilistic Calibrated Circuits (PCCs), a novel post-hoc recalibration technique that provably retains the structure and tractability of PCs while reducing miscalibration. This section provides a high-level overview of the core concepts, while a more comprehensive treatment will be presented in an upcoming publication.</p> <h4 id="quantifying-the-miscalibration">Quantifying the Miscalibration</h4> <p>To diagnose the extent of miscalibration, Probability Integral Transform (PIT) can be utilized. For a continuous random variable $X$ and its cumulative distribution function (CDF) $F_X$, the random variable $Z = F_X(X)$ is uniformly distributed $Z \sim \mathcal{U}(0, 1)$.</p> <p>The calibration of the conditional distribution $p(x\mid y)$ can be checked by calculating the PIT values for a recalibration dataset $(x_i, y_i),\ i = 1,\dots,N$</p> \[z_i = F_{\text{PC}}(x_i \mid y_i) = \int_{-\infty}^{x_i} p_{\text{PC}}(x' \mid y_i) dx',\ i = 1, \dots, N\] <p>Thanks to the properties of PCs (marginalization and integration), the calculation of $F_{PC}$ is exact and efficient. The resulting histogram of PIT values reveals the nature of the error:</p> <ul> <li>Uniform Distribution: The model is <strong>perfectly calibrated</strong>.</li> <li>U-Shape: The model is <strong>under-dispersed</strong> (distribution too narrow, uncertainty underestimated).</li> <li>Inverted U-Shape: The model is <strong>over-dispersed</strong> (distribution too broad, uncertainty overestimated).</li> <li>Systematic Shift: The mean of the prediction is <strong>systematically incorrect</strong>.</li> </ul> <p>To formally quantify the deviation from uniformity, we can calculate the distance between the empirical CDF of the PIT values, $\hat{F}_{\text{cal}}$, and the ideal uniform CDF. This calibration error, $E_{\text{cal}} = d(\hat{F}_{\text{cal}}(z),\mathcal{U}(0,1))$, can serve as the objective function for optimization in PCCs.</p> <h2 id="scaling-circuit-architectures-to-high-dimensions">Scaling Circuit Architectures to High Dimensions</h2> <p>For years, a major criticism of Probabilistic Circuits (PCs) was their lack of scalability. Although they provided exact inference, they struggled to model the complex, high-dimensional dependencies found in data like images or natural language—a domain where Deep Neural Networks (DNNs) excelled. This led to the prevailing belief in a rigid trade-off: one could have tractability (PCs) or expressiveness (DNNs), but not both. However, architectural innovations and new computational frameworks developed in recent research (2023–2025) have largely dismantled this dichotomy, enabling PCs to scale massively<d-cite key="peharz_einsum_2020,peharz_probabilistic_2023,liu_scaling_2025,zhang_scaling_2025"></d-cite>.</p> <h3 id="scaling-via-vectorization">Scaling via Vectorization</h3> <p>The traditional implementation of PCs involved sparse, irregular graph structures that were essentially pointer-chasing operations. This is efficient on CPUs for small models but not suited for modern GPUs, which rely on dense matrix multiplications and coherent memory access.</p> <p>Einsum Networks (EiNets) represent a paradigm shift in PC implementation<d-cite key="peharz_einsum_2020,peharz_probabilistic_2023"></d-cite>. The core insight of EiNets is to reformulate the execution of sum and product layers using the Einstein summation (einsum) convention, a standard operation in tensor algebra libraries like PyTorch and TensorFlow.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_pc_layering-480.webp 480w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_pc_layering-800.webp 800w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_pc_layering-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_pc_layering.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> An illustration of grouping nodes with the same topological depth into disjoint subsets. The forward- and backward-pass can be carried out independently on nodes within the same layer / subset. Illustration obtained from<d-cite key="liu_scaling_2025"></d-cite>. </div> <p>Instead of processing nodes individually, EiNets organize nodes into layers. A ‘‘product layer’’ can be viewed as a mixing operation that can be computed via element-wise multiplication and reshaping of large tensors. A ‘‘sum layer’’ becomes a tensor contraction (matrix multiplication).</p> <p>By combining these monolithic tensor operations, EiNets allow PCs to utilize the massive parallelism of GPUs. This vectorization enables the training of PCs with millions of parameters and hundreds of layers, achieving density estimation performance on benchmarks like ImageNet that rivals intractable deep generative models<d-cite key="liu_scaling_2025"></d-cite>.</p> <h3 id="scaling-via-sparse-monarch-metrics">Scaling via Sparse Monarch Metrics</h3> <p>While EiNets solved the computation speed, parameter efficiency remained a challenge. Fully dense sum layers imply that every latent component connects to every child, leading to quadratic parameter growth.</p> <p>Recent work has introduced Monarch Matrices to parameterize the sum blocks in PCs<d-cite key="zhang_scaling_2025"></d-cite>. Monarch matrices are a class of structured sparse matrices that are highly expressive (capable of representing permutations and Fast Fourier Transforms) yet computationally efficient.</p> <p>By replacing dense weight matrices in sum layers with products of sparse Monarch factors, Zhang et al. have reduced the memory and computation footprint of PCs significantly.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_monarch_hclt-480.webp 480w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_monarch_hclt-800.webp 800w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_monarch_hclt-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_monarch_hclt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The figure shows the bits-per-dimension (BPD) as a function of the FLOPs during training per pixel. The Hidden Chow-Liu Tree (HCLT) serves as base architecture. As the computational budget increases, the monarch-version of the HCLT shows a better efficiency as the default version. Illustration obtained from<d-cite key="zhang_scaling_2025"></d-cite>. </div> <p>This Monarch Parameterization approach has enabled ‘‘unprecedented scaling’’, allowing PCs to achieve state-of-the-art generative modeling performance on challenging benchmarks like Text8 and ImageNet 32x32, demonstrating superior scaling laws (better performance for fewer FLOPs) compared to traditional dense parameterizations<d-cite key="zhang_scaling_2025"></d-cite>.</p> <h3 id="restructuring-of-fitted-circuits">Restructuring of Fitted Circuits</h3> <p>A historical limitation of PCs was ‘‘structure lock-in’’. Once a PC was trained with a specific vtree (variable decomposition), it was difficult to perform operations with other circuits having different structures.</p> <p>New algorithms for restructuring PCs have emerged<d-cite key="zhang_restructuring_2025"></d-cite>. These algorithms allow a structured-decomposable PC to be transformed into a new PC that respects a target vtree while representing the same distribution.</p> <p>The restructuring process involves converting the original PC into an equivalent Bayesian Network with latent variables, identifying the conditional independencies required by the target vtree, and then recursively constructing the new circuit layers.</p> <p>This breakthrough allows for dynamic inference optimization. A large, complex PC trained for high expressiveness can be ‘‘compiled’’ or restructured into a shallower, optimized circuit for faster inference on edge devices. It also enables the multiplication of circuits with different structures, which is essential for ensemble methods where different models might learn different structural dependencies<d-cite key="zhang_restructuring_2025"></d-cite>.</p> <h2 id="applications-of-probabilistic-circuits-for-uq">Applications of Probabilistic Circuits for UQ</h2> <p>To conclude this post, we want to discuss some recent applications of PCs for UQ. The frontier of PC research is no longer about replacing neural networks but integrating with them. The concept of Probabilistic Neural Circuits (PNCs) has emerged, blending the learnable features of deep learning with the tractable reasoning of circuits<d-cite key="martires_probabilistic_2024"></d-cite>.</p> <h3 id="probabilistic-flow-circuits">Probabilistic Flow Circuits</h3> <p>A promising hybrid architecture involves integrating Probabilistic Circuits with Normalizing Flows (NFs)<d-cite key="sidheekh_probabilistic_2023"></d-cite>. This synergy addresses the complementary limitations of both architectures: Normalizing Flows are highly effective at modeling continuous, local correlations through diffeomorphic transformations but lack mechanisms for handling global discrete structure and efficient marginalization. Conversely, PCs excel at capturing global structure via mixture models and performing exact marginalization but can be inefficient at modeling complex local continuous manifolds, often requiring an excessive number of mixture components to approximate them.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_probabilistic_flow_circuits-480.webp 480w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_probabilistic_flow_circuits-800.webp 800w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_probabilistic_flow_circuits-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_probabilistic_flow_circuits.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The illustraion shows a comparisson between a PC and a PFC on a donut-shaped task. The blue and red colors depict the distributions captured by the leaf nodes while the green dots illustrated the joint distribution. The PFC is able to capture the target data better, due to its multi-modal leaf densities. Illustration obtained from<d-cite key="sidheekh_probabilistic_2023"></d-cite>. </div> <p>Probabilistic Flow Circuits (PFCs) bridge this gap by replacing the standard univariate leaf distributions of a PC with flexible, invertible flow transformations. This integration yields a powerful dual-structure model. The PC backbone manages the multimodal, discrete structure of the data (such as distinct object categories in an image), while the flow-based leaves model the continuous manifold of variations (such as pixel intensities) within each category. A naive integration of flows would violate the decomposability required for tractable inference, as flows inherently couple variables. To preserve the circuit’s marginalization guarantees, recent theoretical work introduces structural constraints such as $\tau$-decomposability<d-cite key="sidheekh_building_2024"></d-cite>. These conditions ensure that flow transformations are applied only to disjoint subsets of variables in a way that does not entangle the global independence structure maintained by the circuit. This architecture enables PFCs to improve density estimation performance while retaining the capability to answer complex probabilistic queries, such as marginals and conditionals, that are typically intractable for standalone Normalizing Flows.</p> <p>Interestingly, despite originating from distinct research motivations, the authors work on Probabilistic Calibrated Circuits (discussed above) can be viewed as a specialized subclass of Probabilistic Flow Circuits. While PFCs generally employ flows to enhance the flexibility of density estimation, PCCs utilize specific monotonic transformations at the leaves to minimize calibration error. Thus, the PCC framework effectively instantiates a Probabilistic Flow Circuit where the flow transformations are constrained by the objective of post-hoc uncertainty calibration.</p> <h3 id="multi-token-prediction-with-probabilistic-circuits">Multi-Token Prediction with Probabilistic Circuits</h3> <p>Another interesting application and perhaps the most high-impact application of PCs in 2025 involves their integration into the training and inference of Large Language Models (LLMs), specifically in the domain of Multi-Token Prediction (MTP).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_mtpc_structures-480.webp 480w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_mtpc_structures-800.webp 800w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_mtpc_structures-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_mtpc_structures.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Different dependency structures over sequences of tokens. The input tokens are grouped into the coloured layers. Illustration taken from<d-cite key="grivas_fast_2025"></d-cite>. </div> <p>Standard LLMs are autoregressive, that is, they predict the next token $x_{t+1}$ given the history $x_{1:t}$. Generating a sequence of length $L$ requires $L$ sequential forward passes, a process bound by memory bandwidth and thus inherently slow. Speculative Decoding addresses this by employing a lightweight “draft” model to predict a chunk of $K$ tokens, which are subsequently verified in parallel by the large target model. However, conventional draft models often prioritize speed over expressiveness by assuming independence among the $K$ predicted tokens. This approximation sacrifices accuracy, leading to lower acceptance rates and reduced speedups.</p> <p>To bridge this gap, recent work introduces MTP with PCs (MTPC)<d-cite key="grivas_fast_2025"></d-cite>. This framework utilizes a PC to model the joint distribution of the next $K$ tokens, $P(x_{t+1}, \dots, x_{t+K} \mid x_{1:t})$.</p> <p>PCs are uniquely suited for this task because they offer a tractable mechanism to model complex dependencies between future tokens (e.g., capturing that “San” strongly implies “Francisco”) without the computational overhead of a full Transformer. A PC can evaluate the likelihood of candidate sequences with extreme efficiency.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_mtpc-480.webp 480w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_mtpc-800.webp 800w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_mtpc-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_mtpc.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The authors explored the trade-off between latency and expressiveness with different MTP designs with respect to different PC architectures (FF, CP, HMM, BTree), as well as shared layers between draft and varifying models in self-speculative decoding. Illustration taken from<d-cite key="grivas_fast_2025"></d-cite>. </div> <p>The MTPC framework systematically explores a spectrum of PC architectures to balance expressiveness and latency:</p> <ul> <li>Fully Factorized (FF): The baseline approach that assumes independence between tokens. While fast, it suffers from low acceptance rates due to its inability to model correlations.</li> <li>Canonical Polyadic (CP): Introduces a single latent variable $Z$ with $r$ states to model the joint distribution as a mixture of independent components. This captures shared global context through the mixture weights while keeping the structure shallow for fast inference.</li> <li>Hidden Markov Models (HMM): A deeper PC structure that models sequential dependencies explicitly via latent state transitions $z_1 \to z_2 \to \dots \to z_K$. This offers higher expressiveness by capturing local dependencies between adjacent tokens but incurs increased latency due to the sequential summation.</li> <li>Binary Tree Factorizations (BTree): A novel hierarchical structure that recursively splits the token window. This architecture strikes an optimal balance between depth and width, efficiently capturing long-range dependencies among the $K$ tokens while enabling parallel sampling.</li> </ul> <p>Empirical evaluations retrofitting EvaByte, a byte-level LLM, with MTPC demonstrate the efficacy of this approach<d-cite key="grivas_fast_2025"></d-cite>. MTPC increases generation throughput by a factor of $5.47$ compared to standard autoregressive generation, achieving a $1.22$ speedup over MTP baselines that rely on independence assumptions. This significant performance gain stems from the fact that the PC-based draft model is sufficiently expressive to generate high-quality drafts, which are accepted by the verifier model significantly more often than those from independent drafters. These results establish PCs as a viable, real-time accelerator for foundation models, capable of efficiently capturing the local correlations of language and byte sequences.</p> <h3 id="spn-guided-latent-space-manipulation">SPN-Guided Latent Space Manipulation</h3> <p>In the domain of medical AI, “explainability” is not merely a desirable feature but a crucial safety requirement. Clinicians need to understand the rationale behind a model’s decision, for instance, why a specific scan was diagnosed as containing a tumor. A powerful technique for providing such insights is the generation of counterfactual explanations: answering the question, “What would this scan look like if the patient were healthy?”</p> <p>However, generating plausible counterfactuals is challenging; standard methods often produce unrealistic images that trick the classifier but are medically meaningless. To address this, Siekiera et al. propose SPN-Guided Latent Space Manipulation, utilizing the rigorous density estimation capabilities of PCs to generate high-fidelity counterfactuals<d-cite key="siekiera_counterfactual_2025"></d-cite>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_spn_latent_vae-480.webp 480w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_spn_latent_vae-800.webp 800w,/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_spn_latent_vae-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-probabilistic-circuits-for-uncertainty-quantification/screenshot_spn_latent_vae.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The illustration shows the use of a SPN to model the latent distribution of a VAE. Based on the counterfactual-frontier ($\mathbf{z}\_{cf}$), plausuble counterfactial embeddings are found. By propagating such through the decoder, counterfactual examples are generated. Illustration obtained from<d-cite key="siekiera_counterfactual_2025"></d-cite>. </div> <p>The approach is built upon a hybrid architecture that integrates an SPN into the latent space of a semi-supervised Variational Autoencoder (VAE). First, a VAE is trained to compress high-dimensional medical images into a lower-dimensional latent space $\mathbf{z}$. Unlike standard VAEs, which assume a simple Gaussian prior and often fail to capture complex data manifolds, this method employs an SPN to model the true, complex distribution of latent vectors, $P_{\text{SPN}}(\mathbf{z} \mid y)$, where $y$ denotes the class label (e.g., “Healthy” or “Sick”). The SPN serves a dual purpose: it acts as a flexible prior describing the latent topology and as a classifier $P(y \mid \mathbf{z})$, ensuring the latent space is structured according to the diagnostic classes.</p> <p>To generate a counterfactual for a patient diagnosed as “Sick” ($y_{\text{orig}}$), the model optimizes a new latent vector $\mathbf{z}_{cf}$ that flips the classification to “Healthy” ($y_{\text{target}}$). This optimization is governed by three competing objectives: validity, proximity, and plausibility. First, the model maximizes the SPN’s predicted probability for the target class, $P_{\text{SPN}}(y_{\text{target}} \mid \mathbf{z}_{cf})$, to ensure the diagnosis changes. Second, it minimizes the distance $\vert \mathbf{z}_{cf} - \mathbf{z}_{\text{orig}} \vert$ to guarantee that the counterfactual remains semantically similar to the original patient scan. Finally, and crucially, the optimization maximizes the likelihood of the vector under the SPN prior, $P_{\text{SPN}}(\mathbf{z}_{cf})$. This constraint forces the search into the high-density regions of the “Healthy” distribution, effectively preventing the generation of OOD or hallucinated samples.</p> <p>Experiments on the CheXpert dataset demonstrate that this SPN-guided approach produces anatomically plausible alterations, such as the specific removal of lung opacities, that are far more stable and interpretable than those generated by baseline DNN methods. While simple MLP classifiers require aggressive regularization to avoid adversarial noise, the SPN’s robust density modeling naturally guides the generation toward semantically meaningful counterfactuals without such fragile tuning.</p> <h2 id="a-personal-note">A Personal Note</h2> <p>To conclude, we have shown how recent advancements have positioned PCs as a scalable and powerful solution for UQ. These models not only resemble a robust, tractable method for assessing uncertainty but also enable innovative applications across diverse fields, highlighted by a few examples above. We anticipate a particularly transformative role for PCs in the future, specifically their integration into live systems to boost reliability and provide deeper interpretability. We hope this overview stimulates further research and practical deployment of PCs for UQ. Please note, this post is by far not a comprehensive survey of all works in this rapidly evolving field. It is mere a selection of key ideas and applications that we found particularly interesting and illustrative.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-probabilistic-circuits-for-uncertainty-quantification.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-long-context/">Text-as-Image, A Visual Encoding Approach for Long-Context Understanding</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/vis-llm-latent-geometry/">Visualizing LLM Latent Space Geometry Through Dimensionality Reduction</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/unigramlm-manual/">UnigramLM - An Attempt at Writing the Missing Manual</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/symbolic-connect/">From Dense Monoliths to Modular Minds: The Rise of Symbolic Routing in LLMs</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>