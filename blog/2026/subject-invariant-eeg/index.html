<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Decoupling Hypothesis: Attempting Subject-Invariant EEG Representation Learning via Auxiliary Injection | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="We explore several ideas for learning subject-invariant EEG representations for reaction time and psychopathology prediction using only 2-second windows in the NeurIPS 2025 EEG Challenge. The core of our approach is the Decoupling Hypothesis: an autoencoder framework where we attempt to disentangle subject-specific artifacts and long-term temporal trends (such as fatigue) from the neural signal by explicitly injecting 'nuisance' variables (like demographics and sequence position) into the decoder. This method aimed to force a purely convolutional encoder to learn slow, sequential features without relying on computationally expensive Recurrent or Attention mechanisms. This blog discusses the ideas that seemed promising but ultimately did not work as intended—and why."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/subject-invariant-eeg/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "The Decoupling Hypothesis: Attempting Subject-Invariant EEG Representation Learning via Auxiliary Injection",
            "description": "We explore several ideas for learning subject-invariant EEG representations for reaction time and psychopathology prediction using only 2-second windows in the NeurIPS 2025 EEG Challenge. The core of our approach is the Decoupling Hypothesis: an autoencoder framework where we attempt to disentangle subject-specific artifacts and long-term temporal trends (such as fatigue) from the neural signal by explicitly injecting 'nuisance' variables (like demographics and sequence position) into the decoder. This method aimed to force a purely convolutional encoder to learn slow, sequential features without relying on computationally expensive Recurrent or Attention mechanisms. This blog discusses the ideas that seemed promising but ultimately did not work as intended—and why.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "https://www.Anonymous.com/",
                "affiliations": [
                  {
                    "name": "Anonymous",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>The Decoupling Hypothesis: Attempting Subject-Invariant EEG Representation Learning via Auxiliary Injection</h1> <p>We explore several ideas for learning subject-invariant EEG representations for reaction time and psychopathology prediction using only 2-second windows in the NeurIPS 2025 EEG Challenge. The core of our approach is the Decoupling Hypothesis: an autoencoder framework where we attempt to disentangle subject-specific artifacts and long-term temporal trends (such as fatigue) from the neural signal by explicitly injecting 'nuisance' variables (like demographics and sequence position) into the decoder. This method aimed to force a purely convolutional encoder to learn slow, sequential features without relying on computationally expensive Recurrent or Attention mechanisms. This blog discusses the ideas that seemed promising but ultimately did not work as intended—and why.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#the-hbn-dataset-challenge">The HBN Dataset &amp; Challenge</a> </div> <div> <a href="#existing-self-supervised-pretraining-approaches-for-eeg-and-our-hypothesis">Existing Self-Supervised Pretraining Approaches for EEG and Our Hypothesis</a> </div> <div> <a href="#methodology">Methodology</a> </div> <ul> <li> <a href="#architectural-motivation">Architectural Motivation</a> </li> <li> <a href="#the-encoder-design">The Encoder Design</a> </li> <li> <a href="#auxiliary-injection-disentanglement">Auxiliary Injection &amp; Disentanglement</a> </li> <li> <a href="#spatial-scaling-via-adjacency">Spatial Scaling via Adjacency</a> </li> <li> <a href="#multi-task-pseudo-labeling">Multi-Task Pseudo-Labeling</a> </li> </ul> <div> <a href="#loss-landscape">Loss Landscape</a> </div> <div> <a href="#results-retrospective">Results &amp; Retrospective</a> </div> <ul> <li> <a href="#performance">Performance</a> </li> <li> <a href="#what-didn-t-work-and-why">What didn't work (and why)</a> </li> <li> <a href="#future-steps">Future Steps</a> </li> </ul> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Decoding the human brain is difficult; decoding it from short, noisy snippets of electrical activity is even harder.</p> <p>We recently participated in the NeurIPS 2025 EEG Benchmark Competition <d-cite key="aristimunha2025eeg"></d-cite>, a rigorous challenge designed to test the limits of EEG representation learning. The goal was to predict behavioral metrics (Reaction Times) and clinical diagnostics (Psychopathology Factors) using only 2-second windows of raw EEG signals.</p> <p>Out of many participating teams, our experimental architecture achieved:</p> <ul> <li> <strong>Rank 54</strong> in Challenge 1 (Reaction Time Prediction)</li> <li> <strong>Rank 16</strong> in Challenge 2 (Psychopathology Prediction)</li> </ul> <p>While we didn’t take the top spot, our approach attempted to solve a fundamental problem in neuroscience: <strong>Subject Invariance</strong>. A model trained on raw signals often overfits to subject-specific artifacts (like skull thickness or hair density) rather than learning the underlying neural dynamics required for downstream tasks.</p> <p>The core challenge with short, 2-second windows is capturing slow morphological trends in the EEG. While a 200-sample window (at 100 Hz) easily captures higher frequencies like Beta (13-30 Hz) or Alpha (8-12 Hz), it fundamentally limits the resolution of slow waves such as deep sleep Delta waves (~0.5–2 Hz).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-subject-invariant-eeg/brainwaves-480.webp 480w,/2026/assets/img/2026-04-27-subject-invariant-eeg/brainwaves-800.webp 800w,/2026/assets/img/2026-04-27-subject-invariant-eeg/brainwaves-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-subject-invariant-eeg/brainwaves.png" class="img-fluid h-75" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 0: Visualization of different types of brainwaves, which are electrical pulses in the brain that communicate information, categorized by their frequency. Each wave type is depicted with its characteristic pattern, and a timeline at the bottom provides a scale for one second.</figcaption> </figure> <p>Typically, capturing these long-range temporal dependencies requires explicit sequential models like Recurrent Neural Networks (RNNs) or Attention mechanisms.</p> <p>In this post, we discuss a “cool idea that didn’t quite work out”: Forcing representation invariance and long-term trend learning by injecting “nuisance” variables (demographics, sequence position) only at the decoder stage. Our hypothesis was: If the decoder is explicitly given the window’s temporal position within the overall sequence, the CNN Encoder might be forced to extract and encode the slow, sequential features needed for a proper reconstruction, thus effectively bypassing the need for computationally expensive RNNs to learn temporal context.</p> <p>This attempt to decouple subject-specific noise (like age) and long-range sequential trends (like fatigue) from the pure neural dynamics forms the backbone of our Decoupling Hypothesis.</p> <h2 id="the-hbn-dataset--challenge">The HBN Dataset &amp; Challenge</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-subject-invariant-eeg/eeg_challenge_2025-480.webp 480w,/2026/assets/img/2026-04-27-subject-invariant-eeg/eeg_challenge_2025-800.webp 800w,/2026/assets/img/2026-04-27-subject-invariant-eeg/eeg_challenge_2025-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-subject-invariant-eeg/eeg_challenge_2025.png" class="img-fluid h-80" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 1: HBN-EEG Dataset and Data split. A. EEG is recorded using a 128-channel system during active tasks (i.e., with user input) or passive tasks. B. The psychopathology and demographic factors. C. The dataset split into Train, Test, and Validation.</figcaption> </figure> <d-cite key="aristimunha2025eeg"></d-cite> <p>The competition utilized the Healthy Brain Network (HBN) dataset <d-cite key="alexander2017hbn"></d-cite>, comprising recordings from over 3,000 participants.</p> <p>The data spans six distinct cognitive tasks, categorized into Passive and Active states:</p> <ol> <li> <strong>Passive Tasks (Sensory Processing):</strong> The subject simply perceives stimuli. <ul> <li> <em>Resting State (RS):</em> Measuring baseline activity (eyes open/closed).</li> <li> <em>Surround Suppression (SuS):</em> Visual processing of flashing disks.</li> <li> <em>Movie Watching (MW):</em> Emotional and narrative processing (clips from <em>Despicable Me</em>, etc.).</li> </ul> </li> <li> <strong>Active Tasks (Cognitive Load + Motor Response):</strong> The subject must think and react. <ul> <li> <em>Contrast Change Detection (CCD):</em> Attention vigilance.</li> <li> <em>Sequence Learning (SL):</em> Working memory and pattern recognition.</li> <li> <em>Symbol Search (SyS):</em> High-speed processing and executive function.</li> </ul> </li> </ol> <p>The tasks are very well explained in detail on the <a href="https://eeg2025.github.io/data/" rel="external nofollow noopener" target="_blank">competition page</a>. Different tasks activate different brain regions; we hoped to exploit this diversity.</p> <p>The competition required predicting:</p> <ul> <li> <strong>Reaction Times</strong> (RT)</li> <li><strong>Four Psychopathology Factors</strong></li> </ul> <p>from a <strong>2-second EEG window</strong>, sampled at <strong>100 Hz</strong>, across <strong>129 channels</strong>, where the 129th channel is a reference channel.</p> <p><strong>No preprocessing was allowed</strong> — filtering, artifact removal, normalization, etc., had to be learned implicitly inside the model.</p> <p>Thus the core challenge was:</p> <blockquote> <p><strong>Can we learn subject-invariant, task-invariant EEG embeddings from only 2 seconds of signal?</strong></p> </blockquote> <h2 id="existing-self-supervised-pretraining-approaches-for-eeg-and-our-hypothesis">Existing Self-Supervised Pretraining Approaches for EEG and Our Hypothesis</h2> <p>Self-supervised learning (SSL) has become the standard approach for EEG representation learning, largely due to the scarcity of high-quality labeled datasets and the large inter-subject variability. Surveys on EEG and biomedical SSL <d-cite key="weng2023ssl_eeg_survey"></d-cite>, <d-cite key="ding2023ssl_biomedical_review"></d-cite> group existing methods into a few dominant paradigms.</p> <p><strong>Masked reconstruction methods</strong> adapt Masked Autoencoders (MAE) <d-cite key="he2022masked"></d-cite> to EEG by masking temporal segments or channels and reconstructing the missing signal. This strategy underpins several recent EEG foundation models, including EEGPT <d-cite key="eegpt2024arxiv"></d-cite>.</p> <p><strong>Contrastive learning</strong> remains the most widely used family of SSL techniques, extending SimCLR <d-cite key="chen2020simple"></d-cite> with EEG-specific augmentations such as channel dropout, temporal jittering, and filtering. Representative approaches include CPC <d-cite key="oord2018cpc"></d-cite>, TS-TCC <d-cite key="tstcc2021"></d-cite>, and CL-TCN <d-cite key="banville2021cltcn"></d-cite>.</p> <p><strong>Bootstrap and teacher–student methods</strong> (e.g., BYOL <d-cite key="byol2020"></d-cite> and VICReg <d-cite key="vicreg2022"></d-cite>) have also been adapted to avoid the negatives required in contrastive learning and stabilize training on noisy EEG.</p> <p><strong>Clustering-driven and prototype-based SSL</strong> <d-cite key="prototype2021"></d-cite> aim to discover latent neural states by grouping or assigning EEG segments to learned prototypes without labels.</p> <p>Finally, <strong>large-scale EEG foundation models</strong> such as EEGPT <d-cite key="eegpt2024arxiv"></d-cite> and BC-SSL <d-cite key="bcssl2023"></d-cite> combine multiple SSL paradigms—masked prediction, temporal contrast, clustering, and cross-task invariance—to produce universal EEG encoders.</p> <h3 id="our-hypothesis-context-injection-via-decoupling">Our Hypothesis: Context Injection via Decoupling</h3> <p>The traditional solution to modeling long-range dependencies is to employ Recurrent Neural Networks (RNNs) or Transformers. However, attention mechanisms are compute-intensive, and the non-uniformity of task lengths makes designing a robust RNN architecture difficult.</p> <h3 id="core-idea">Core Idea</h3> <p>We designed an autoencoder where:</p> <ul> <li>The <strong>encoder</strong> (E(x)) receives only the raw 2-second EEG window.</li> <li>The <strong>decoder</strong> (D(z, a)) receives both the latent (z) and an auxiliary embedding (a) containing: <ul> <li>demographics,</li> <li>coarse task identity, and</li> <li>manually constructed sequence-position features.</li> </ul> </li> </ul> <p>The design intentionally applies <strong>dropout to the latent (z)</strong> but <strong>not</strong> to the auxiliary input <strong>(a)</strong>:</p> <blockquote> <p>This forces the decoder to rely on (a) for static, slow-varying information (subject factors, task stage), compelling the encoder to encode only the <em>residual fast neural dynamics</em>—the part we want to be subject- and task-invariant.</p> </blockquote> <p>We hypothesized that subject-invariant embeddings require explicitly factoring out three specific biases:</p> <ol> <li> <p><strong>Task-level cognitive context</strong><br> A 2-second window of a “sad movie” implies different neural states than a “math problem.”</p> </li> <li> <p><strong>Demographic differences</strong><br> Isolating age-related amplitude shifts (e.g., high-amplitude waves in children) to prevent them from being confounded with pathology.</p> </li> <li> <p><strong>Long-range sequence position</strong><br> Explicitly signaling the start vs. end of a task to force the CNN to learn slow morphological trends (like habituation) that usually require RNNs.</p> </li> </ol> <p>Our approach therefore aimed to approximate the benefits of long-range sequence modeling—<strong>without</strong> an RNN/Transformer—by making the decoder responsible for positional and demographic variation, and the encoder responsible for invariant neural representation learning.</p> <h2 id="methodology">Methodology</h2> <h3 id="architectural-motivation">Architectural Motivation</h3> <p>A single 2-second EEG window contains only fast, local neural dynamics, while the behavioral and cognitive variables we aim to predict (reaction time, task state, fatigue, engagement) depend on <strong>slower, longer-range processes</strong> that unfold across minutes. This mismatch creates a fundamental challenge: any encoder that only sees isolated 2-second segments is forced to infer complex context without access to the underlying temporal structure.</p> <p>The architecture addresses this by explicitly <strong>separating what the encoder should learn (subject-invariant neural dynamics)</strong> from what the decoder must reconstruct (context-dependent variations). This leads to three core design principles:</p> <ul> <li><strong>Inject longer-term position information into the decoder, not the encoder</strong></li> <li><strong>Encourage invariance to demographics via an auxiliary encoder</strong></li> <li><strong>Predict task-specific pseudo-labels to force richer latent structure</strong></li> </ul> <p>This led to a hybrid architecture consisting of:</p> <ul> <li> <strong>Multi-branch CNN encoder</strong> capturing multi-scale oscillatory features</li> <li> <strong>Latent bottleneck</strong> with strong regularization</li> <li> <strong>Auxiliary encoder</strong> injecting demographics and sequence-position info</li> <li> <strong>Decoder</strong> conditioned on both EEG latent and auxiliary latent</li> <li> <strong>MTL heads</strong> ensuring structured latent supervision</li> <li> <strong>Spatial reweighting</strong> using electrode geometry</li> <li> <strong>Contrastive + orthogonality losses</strong> ensuring disentangled representations</li> </ul> <h3 id="the-encoder-design">The Encoder Design</h3> <p>We have the raw EEG input of size $(B, 129, 200)$, where B is the batch size, 129 are the signal-channels (last channel is the reference channel) and 200 is the time dimension of each channel. We split the channel dimension into size 128 and 1, the last reference channel. We then broadcast the reference channel to match the size of the other channels, i.e., 128, and stack the two components onto a new dimension resulting in a tensor of size $(B, 128, 200, 2)$.</p> \[X' \in \mathbb{R}^{B \times 128 \times 200 \times 2}\] <p>We employ a multi-scale CNN approach to capture temporal features at different frequencies:</p> <ol> <li> <p><strong>Multi-Kernel Convolution:</strong> Three parallel CNN branches with kernel sizes of $[1, 15, 45]$ (corresponding to immediate, alpha/beta, and delta/theta band scales). \(H_0 = \text{Concat}(H_{k=1}, H_{k=15}, H_{k=45})\)</p> </li> <li> <strong>Orthogonal Feature Extraction:</strong> We utilize two distinct convolutional branches with differing dilation rates $(1, 2, 4, 16)$ to capture long-range dependencies.</li> <li> <strong>Orthogonality Constraint:</strong> To ensure these branches learn distinct features, we minimize the Frobenius norm of their product.</li> </ol> \[\mathcal{L}_{\text{ortho}} = \left\| H_1^\top H_2 \right\|_F^2\] <p>where ( $ H_1, H_2 \in \mathbb{R}^{B \times T \times d} $ ) are flattened feature maps.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-subject-invariant-eeg/model_architecture-480.webp 480w,/2026/assets/img/2026-04-27-subject-invariant-eeg/model_architecture-800.webp 800w,/2026/assets/img/2026-04-27-subject-invariant-eeg/model_architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-subject-invariant-eeg/model_architecture.png" class="img-fluid w-80 h-70" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 2: The proposed architecture. Note the Auxiliary Encoder injecting demographics and sequence position directly into the latent space before decoding.</figcaption> </figure> <h3 id="auxiliary-injection--disentanglement">Auxiliary Injection &amp; Disentanglement</h3> <p>The decoder must know which part of the original EEG sequence the 2-second window corresponds to.</p> <p>We manually constructed positional encodings that representing EEG’s temporal structure:</p> <ul> <li><strong>Linear ramp</strong></li> <li><strong>Exponential decay</strong></li> <li><strong>Sinusoidal positional encoding</strong></li> </ul> <p>and concatenated them with demographic and task information:</p> \[z_{\text{aux}} = f_{\theta}([d_{\text{demo}}, p_{\text{pos}}, t_{\text{task}}])\] <p>where ( $ f_{\theta} $ ) is a small MLP producing a <strong>32-D vector</strong>.</p> <p>This was the core of our “Cool Idea.” We postulated that if we fed demographic data (age, sex, handedness), task information (one hot encoding representing the task) and the positional encoding (sequence index) into the decoder but not the encoder, we can force the encoder to focus on subject independent signals and we thought that the CNNs would be able to extract sequence based features without the use of RNNs.</p> \[Encoder: z_{eeg} = E(x_{raw})\] \[Aux Encoder: z_{aux} = E_{aux}(x_{demo}, x_{pos})\] \[Decoder: \hat{x} = D(Concat(z_{eeg}, z_{aux}))\] <p><strong>This auxiliary embedding was appended to the encoder without dropout</strong>, forcing the model to use auxiliary signals for positioning. By applying dropout to $z_{eeg}$ but not to $z_{aux}$, we forced the Decoder to rely on $z_{aux}$ for the “easy” reconstruction details (like general signal amplitude related to age), thereby forcing the Encoder to focus purely on the residual neurological signals that are independent of demographics.</p> <h3 id="spatial-scaling-via-adjacency">Spatial Scaling via Adjacency</h3> <p>EEG electrodes have a physical geometry. Neighboring electrodes often capture redundant signals from the same brain lobe. To introduce spatial awareness, we scale the latent representations based on the physical distance of electrodes.</p> <p>To introduce <strong>spatial awareness</strong>, we scale the latent representations based on the physical distance of electrodes. Let $D_{norm} \in \mathbb{R}^{C}$ be the normalized distance of electrodes from a center point. We apply an inverse scaling:</p> \[Z'_{c} = Z_{c} \cdot \frac{1}{D_{norm, c} + \epsilon}\] <p>This acts as a soft attention mechanism, upweighting signals from sparser regions or distinct lobes before they enter the dense layers of the decoder.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-subject-invariant-eeg/before-and-after_visualization_soft_attention_mechanism-480.webp 480w,/2026/assets/img/2026-04-27-subject-invariant-eeg/before-and-after_visualization_soft_attention_mechanism-800.webp 800w,/2026/assets/img/2026-04-27-subject-invariant-eeg/before-and-after_visualization_soft_attention_mechanism-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-subject-invariant-eeg/before-and-after_visualization_soft_attention_mechanism.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 3: Stabilization effect of the soft attention mechanism using spatial information; before-and-after visualizations.</figcaption> </figure> <h3 id="multi-task-pseudo-labeling">Multi-Task Pseudo-Labeling</h3> <p>Each task contains additional metadata such as correctness, contrast values, movie identifiers, etc. To further regularize the latent space, we utilized these known task structures to generate “pseudo-labels.” Even though the competition goal was reaction time, predicting the <em>state</em> of the experiment forces the model to recognize which neural circuits are active.</p> <div class="row"> <div class="col-sm-3 d-flex align-items-center justify-content-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-subject-invariant-eeg/mtl_head-480.webp 480w,/2026/assets/img/2026-04-27-subject-invariant-eeg/mtl_head-800.webp 800w,/2026/assets/img/2026-04-27-subject-invariant-eeg/mtl_head-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-subject-invariant-eeg/mtl_head.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 4: Task-specific prediction head.</figcaption> </figure> </div> <div class="col-sm-9"> <p>We attached 6 task-specific heads to the encoder's latent.</p> <table> <thead> <tr> <th>Task</th> <th>Pseudo-Labels Generated</th> </tr> </thead> <tbody> <tr> <td><strong>Contrast Change</strong></td> <td> <code>Contrast_left_side</code> (binary), <code>Contrast_correct</code>, <code>Reaction_time</code> </td> </tr> <tr> <td><strong>Symbol Search</strong></td> <td> <code>Contrast_correct</code> (binary)</td> </tr> <tr> <td><strong>Surround Supp.</strong></td> <td> <code>Background_type</code>, <code>Foreground_contrast</code>, <code>Stimulus_cond</code> </td> </tr> <tr> <td><strong>Seq. Learning</strong></td> <td> <code>Correct_count</code>, <code>Target_count</code>, <code>Learning_phase</code> (binary)</td> </tr> <tr> <td><strong>Resting State</strong></td> <td> <code>Eyes_closed</code> (binary - derived from onset times)</td> </tr> <tr> <td><strong>Movies</strong></td> <td>One-hot encoded movie segment identifiers</td> </tr> </tbody> </table> </div> </div> <h2 id="loss-landscape">Loss Landscape</h2> <p>The training objective was a complex balancing act of four distinct loss functions. We define the total loss $\mathcal{L}_{total}$ as:</p> \[\mathcal{L}_{total} = \lambda_{recon}\mathcal{L}_{recon} + \lambda_{scl}\mathcal{L}_{scl} + \lambda_{mtl}\mathcal{L}_{mtl} + \lambda_{ortho}\mathcal{L}_{ortho}\] <p>Where:</p> <ul> <li>$\mathcal{L}_{recon}$: MSE Reconstruction loss of the signal.</li> <li>$\mathcal{L}_{scl}$: Supervised Contrastive Loss <d-cite key="khosla2020supcon"></d-cite> to cluster embeddings of the same task type.</li> <li>$\mathcal{L}_{mtl}$: Multi-Task Learning loss (masked sum of BCE/MSE/CrossEntropy for pseudo-labels).</li> <li>$\mathcal{L}_{ortho}$: Orthogonality loss to force diverse feature extraction between the dilation branches.</li> </ul> <p>The weighting was crucial. We used $\lambda_{recon}=1.0$, $\lambda_{mtl}=1.0$, but kept contrastive loss low ($\lambda_{scl}=0.001$) to prevent collapse, and $\lambda_{ortho}=0.1$.</p> <h2 id="results--retrospective">Results &amp; Retrospective</h2> <p>We entered the competition late (4 weeks prior to deadline), limiting our ability to tune hyperparameters. However, the results provided a clear picture of what works and what doesn’t.</p> <h3 id="performance">Performance</h3> <div class="row"> <div class="col-sm-6 d-flex justify-content-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-subject-invariant-eeg/challenge1_score-480.webp 480w,/2026/assets/img/2026-04-27-subject-invariant-eeg/challenge1_score-800.webp 800w,/2026/assets/img/2026-04-27-subject-invariant-eeg/challenge1_score-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-subject-invariant-eeg/challenge1_score.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 5: Our score and comparison to near-by entries in Challenge 1.</figcaption> </figure> </div> <div class="col-sm-6 d-flex justify-content-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-subject-invariant-eeg/challenge2_score-480.webp 480w,/2026/assets/img/2026-04-27-subject-invariant-eeg/challenge2_score-800.webp 800w,/2026/assets/img/2026-04-27-subject-invariant-eeg/challenge2_score-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-subject-invariant-eeg/challenge2_score.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 6: Our score and comparison to near-by entries in Challenge 2.</figcaption> </figure> </div> </div> <p>The metric used for both regression challenges 1 and 2 were the normalized root mean square error for the response time prediction and the psychopathology factor prediction.</p> <table> <thead> <tr> <th style="text-align: left">Metric</th> <th style="text-align: left">Our Score</th> <th style="text-align: left">Top scores</th> <th style="text-align: left">Rank</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Challenge 1 (Reaction Time)</strong></td> <td style="text-align: left"><em>0.95961</em></td> <td style="text-align: left"><em>0.88668</em></td> <td style="text-align: left"><strong>54</strong></td> </tr> <tr> <td style="text-align: left"><strong>Challenge 2 (Psychopathology)</strong></td> <td style="text-align: left"><em>0.99786</em></td> <td style="text-align: left"><em>0.97843</em></td> <td style="text-align: left"><strong>16</strong></td> </tr> </tbody> </table> <h3 id="what-didnt-work-and-why">What didn’t work (and why)</h3> <ol> <li> <p><strong>Manual Positional Encoding was too simple.</strong> We assumed brain fatigue or habituation scales linearly or exponentially over a task. In reality, attention fluctuates in complex, non-linear waves. Our “Linear Ramp” auxiliary input likely confused the decoder more than it helped, as the brain states didn’t align perfectly with simple time indices.</p> </li> <li> <p><strong>Demographics are harder to disentangle.</strong> We hoped the auxiliary injection would strip age-related amplitude differences from the latent space. However, age impacts not just amplitude, but frequency coupling and signal complexity. A simple concatenation in the decoder wasn’t expressive enough to capture these non-linear relationships, meaning the encoder still retained (and overfit to) demographic shifts.</p> </li> <li> <p><strong>Orthogonality constraints were too rigid.</strong> Forcing the two CNN branches to be orthogonal ($\mathcal{L}_{ortho}$) was intended to encourage diversity. In practice, EEG features are highly correlated. By penalizing correlation, we may have forced the model to discard useful, albeit redundant, information.</p> </li> </ol> <h3 id="future-steps">Future Steps</h3> <p>The challenge highlighted the need for <strong>foundation models for EEG</strong> that integrate multi-timescale structure and spatial geometry more organically.</p> <p>Moving forward, we plan to:</p> <ol> <li> <strong>Benchmark on the Test Set:</strong> Once the full labels are released, we will verify if the spatial scaling offers robustness on out-of-distribution subjects.</li> <li> <strong>Replace Manual Auxiliaries:</strong> Instead of manually coding positions, we are exploring <strong>Learnable Time Embeddings</strong> (like in Transformers) that can adapt to the non-linear fatigue patterns of the brain.</li> <li> <strong>Graph Neural Networks:</strong> Replace the distance-based scaling with an explicit Graph Attention Network (GAT) to model the electrode topology dynamically.</li> </ol> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-subject-invariant-eeg.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-vlms-waste-their-vision/">Why vlms waste their vision</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>