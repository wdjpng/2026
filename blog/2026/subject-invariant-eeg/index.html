<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Decoupling Hypothesis: Attempting Subject-Invariant EEG Representation Learning via Auxiliary Injection | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="We explore several ideas for learning subject-invariant EEG representations for reaction time and psychopathology prediction using only 2-second windows in the NeurIPS 2025 EEG Challenge. The core of our approach is the Decoupling Hypothesis: an autoencoder framework where we attempt to disentangle subject-specific artifacts and long-term temporal trends (such as fatigue) from the neural signal by explicitly injecting 'nuisance' variables (like demographics and sequence position) into the decoder. This method aimed to force a purely convolutional encoder to learn slow, sequential features without relying on computationally expensive Recurrent or Attention mechanisms. This blog discusses the ideas that seemed promising but ultimately did not work as intended—and why."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/subject-invariant-eeg/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "The Decoupling Hypothesis: Attempting Subject-Invariant EEG Representation Learning via Auxiliary Injection",
            "description": "We explore several ideas for learning subject-invariant EEG representations for reaction time and psychopathology prediction using only 2-second windows in the NeurIPS 2025 EEG Challenge. The core of our approach is the Decoupling Hypothesis: an autoencoder framework where we attempt to disentangle subject-specific artifacts and long-term temporal trends (such as fatigue) from the neural signal by explicitly injecting 'nuisance' variables (like demographics and sequence position) into the decoder. This method aimed to force a purely convolutional encoder to learn slow, sequential features without relying on computationally expensive Recurrent or Attention mechanisms. This blog discusses the ideas that seemed promising but ultimately did not work as intended—and why.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "https://www.Anonymous.com/",
                "affiliations": [
                  {
                    "name": "Anonymous",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>The Decoupling Hypothesis: Attempting Subject-Invariant EEG Representation Learning via Auxiliary Injection</h1> <p>We explore several ideas for learning subject-invariant EEG representations for reaction time and psychopathology prediction using only 2-second windows in the NeurIPS 2025 EEG Challenge. The core of our approach is the Decoupling Hypothesis: an autoencoder framework where we attempt to disentangle subject-specific artifacts and long-term temporal trends (such as fatigue) from the neural signal by explicitly injecting 'nuisance' variables (like demographics and sequence position) into the decoder. This method aimed to force a purely convolutional encoder to learn slow, sequential features without relying on computationally expensive Recurrent or Attention mechanisms. This blog discusses the ideas that seemed promising but ultimately did not work as intended—and why.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#the-hbn-dataset-challenge">The HBN Dataset &amp; Challenge</a> </div> <div> <a href="#the-hypothesis">The Hypothesis</a> </div> <div> <a href="#methodology">Methodology</a> </div> <ul> <li> <a href="#the-encoder-design">The Encoder Design</a> </li> <li> <a href="#auxiliary-injection-disentanglement">Auxiliary Injection &amp; Disentanglement</a> </li> <li> <a href="#spatial-scaling-via-adjacency">Spatial Scaling via Adjacency</a> </li> <li> <a href="#multi-task-pseudo-labeling">Multi-Task Pseudo-Labeling</a> </li> </ul> <div> <a href="#loss-landscape">Loss Landscape</a> </div> <div> <a href="#results-discussion">Results &amp; Discussion</a> </div> <div> <a href="#references">References</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Decoding the human brain is difficult; decoding it from short, noisy snippets of electrical activity is even harder.</p> <p>We recently participated in the NeurIPS 2025 EEG Benchmark Competition <d-cite key="aristimunha2025eeg"></d-cite>, a rigorous challenge designed to test the limits of EEG representation learning. The goal was to predict behavioral metrics (Reaction Times) and clinical diagnostics (Psychopathology Factors) using only 2-second windows of raw EEG signals.</p> <p>Out of many participating teams, our experimental architecture achieved:</p> <ul> <li> <strong>Rank 54</strong> in Challenge 1 (Reaction Time Prediction)</li> <li> <strong>Rank 16</strong> in Challenge 2 (Psychopathology Prediction)</li> </ul> <p>While we didn’t take the top spot, our approach attempted to solve a fundamental problem in neuroscience: <strong>Subject Invariance</strong>. A model trained on raw signals often overfits to subject-specific artifacts (like skull thickness or hair density) rather than learning the underlying neural dynamics required for downstream tasks.</p> <p>The core challenge with short, 2-second windows is capturing slow morphological trends in the EEG. While a 200-sample window (at 100 Hz) easily captures higher frequencies like Beta (13-30 Hz) or Alpha (8-12 Hz), it fundamentally limits the resolution of slow waves such as deep sleep Delta waves (~0.5–2 Hz).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-subject-invariant-eeg/brainwaves-480.webp 480w,/2026/assets/img/2026-04-27-subject-invariant-eeg/brainwaves-800.webp 800w,/2026/assets/img/2026-04-27-subject-invariant-eeg/brainwaves-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-subject-invariant-eeg/brainwaves.png" class="img-fluid h-75" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 0: Visualization of different types of brainwaves, which are electrical pulses in the brain that communicate information, categorized by their frequency. Each wave type is depicted with its characteristic pattern, and a timeline at the bottom provides a scale for one second.</figcaption> </figure> <p>Typically, capturing these long-range temporal dependencies requires explicit sequential models like Recurrent Neural Networks (RNNs) or Attention mechanisms.</p> <p>In this post, we discuss a “cool idea that didn’t quite work out”: Forcing representation invariance and long-term trend learning by injecting “nuisance” variables (demographics, sequence position) only at the decoder stage. Our hypothesis was: If the decoder is explicitly given the window’s temporal position within the overall sequence, the CNN Encoder might be forced to extract and encode the slow, sequential features needed for a proper reconstruction, thus effectively bypassing the need for computationally expensive RNNs to learn temporal context.</p> <p>This attempt to decouple subject-specific noise (like age) and long-range sequential trends (like fatigue) from the pure neural dynamics forms the backbone of our Decoupling Hypothesis.</p> <h2 id="the-hbn-dataset--challenge">The HBN Dataset &amp; Challenge</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-subject-invariant-eeg/eeg_challenge_2025-480.webp 480w,/2026/assets/img/2026-04-27-subject-invariant-eeg/eeg_challenge_2025-800.webp 800w,/2026/assets/img/2026-04-27-subject-invariant-eeg/eeg_challenge_2025-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-subject-invariant-eeg/eeg_challenge_2025.png" class="img-fluid h-80" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 1: EEG Challenge 2025.</figcaption> </figure> <d-cite key="aristimunha2025eeg"></d-cite> <p>The competition utilized the Healthy Brain Network (HBN) dataset <d-cite key="alexander2017hbn"></d-cite>, comprising recordings from over 3,000 participants.</p> <p>The data spans six distinct cognitive tasks, categorized into Passive and Active states:</p> <ol> <li> <strong>Passive Tasks (Sensory Processing):</strong> The subject simply perceives stimuli. <ul> <li> <em>Resting State (RS):</em> Measuring baseline activity (eyes open/closed).</li> <li> <em>Surround Suppression (SuS):</em> Visual processing of flashing disks.</li> <li> <em>Movie Watching (MW):</em> Emotional and narrative processing (clips from <em>Despicable Me</em>, etc.).</li> </ul> </li> <li> <strong>Active Tasks (Cognitive Load + Motor Response):</strong> The subject must think and react. <ul> <li> <em>Contrast Change Detection (CCD):</em> Attention vigilance.</li> <li> <em>Sequence Learning (SL):</em> Working memory and pattern recognition.</li> <li> <em>Symbol Search (SyS):</em> High-speed processing and executive function.</li> </ul> </li> </ol> <p>The tasks are very well explained in detail on the <a href="https://eeg2025.github.io/data/" rel="external nofollow noopener" target="_blank">competition page</a>. Different tasks activate different brain regions; we hoped to exploit this diversity.</p> <p>The competition required predicting:</p> <ul> <li> <strong>Reaction Times</strong> (RT)</li> <li><strong>Four Psychopathology Factors</strong></li> </ul> <p>from a <strong>2-second EEG window</strong>, sampled at <strong>100 Hz</strong>, across <strong>129 channels</strong>, where the 129th channel is a reference channel.</p> <p><strong>No preprocessing was allowed</strong> — filtering, artifact removal, normalization, etc., had to be learned implicitly inside the model.</p> <p>Thus the core challenge was:</p> <blockquote> <p><strong>Can we learn subject-invariant, task-invariant EEG embeddings from only 2 seconds of signal?</strong></p> </blockquote> <h2 id="existing-self-supervised-pretraining-approaches-for-eeg-and-our-hypothesis">Existing Self-Supervised Pretraining Approaches for EEG and Our Hypothesis</h2> <p>Self-supervised learning (SSL) has recently become the dominant paradigm for EEG pretraining, driven by the difficulty of obtaining large-scale labeled datasets.<br> Systematic surveys <d-cite key="weng2023ssl_eeg_survey"></d-cite>, <d-cite key="ding2023ssl_biomedical_review"></d-cite> categorize EEG SSL approaches into several broad families.</p> <h4 id="1-masked-predictionbased-methods">1. Masked Prediction–based Methods</h4> <p>Inspired by Masked Autoencoders (MAE) <d-cite key="he2022masked"></d-cite>, EEG variants randomly mask temporal segments or channels and reconstruct the missing signal.<br> Examples include Masked Reconstruction models and channel dropout reconstruction used in EEGPT <d-cite key="eegpt2024arxiv"></d-cite>.</p> <h4 id="2-contrastive-learning">2. Contrastive Learning</h4> <p>Contrastive learning (SimCLR-style) <d-cite key="chen2020simple"></d-cite> remains the most widely used EEG SSL framework.<br> Common contrastive EEG methods include:</p> <ul> <li> <strong>CPC / contrastive predictive coding</strong> <d-cite key="oord2018cpc"></d-cite> </li> <li> <strong>TS-TCC (Time-Series Temporal Contrastive Coding)</strong> <d-cite key="tstcc2021"></d-cite> </li> <li> <strong>CL-TCN / temporal instance discrimination</strong> <d-cite key="banville2021cltcn"></d-cite> </li> </ul> <p>These approaches create augmentations such as jittering, time-warping, filtering, or channel dropout to enforce invariance.</p> <h4 id="3-bootstrap--teacherstudent-ssl">3. Bootstrap / Teacher–Student SSL</h4> <p>Non-contrastive methods such as BYOL and VICReg have EEG extensions:</p> <ul> <li> <strong>BYOL-EEG</strong> (bootstrapping features without negatives) <d-cite key="byol2020"></d-cite> </li> <li> <strong>VICReg-style EEG regularization</strong> <d-cite key="vicreg2022"></d-cite> </li> </ul> <p>Such models avoid instability arising from contrastive negatives.</p> <h4 id="4-clustering-driven-ssl">4. Clustering-Driven SSL</h4> <p>Deep clustering and prototype assignment (DeepCluster-style) have been adapted to EEG:</p> <ul> <li><strong>EEG-MixMatch clustering</strong></li> <li> <strong>Prototype-contrastive EEG objectives</strong> <d-cite key="prototype2021"></d-cite> </li> </ul> <p>These aim to group similar neural states without labels. These methods encourage representations that align across physiological modalities.</p> <h4 id="5-large-scale-foundation-models">5. Large-Scale Foundation Models</h4> <p>Recent EEG foundation models such as <strong>EEGPT</strong> <d-cite key="eegpt2024arxiv"></d-cite> and <strong>BC-SSL</strong> <d-cite key="bcssl2023"></d-cite> combine several SSL components—masked modeling, temporal contrastive pretraining, and cross-task invariance—to produce universal EEG embeddings.</p> <p>Overall, SSL for EEG spans masked modeling, contrastive learning, non-contrastive bootstrapping, clustering, cross-modal pretraining, and large-scale foundation models.</p> <h4 id="our-hypothesis">Our Hypothesis</h4> <p>However, given the massive inter-subject variability and the varying, often irregular, lengths of the six different tasks in the HBN dataset, we hypothesized that the sequence position within a task is a strong and inconsistent bias we must account for.</p> <p>The traditional approach to handling sequence-dependent biases and long-range dependencies (like fatigue) is to use Recurrent Neural Networks (RNNs) or Transformer-based Attention mechanisms. While Attention mechanisms are powerful, they are highly compute-intensive, a significant limitation given our restricted budget. Furthermore, the non-uniformity of task lengths made designing a single, robust RNN architecture challenging.</p> <p><strong>Our Core Idea:</strong> We proposed a computationally lighter solution: Use an Autoencoder where the Encoder $E(x)$ sees only the EEG, but the Decoder $D(z, a)$ sees the latent $z$ <em>plus</em> an auxiliary embedding $a$ containing demographics, task information and sequence position.</p> <p><strong>The Crux of the Experiment</strong>: By manually constructing and passing the sequence position $a$ to the decoder, we aimed to test whether this could <em>implicitly force the CNN Encoder to learn the required slow, long-term morphological features</em> that an RNN would typically capture, all while avoiding the high computational cost of a full attention model.</p> <p>By applying dropout to $z$ but <em>not</em> to $a$ during training, we create an information bottleneck that incentivizes the decoder to rely on $a$ for static/positional information, forcing $z$ to encode only the <strong>residual neural dynamics</strong>—the pure brain activity we actually care about.</p> <p>This led to our full hypothesis: Learning subject-invariant, task-invariant EEG embeddings requires incorporating:</p> <ol> <li> <strong>Task-Level Cognitive Context:</strong> A 2-second window of a “sad movie” looks different from a 2-second window of a “math problem.” Without context, the model struggles to interpret the waveforms.</li> <li> <strong>Recording-Specific Demographics:</strong> A 5-year-old’s brain waves are higher amplitude and slower than a 20-year-old’s. If the model doesn’t account for age, it might mistake “youth” for “pathology.”</li> <li> <strong>Manual Long-Range Position Information:</strong> A 2-second window at the <em>start</em> of a task (fresh) is different from one at the <em>end</em> (fatigued). To substitute for computationally expensive RNNs and force the CNN encoder to learn slow morphological trends.</li> </ol> <h2 id="methodology">Methodology</h2> <h3 id="architectural-motivation">Architectural Motivation</h3> <p>A single 2-second window is insufficient to predict behavioral or clinical outcomes.<br> Thus, the architecture was designed to:</p> <ul> <li><strong>inject longer-term position information into the decoder, not the encoder</strong></li> <li><strong>encourage invariance to demographics via an auxiliary encoder</strong></li> <li><strong>predict task-specific pseudo-labels to force richer latent structure</strong></li> </ul> <p>This led to a hybrid architecture consisting of:</p> <ul> <li>A <strong>multi-branch CNN encoder</strong> </li> <li>A <strong>latent bottleneck</strong> regularized with dropout</li> <li>An <strong>auxiliary encoder</strong> (demographics + positional encoding)</li> <li>A <strong>decoder</strong> reconstructing EEG</li> <li>Six <strong>task-specific MTL heads</strong> predicting pseudo-labels</li> <li> <strong>Contrastive loss</strong> across tasks</li> <li> <strong>Orthogonality constraints</strong> on feature subspaces</li> <li><strong>Electrode distance re-weighting</strong></li> </ul> <h3 id="the-encoder-design">The Encoder Design</h3> <p>The encoder takes the raw EEG input $(B, 129, 200)$. We first broadcast the reference channel and stack it, resulting in $(B, 128, 200, 2)$.</p> \[X' \in \mathbb{R}^{B \times 128 \times 200 \times 2}\] <p>We employ a multi-scale CNN approach to capture temporal features at different frequencies:</p> <ol> <li> <p><strong>Multi-Kernel Convolution:</strong> Three parallel CNN branches with kernel sizes of $[1, 15, 45]$ (corresponding to immediate, alpha/beta, and delta/theta band scales). \(H_0 = \text{Concat}(H_{k=1}, H_{k=15}, H_{k=45})\)</p> </li> <li> <strong>Orthogonal Feature Extraction:</strong> We utilize two distinct convolutional branches with differing dilation rates $(1, 2, 4, 16)$ to capture long-range dependencies.</li> <li> <strong>Orthogonality Constraint:</strong> To ensure these branches learn distinct features, we minimize the Frobenius norm of their product.</li> </ol> \[\mathcal{L}_{\text{ortho}} = \left\| A^\top B \right\|_F^2\] <p>where ( $ A, B \in \mathbb{R}^{B \times T \times d} $ ) are flattened feature maps.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-subject-invariant-eeg/model_architecture-480.webp 480w,/2026/assets/img/2026-04-27-subject-invariant-eeg/model_architecture-800.webp 800w,/2026/assets/img/2026-04-27-subject-invariant-eeg/model_architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-subject-invariant-eeg/model_architecture.png" class="img-fluid w-80 h-70" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 2: The proposed architecture. Note the Auxiliary Encoder injecting demographics and sequence position directly into the latent space before decoding.</figcaption> </figure> <h3 id="auxiliary-injection--disentanglement">Auxiliary Injection &amp; Disentanglement</h3> <p>The decoder must know which part of the original EEG sequence the 2-second window corresponds to.</p> <p>We manually constructed:</p> <ul> <li><strong>Linear ramp</strong></li> <li><strong>Exponential decay</strong></li> <li><strong>Sinusoidal positional encoding</strong></li> </ul> <p>and concatenated them with demographic and task information:</p> \[z_{\text{aux}} = f_{\theta}([d_{\text{demo}}, p_{\text{pos}}, t_{\text{task}}])\] <p>where ( $ f_{\theta} $ ) is a small MLP producing a <strong>32-D vector</strong>.</p> <p>This was the core of our “Cool Idea.” We postulated that if we fed demographic data (age, sex, handedness), task information (one hot encoding representing the task) and the positional encoding (sequence index) into the decoder but not the encoder, we can force the encoder to focus on subject independent signals and we thought that the CNNs would be able to extract sequence based features without the use of RNNs.</p> \[Encoder: z_{eeg} = E(x_{raw})\] \[Aux Encoder: z_{aux} = E_{aux}(x_{demo}, x_{pos})\] \[Decoder: \hat{x} = D(Concat(z_{eeg}, z_{aux}))\] <p><strong>This auxiliary embedding was appended to the encoder without dropout</strong>, forcing the model to use auxiliary signals for positioning. By applying dropout to $z_{eeg}$ but not to $z_{aux}$, we forced the Decoder to rely on $z_{aux}$ for the “easy” reconstruction details (like general signal amplitude related to age), thereby forcing the Encoder to focus purely on the residual neurological signals that are independent of demographics.</p> <h3 id="spatial-scaling-via-adjacency">Spatial Scaling via Adjacency</h3> <p>EEG electrodes have a physical geometry. Neighboring electrodes often capture redundant signals from the same brain lobe. To introduce spatial awareness, we scale the latent representations based on the physical distance of electrodes.</p> <p>To introduce <strong>spatial awareness</strong>, we scale the latent representations based on the physical distance of electrodes. Let $D_{norm} \in \mathbb{R}^{C}$ be the normalized distance of electrodes from a center point. We apply an inverse scaling:</p> \[Z'_{c} = Z_{c} \cdot \frac{1}{D_{norm, c} + \epsilon}\] <p>This acts as a soft attention mechanism, upweighting signals from sparser regions or distinct lobes before they enter the dense layers of the decoder.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-subject-invariant-eeg/before-and-after_visualization_soft_attention_mechanism-480.webp 480w,/2026/assets/img/2026-04-27-subject-invariant-eeg/before-and-after_visualization_soft_attention_mechanism-800.webp 800w,/2026/assets/img/2026-04-27-subject-invariant-eeg/before-and-after_visualization_soft_attention_mechanism-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-subject-invariant-eeg/before-and-after_visualization_soft_attention_mechanism.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 3: Stabilization effect of the soft attention mechanism using spatial information; before-and-after visualizations.</figcaption> </figure> <h3 id="multi-task-pseudo-labeling">Multi-Task Pseudo-Labeling</h3> <p>Each task contains additional metadata such as correctness, contrast values, movie identifiers, etc. To further regularize the latent space, we utilized these known task structures to generate “pseudo-labels.” Even though the competition goal was reaction time, predicting the <em>state</em> of the experiment forces the model to recognize which neural circuits are active.</p> <div class="row"> <div class="col-sm-3 d-flex align-items-center justify-content-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-subject-invariant-eeg/mtl_head-480.webp 480w,/2026/assets/img/2026-04-27-subject-invariant-eeg/mtl_head-800.webp 800w,/2026/assets/img/2026-04-27-subject-invariant-eeg/mtl_head-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-subject-invariant-eeg/mtl_head.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 4: Task-specific prediction head.</figcaption> </figure> </div> <div class="col-sm-9"> <p>We attached 6 task-specific heads to the encoder's latent.</p> <table> <thead> <tr> <th>Task</th> <th>Pseudo-Labels Generated</th> </tr> </thead> <tbody> <tr> <td><strong>Contrast Change</strong></td> <td> <code>Contrast_left_side</code> (binary), <code>Contrast_correct</code>, <code>Reaction_time</code> </td> </tr> <tr> <td><strong>Symbol Search</strong></td> <td> <code>Contrast_correct</code> (binary)</td> </tr> <tr> <td><strong>Surround Supp.</strong></td> <td> <code>Background_type</code>, <code>Foreground_contrast</code>, <code>Stimulus_cond</code> </td> </tr> <tr> <td><strong>Seq. Learning</strong></td> <td> <code>Correct_count</code>, <code>Target_count</code>, <code>Learning_phase</code> (binary)</td> </tr> <tr> <td><strong>Resting State</strong></td> <td> <code>Eyes_closed</code> (binary - derived from onset times)</td> </tr> <tr> <td><strong>Movies</strong></td> <td>One-hot encoded movie segment identifiers</td> </tr> </tbody> </table> </div> </div> <h2 id="loss-landscape">Loss Landscape</h2> <p>The training objective was a complex balancing act of four distinct loss functions. We define the total loss $\mathcal{L}_{total}$ as:</p> \[\mathcal{L}_{total} = \lambda_{recon}\mathcal{L}_{recon} + \lambda_{scl}\mathcal{L}_{scl} + \lambda_{mtl}\mathcal{L}_{mtl} + \lambda_{ortho}\mathcal{L}_{ortho}\] <p>Where:</p> <ul> <li>$\mathcal{L}_{recon}$: MSE Reconstruction loss of the signal.</li> <li>$\mathcal{L}_{scl}$: Supervised Contrastive Loss <d-cite key="khosla2020supcon"></d-cite> to cluster embeddings of the same task type.</li> <li>$\mathcal{L}_{mtl}$: Multi-Task Learning loss (masked sum of BCE/MSE/CrossEntropy for pseudo-labels).</li> <li>$\mathcal{L}_{ortho}$: Orthogonality loss to force diverse feature extraction between the dilation branches.</li> </ul> <p>Given two feature matrices $A$ and $B$ from the parallel branches, the orthogonal loss is defined as:</p> \[\mathcal{L}_{ortho} = || A^T B ||_F^2 \approx 0\] <p>The weighting was crucial. We used $\lambda_{recon}=1.0$, $\lambda_{mtl}=1.0$, but kept contrastive loss low ($\lambda_{scl}=0.001$) to prevent collapse, and $\lambda_{ortho}=0.1$.</p> <h2 id="results--retrospective">Results &amp; Retrospective</h2> <p>We entered the competition late (4 weeks prior to deadline), limiting our ability to tune hyperparameters. However, the results provided a clear picture of what works and what doesn’t.</p> <h3 id="performance">Performance</h3> <div class="row"> <div class="col-sm-6 d-flex justify-content-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-subject-invariant-eeg/challenge1_score-480.webp 480w,/2026/assets/img/2026-04-27-subject-invariant-eeg/challenge1_score-800.webp 800w,/2026/assets/img/2026-04-27-subject-invariant-eeg/challenge1_score-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-subject-invariant-eeg/challenge1_score.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 5: Our score and comparison to near-by entries in Challenge 1.</figcaption> </figure> </div> <div class="col-sm-6 d-flex justify-content-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-subject-invariant-eeg/challenge2_score-480.webp 480w,/2026/assets/img/2026-04-27-subject-invariant-eeg/challenge2_score-800.webp 800w,/2026/assets/img/2026-04-27-subject-invariant-eeg/challenge2_score-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-subject-invariant-eeg/challenge2_score.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 6: Our score and comparison to near-by entries in Challenge 2.</figcaption> </figure> </div> </div> <table> <thead> <tr> <th style="text-align: left">Metric</th> <th style="text-align: left">Our Score</th> <th style="text-align: left">Top scores</th> <th style="text-align: left">Rank</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Challenge 1 (Reaction Time)</strong></td> <td style="text-align: left"><em>0.95961</em></td> <td style="text-align: left"><em>0.88668</em></td> <td style="text-align: left"><strong>54</strong></td> </tr> <tr> <td style="text-align: left"><strong>Challenge 2 (Psychopathology)</strong></td> <td style="text-align: left"><em>0.99786</em></td> <td style="text-align: left"><em>0.97843</em></td> <td style="text-align: left"><strong>16</strong></td> </tr> </tbody> </table> <h3 id="what-didnt-work-and-why">What didn’t work (and why)</h3> <ol> <li> <p><strong>Manual Positional Encoding was too simple.</strong> We assumed brain fatigue or habituation scales linearly or exponentially over a task. In reality, attention fluctuates in complex, non-linear waves. Our “Linear Ramp” auxiliary input likely confused the decoder more than it helped, as the brain states didn’t align perfectly with simple time indices.</p> </li> <li> <p><strong>Demographics are harder to disentangle.</strong> We hoped the auxiliary injection would strip age-related amplitude differences from the latent space. However, age impacts not just amplitude, but frequency coupling and signal complexity. A simple concatenation in the decoder wasn’t expressive enough to capture these non-linear relationships, meaning the encoder still retained (and overfit to) demographic shifts.</p> </li> <li> <p><strong>Orthogonality constraints were too rigid.</strong> Forcing the two CNN branches to be orthogonal ($\mathcal{L}_{ortho}$) was intended to encourage diversity. In practice, EEG features are highly correlated. By penalizing correlation, we may have forced the model to discard useful, albeit redundant, information.</p> </li> </ol> <h3 id="future-steps">Future Steps</h3> <p>The challenge highlighted the need for <strong>foundation models for EEG</strong> that integrate multi-timescale structure and spatial geometry more organically.</p> <p>Moving forward, we plan to:</p> <ol> <li> <strong>Benchmark on the Test Set:</strong> Once the full labels are released, we will verify if the spatial scaling offers robustness on out-of-distribution subjects.</li> <li> <strong>Replace Manual Auxiliaries:</strong> Instead of manually coding positions, we are exploring <strong>Learnable Time Embeddings</strong> (like in Transformers) that can adapt to the non-linear fatigue patterns of the brain.</li> <li> <strong>Graph Neural Networks:</strong> Replace the distance-based scaling with an explicit Graph Attention Network (GAT) to model the electrode topology dynamically.</li> </ol> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-subject-invariant-eeg.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/vis-llm-latent-geometry/">Visualizing LLM Latent Space Geometry Through Dimensionality Reduction</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/symbolic-connect/">From Dense Monoliths to Modular Minds: The Rise of Symbolic Routing in LLMs</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/style-representations/">Artistic Style and the Play of Neural Style Representations</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/spatial-awareness/">Where's the Chicken? Unpacking Spatial Awareness in Vision-Language Models</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>