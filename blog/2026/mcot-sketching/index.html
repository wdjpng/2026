<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Research Directions in Multimodal Chain-of-Thought (MCoT) with Sketching | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="This article explores adding sketching to Multimodal Chain-of-Thought (MCoT)reasoning to enhance AI capabilities. It reviews past methods, identifies key gaps such as the lack of sketch-rationale datasets, and proposes advancing the field through targeted data collection, unified multimodal models, and reinforcement learning. Ethical considerations include mitigating cultural bias and visual misrepresentation in generated sketches."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/mcot-sketching/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Research Directions in Multimodal Chain-of-Thought (MCoT) with Sketching",
            "description": "This article explores adding sketching to Multimodal Chain-of-Thought (MCoT)reasoning to enhance AI capabilities. It reviews past methods, identifies key gaps such as the lack of sketch-rationale datasets, and proposes advancing the field through targeted data collection, unified multimodal models, and reinforcement learning. Ethical considerations include mitigating cultural bias and visual misrepresentation in generated sketches.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Research Directions in Multimodal Chain-of-Thought (MCoT) with Sketching</h1> <p>This article explores adding sketching to Multimodal Chain-of-Thought (MCoT)reasoning to enhance AI capabilities. It reviews past methods, identifies key gaps such as the lack of sketch-rationale datasets, and proposes advancing the field through targeted data collection, unified multimodal models, and reinforcement learning. Ethical considerations include mitigating cultural bias and visual misrepresentation in generated sketches.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#motivation-to-incorporate-drawing-capabilities-into-ai">Motivation to incorporate drawing capabilities into AI</a> </div> <div> <a href="#related-work">Related work</a> </div> <div> <a href="#future-research-for-mcot-with-sketching">Future research for MCoT with sketching</a> </div> <ul> <li> <a href="#creating-a-new-mcot-sketch-dataset">Creating a new MCoT sketch dataset</a> </li> <li> <a href="#advancing-mcot-with-unified-mllms">Advancing MCoT with unified MLLMs</a> </li> <li> <a href="#improving-mcot-with-reinforcement-learning-rl-and-test-time-scaling">Improving MCoT with reinforcement learning (RL) and test-time scaling</a> </li> </ul> <div> <a href="#impact">Impact</a> </div> <div> <a href="#appendix-a-mcot-foundations">Appendix A MCoT foundations</a> </div> <div> <a href="#appendix-b-mcot-template">Appendix B MCoT template</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Drawing and sketching are cognitive tools that humans use not only to express and communicate thoughts, but also to generate new ones <d-cite key="Fan"></d-cite>. For this matter, we would like to equip any intelligent system with the same ability to improve and help it communicate its reasoning. First steps in this direction have been proposed within the field of Multimodal Chain-of-Thought (MCoT) where reasoning steps are enriched with data from different modalities, such as visuals. Therefore, future research on sketching should advance the design of MCoT reasoning strategies. Improving Multimodal Large Language Models (MLLMs) that perform such cross-modal reasoning is also relevant.</p> <h2 id="motivation-to-incorporate-drawing-capabilities-into-ai">Motivation to incorporate drawing capabilities into AI</h2> <p>Humans express and communicate ideas visually through drawing and sketching, which is a quick and loose form of drawing. Drawing is a representation of thought, but also an activity that can support ongoing cognition <d-cite key="Fan"></d-cite>. Drawing and sketching precede writing: The first documented drawings date back as far as 64,000 years <d-cite key="Hoffmann"></d-cite>. For that reason, Fan et al. <d-cite key="Fan"></d-cite> argue that drawing is one of the most enduring and versatile cognitive tools from which humans have benefited.</p> <p>One explanation for the power of drawing and sketching can be derived from cognitive enhancement and offloading strategies. According to Morrison and Richmond <d-cite key="Morrison2020"></d-cite>, technologies are used as external memories, facilitating other tasks by freeing up memory. Similarly, Osiurak et al. <d-cite key="Osiurak2018"></d-cite> show that tools such as maps can extend human’s cognitive abilities.</p> <p>Given the relevance of drawing and sketching for human thought, expression, and communication, we would want to equip any AI with the capability to also use this tool to advance and share its own ideas. Sketching can not only be a window into how AI models process information, but it is fair to assume that it can also support their reasoning.</p> <p>Reasoning in large language models (LLMs) has been greatly improved with in-context learning (ICL) <d-cite key="Min2022RethinkingTR"></d-cite> and Chain-of-Thought (CoT) techniques <d-cite key="Nye, Wei"></d-cite>. ICL helps models with additional information added to the input to find appropriate responses for a given task. With CoT, the contextual information is specifically extended by a simulation of human reasoning steps, where a task is divided into subtasks for which intermediate solutions are given so that the model can derive its final answer from them. This can be achieved by eliciting reasoning through prompting, as with ’think step-by-step’ prompts (Zero-Shot-CoT <d-cite key="Kojima2022LargeLM"></d-cite>), or by providing the model with an explicit reasoning demonstration (also called a rationale) for a given problem (Few-Shot-CoT <d-cite key="Wei"></d-cite>).</p> <p>CoT has been extended with multimodal information <d-cite key="Wang2024ExploringTR, Wang2025MultimodalCR"></d-cite> where models receive more than text to guide them toward a correct answer. This information can consist of visual, auditory, or spatio-temporal data. Sketches would be additional visual information. They could also help models to offload complex tasks and retain intermediate memories, for example, of subtasks. Therefore, an implementation of the capability to sketch in order to enhance models’ reasoning abilities should expand existing research in MCoT. A detailed account of MCoT is given in <strong>Appendix A</strong>.</p> <h2 id="related-work">Related work</h2> <p>Several recent approaches explore MCoT reasoning, though most do not fully integrate sketch generation into the reasoning process.</p> <p>Zhang et al. <d-cite key="Zhang2023MultimodalCR"></d-cite> propose a two-stage framework for multiple-choice reasoning for text and image inputs where a FLAN-AlpacaBase model <d-cite key="taori_alpaca_2023, Zheng2023JudgingLW"></d-cite> first produces a rationale, then derives the answer. Fusing text and image features from the input improves performance, but the system cannot generate new visual content. This limits applicability to reasoning scenarios that benefit from active visual exploration, such as diagram construction in geometry or mechanical design tasks.</p> <p>Meng et al. <d-cite key="Meng2023ChainOI"></d-cite> extend CoT by having an LLM produce symbolic sketch-like diagrams (e.g., with SVG), rendered into images and re-encoded for reasoning. Their ’think image by image’ approach helps, for example, with geometric tasks. However, this gain comes at the cost of operational complexity: the pipeline depends on separate LLMs, rendering engines, and encoders, creating latency and integration challenges. Unified MLLMs avoid such fragmentation and may better support generalization by learning a shared latent space for both text and sketches.</p> <p>In contrast to the previous two approaches, Liao et al. <d-cite key="Liao2025ImageGenCoTET"></d-cite> fine-tune unified MLLMs (SEED-LLaMA <d-cite key="Ge2023MakingLS"></d-cite> and SEED-X <d-cite key="Ge2024SEEDXMM"></d-cite>) on their ImageGen-CoT dataset. Reasoning steps of their models precede image generation. Test-time scaling is applied to select better outputs. While they demonstrate high-quality image generation, their evaluation focuses on aesthetics and relevance rather than measurable reasoning improvement. For reasoning-centric applications, visual fidelity without explicit reasoning gains may be insufficient.</p> <p>Hu et al. <d-cite key="Hu2024VisualSS"></d-cite> and Vinker et al. <d-cite key="Vinker2024SketchAgentLS"></d-cite> develop agentic strategies (Sketchpad, Sketchagent) where models like GPT-4o <d-cite key="Hurst2024GPT4oSC"></d-cite> or Claude3.5-Sonnet <d-cite key="TheC3"></d-cite> can decide to produce or modify sketches during problem-solving by leveraging external vision models, Python or a domain-specific language (DSL) for sketches. Models with Sketchpad iterate over a ’thought’, ’action’ (to inject sketches), and ’observation’ pattern. With this approach, Hu et al. <d-cite key="Hu2024VisualSS"></d-cite> show that allowing models to decide to insert sketches during reasoning leads to notable performance gains. However, the framework relies on external vision models to rather enhance or dissect images and a Python sketch representation, which may not capture the nuances of freehand or abstract sketches common in human reasoning.</p> <p>A truly multimodal approach for sketches would not use Python or DSLs to ’implicitly’ generate figures that the model ingests as textual input. However, few multimodal datasets that combine visuals with rationales exist. While QuickDraw <d-cite key="Jongejan_quick_draw"></d-cite> provides scale and diversity in sketch data, its lack of accompanying rationales prevents multimodal alignment learning. ScienceQA <d-cite key="lu2022learn"></d-cite> and ImageGen-CoT <d-cite key="Liao2025ImageGenCoTET"></d-cite> offer strong rationale-image pairs, but the absence of sketches means they primarily serve full-image reasoning rather than schematic reasoning. This gap suggests that the field currently lacks a dataset that balances sketch simplicity with reasoning, a pairing that could uniquely advance MCoT.</p> <p>Overall, existing MCoT work shows that visual information, including sketches, can aid reasoning. However, limitations remain: most systems either consume but do not create sketches, focus on image quality rather than reasoning improvement, or require orchestration of multiple models instead of unified generation. Furthermore, appropriate datasets with sketches in combination with rationales are lacking.</p> <h2 id="future-research-for-mcot-with-sketching">Future research for MCoT with sketching</h2> <p>Given the power of visual information for reasoning tasks, as shown by <d-cite key="Zhang2023MultimodalCR, Meng2023ChainOI, Liao2025ImageGenCoTET, Hu2024VisualSS"></d-cite>, some of the shortcomings of existing MCoT approaches can be addressed to better incorporate sketching in future research.</p> <h3 id="creating-a-new-mcot-sketch-dataset">Creating a new MCoT sketch dataset</h3> <p>To facilitate the training of MLLMs, the lack of an appropriate dataset with sketching and rationales is a limitation.</p> <p>Sketch data should be gathered and grouped within different categories, depending on the downstream task (consider Figure 1). In experimental studies with humans, Huey et al. <d-cite key="Huey2023VisualEP"></d-cite> point out that drawings differ according to their intended goal: visual explanations by the participants emphasized moving and interactive parts, while their visual depictions focused on salient features. Hu et al. <d-cite key="Hu2024VisualSS"></d-cite> show that adding auxiliary lines to geometric figures helps multimodal models such as GPT-4o to infer correct answers about these figures. Fan et al. <d-cite key="Fan"></d-cite> highlight that not all drawings are faithful depictions, but can also be abstractions whose meanings are conveyed by cultural conventions.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-mcot-sketching/sketches-480.webp 480w,/2026/assets/img/2026-04-27-mcot-sketching/sketches-800.webp 800w,/2026/assets/img/2026-04-27-mcot-sketching/sketches-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-mcot-sketching/sketches.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Figure 1:</strong> Different types of sketches and drawings: (a) depicts a geometric form that has an auxiliary line, (b) emphasizes moving parts of a machine, (c) depicts the same machine in more detail, (d) represents figures from tetris whose next moves are indicated with arrows, (e) is a conventional sketch of a heart that does not resemble actual human hearts.</p> <p>To integrate sketches into a CoT, training data should not only consist of images of drawings and sketches, but combine these with textual rationales. This would enable multimodal alignment between visual and linguistic reasoning steps. A typical template for this data could consist of instruction <em>I</em>, query <em>Q</em>, rationale <em>R</em>, and answer <em>A</em> where we could further divide <em>R</em> into ’thought’, ’sketch’, and ’observation’ with respective special tokens to guide the model, loosely following Hu et al. <d-cite key="Hu2024VisualSS"></d-cite>. An example template is given in <strong>Appendix B</strong>. Since ScienceQA and ImageGen-CoT already pair images with rationales, they could be extended with sketches to strengthen visual-textual alignment for their tasks.</p> <h3 id="advancing-mcot-with-unified-mllms">Advancing MCoT with unified MLLMs</h3> <p>To avoid multi-model orchestration and to leverage potential transfer-learning effects, further advancing reasoning of MLLMs with sketches is a promising direction. However, there exist only a few MLLMs <d-cite key="Yu2023ScalingAM, Zhao2025R1OmniEO, Zhang2023MultimodalCR, swerdlow2025unidisc"></d-cite> that can potentially handle sketch-to-text as well as text-to-sketch tasks within a unified architecture (consider Figure 2). The majority of current approaches such as Sketchpad pair VLMs such as Flamingo <d-cite key="Alayrac2022FlamingoAV"></d-cite>, PaLM-E <d-cite key="Driess2023PaLMEAE"></d-cite>, LLAVA <d-cite key="Liu2023VisualIT"></d-cite>, GPT-4o <d-cite key="Hurst2024GPT4oSC"></d-cite>, or Claude3-Opus and Claude3.5-Sonnet <d-cite key="TheC3"></d-cite> with text-to-image models.</p> <p>Unified MLLMs can be divided into autoregressive (AR) and diffusion-based MLLMs. For example, CM3Leon <d-cite key="Yu2023ScalingAM"></d-cite> from Meta is a Transfomer-based AR decoder that can generate both text and images. It is built on the CM3 model <d-cite key="Aghajanyan2022CM3AC"></d-cite>. CM3Leon has been trained on text-guided image editing, image-to-image grounding tasks where visual features can be derived from images, and text-to-image generations.</p> <p>Swerdlow et al. <d-cite key="swerdlow2025unidisc"></d-cite> introduce a unified multimodal discrete diffusion model (UniDisc). While the model’s architecture consists of a Transformer (bidirectional) decoder, its training goal is not to auto-regressively predict the next tokens in a sequential manner (e.g., left to right for text or top to bottom for image patch rasters), but to predict the distribution of tokens via a denoising process that allows parallel predictions as well as later refinements. The training of UniDisc is realized with a denoising process of corrupted inputs (masking). In contrast to continuous diffusion models, Swerdlow et al. <d-cite key="swerdlow2025unidisc"></d-cite> use discrete noising and denoising for both images and texts. Swerdlow et al. <d-cite key="swerdlow2025unidisc"></d-cite> show that UniDisc outperforms the same architecture without a diffusion objective with respect to image and text classification tasks. The model is also capable of inpainting and infilling missing parts of an input, which no AR model can do. However, these performance gains come at a cost: UniDisc requires 13.2 times longer than its AR counterpart to reach equivalent loss levels <d-cite key="swerdlow2025unidisc"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-mcot-sketching/mcot-480.webp 480w,/2026/assets/img/2026-04-27-mcot-sketching/mcot-800.webp 800w,/2026/assets/img/2026-04-27-mcot-sketching/mcot-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-mcot-sketching/mcot.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Figure 2:</strong> MCoT involving sketches with a Multimodal Large Language Model (MLLM). Black arrows represent sequential auto-regressive processing, while blue arrows illustrate the bidirectionality of diffusion models. The model’s reasoning is guided by special tokens, such as &lt;think&gt;.</p> <p>Models like UniDisc provide an interesting model class for MCoT. While current diffusion language models (DLMs) might not rival AR LLMs due to training inefficiencies <d-cite key="swerdlow2025unidisc"></d-cite> or speed <d-cite key="dream2025"></d-cite>, the strength of multimodal DLMs in handling and generating multimodal data – as shown by Swerdlow et al. <d-cite key="swerdlow2025unidisc"></d-cite> – warrants further research. Their ability to inpaint and infill would be particularly helpful for amending visualizations, which is a core aspect of explanatory sketching. Research in this direction could be informed by Diffusion-of-Thought (DoT) proposed by Ye et al. <d-cite key="Ye2024DiffusionOT"></d-cite>, who fine-tune a DLM for CoT. However, diffusion models require a fixed output size. This is a challenge that needs to be addressed to allow versatile reasoning over different tasks.</p> <h3 id="improving-mcot-with-reinforcement-learning-rl-and-test-time-scaling">Improving MCoT with reinforcement learning (RL) and test-time scaling</h3> <p>Existing work on MCoT <d-cite key="Zhang2023MultimodalCR, Meng2023ChainOI, Liao2025ImageGenCoTET"></d-cite> has mainly relied on supervised fine-tuning (SFT). However, other work in <em>reasoning</em> has shown that RL leads to improvements <d-cite key="DeepSeekAI2025DeepSeekR1IR, Ranaldi2025MultilingualRV, Zhao2025R1OmniEO"></d-cite>. Therefore, MCoT could be advanced with Direct Preference Optimization (DPO) <d-cite key="Rafailov2023DirectPO"></d-cite>, Reinforcement Learning with Verifiable Rewards (RLVR) <d-cite key="DeepSeekAI2025DeepSeekR1IR"></d-cite> and Group Relative Policy Optimization (GRPO) <d-cite key="Shao2024DeepSeekMathPT"></d-cite> strategies. One straight-forward application would be to use RLVR with GRPO, following Deepseek’s R1 <d-cite key="DeepSeekAI2025DeepSeekR1IR"></d-cite>, to reward accuracy (\(R_{acc}\)) and format (\(R_{format}\)) for rationales and answers based on generated sketches.</p> <p>An appropriate reward for the generation of sketches could leverage AR-GRPO for autoregressive MLLMs <d-cite key="Yuan2025ARGRPOTA"></d-cite>. AR-GRPO realizes rewards for the generation of images with a multi-faceted reward function that ensures (a) consistency with the textual input condition through CLIP <d-cite key="Radford2021LearningTV"></d-cite> and Human Preference Score v2 <d-cite key="Wu2023HumanPS"></d-cite>, (b) image quality with MANIQA <d-cite key="Yang2022MANIQAMA}"></d-cite>, and (c) a further realism reward through a VLM, such as Qwen2.5-VL-3B-Instruct <d-cite key="Bai2025Qwen25VLTR"></d-cite>. This function is used with GRPO to improve the quality of generated images. Since the proposed rewards by Yuan et al. <d-cite key="Yuan2025ARGRPOTA"></d-cite> focus on overall quality, a specific reward should be conceived for sketches. For example, a sketch can consist of a hierarchy of strokes whose meaning can be of different importance. It would be interesting to incorporate this somehow into the reward: Should sketches with a limited amount of strokes be prioritized?</p> <p>In the wake of Liao et al. <d-cite key="Liao2025ImageGenCoTET"></d-cite>, existing MCoT could be further improved with Test-time scaling methods, sampling more CoTs and sketches to select the best candidates with an appropriate scoring method. This approach could also be used with agentic frameworks that pair VLMs with image generators and would not require any additional training of the models.</p> <p>Beyond standard accuracy on downstream tasks, evaluation should measure how sketches contribute to the reasoning process. This includes interpretability (e.g., can a human follow the model’s reasoning with a sketch?), task completion time (one of the biggest bottlenecks because image generation requires many tokens), error localization, and robustness under noisy or incomplete inputs. Additionally, user studies could assess subjective clarity and helpfulness of generated sketches.</p> <h2 id="impact">Impact</h2> <p>MLLMs with sketching would have an impact on AI in different domains. For example, agentic systems such as Auto-GUI <d-cite key="Zhang2023YouOL"></d-cite> that interact with graphical user interfaces or websites could be enhanced by providing them with additional visual information with sketches. Similarly, embodied AI systems, such as EmbodiedGPT <d-cite key="Mu2023EmbodiedGPTVP"></d-cite> whose backbone uses a combination of vision and language models that help navigate the real world, could reason about their surroundings using sketches. MLLMs for STEM education could also benefit from the ability to make their reasoning more transparent with additional drawings as proposed in Meng et al. <d-cite key="Meng2023ChainOI"></d-cite>. In sum, sketching would help all reasoning models not only to enhance their thoughts, but also communicate them with more than one modality.</p> <p>As with language, sketches are not neutral representations. The ability of AI systems to generate and reason with sketches introduces risks of cultural bias, visual misrepresentation, and domain-specific inaccuracies. For example, the “heart” symbol in Figure 1(e) is globally recognized in popular culture but anatomically incorrect; in medical education, reasoning over such a schematic could reinforce misconceptions. Similar issues may arise if models default to culturally specific diagrammatic conventions, omit critical features due to dataset biases, or overgeneralize from training examples.</p> <p>Ethical safeguards should address the entire MCoT-with-sketching workflow. Dataset curation must ensure diversity of styles, cultural perspectives, and schematic conventions. Annotation guidelines should clarify the intended use and accuracy requirements of sketches. Model evaluation should include bias detection for visual outputs, alongside interpretability checks so users can trace how a sketch influenced reasoning.</p> <h2 id="appendix-a-mcot-foundations">Appendix A MCoT foundations</h2> <p>Following Wang et al. <d-cite key="Wang2025MultimodalCR"></d-cite>, we can define prompt, instruction, query, answer, and rationale with \(P\) , \(I\), \(Q\), \(A\), and \(R\), which are all token sequences. A Chain-of-Thought (CoT) would be:</p> <p>\begin{equation} P_{CoT} = {I, (x_1, e_1, y_1), …, (x_n, e_n, y_n)} \end{equation}</p> <p>where \(x_i \in Q\) and \(y_i \in A\) are questions with corresponding answers and \(e_i \in R\) is an example rationale. The joint probability of generating an answer A and a rationale R given the prompt \(P_{CoT}\) and a query \(Q\) would be <d-cite key="Wang2025MultimodalCR"></d-cite>:</p> <p>\begin{equation} p(A, R |P_{CoT}, Q) = p(R |P_{CoT}, Q) \cdot p(A |P_{CoT}, Q, R) \end{equation}</p> <p>where the model should output rationale \(R\) with the tokens \(r_1, ..., r_i\) before arriving at the answer \(A\) consisting of the tokens \(a_1, ..., a_i\). The goal in training a reasoning model \(F\) is to jointly maximize the likelihood of equation (2).</p> <p>Finally, all components \(P\), \(Q\), \(A\), and \(R\) can be enriched with multimodal information \(\mathcal{M}\). For example with MCoT, a rationale \(R\) should handle \(\mathcal{M}\) input and generate multimodal information (e.g., a sketch) as well as text \(T\), that is, \(R\in\{M, M\oplus T\}\) <d-cite key="Wang2025MultimodalCR"></d-cite>.</p> <h2 id="appendix-b-mcot-template">Appendix B MCoT template</h2> <p><code class="language-plaintext highlighter-rouge"> { "instruction": "Find proofs for geometry problems.", "query": "Prove the angles of ABC provided in the attached image sum to 180. &lt;image&gt; VT_011 VT_115 VT_563 VT_101 ... VT_909 &lt;/image&gt;", "rationale": "&lt;think&gt; I need to figure out how ABC are related in the image. The image shows a triangle. I need to prove that the angles of the triangle sum to 180. To find an answer, I draw a triangle: Let's call it ABC. &lt;sketch&gt; VT_421 VT_105 VT_983 VT_002 ... VT_778 &lt;/sketch&gt; I extend the sides from A to B, from A to C, and from B to C. &lt;sketch&gt; VT_421 VT_105 VT_983 VT_001 ... VT_708 &lt;/sketch&gt; I draw a line parallel to AB through point C. &lt;sketch&gt; VT_420 VT_105 VT_983 VT_001 ... VT_718 &lt;/sketch&gt; &lt;observe&gt; The angles at point C created by the parallel line correspond to the interior angles at points A and B. When I add those angles up, they form a straight line at point C, which measures 180. Since those angles correspond exactly to the three interior angles of the triangle, the sum of the interior angles is 180. &lt;/observe&gt; This proof follows from the alternate interior angles theorem. &lt;/think&gt;", "answer": "The alternate interior angles theorem shows that all angles at point C created by the parallel line sum to 180. They further correspond to the interior angles at points A and B. Therefore, the angles of ABC provided in the attached image sum to 180." } </code></p> <p>MCoT template with instruction \(I\), query \(Q\), rationale \(R\), and answer \(A\) where \(R\) is further divided into “thought”, “sketch”, and “observation” with respective special tokens to guide the model. VT_n tokens correspond to image tokens.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-mcot-sketching.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/general-agent-evaluation/">Ready For General Agents? Let's Test It.</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-vlms-waste-their-vision/">Why vlms waste their vision</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>