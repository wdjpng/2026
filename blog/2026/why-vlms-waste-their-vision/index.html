<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Why vlms waste their vision | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Despite the robustness of standalone vision encoders, they often collapse to near-chance performance within Vision Language Models (VLMs) by ignoring visual data in favor of language priors. We investigate this paradox by reconciling conflicting theoretical and empirical literature through the lens of attention budgets and information exchange rates. Ultimately, we propose a new mental model that explains why standard multimodal fusion fails and how to restore effective integration."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/why-vlms-waste-their-vision/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Why vlms waste their vision",
            "description": "Despite the robustness of standalone vision encoders, they often collapse to near-chance performance within Vision Language Models (VLMs) by ignoring visual data in favor of language priors. We investigate this paradox by reconciling conflicting theoretical and empirical literature through the lens of attention budgets and information exchange rates. Ultimately, we propose a new mental model that explains why standard multimodal fusion fails and how to restore effective integration.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Why vlms waste their vision</h1> <p>Despite the robustness of standalone vision encoders, they often collapse to near-chance performance within Vision Language Models (VLMs) by ignoring visual data in favor of language priors. We investigate this paradox by reconciling conflicting theoretical and empirical literature through the lens of attention budgets and information exchange rates. Ultimately, we propose a new mental model that explains why standard multimodal fusion fails and how to restore effective integration.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#background">Background</a> </div> <div> <a href="#the-image-text-exchange-rate">The image-text exchange rate</a> </div> <div> <a href="#measuring-modality-usage">Measuring Modality Usage</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#dependency-test-minimal-exchange-rate-criterion">Dependency Test (Minimal Exchange-Rate Criterion)</a> </li> <li> <a href="#counterfactual-override-test-text-dominance-detection">Counterfactual Override Test (Text Dominance Detection)</a> </li> <li> <a href="#cross-modal-consistency-test-fusion-quality-measurement">Cross-Modal Consistency Test (Fusion Quality Measurement)</a> </li> <li> <a href="#perturbation-sensitivity-test-attention-budget-probe">Perturbation Sensitivity Test (Attention Budget Probe)</a> </li> <li> <a href="#information-contribution-test-exchange-rate-in-bits">Information Contribution Test (Exchange Rate in Bits)</a> </li> <li> <a href="#uncertainty-reduction-test-decision-level-synergy">Uncertainty Reduction Test (Decision-Level Synergy)</a> </li> </ul> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="background">Background</h2> <p>To understand why VLMs behave so strangely, we need both theory that specifies what should happen when we combine modalities, and empirical evidence that reveals what actually happens in current systems. A growing body of work now makes it clear that these two perspectives are in tension.</p> <p>On the theoretical side, Better Together <d-cite key="gupta2025bettertogetherleveragingunpaired"></d-cite>gives a strikingly optimistic view of multimodality. They show that adding an auxiliary modality can only increase or preserve the Fisher information<d-footnote> Fisher Information measures how much a model’s observations constrain its belief about the correct answer. Informally, it quantifies how sharply the model’s output distribution changes in response to changes in the underlying signal. More Fisher information means less uncertainty and more decisive predictions. </d-footnote> available to a model, and empirically demonstrate that unpaired text, audio, or images consistently improve downstream performance of unimodal encoders. In this picture, modalities behave like complementary measurements of the same underlying signal: more channels should never hurt, and multimodal pretraining becomes a general recipe for building stronger unimodal models.</p> <p>The empirical story from large VLMs is far less rosy. Hidden in Plain Sight<d-cite key="fu2025hidden"></d-cite> finds that many state-of-the-art VLMs barely use their visual representations at all: scrambling, masking, or even replacing the image often leads to negligible changes in the output across a range of downstream tasks. Related work in vision–language reasoning and abstract shape recognition echoes the same theme: models that appear multimodal by design often solve benchmarks using predominantly linguistic shortcuts with minimal dependence on pixels<d-cite key="hemmat2024hiddenplainsightevaluating"></d-cite>. This mirrors older observations from multimodal surveys and VQA: text only baselines can rival full models, and spurious language priors frequently dominate over visual evidence<d-cite key="baltrušaitis2017multimodalmachinelearningsurvey"></d-cite>. These findings suggest that the optimistic guarantees of theory rely on assumptions that real systems routinely violate: that modalities are fused without distortion, that their contributions are balanced, and that the model has enough capacity to process them jointly rather than letting one overwhelm the other.</p> <p>Taken together, this body of work exposes a conceptual gap. We have strong theoretical results predicting monotonic gains from adding modalities, and equally strong empirical evidence that large models often ignore, underweigh or sometimes even misuse those modalities. We lack principles and metrics for characterizing how information from each modality is integrated, and how much influence each modality actually has on the final prediction. We aim to address this gap by viewing multimodal models through the lens of an information exchange rate i.e how many “bits” of effective evidence an image contributes relative to text and a finite attention budget which is how much representational and computational capacity the model allocates to each modality as context grows. Having these tools, we can begin to diagnose not just whether multimodal fusion fails, but why it fails and when it silently collapses into text-only behavior despite having a powerful vision backbone.</p> <h2 id="the-image-text-exchange-rate">The image-text exchange rate</h2> <p>If multimodality is supposed to help, how much does each modality contribute? And can we say anything quantitative about when one modality should override another? A useful way to think about this is through what we’ll call the information exchange rate: how many bits of useful information does an image contribute relative to text, and how does a VLM decide which to trust when the two compete.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-why-vlms-waste-their-vision/diagram_info_exchange_rate-480.webp 480w,/2026/assets/img/2026-04-27-why-vlms-waste-their-vision/diagram_info_exchange_rate-800.webp 800w,/2026/assets/img/2026-04-27-why-vlms-waste-their-vision/diagram_info_exchange_rate-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-why-vlms-waste-their-vision/diagram_info_exchange_rate.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 1: A mental model for the image-text exchange rate presented with the analogy of a balance scale. Image generated with Nano banana pro.</figcaption> </figure> <p>The optimistic view, grounded in theory like Better Together<d-cite key="gupta2025bettertogetherleveragingunpaired"></d-cite>, is that modalities should behave like complementary information channels. Under this lens, an image and a sentence each provide different perspective of the same world. So combining them should give the model more Fisher information and in turn, a better estimate of the target. In an ideal multimodal system, adding auxiliary text is like adding more windows to the same reality, in other words it should never make the model worse. However prior work suggest a more complicated reality. Studies on modality dominance<d-cite key="baltrušaitis2017multimodalmachinelearningsurvey"></d-cite>, linguistic priors in VQA<d-cite key="agrawal2018dontjustassumelook"></d-cite>, and error analysis of image–to-text benchmarks<d-cite key="jabri2016revisitingvisualquestionanswering"></d-cite> consistently show that models learn uneven “exchange rates” between modalities. In many architectures, textual signals are treated as cheap and abundant, while visual signals though rich are compressed into a handful of tokens and passed through shallow fusion layers. The result is that text often overwhelms vision, even when the image provides the decisive evidence for more complex tasks.</p> <p>This perspective helps reconcile the tension between theory and practice. The theory assumes each modality contributes information cleanly; the empirical literature shows that models frequently assign skewed exchange rates, making one modality effectively “expensive” and the other “free”. When this happens, adding more text doesn’t just fail to help it actively distorts the model’s internal representation, drowning out the visual evidence entirely.</p> <h2 id="measuring-modality-usage">Measuring Modality Usage</h2> <p>Up to this point, we’ve framed multimodal behavior in terms of exchange rates: how much effective evidence an image contributes relative to text. But an exchange rate does not exist in a vacuum. In any real system, information must be processed through a finite computational pathway and in transformer based VLMs, that pathway is governed by attention.</p> <p>This is where the notion of an attention budget becomes essential. A VLM does not have unlimited capacity to process every modality equally. Instead, it allocates a finite amount of representational and computational capacity across tokens. As context grows, this budget must be divided implicitly among text tokens and visual tokens. When one modality is abundant and native to the architecture (text) and the other is sparse and compressed (images), this allocation can become highly skewed.</p> <p>Seen this way, the information exchange rate between image and text is enforced mechanically by attention. A modality whose tokens consistently receive less attention will have less opportunity to influence intermediate representations and, ultimately, the model’s predictions. In other words, a modality’s effective exchange rate is bounded by how much attention budget it receives.</p> <p>We propose that the missing link is that modern VLMs may violate the implicit assumptions of multimodal theory not at the level of representation quality, but at the level of capacity allocation. Visual information may exist, but never meaningfully participate in the computation that produces the output. This brings us to a natural question:</p> <blockquote> How can we tell whether a model is actually spending attention budget on vision, and whether the image has a meaningful exchange rate relative to text? </blockquote> <p>Answering this does not require new benchmarks or training regimes. It requires is a way to probe whether visual information changes the model’s internal computation and output distribution in measurable ways. Below, we outline a small set of diagnostic tests as ways of operationalizing exchange rates and attention budgets in real models.</p> <p>Broadly, within this framework, a model “uses” an image only if visual input meaningfully participates in the computation under a finite attention budget. Concretely:</p> <p>A fair exchange rate means the image shifts predictions or reduces uncertainty relative to text alone.</p> <p>An attention budget failure means that as textual context grows, visual tokens lose influence even when they are informative.</p> <p>A fusion bottleneck means that image signals never couple with language representations in a way that affects downstream reasoning.</p> <p>The tests below should be read as different lenses on the same underlying phenomenon: whether vision is allowed to compete for attention and influence on equal terms with text.</p> <h2 id="dependency-test-minimal-exchange-rate-criterion">Dependency Test (Minimal Exchange-Rate Criterion)</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-why-vlms-waste-their-vision/dependency_failure-480.webp 480w,/2026/assets/img/2026-04-27-why-vlms-waste-their-vision/dependency_failure-800.webp 800w,/2026/assets/img/2026-04-27-why-vlms-waste-their-vision/dependency_failure-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-why-vlms-waste-their-vision/dependency_failure.jpeg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 2: Visual representation of a model's functional dependence failure where changing the image does not alter the model's output logits. Image generated with Nano banana pro.</figcaption> </figure> <p>The most basic requirement for visual usage is <strong>functional dependence</strong>:</p> \[f(x_{\text{img}}, x_{\text{text}}) \neq f(x'_{\text{img}}, x'_{\text{text}})\] <p>for $(x_{\text{img}} \neq x’_{\text{img}})$.</p> <p>Here:</p> <ul> <li>$f(\cdot)$ denotes the model’s forward computation (e.g., the output logits),</li> <li>$x_{\text{img}}$ and $x’_{\text{img}}$ denote two different images, and</li> <li>$x_{\text{text}}$ denotes the textual input, held fixed.</li> </ul> <p>By <em>functional dependence</em>, we mean that the model’s output must change as a function of the image input—holding all other inputs constant. If varying the image does not alter the output, then the image does not exert a causal influence on the computation, regardless of how rich its internal representation may be. The effective exchange rate of vision is zero, no amount of visual information survives the fusion pathway. This is the extreme failure mode documented in <em>Hidden in Plain Sight</em><d-cite key="fu2025hidden"></d-cite>, where image scrambling or replacement leaves predictions almost unchanged.</p> <p>From the attention-budget perspective, this corresponds to a system where visual tokens receive so little attention that their contribution is effectively erased before reaching the decision layers.</p> <h2 id="counterfactual-override-test-text-dominance-detection">Counterfactual Override Test (Text Dominance Detection)</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-why-vlms-waste-their-vision/coounterfactual_failure-480.webp 480w,/2026/assets/img/2026-04-27-why-vlms-waste-their-vision/coounterfactual_failure-800.webp 800w,/2026/assets/img/2026-04-27-why-vlms-waste-their-vision/coounterfactual_failure-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-why-vlms-waste-their-vision/coounterfactual_failure.jpeg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 3: Counterfactual Override test scenario showing the tension between text-implied class and vision implied class</figcaption> </figure> <p>In less extreme cases, the model may attend to vision weakly, but not strongly enough to override text when the two disagree. Let $c_i$ denote the class implied by the image and $c_t$ denote the class suggested by the text. If the system is genuinely using visual information, we expect:</p> \[P(c_i \mid x_{\text{img}}, x_{\text{text}}) &gt; P(c_t \mid x_{\text{img}}, x_{\text{text}})\] <p>Here, $P(c \mid x_{\text{img}}, x_{\text{text}})$ denotes the model’s predicted probability for candidate output $c$ given both visual input $x_{\text{img}}$ and textual input $x_{\text{text}}$.</p> <p>Failure here signals that the <strong>exchange rate skews heavily toward text</strong>, a phenomenon repeatedly observed in VQA models<d-cite key="goyal2017makingvvqamatter"></d-cite><d-cite key="agrawal2018dontjustassumelook"></d-cite>. Even large VLMs still display this “linguistic gravity” i.e text drags predictions toward strong priors regardless of what the pixels say.</p> <h2 id="cross-modal-consistency-test-fusion-quality-measurement">Cross-Modal Consistency Test (Fusion Quality Measurement)</h2> <p>Even if vision sometimes shifts outputs, it may still fail to interact with language representations in a meaningful way. If a model truly integrates modalities, image and text representations should be informationally entangled, rather than processed in isolation. Let $h_{\text{img}}$ and $h_{\text{text}}$ denote the internal representations derived from the image and text, respectively, and let $y$ denote the target prediction.<br> We use $I$ to denote <em>conditional mutual information</em>, which measures how much knowing one representation reduces uncertainty about the other given the task outcome.</p> <p>Formally, for effective multimodal fusion:</p> \[I(h_{\text{img}} ; h_{\text{text}} \mid y) &gt; 0.\] <p>Low conditional mutual information indicates late-fusion collapse: modalities are processed independently and combined only superficially. In exchange-rate terms, this is a broken market i.e the modalities never negotiate shared evidence. In attention-budget terms, it suggests that attention flows remain separated across modality-specific pathways.</p> <h2 id="perturbation-sensitivity-test-attention-budget-probe">Perturbation Sensitivity Test (Attention Budget Probe)</h2> <p>Attention budgets are not directly visible, but they leave fingerprints. If vision receives meaningful attention, then small, structured perturbations to the image should affect the output. Let $f(\cdot)$ denote the model’s forward computation, $x_{\text{img}}$ the original image input, and $\delta$ a small, semantically meaningful image perturbation (e.g., masking, cropping, or color shift).<br> We define the sensitivity of the output to visual perturbations as:</p> \[\Delta f = \lVert f(x_{\text{img}}) - f(x_{\text{img}} + \delta) \rVert &gt; 0.\] <p>Here, $\lVert \cdot \rVert$ denotes a suitable norm measuring change in the model’s output (e.g., logit distance or probability shift). When outputs are insensitive to such perturbations, it suggests that the model allocates little effective capacity to processing visual tokens. This provides a behavioral proxy for attention-budget collapse and connects naturally to robustness and saliency analyses with the key distinction that the focus here is modality-level influence, not pixel-level explanation.</p> <h2 id="information-contribution-test-exchange-rate-in-bits">Information Contribution Test (Exchange Rate in Bits)</h2> <p>From the perspective of theoretical works like Better Together<d-cite key="gupta2025bettertogetherleveragingunpaired"></d-cite>, adding a modality should never increase uncertainty. Let $(Y)$ denote the target prediction (e.g., a class label or answer), and let<br> $(x_{\text{text}})$ and $(x_{\text{img}})$ denote the textual and visual inputs, respectively.<br> We use $(H(\cdot))$ to denote <em>entropy</em>, which measures uncertainty in the model’s predictive distribution.</p> <p>For a well-functioning fusion mechanism:</p> \[H(Y \mid x_{\text{text}}, x_{\text{img}}) &lt; H(Y \mid x_{\text{text}}).\] <p>When adding an image (or additional text) increases entropy or reduces accuracy, this indicates negative exchange rates, a regime where the modality injects noise rather than signal. This directly violates the guarantees assumed in works like Better Together<d-cite key="gupta2025bettertogetherleveragingunpaired"></d-cite> and exposes where theory and practice diverge.</p> <h2 id="uncertainty-reduction-test-decision-level-synergy">Uncertainty Reduction Test (Decision-Level Synergy)</h2> <p>Finally, even when accuracy remains unchanged, visual information should make the model more certain about its prediction rather than less.</p> <p>Let $c$ index candidate outputs (e.g., class labels or answer choices), and let<br> $P(c \mid x_{\text{text}}, x_{\text{img}})$ denote the model’s predicted probability for output $c$ given both the textual input $x_{\text{text}}$ and the visual input $x_{\text{img}}$.<br> We define the model’s confidence under multimodal input as:</p> \[C_{\text{img+text}} \;=\; \max_{c} P(c \mid x_{\text{text}}, x_{\text{img}}).\] <p>A model can be said to <em>use</em> vision if adding the image does not reduce this confidence, i.e.,</p> \[C_{\text{img+text}} \;\ge\; C_{\text{text-only}},\] <p>consistently across samples.</p> <p>Here, $C$ represents the model’s confidence in its most likely output.<br> If confidence instead decreases or the predictive distribution becomes more diffuse when vision is added, the model is operating with a <strong>malformed exchange rate</strong>, a regime where visual input increases epistemic uncertainty instead of reducing it.<br> From an attention-budget perspective, this suggests that capacity is being spent on conflicting or weakly integrated visual cues rather than sharpening the model’s belief.</p> <h2 id="conclusion">Conclusion</h2> <p>The paradox of “blind” Vision Language Models is not merely a failure of training data or encoder quality; it is a structural failure of resource allocation. As we have explored, the optimistic guarantees of works like Better Together crash against the harsh reality of Hidden in Plain Sight<d-cite key="fu2025hidden"></d-cite> because current architectures lack the mechanisms to enforce a fair information exchange rate.</p> <p>By viewing multimodal fusion through the lens of attention budgets, we can see that text and vision do not simply cooperate but compete. In this internal economy, language priors are often “cheap” and abundant, while visual evidence is “expensive” to process and easy to discard. When the exchange rate is skewed, models inevitably learn to bankrupt their visual pathways, collapsing into text-only reasoning even while retaining powerful vision backbones.</p> <p>The diagnostic tests proposed here, ranging from functional dependence to perturbation sensitivity, offer a way to move beyond deceptive top-line metrics. They allow us to ask not just if a model got the right answer, but how it arrived there. Ultimately, solving this paradox requires more than just scaling up. It demands a shift in design philosophy. We must move away from architectures that assume fusion just works, and toward systems that explicitly manage the attention economy. Future work must focus on enforcing a non-zero exchange rate for vision, ensuring that pixels are not just present in the input, but are solvent in the computation. Only then can we build VLMs that don’t just look, but actually see.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-why-vlms-waste-their-vision.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-long-context/">Text-as-Image, A Visual Encoding Approach for Long-Context Understanding</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>