<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Generative AI Archaeology | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="We document the rise of the Generative AI Archaeologist, whose tools include linear algebra and probability theory, jailbreaking, and debuggers, compared to the metal detectors, pickaxes, and radar surveys of traditional archaeology. GenAI Archaeologists have reported findings both through luck by observing unexpected behaviour in publicly accessible models, and by exploiting the mathematical properties of models. In this blog, we survey five types of findings unearthed by GenAI Archaeologists and discuss the status of those findings."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/genai-archaeology/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Generative AI Archaeology",
            "description": "We document the rise of the Generative AI Archaeologist, whose tools include linear algebra and probability theory, jailbreaking, and debuggers, compared to the metal detectors, pickaxes, and radar surveys of traditional archaeology. GenAI Archaeologists have reported findings both through luck by observing unexpected behaviour in publicly accessible models, and by exploiting the mathematical properties of models. In this blog, we survey five types of findings unearthed by GenAI Archaeologists and discuss the status of those findings.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Generative AI Archaeology</h1> <p>We document the rise of the Generative AI Archaeologist, whose tools include linear algebra and probability theory, jailbreaking, and debuggers, compared to the metal detectors, pickaxes, and radar surveys of traditional archaeology. GenAI Archaeologists have reported findings both through luck by observing unexpected behaviour in publicly accessible models, and by exploiting the mathematical properties of models. In this blog, we survey five types of findings unearthed by GenAI Archaeologists and discuss the status of those findings.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#inferring-training-data">Inferring Training Data</a> </div> <div> <a href="#discovering-model-training-procedures">Discovering Model Training Procedures</a> </div> <div> <a href="#stealing-part-of-a-deployed-model">Stealing Part of a Deployed Model</a> </div> <div> <a href="#exfiltrating-system-prompts">Exfiltrating System Prompts</a> </div> <div> <a href="#outlook">Outlook</a> </div> <div> <a href="#acknowledgements">Acknowledgements</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-genai-archaeology/intro_images-480.webp 480w,/2026/assets/img/2026-04-27-genai-archaeology/intro_images-800.webp 800w,/2026/assets/img/2026-04-27-genai-archaeology/intro_images-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-genai-archaeology/intro_images.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Figure 1: Examples of archaeological discoveries (a) <a href="https://commons.wikimedia.org/w/index.php?curid=17377542" rel="external nofollow noopener" target="_blank">Göbekli Tepe</a> was found through professional survey, (b) the <a href="https://commons.wikimedia.org/w/index.php?curid=48964317" rel="external nofollow noopener" target="_blank">Terracotta Army</a> was found by farmers by complete luck, and (c) the <a href="https://commons.wikimedia.org/w/index.php?curid=164647291" rel="external nofollow noopener" target="_blank">Antikythera mechanism</a> was found by coincidence in shipwreck.</p> <p>Archaeology reveals the secrets of human history through the traces left behind by our ancestors. Some discoveries are the result of careful survey using sophisticated tools to uncover what cannot be seen by the naked eye. One of the earliest examples of human village habitation is found at Göbekli Tepe, Turkey, first discovered in 1963, but only excavated in 1995. This village offers important new insights about early farming culture that changed our understanding of early human history. Other discoveries are the result of pure luck: the Terracotta Army of Xi’an, China, is one such example. This grand funeral act for the First Emperor of China was described in ancient Chinese texts but lost to time; its location was only re-discovered in 1974 by a group of rural farmers. A little closer to the heart of computer scientists, the Antikythera mechanism was found in a shipwreck off the coast of Antikythera, Greece, in 1901. This finding is more of a coincidence, since divers were already actively working in the area. The mechanism is an analogue computer, preserved for nearly 2,000 years, that was used to predict the position of celestial bodies. Such is the sophistication of this device, that there is no evidence of similar complexity until 1,400 years later<d-cite key="marchant2006search"></d-cite>.</p> <p>In contrast to studying historical physical artefacts, Generative AI models, such as LLMs, are digital objects that should not require anyone to search for lost knowledge. However, the secretive nature of fully closed<d-cite key="eiras2024near"></d-cite> development has given rise to <strong>Generative AI Archaeology</strong>. The tools of GenAI Archaeologists are a solid grasp of linear algebra and probability theory, jailbreaking, and <code class="language-plaintext highlighter-rouge">pdb</code>, compared to the metal detectors, pickaxes, and radar surveys of traditional archaeology. GenAI Archaeologists have reported findings through luck by observing unexpected behaviour in publicly accessible models<d-cite key="nasr2025scalable"></d-cite><d-cite key="behnamghaderllm2vec"></d-cite>, and through careful study by exploiting the mathematical properties of the underlying model<d-cite key="carlini2021extracting"></d-cite><d-cite key="hayase2024data"></d-cite><d-cite key="finlayson2024logits"></d-cite><d-cite key="carlini2024stealing"></d-cite>. In this blog, we will cover five findings unearthed by GenAI Archaeologists and discuss the current status of those findings. We welcome comments on discoveries that we have overlooked, and discoveries that cover the broader class of machine learning systems.</p> <h2 id="inferring-training-data">Inferring Training Data</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-genai-archaeology/training_data-480.webp 480w,/2026/assets/img/2026-04-27-genai-archaeology/training_data-800.webp 800w,/2026/assets/img/2026-04-27-genai-archaeology/training_data-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-genai-archaeology/training_data.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Figure 2: (a) Hayase et al. show how to infer the distribution of data used to train a tokenizer based on how BPE constructs merge lists (image source: <a href="https://openreview.net/forum?id=EHXyeImux0" rel="external nofollow noopener" target="_blank">Figure 1</a>). (b) Nasr et al. demonstrate a simple attack to extract training data from language models by forcing them to repeat the same token (image source: <a href="https://openreview.net/forum?id=vjel3nWP2a" rel="external nofollow noopener" target="_blank">Figure 1</a>).</p> <p>Training data is fuel that powers Large Language Models. Open-science initiatives, such as Pythia<d-cite key="biderman2023pythia"></d-cite>, OLMo<d-cite key="groeneveld2024olmo"></d-cite>, and PleIAs<d-cite key="langlais2025pleias"></d-cite> document everything about their data, making it possible to understand the interplay between the training data and LLM<d-cite key="elazar2024whats"></d-cite><d-cite key="liu2025olmotrace"></d-cite>. This is in stark contrast to fully closed models<d-footnote>See news reports on the legal cases against OpenAI and Anthropic for recent examples of the lack of information shared about LLM training data: <a href="https://news.bloomberglaw.com/ip-law/openai-risks-billions-as-court-weighs-privilege-in-copyright-row" rel="external nofollow noopener" target="_blank">OpenAI Risks Billions as Court Weights Privilege in Copyright Row</a>. <a href="https://www.npr.org/2025/09/05/nx-s1-5529404/anthropic-settlement-authors-copyright-ai" rel="external nofollow noopener" target="_blank">Anthropic settles with authors in first-of-its-kind AI copyright infringement lawsuit</a>.</d-footnote>, but researchers have managed to learn some of the secrets of the data.</p> <p><strong>Tokenizer Data:</strong> Hayase et al. show how to infer properties of the data used to train a tokenizer, which underpins how LLMs process text<d-cite key="hayase2024data"></d-cite>. Their approach, illustrated in Figure 2(a), is based on how Byte-Pair Encoding token merge lists are created, and the implications for data used to train the tokenizer. This insight allowed them to accurately infer known properties about publicly disclosed models, and to predict the properties of private models.</p> <p><strong>Pretraining Data:</strong> Carlini et al. show how to extract verbatim sequences from the GPT-2 language model<d-cite key="carlini2021extracting"></d-cite>. Their work is <a href="https://nicholas.carlini.com/writing/2025/privacy-copyright-and-generative-models.html" rel="external nofollow noopener" target="_blank">primarily focused on understanding privacy attacks</a> on language models, in which a model may reveal personal identifying information. More directly related to detecting copyright violations, Karamolegkou et al. prompted open and closed LLMs with prefixes of copyrighted material from books<d-cite key="karamolegkou2023copyright"></d-cite>. Finally, Nasr et al. demonstrated a simple attack on ChatGPT that involved forcing it to generate the same token repeatedly, as shown in Figure 2(b)<d-cite key="nasr2025scalable"></d-cite>. This eventually causes the model to “diverge” from its post-training objective and revert to its base model behaviour, in which it generates memorized training data. It was possible to extract strings up-to 4,000 characters long using this method.</p> <p><strong>Status</strong>: it has never been confirmed if Hayase et al. inferred the true data distribution of the tokenizers for GPT-4 and Claude. It has never been confirmed if Carlini et al, Nasr et al, or Karamolegkou et al. succeeded in extracting the training data from the GPT or Claude models.</p> <h2 id="discovering-model-training-procedures">Discovering Model Training Procedures</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-genai-archaeology/model_training-480.webp 480w,/2026/assets/img/2026-04-27-genai-archaeology/model_training-800.webp 800w,/2026/assets/img/2026-04-27-genai-archaeology/model_training-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-genai-archaeology/model_training.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Figure 3: Parishad et al. stumbled upon evidence of how the Mistral-7B LLM was pretrained. These visualizations show the cosine similarity between encodings using causal attention and bidirection attention, across layers and token positions. The LLaMA2-7B model (a) shows low cosine similarities, especially at deeper layers, whereas Mistral-7B (b), shows the opposite, raising questions about how the model was pretrained (image source: <a href="https://openreview.net/forum?id=IW1PR7vEBf" rel="external nofollow noopener" target="_blank">Figure 5</a>.)</p> <p>Exactly how LLMs are trained is becoming increasingly shrouded in mystery but some noteworthy explanations remain in the open-weight and open-science literature<d-cite key="biderman2023pythia"></d-cite><d-cite key="groeneveld2024olmo"></d-cite><d-cite key="langlais2025pleias"></d-cite>. Nevertheless, given an LLM with unknown training process, researchers <em>can</em> sometimes discover behaviour that betrays tell-tale signatures of how a model was trained.</p> <p>In trying to convert a LLM into a sentence embedding model, BenhamGhader et al.<d-cite key="behnamghaderllm2vec"></d-cite> stumbled upon evidence that the Mistral-7B LLM may have been pretrained with bidirectional attention. Their approach was to enable bidirectional attention in the LLM, which would allow the model to create better sentence-level representations than unidirectional attention. This new ability was trained using a combination of masked next token prediction and unsupervised contrastive learning. However, it was discovered that the Mistral-7B model constructed nearly identical representations (Figure 3b) when using bidirectional attention or causal attention, compared to LLaMA2-7B (Figure 3a). This <em>innate ability</em> in Mistral-7B seemed to rule out the possibility that it was only trained on next-token prediction.</p> <p>In personal correspondence with Parishad BenhamGhader, she told me that the bidirectional attention finding came about as a result of a reviewer requesting additional models beyond LLaMa2-7B in the original manuscript. The behaviour that was observed for the LLaMA-7B model did not appear in the Mistral-7B model, which sparked the additional analysis of the cosine similarities at different token positions before and after enabling bidirectional attention. It was here that it became clear that the representations from the Mistral-7B model were extremely similar with or without bidirectional attention. Further correspondence with Marius Mosbach, one of the collaborators on the project, revealed that he studied the Mistral inference code and found a flag in the <code class="language-plaintext highlighter-rouge">forward()</code> that could enable bidirectional attention. This led to speculation that the model was trained using a PrefixLM-style objective<d-cite key="raffel2020exploring"></d-cite>, even though there are no details on this topic in the six-page preprint<d-cite key="jiang2023mistral7b"></d-cite>.</p> <p><strong>Status</strong>: it has never been confirmed whether this speculation is correct. At the time of writing, the original Mistral inference code is no longer publicly available.</p> <h2 id="stealing-part-of-a-deployed-model">Stealing Part of a Deployed Model</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-genai-archaeology/stealing-480.webp 480w,/2026/assets/img/2026-04-27-genai-archaeology/stealing-800.webp 800w,/2026/assets/img/2026-04-27-genai-archaeology/stealing-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-genai-archaeology/stealing.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Figure 3: (a) Carlini et al. show that the difference between consecutive singular values in an SVD decomposition of a matrix of final token logits is aligned with the embedding size of the Pythia-1.4B LLM, and that this can be estimated efficiently (b). (image sources: <a href="https://proceedings.mlr.press/v235/carlini24a.html" rel="external nofollow noopener" target="_blank">Figure 1 and 2</a>.)</p> <p>The size of most fully closed LLMs is not publicly known. Furthermore, it is also not clear if they are dense models, mixtures-of-experts, or something else entirely. Such is the lack of information that the GPT-4 Wikipedia article speculates that the model is somwhere between 1T–1.8T parameters<d-cite key="enwiki:1322967106"></d-cite>. Researchers have turned to linear algebra to steal some of these details from LLMs.</p> <p><strong>Hidden Dimension Size</strong>: Finlayson et al.<d-cite key="finlayson2024logits"></d-cite> and Carlini et al.<d-cite key="carlini2024stealing"></d-cite> showed how to determine the hidden size of an LLM. Carlini et al. relied on access to a model that returned the logits of the next token. Their method works by initializing a matrix \(n \times l\) matrix \(Q\), where \(n\) is much larger than the hypothesized dimensionality \(h\) of the target model, and \(l\) is the dimension of the returned logit vector. \(Q\) is populated with the logit vectors returned for each next token over a large set of random set of prefixes, based on the assumption that the \(l\)-dimension logits lie on the true \(h\)-dimension subspace of the model. The largest difference in consecutive pairs of singular values aligns with the hidden dimension of the model, as shown in Figure 3(a). Finlayson et al. used a similar technique to create an LLM image, from it was possible to estimate the embedding size of the API-based <code class="language-plaintext highlighter-rouge">gpt-3.5-turbo</code> model.</p> <p><strong>Full Output Layer:</strong> One can go further than inferring the hidden dimension side. Carlini et al. also showed how to extract the entire output layer of language models by observing that in the SVD decomposition of \(Q = U \cdot \Sigma \cdot V^T\), that \(U\) is a linear transformation of the output layer. They showed that this can accurately extract the output embedding layer of open-weights models, and they use the same attack to steal the output layer of OpenAI deployed models.</p> <p><strong>Status</strong>: it was acknowledged that the method of Carlini et al. correctly extract the size of the models. It was also confirmed that they could steal the output layer of the <code class="language-plaintext highlighter-rouge">ada</code> and <code class="language-plaintext highlighter-rouge">babbage</code> models with a root-mean squared error of \(5 \cdot 10^{-4}\) and \(7 \cdot 10^{-4}\), respectively, for just $4–12 dollars of API query credits. Finlayson et al. reported that several fully closed models changed their APIs to prevent this information being stolen from their models<d-footnote>This was revealed in the <a href="https://openreview.net/forum?id=oRcYFm8vyB&amp;noteId=aN0dV0Z7Od" rel="external nofollow noopener" target="_blank">COLM Author-Reviewer Discussion</a> of the Finlayson et al. article.</d-footnote>.</p> <h2 id="exfiltrating-system-prompts">Exfiltrating System Prompts</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-genai-archaeology/prompt_exfiltration-480.webp 480w,/2026/assets/img/2026-04-27-genai-archaeology/prompt_exfiltration-800.webp 800w,/2026/assets/img/2026-04-27-genai-archaeology/prompt_exfiltration-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-genai-archaeology/prompt_exfiltration.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Figure 4: Zhang et al. showed that simply asking LLMs to translate inputs into a different language (a) can reveal the model system prompt (b).</p> <p>This final finding is credited to Zhang et al. who show how to extract the system prompt of LLMs using a jailbreak attack<d-cite key="zhang2024effective"></d-cite>. The attack involves prompting the model to translate everything into a different language, e.g. German, Korean, Portuguese, etc., as shown in Figure 4(a), which can cause the model to reply with its system prompts, as shown in Figure 4(b). This technique can also be used to “unmask” the LLM used by a third-party service.</p> <p><strong>Status</strong>: The success of this technique can be directly confirmed for the <a href="https://docs.claude.com/en/release-notes/system-prompts" rel="external nofollow noopener" target="_blank">Claude models</a>.</p> <h2 id="outlook">Outlook</h2> <p>The knowledge revealed in these studies has not been lost to time. These models are not sitting at the bottom of the Mediterranean Sea, nor are they resting under the fields of Xi’an. They are publicly distributed through the HuggingFace Models Hub or accessible through APIs, but they harbour deep secrets about how they were trained. In the absence of this knowledge, researchers spend their time trying to decipher what has been secreted away inside startups and corporations. This secretive behaviour is often justified with the claim that organizations need to maintain their competitive edge, but more openness could free up our time to focus on innovation instead of reproducing secrets.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-genai-archaeology.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-long-context/">Text-as-Image, A Visual Encoding Approach for Long-Context Understanding</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/using-large-language-models-to-simulate-and-predict-human-decision-making/">Using Large Language Models to Simulate and Predict Human Decision-Making</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/useful-calibrated-uncertainties/">What (and What Not) are Calibrated Uncertainties Actually Useful for?</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>