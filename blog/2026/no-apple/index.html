<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> There is No üçé in Timeseries: Rethinking TSFM through the Lens of Invariance | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Timeseries foundation models (TSFMs) are trained on scrape-everything-on-the-interet paradigm, yet the internet is only semantically complete for text and images, not for timeseries, leading to poor performance. It contains countless texts and images of üçé, but there is no timeseries that captures the concept of üçé. This mismatch calls for a different pretraining strategy for timeseries, and we argue that the correct organising principle is invariance."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/no-apple/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "There is No üçé in Timeseries: Rethinking TSFM through the Lens of Invariance",
            "description": "Timeseries foundation models (TSFMs) are trained on scrape-everything-on-the-interet paradigm, yet the internet is only semantically complete for text and images, not for timeseries, leading to poor performance. It contains countless texts and images of üçé, but there is no timeseries that captures the concept of üçé. This mismatch calls for a different pretraining strategy for timeseries, and we argue that the correct organising principle is invariance.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>There is No üçé in Timeseries: Rethinking TSFM through the Lens of Invariance</h1> <p>Timeseries foundation models (TSFMs) are trained on scrape-everything-on-the-interet paradigm, yet the internet is only semantically complete for text and images, not for timeseries, leading to poor performance. It contains countless texts and images of üçé, but there is no timeseries that captures the concept of üçé. This mismatch calls for a different pretraining strategy for timeseries, and we argue that the correct organising principle is invariance.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#the-incompleteness-of-timeseries-data">The Incompleteness of Timeseries Data</a> </div> <div> <a href="#timeseries-invariance-ontology">Timeseries Invariance Ontology</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="the-incompleteness-of-timeseries-data">The Incompleteness of Timeseries Data</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-no-apple/vizabs2-480.webp 480w,/2026/assets/img/2026-04-27-no-apple/vizabs2-800.webp 800w,/2026/assets/img/2026-04-27-no-apple/vizabs2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-no-apple/vizabs2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The ‚Äúscrape everything on the internet‚Äù paradigm succeeds in vision and language because web-scale image and text corpora collectively span most of the human-relevant representational space. In contrast, timeseries data occupy a complementary subspace‚Äîcapturing dynamics, rhythms, and control signals that rarely overlap with such semantic concepts. The slice of reality containing the notion of üçé ‚Äúapple‚Äù, for example, is densely represented in image and text corpora but absent from timeseries. We picked the üçé "apple" example in direct reference to the first equation in the <a href="https://phillipi.github.io/prh/" rel="external nofollow noopener" target="_blank"> Platonic Represntation Hypothesis (PRH) </a> webpage. </div> <p><strong>Foundation models (FM) are trained on ‚Äúscrape everything (on the internet)‚Äù corpus.</strong> This has been proven effective in the domain of computer vision (CV) and natural language processing (NLP). Timeseries foundation models (TSFMs) are gaining popularity by following this paradigm. However, this has recently been questioned due to its poor performance over a fully supervised baseline, hence the missing BERT moment <d-cite key="xu2025specialized"></d-cite>. This paper argues that the ‚Äúscrape everything‚Äù approach is effective for text and image data but fundamentally unsuitable for time-series.</p> <p><strong>Image and text data on the internet are near human-representationally complete.</strong> This stems from bias in data collection due to human behaviour. Humans naturally document the world through writing, photography, and video. As a result, the internet is saturated with multimodal data that forms an almost complete, human-relevant representation of the world. This completeness is not merely an aspect of scale but of semantic coverage: the web collectively encodes nearly every concept, object, and event that humans find meaningful. Foundation models trained on such data therefore perform well on a wide range of human-relevant tasks <d-cite key="awais2025foundation"></d-cite>. </p> <p><strong>The Platonic Representation Hypothesis (PRH)<d-cite key="huh2024PRH"></d-cite> provides supporting evidence for the completeness of the image and text corpus.</strong> The PRH posits that ‚Äúneural networks, trained with different objectives on different data and modalities, are converging to a shared statistical model of reality in their representation spaces.‚Äù Such convergence is only possible if each pretraining dataset independently provides a sufficiently complete capture of reality. </p> <p><strong>However, timeseries data are not human-representationally complete.</strong> They are collected precisely to capture aspects of reality that images and text cannot, serving as complementary rather than overlapping modalities. Borrowing the example in the original PRH paper, the concept of ‚Äúapple‚Äù is richly represented in both text and images through abundant descriptions and visuals, yet no time-series signal conveys its meaning: <strong>there is no üçé ‚Äúapple‚Äù in timeseries</strong>. As shown in the figure above, üçé ‚Äúapple‚Äù lies within the intersection of image and text, but outside time-series data. The reverse also applies; many time-series phenomena cannot be effectively expressed through language. Road traffic, for example, is typically described only in broad qualitative categories such as free-flowing, congested, or peak/off-peak. This complementary configuration explains why the ‚Äúscrape-everything‚Äù paradigm succeeds for text and images but fails for timeseries. Even with perfect coverage and training, a timeseries foundation model would lack emergent common-sense or zero-shot behaviours, since the semantic and physical regularities underpinning real-world understanding are absent from timeseries measurements.</p> <p><strong>Existing TSFM pretraining datasets are incomplete.</strong> Because human-collected timeseries data only capture a narrow slice of reality, this incompleteness inevitably carries over into the TSFM pretraining datasets derived from them. These datasets are assembled through opportunistic and ad hoc processes, guided by data availability rather than by a systematic effort to achieve representational completeness of the real world. Some draw from previously established collections such as the Time Series Pile <d-cite key="goswami2024moment"></d-cite>, LOTSA <d-cite key="woo2024uni2ts"></d-cite>, and UTSD <d-cite key="liu2024timer"></d-cite>, which themselves aggregate prior benchmarks like the Monash Forecasting Archive <d-cite key="godahewa2021monash"></d-cite> and Informer datasets <d-cite key="zhou2021informer"></d-cite>. Others augment public data with in-house sources. For instance, TimesFM <d-cite key="das2024timesFM"></d-cite> relies on Google Trends, TimeHF <d-cite key="qi2025timeHF"></d-cite> incorporates JD supply-chain sales covering over 20 000 products,and BOOM <d-cite key="cohen2025toto"></d-cite> pretrains on telemetry metrics from DataDog‚Äôs cloud systems. In addition, several models expand coverage through synthetic generation: TimeHF applies data augmentation, Chronos <d-cite key="ansari2024chronos"></d-cite> and WaveToken <d-cite key="masserano2025wavetoken"></d-cite> introduce Gaussian process‚Äìbased synthetic series, and BOOM uses procedurally generated rule-based signals.</p> <p><strong>Efforts to mitigate this incompleteness remain superficial.</strong> The Time Series Pile seeks broader domain coverage through aggregation rather than systematic design. MOIRAI enforces a 0.1% per-dataset cap within LOTSA‚Äîan ad hoc constraint that limits dominance without addressing underlying representational bias. TimeHF supplements its retail-focused corpus with public weather and energy datasets. BOOM integrates open benchmarks with telemetry logs, incrementally increasing domain variance. None of these approaches constitute a systematic effort toward representational completeness. As a result, TSFMs continue to underperform in zero-shot settings, lacking the emergent reasoning and world-aligned understanding that arise only from exposure to semantically complete data.</p> <p>The implication of these observations are is that TSFM models require a data paradigm fundamentally distinct from those of vision and language. Progress will not come from scaling alone but from constructing datasets that systematically span the diversity of physical and behavioural processes underlying the world‚Äôs dynamics. Identifying the combination of timeseries domains that together approximate a world-complete representation of reality remains an open research question. In the following section, we argue that <strong>the lens of invariance provides a principled framework for pursuing such completeness</strong>.</p> <h2 id="timeseries-invariance-ontology">Timeseries Invariance Ontology</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-no-apple/ecg-480.webp 480w,/2026/assets/img/2026-04-27-no-apple/ecg-800.webp 800w,/2026/assets/img/2026-04-27-no-apple/ecg-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-no-apple/ecg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-no-apple/San_Clemente._Temperaturas-480.webp 480w,/2026/assets/img/2026-04-27-no-apple/San_Clemente._Temperaturas-800.webp 800w,/2026/assets/img/2026-04-27-no-apple/San_Clemente._Temperaturas-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-no-apple/San_Clemente._Temperaturas.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-no-apple/waveform-480.webp 480w,/2026/assets/img/2026-04-27-no-apple/waveform-800.webp 800w,/2026/assets/img/2026-04-27-no-apple/waveform-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-no-apple/waveform.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Illustration of invariances in various domains in timeseries. Minor temporal shifts preserve meaning in ECG signals through heart rate variability, yet the same shifts in meteorological data misalign seasonal cycles. Similarly, vertical inversion leaves audio waveforms perceptually unchanged, but in ECG it indicates lead reversal, and in climate data it represents a physically invalid temperature inversion. (Images adapted from publicly available sources.) </div> <p>A defining property of timeseries domains is the presence of domain-specific invariances and equivariances. These are transformations that preserve a signal‚Äôs underlying dynamics, or at least should not alter its semantic or predictive identity~\cite{bronstein2021GDL}. Identifying and leveraging these invariances is essential for both model design and dataset construction.<br> Different timeseries domains exhibit distinct invariance structures, yet existing benchmarks and pretraining corpora neither enumerate nor balance them.<br> As a result, current models risk overfitting to superficial correlations rather than learning robust representations that generalise across domains.<br> This section outlines several illustrative examples.</p> <p><strong>Spectral Invariance.</strong> In high-frequency signals such as audio, vibration, and speech waveforms, information is primarily encoded in spectral composition.<br> These domains exhibit spectral invariance, where small shifts or scalings in frequency content do not alter the underlying semantics.<br> For instance, the spoken word ‚Äúhello‚Äù remains identifiable despite differences in pitch or volume.<br> When a pretraining corpus is dominated by slow-varying, seasonal domains such as economics or climate data, models may never encounter such invariance classes, limiting cross-modal generalisation and the emergence of temporal reasoning. Conversely, models that internalise spectral invariance too broadly may misinterpret meaningful frequency shifts in domains where they encode genuine structural change‚Äîfor example, economic cycles, market volatility, or hydrological regime shifts.</p> <p><strong>Amplitude Invariance.</strong> In many sensing contexts, scaling intensity or measurement units does not change the underlying behaviour. Devices such as microphones, accelerometers, or pressure sensors often have adjustable or drifting gain, making relative changes more informative than absolute values. However, amplitude invariance is not universal. In domains such as energy load, traffic, or physiology, amplitude itself carries semantic meaning‚Äîsignalling demand surges, congestion, or heartbeat anomalies. Pretraining on corpora that disregard amplitude sensitivity may therefore erase meaningful information, while training exclusively on amplitude-sensitive domains can have the opposite effect, producing models that fail to recognise shape- or pattern-level equivalences across sensors. Balanced exposure to both invariance regimes is necessary for timeseries foundation models to generalise across heterogeneous sensing conditions.</p> <p><strong>Shape (Morphological) Invariance.</strong> In certain domains, local waveform shape rather than absolute duration or amplitude defines identity.<br> This property is characteristic of ECG traces, gesture dynamics, and speech signals, where transient motifs at multiple scales convey meaning.<br> By contrast, domains such as climate or retail sales rely more on aggregate magnitudes.<br> Models that fail to capture morphological invariance struggle to recognise recurring motifs that vary in scale or duration.</p> <p><strong>Elastic Morphological Invariance.</strong> In certain domains, the local waveform shape, rather than absolute duration or amplitude,defines identity. Signals such as ECG traces, gestures, and trajectories share this property: the same underlying pattern may occur at different speeds or with local temporal distortions. This reflects not only morphological invariance but also temporal warping invariance, where nonlinear time deformations preserve semantic identity. Dynamic Time Warping (DTW) and related alignment methods embody this assumption, enabling comparison of signals that differ in rate or alignment. By contrast, domains such as climate, finance, or control systems encode meaning in the precise timing of events, where warping destroys causal structure. Models that fail to capture these dual invariances struggle to recognise recurring motifs under variable durations, while those that overgeneralise time-warping risk erasing meaningful chronological information.</p> <p><strong>Distributional (Stochastic) Invariance.</strong><br> A distinct form of invariance arises in stochastic domains, where the semantic identity of a process is defined not by an individual trajectory but by its underlying probability law.<br> Temporal permutations or alternative realisations preserve process identity: white noise, Brownian motion, or AR(1) dynamics remain equivalent under resampling.<br> Such invariance characterises domains like turbulence, diffusion, random walks, and financial returns, where statistical moments and spectral densities define behaviour.<br> Deterministic domains such as ECG or traffic lack this property, as the specific sequence of events carries meaning.</p> <p><strong>Parametric Invariance.</strong><br> In many dynamical systems, the governing parameters remain stable within local regimes, such as viscosity in fluid flow, friction coefficients in mechanics, or reaction rates in chemistry.<br> A model capable of inferring these latent constants demonstrates deeper understanding of the underlying process, effectively reconstructing its governing equations from data.<br> Domains like fluid dynamics, epidemiology, and controlled mechanical systems exhibit strong parametric invariance, making them amenable to structured pretraining.<br> Conversely, domains subject to frequent regime changes (e.g. speech, finance, and traffic) lack persistent constants and exhibit piecewise or context-dependent dynamics. Models overfitted to constant-regime data may therefore be fragile to transient changes in such domains.</p> <p><strong>Toward a Timeseries Invariance Ontology.</strong> The diversity of invariances across timeseries domains underscores that there is no single symmetry governing temporal data. To move beyond opportunistic dataset aggregation, the community must develop a formal ontology of timeseries invariances. Such an ontology would not only guide model architectures and evaluation but also inform how data should be collected, balanced, or even synthetically generated to ensure comprehensive coverage of invariance space. A world-complete timeseries corpus must therefore be designed, not merely scraped‚Äîcurated to represent the full spectrum of dynamical symmetries that define temporal reality.</p> <p><strong>An ontology of timeseries invariance from first principles</strong> could be a good starting point. Formally, any invariance can be expressed as the set of transformations \(g\) such that \(T(g \cdot x)=T(x)\) , where \(T\) is a downstream task or representation operator. From this foundation, one can systematically generate invariance candidates from a canonical set of primitive transformations on timeseries: time-axis operations (shift, scaling, reversal, monotone warping); value-space operations (affine, monotone, diffeomorphic, or stochastic channels); index-space operations (permutations, isometries, graph automorphisms, or topology-preserving maps); and dynamical or causal symmetries (structural conjugacy, conservation, symplectic, or causal-mechanism invariance). Closing these transformation sets under composition yields a complete invariance group for temporal data‚Äîan explicit ontology that can directly guide both dataset curation and model design.</p> <h2 id="conclusion">Conclusion</h2> <p>TSFMs underperform not only because of limited scale or model capacity, but because timeseries corpora are structurally incomplete. They lack the human-semantic coverage that makes web-scale text and image datasets so powerful; i.e. there is no ‚Äúapple‚Äù in timeseries. The ‚Äúscrape everything‚Äù strategy, successful in language and vision, cannot yield emergent understanding or robust zero-shot reasoning in the temporal domain. Progress therefore requires a shift from ad hoc aggregation to deliberate design: datasets must be curated (and, where appropriate, synthetically generated) to systematically span the space defined by an ontology of timeseries invariances. Constructing this ontology from first principles ensures completeness, grounding it in the full range of transformations that preserve temporal semantics. Only under this paradigm can TSFMs develop the aligned structure necessary for generalisation, reasoning, and truly emergent behaviour.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-no-apple.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-vlms-waste-their-vision/">Why vlms waste their vision</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>