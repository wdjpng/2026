<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Adversarial Conditioning Paradox: Why Attacked Inputs Are More Stable, Not Less | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Adversarial inputs exhibit systematically lower Jacobian condition numbers at early transformer layers—the opposite of our initial hypothesis that attacks exploit unstable regions. This paradox reveals that adversarial attacks succeed by finding well-conditioned directions that cross decision boundaries."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/adversarial-conditioning-paradox/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "The Adversarial Conditioning Paradox: Why Attacked Inputs Are More Stable, Not Less",
            "description": "Adversarial inputs exhibit systematically lower Jacobian condition numbers at early transformer layers—the opposite of our initial hypothesis that attacks exploit unstable regions. This paradox reveals that adversarial attacks succeed by finding well-conditioned directions that cross decision boundaries.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous Authors",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Anonymous Institution",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>The Adversarial Conditioning Paradox: Why Attacked Inputs Are More Stable, Not Less</h1> <p>Adversarial inputs exhibit systematically lower Jacobian condition numbers at early transformer layers—the opposite of our initial hypothesis that attacks exploit unstable regions. This paradox reveals that adversarial attacks succeed by finding well-conditioned directions that cross decision boundaries.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#the-adversarial-conditioning-paradox">The Adversarial Conditioning Paradox</a> </div> <div> <a href="#background-and-related-work">Background and Related Work</a> </div> <ul> <li> <a href="#adversarial-attacks-on-nlp">Adversarial Attacks on NLP</a> </li> <li> <a href="#detection-methods">Detection Methods</a> </li> <li> <a href="#jacobian-conditioning">Jacobian Conditioning</a> </li> </ul> <div> <a href="#methods">Methods</a> </div> <ul> <li> <a href="#attack-generation">Attack Generation</a> </li> <li> <a href="#conditioning-analysis">Conditioning Analysis</a> </li> <li> <a href="#spectral-conditioning-monitor">Spectral Conditioning Monitor</a> </li> </ul> <div> <a href="#results">Results</a> </div> <ul> <li> <a href="#layer-wise-analysis">Layer-wise Analysis</a> </li> <li> <a href="#attack-specific-patterns">Attack-specific Patterns</a> </li> <li> <a href="#detection-performance">Detection Performance</a> </li> </ul> <div> <a href="#discussion">Discussion</a> </div> <ul> <li> <a href="#geometric-interpretation">Geometric Interpretation</a> </li> <li> <a href="#implications-for-defense">Implications for Defense</a> </li> </ul> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Adversarial attacks on NLP systems pose a growing security concern. Attacks such as TextFooler <d-cite key="jin2020bert"></d-cite>, BERT-Attack <d-cite key="li2020bert"></d-cite>, and PWWS <d-cite key="ren2019generating"></d-cite> generate inputs that appear semantically similar to clean text yet cause misclassification. These attacks are specifically optimized to minimize embedding distance while maximizing prediction change—making them invisible to similarity-based detection methods.</p> <p>We set out to investigate whether <strong>Jacobian conditioning analysis</strong> could provide an alternative detection signal. The condition number κ of a layer’s Jacobian measures the ratio of maximum to minimum singular values, capturing how much the layer amplifies perturbations in different directions.</p> <div class="l-body"> <div style="background-color: #f0f0f0; padding: 15px; border-left: 4px solid #333; margin: 20px 0;"> <strong>Initial Hypothesis:</strong> Adversarial inputs should exhibit <em>high</em> condition numbers, indicating they occupy ill-conditioned regions where small perturbations cause disproportionately large output changes. </div> </div> <p>This hypothesis seemed natural. Adversarial attacks succeed by finding perturbations that cause large prediction shifts. High conditioning (large κ) would indicate sensitivity to perturbation—exactly what adversarial examples exploit.</p> <p><strong>We found the opposite.</strong></p> <h2 id="the-adversarial-conditioning-paradox">The Adversarial Conditioning Paradox</h2> <p>Across three different attack types—word-level substitution (TextFooler, PWWS) and character-level perturbation (DeepWordBug)—adversarial inputs show systematically <em>lower</em> condition numbers at Layer 1 of BERT compared to clean inputs.</p> <div class="l-body"> <figure> <img src="/2026/assets/img/2026-04-27-adversarial-conditioning-paradox/layer_conditioning.png" alt="Layer-wise condition numbers"> <figcaption>Figure 1: Average condition numbers across transformer layers for clean vs adversarial inputs. Adversarial examples show consistently lower κ at early layers.</figcaption> </figure> </div> <p>The effect is statistically significant:</p> <ul> <li> <strong>TextFooler</strong>: AUC = 0.72, p = 0.001</li> <li> <strong>DeepWordBug</strong>: AUC = 0.75, p = 0.001</li> <li> <strong>PWWS</strong>: AUC = 0.59, p = 0.29 (directionally consistent)</li> </ul> <p>Meanwhile, cosine distance—the metric attacks explicitly minimize—fails completely (AUC ≈ 0.25).</p> <p>This <strong>adversarial conditioning paradox</strong> demands explanation. Why would adversarial inputs be <em>more</em> numerically stable, not less? And why does this pattern hold across fundamentally different attack strategies?</p> <p>We propose a geometric interpretation: adversarial attacks succeed not by exploiting instability, but by finding <strong>well-conditioned perturbation directions that happen to cross decision boundaries</strong>. Ill-conditioned directions would make the attack optimization unstable—small changes in the perturbation would cause unpredictable output swings, making it difficult to reliably flip predictions. Instead, attacks implicitly select for smooth, stable paths to misclassification.</p> <h2 id="background-and-related-work">Background and Related Work</h2> <h3 id="adversarial-attacks-on-nlp">Adversarial Attacks on NLP</h3> <p>Adversarial attacks on text classifiers seek to find inputs that cause misclassification while preserving semantic content. We study three attack families:</p> <p><strong>TextFooler</strong> <d-cite key="jin2020bert"></d-cite> uses a greedy search that identifies important words via deletion and replaces them with semantically similar alternatives from a counter-fitted embedding space. The attack explicitly constrains substitutions to maintain sentence similarity.</p> <p><strong>PWWS</strong> <d-cite key="ren2019generating"></d-cite> combines word importance ranking with WordNet-based synonym substitution, using probability-weighted saliency to prioritize replacements. Unlike TextFooler, it uses a fixed synonym dictionary rather than embedding-based similarity.</p> <p><strong>DeepWordBug</strong> <d-cite key="gao2018black"></d-cite> operates at the character level, introducing typos, character swaps, and insertions. This attack is geometrically distinct from word-level attacks—it perturbs within the token embedding space rather than substituting between discrete tokens.</p> <h3 id="detection-methods">Detection Methods</h3> <p>Prior work on adversarial detection in NLP includes:</p> <ul> <li>Perplexity-based methods <d-cite key="mozes2021frequency"></d-cite> </li> <li>Frequency-based analysis <d-cite key="pruthi2019combating"></d-cite> </li> <li>Certified robustness <d-cite key="jia2019certified"></d-cite> </li> <li>Ensemble disagreement approaches</li> </ul> <p>These methods operate on <strong>external properties</strong> of inputs. Our approach differs: we analyze <strong>internal geometric properties</strong> of how the model processes inputs, specifically the conditioning of layer-wise Jacobians.</p> <h3 id="jacobian-conditioning">Jacobian Conditioning</h3> <p>The condition number κ of a matrix J is defined as:</p> \[\kappa(J) = \frac{\sigma_{\max}(J)}{\sigma_{\min}(J)}\] <p>where σ_max and σ_min are the maximum and minimum singular values. For the Jacobian of a neural layer, κ captures how uniformly the layer responds to perturbations:</p> <ul> <li>High κ indicates ill-conditioning: some directions are amplified much more than others</li> <li>Low κ indicates well-conditioning: all directions are treated more uniformly</li> </ul> <h2 id="methods">Methods</h2> <h3 id="attack-generation">Attack Generation</h3> <p>We generate adversarial examples on the SST-2 sentiment classification task using:</p> <ol> <li> <strong>TextFooler</strong>: Word substitution via embedding similarity</li> <li> <strong>PWWS</strong>: WordNet-based synonym replacement</li> <li> <strong>DeepWordBug</strong>: Character-level perturbations</li> </ol> <p>All attacks use default parameters from TextAttack <d-cite key="morris2020textattack"></d-cite> library. We generate 1000 successful adversarial examples per attack type, requiring:</p> <ul> <li>Successful label flip</li> <li>Semantic similarity &gt; 0.8 (for word-level attacks)</li> <li>Edit distance &lt; 30 characters (for character-level attacks)</li> </ul> <h3 id="conditioning-analysis">Conditioning Analysis</h3> <p>For each input (clean or adversarial), we extract:</p> <ol> <li> <p><strong>Layer-wise Jacobians</strong>: For transformer layer l with function f_l, we compute: \(J_l = \frac{\partial f_l(x)}{\partial x}\)</p> </li> <li> <p><strong>Condition numbers</strong>: Using randomized SVD for efficiency: \(\kappa_l = \frac{\sigma_{\max}(J_l)}{\sigma_{\min}(J_l) + \epsilon}\) where ε = 1e-10 for numerical stability.</p> </li> <li> <p><strong>Statistics</strong>: We compute κ for layers {1, 3, 6, 9, 12} of BERT-base.</p> </li> </ol> <h3 id="spectral-conditioning-monitor">Spectral Conditioning Monitor</h3> <p>We implement the Spectral Conditioning Monitor (SCM) algorithm for efficient condition number estimation:</p> <div class="l-body"> <figure> <img src="/2026/assets/img/2026-04-27-adversarial-conditioning-paradox/scm_algorithm.png" alt="SCM Algorithm"> <figcaption>Figure 2: The SCM algorithm efficiently estimates condition numbers using randomized SVD and power iteration.</figcaption> </figure> </div> <h2 id="results">Results</h2> <h3 id="layer-wise-analysis">Layer-wise Analysis</h3> <div class="l-page"> <figure> <img src="/2026/assets/img/2026-04-27-adversarial-conditioning-paradox/kappa_distribution.png" alt="Distribution of condition numbers"> <figcaption>Figure 3: Distribution of condition numbers at Layer 1 for clean vs adversarial inputs across three attack types.</figcaption> </figure> </div> <p><strong>Key findings:</strong></p> <ol> <li> <strong>Layer 1 shows strongest signal</strong>: Adversarial κ consistently lower than clean</li> <li> <strong>Effect diminishes with depth</strong>: By Layer 12, distributions overlap substantially</li> <li> <strong>Cross-attack consistency</strong>: All three attacks show same directional effect</li> </ol> <div class="l-body"> <table> <thead> <tr> <th>Attack</th> <th>Layer 1 κ (Clean)</th> <th>Layer 1 κ (Adv)</th> <th>p-value</th> <th>AUC</th> </tr> </thead> <tbody> <tr> <td>TextFooler</td> <td>23.45 ± 8.32</td> <td>18.73 ± 6.21</td> <td>0.001</td> <td>0.72</td> </tr> <tr> <td>DeepWordBug</td> <td>24.12 ± 9.15</td> <td>17.89 ± 5.43</td> <td>0.001</td> <td>0.75</td> </tr> <tr> <td>PWWS</td> <td>22.78 ± 7.94</td> <td>20.91 ± 7.12</td> <td>0.29</td> <td>0.59</td> </tr> </tbody> </table> <figcaption>Table 1: Condition number statistics at Layer 1 for clean vs adversarial inputs.</figcaption> </div> <h3 id="attack-specific-patterns">Attack-specific Patterns</h3> <p>Different attacks show distinct conditioning signatures:</p> <div class="l-body"> <figure> <img src="/2026/assets/img/2026-04-27-adversarial-conditioning-paradox/attack_patterns.png" alt="Attack-specific patterns"> <figcaption>Figure 4: Attack-specific conditioning patterns across layers reveal different perturbation strategies.</figcaption> </figure> </div> <ul> <li> <strong>TextFooler</strong>: Smooth decay from Layer 1 to 12</li> <li> <strong>DeepWordBug</strong>: Sharp drop at Layer 1, then stabilizes</li> <li> <strong>PWWS</strong>: Gradual change, weakest signal</li> </ul> <h3 id="detection-performance">Detection Performance</h3> <p>ROC analysis shows strong detection capability using Layer 1 conditioning alone:</p> <div class="l-body"> <figure> <img src="/2026/assets/img/2026-04-27-adversarial-conditioning-paradox/roc_curves.png" alt="ROC curves"> <figcaption>Figure 5: ROC curves for adversarial detection using conditioning vs cosine distance.</figcaption> </figure> </div> <p>Remarkably, cosine distance—which attacks explicitly minimize—provides no discriminative signal (AUC ≈ 0.25), while conditioning achieves AUC = 0.72-0.75.</p> <h2 id="discussion">Discussion</h2> <h3 id="geometric-interpretation">Geometric Interpretation</h3> <p>Why do adversarial inputs show <em>lower</em> condition numbers? We propose three complementary explanations:</p> <p><strong>1. Optimization stability:</strong> Ill-conditioned directions would destabilize attack optimization. Small adjustments during the attack search would cause unpredictable output changes, making it difficult to reliably flip predictions. Attacks implicitly select for well-conditioned paths.</p> <p><strong>2. Semantic preservation:</strong> Well-conditioned directions may better preserve semantic content. High-κ directions could correspond to linguistically meaningful variations that attacks must avoid to maintain similarity.</p> <p><strong>3. Decision boundary geometry:</strong> The model’s decision boundaries may be smoother (lower curvature) in well-conditioned regions. Attacks find these smooth crossings rather than sharp, unstable transitions.</p> <h3 id="implications-for-defense">Implications for Defense</h3> <p>Our findings suggest new defense strategies:</p> <ol> <li> <strong>Conditioning-based detection:</strong> Monitor Layer 1 conditioning as a real-time detection signal</li> <li> <strong>Adversarial training:</strong> Include conditioning regularization to eliminate well-conditioned attack paths</li> <li> <strong>Architecture design:</strong> Engineer models with uniform conditioning to reduce attack surface</li> </ol> <p>The paradox also reveals a fundamental trade-off: making models more stable (lower κ) may inadvertently create smoother attack surfaces.</p> <h2 id="conclusion">Conclusion</h2> <p>We document an unexpected phenomenon: adversarial inputs to NLP models exhibit <em>lower</em> Jacobian condition numbers at early layers, contradicting the intuitive hypothesis that attacks exploit unstable regions. This “adversarial conditioning paradox” holds across word-level and character-level attacks, providing a strong detection signal where embedding-based methods fail.</p> <p>Our findings suggest that adversarial attacks succeed not through chaos but through stability—finding well-conditioned directions that smoothly cross decision boundaries. This geometric insight opens new avenues for both understanding and defending against adversarial examples in NLP systems.</p> <p>Future work should investigate:</p> <ul> <li>Whether the paradox extends to other architectures (GPT, RoBERTa)</li> <li>How conditioning evolves during adversarial training</li> <li>Whether attacks can be modified to maintain high conditioning while preserving effectiveness</li> </ul> <p>The code and data for reproducing our experiments are available at [anonymous repository link].</p> <h2 id="acknowledgments">Acknowledgments</h2> <p>We thank the anonymous reviewers for their valuable feedback. This work was supported by [anonymized funding source].</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-adversarial-conditioning-paradox.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-vlms-waste-their-vision/">Why vlms waste their vision</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>