<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> How to Transition from ML to DL in Production - Lessons From the Trenches at Company | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="A large and mature gradient-boosted tree model had been powering Company’s fraud detection for years. We gradually migrated to a pure deep learning model over the past year going through a heterogeneous stacking phase that reached parity before outperforming our boosting model in production. We learned along the way that a simple ResNet can beat sophisticated tabular DL architectures at million-scale (1); stacking is a practical bridge from ML to DL (2); and the biggest wins from DL are often beyond metrics (3)."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/from-ml-to-dl/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "How to Transition from ML to DL in Production - Lessons From the Trenches at Company",
            "description": "A large and mature gradient-boosted tree model had been powering Company’s fraud detection for years. We gradually migrated to a pure deep learning model over the past year going through a heterogeneous stacking phase that reached parity before outperforming our boosting model in production. We learned along the way that a simple ResNet can beat sophisticated tabular DL architectures at million-scale (1); stacking is a practical bridge from ML to DL (2); and the biggest wins from DL are often beyond metrics (3).",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>How to Transition from ML to DL in Production - Lessons From the Trenches at Company</h1> <p>A large and mature gradient-boosted tree model had been powering Company’s fraud detection for years. We gradually migrated to a pure deep learning model over the past year going through a heterogeneous stacking phase that reached parity before outperforming our boosting model in production. We learned along the way that a simple ResNet can beat sophisticated tabular DL architectures at million-scale (1); stacking is a practical bridge from ML to DL (2); and the biggest wins from DL are often beyond metrics (3).</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#background-fraud-detection-at-company">Background - Fraud Detection at Company</a> </div> <div> <a href="#heterogeneous-ensembling-as-a-migration-strategy">Heterogeneous Ensembling as a Migration Strategy</a> </div> <div> <a href="#transition-process">Transition Process</a> </div> <ul> <li> <a href="#offline-experiments">Offline Experiments</a> </li> <li> <a href="#ensembling-as-a-bridge">Ensembling as a Bridge</a> </li> <li> <a href="#live-validation">Live Validation</a> </li> <li> <a href="#scaling-and-transition">Scaling and Transition</a> </li> </ul> <div> <a href="#learnings">Learnings</a> </div> <div> <a href="#future-work">Future Work</a> </div> </nav> </d-contents> <p>It is a widely held belief in the ML community that tree-based models are the only sensible choice for tabular data. Adoption of neural networks for these tasks is usually met with skepticism about their applicability (e.g., latency concerns, GPU usage at inference time, explainability) and performance, since trees typically excel at predictions on tabular data.</p> <p>These concerns are not entirely unfounded, but are often rooted in two issues. Regarding performance, benchmarks usually stem from small-scale academic datasets that favor tree methods and under-resource neural networks. Regarding feasibility, deep learning tends to be conflated with large language models, which require massive infrastructure, multi-billion-parameter models, and complex serving stacks.</p> <p>In this post, we present a real-world case study from Company, a global payments processor. We describe how we migrated our flagship fraud detection model, scoring thousands of transactions per second, from a large, well-tuned boosting model to a pure neural network. We also outline why we believe that, at scale, neural networks can not only match but also outperform gradient-boosted trees on tabular data, and bring substantial ancillary benefits.</p> <p>The transition was not a one-shot replacement. We went through an intermediate ensembling process that improved performance and reduced migration risk before we could fully transition and consolidate on a pure deep learning model. Along the way, we tried many tabular DL architectures from the literature, observed surprising results at our scale, and leaned heavily on intermediate ensembling and pragmatism to deploy the model in production. This report outlines the results and lessons we encountered during this transition.</p> <h2 id="background---fraud-detection-at-company">Background - Fraud Detection at Company</h2> <p>Company processes large volumes of payments for merchants worldwide. A key part of this process is our Fraud Detection Model (FDM), which estimates in real time the likelihood that a payment is fraudulent and approves, challenges, or blocks it accordingly. The cost of errors is high: blocking legitimate payments hurts merchants’ authorization rates and shoppers’ experience, while approving fraudulent ones leads to significant financial losses for merchants.</p> <p>Our fraud detection model consumes a large, evolving feature set combining:</p> <ul> <li>Basic payment-level features</li> <li>Aggregate features, such as rolling statistics across many merchant and shopper dimensions, served by our feature platform.</li> <li>Shopper features, such as histories of payments, refusals, and identifiers across all of our merchants and regions, served by our shopper linking algorithm.</li> <li>Velocity features, such as the number of payment attempts across multiple dimensions, served as timestamp arrays by our velocity database.</li> <li>Structured fields containing payment information in raw string format.</li> </ul> <pre><code class="language-mermaid">graph LR
    PR[Payment Request]

    subgraph "Feature Stores"
        FP[Feature Platform]
        SL[Shopper Linking]
        VDB[Velocity Database]
    end


    subgraph "Features"
        BF[Payment Information]
        SF[PII Fields]
        AF[Payment Aggregations]
        SHF[Shopper History]
        VF[Payment Velocity]
    end

    FDM[FDM]

    PR --&gt; BF
    PR --&gt; SF

    PR -.-&gt; FP
    PR -.-&gt; SL
    PR -.-&gt; VDB

    FP --&gt; AF
    SL --&gt; SHF
    VDB --&gt; VF

    BF --&gt; FDM
    SF --&gt; FDM
    AF --&gt; FDM
    SHF --&gt; FDM
    VF --&gt; FDM
</code></pre> <p>FDM’s ML stack was originally built on a large boosting model based on LightGBM: it was easy to adopt, simple to scale, and excellent on tabular data. As a result, model choice wasn’t an immediate bottleneck; most gains came from feature engineering, deeper platform integration, and expanding model scope, which made deep learning hard to prioritize.</p> <p>Still, we knew a switch would eventually be needed to unlock uplift beyond what boosting could deliver. Many FDM inputs are naturally structured sequences (timestamps, past payments) or raw text. While careful feature engineering helped (e.g., rolling-window shopper aggregates, proxy email features), these signals weren’t fully exploited. Growth also created operational pressure: rising payment volume and feature count strained out-of-core training and constant-memory inference. At the same time, the company’s long-term strategy emphasized deep learning—foundational payments models, model unification, and multi-task learning.</p> <p>We concluded that a well-designed neural network could outperform given the right scale and infrastructure, but a hard cutover from a trusted boosting model would be risky.</p> <h2 id="heterogeneous-ensembling-as-a-migration-strategy">Heterogeneous Ensembling as a Migration Strategy</h2> <p>The observation that tree-based models typically outperform neural networks on tabular data has been explored in recent work<d-cite key="gorishniy2023revisitingdeeplearningmodels"></d-cite><d-cite key="chen2023tromptbetterdeepneural"></d-cite>. This performance gap is often explained by a few key arguments. One is <em>smoothness bias</em>, where standard NNs favor smooth decision boundaries, which can be a poor fit for the piecewise-constant structures common in tabular problems. Another is <em>rotational invariance</em>, as NNs are generally rotation-invariant while trees are sensitive to axis-aligned splits. Finally, there is the issue of <em>scale</em> since trees are sample efficient and handle small to medium datasets extremely well, whereas NNs often require more data and/or careful regularization.</p> <p>The literature also suggests remedies to these limitations, including discretization and binning, embeddings for numerical features, attention-based architectures to mitigate rotational invariance, and augmentation with large-scale training to exploit NNs as universal approximators.</p> <p>We took inspiration from these ideas, but also from industry case studies where NNs or heterogeneous ensembles beat boosting models in production. Migration narratives from ShareChat<d-cite key="Jeunen_2023"></d-cite> for short-video recommendations, Swiggy<d-cite key="swiggy2021learningtorank"></d-cite> for restaurant ranking, Stripe <d-cite key="stripe2020howwebuiltitstriperadar"></d-cite> for payment fraud detection, and internal anecdotes from eBay all point to a similar pattern of leveraging heterogeneous ensembling (stacking) as a temporary model instead of directly migrating from boosting to neural networks.</p> <p>Our production boosting models were already a form of ensembling (albeit homogeneous), as they combine many similar weak trees into a strong model. This approach could be extended through heterogeneous ensembling, in which multiple classes of learners, such as boosting models and neural networks, are combined to solve the same task. Ensembling approaches (e.g., bagging, boosting, stacking) have been extremely effective at winning competitions on tabular data <d-cite key="erickson2025tabarenalivingbenchmarkmachine"></d-cite><d-cite key="holzmüller2025betterdefaultstrongpretuned"></d-cite>. This illustrates the <em>No Free Lunch</em> theorem, which suggests that no single model class can dominate on all learning tasks. We eventually chose to explore heterogeneous ensembling through stacking, in which diverse learners are trained to solve a task, and a meta-learner is trained on the same task using the learners’ predictions as input features.</p> <pre><code class="language-mermaid">graph TD
  A[Input Features]

  subgraph "Base Learners"
      B[Network 1]
      C[Network 2]
      D[Booster 1]
      E[Booster 2]
  end

  A --&gt; B
  A --&gt; C
  A --&gt; D
  A --&gt; E

  F(Meta-Learner)

  B  --&gt; F
  C  --&gt; F
  D  --&gt; F
  E  --&gt; F

  F --&gt; G[Prediction]
</code></pre> <p>Aside from usually yielding superior performance compared to single learner classes, stacking also provides a valuable and safe blueprint for transitioning to deep learning. A simple stacking of our current boosting models with a fairly basic neural network could result in direct but moderate performance uplift. We could then gradually improve the neural network while serving it within a stacking model in production until it was strong enough to be deployed on its own, ensuring a smooth transition and incremental performance gains.</p> <p>Of course, this approach would first have to be validated experimentally on our large fraud detection dataset before this transition plan could be enacted.</p> <h2 id="transition-process">Transition Process</h2> <p>The complete transition of our fraud detection models from ML to DL ranged from October 2024 to September 2025. Given the uncertainties regarding the performance and deployability of neural networks and stacking models, we avoided a large-scale monolithic migration and instead focused on shorter, iterative experiments.</p> <h3 id="offline-experiments">Offline Experiments</h3> <p>We ran an initial feasibility study to evaluate whether a neural network would outperform our current boosting model for fraud detection on our offline benchmarks. Even though feature pruning and engineering efforts seemed promising for bringing additional uplift to neural networks, we kept the experiment’s scope simple by comparing models on the current production feature set.</p> <p>We built a lightweight experimentation loop focused on rapid iteration rather than production readiness. We initially leveraged the <a href="https://github.com/pyg-team/pytorch-frame" rel="external nofollow noopener" target="_blank">PyTorch Frame</a> <d-cite key="hu2024pytorch"></d-cite> library, which conveniently collects popular NN architectures for tabular data from the literature, ranging from simple MLP and ResNet<d-cite key="gorishniy2023revisitingdeeplearningmodels"></d-cite> architectures to novel solutions such as FT-Transformer (attention-based models for tabular data)<d-cite key="gorishniy2023revisitingdeeplearningmodels"></d-cite>, TabNet<d-cite key="arik2020tabnetattentiveinterpretabletabular"></d-cite>, and ExcelFormer <d-cite key="chen2024excelformerneuralnetworksurpassing"></d-cite>. The library also includes several popular encoding schemes for numerical and categorical features. This allowed us to iterate over these architectures to see how they adapted to our problem space and scale.</p> <p>After running multiple training and tuning experiments, we observed that neural networks overall underperformed by 18% to 30% on our internal metrics versus our boosting baselines across a combination of architectures and encoding schemes:</p> <ul> <li> <p>On the architecture side, we surprisingly observed that performance metrics did not significantly differ across architectures: simple architectures such as MLP and ResNet were just as good (or, in this case, just as bad) as complex ones such as FT-Transformer or TabNet, although the latter were shown to bring significant performance uplift on smaller benchmarks <d-cite key="rubachev2024tabredanalyzingpitfallsfilling"></d-cite>. Furthermore, model training time increased significantly with the complexity of the architectures on our then fairly immature GPU cluster, rendering the training of some architectures, like ExcelFormer, prohibitively time-consuming.</p> </li> <li> <p>On the encoding side, we found that an adequate encoding scheme for numerical and categorical features was essential for acceptable performance from neural networks. Here again, complex encoding schemes such as piecewise linear encoding or numerical embeddings <d-cite key="gorishniy2023embeddingsnumericalfeaturestabular"></d-cite> did not bring uplift and sometimes significantly increased training time. A simple combination of standard scaling and masking missing values for numerical features, and learned embeddings for categorical features, gave the best results.</p> </li> </ul> <p>This first run of experiments allowed us to settle, quite surprisingly, on the simplest available architecture: a wide, shallow MLP of around 10M parameters. This architecture yielded superior performance among the architectures we could train (in a close tie with ResNet) and trained relatively fast, taking around 10 hours at the time to converge on our training sets.</p> <p>The surprising finding that a simple MLP was the most stable and scalable architecture for our problem shifted our strategy from finding the best architecture to making a simple architecture good at scale and matches recent findings in the literature <d-cite key="holzmüller2025betterdefaultstrongpretuned"></d-cite><d-cite key="He2014PracticalLF"></d-cite>.</p> <h3 id="ensembling-as-a-bridge">Ensembling as a Bridge</h3> <p>We then turned our efforts to the stacking strategy that would best complement the MLP architecture. We used a simple ridge classifier as our meta-learner and compared two popular forms of stacking:</p> <ul> <li>A <em>simple</em> stacking scheme in which the predictions of the MLP and the boosting model are fed as two inputs to the meta-learner. This is the most basic form of stacking, as the meta-learner receives only two input features with no extra information about the sample it is scoring.</li> <li>A <em>deep</em> stacking scheme in which the predictions of each individual tree from the booster and activations from the second-to-last layer of the MLP are fed to the meta-learner. This allows for a richer representation of the underlying sample being scored <d-cite key="He2014PracticalLF"></d-cite>.</li> </ul> <pre><code class="language-mermaid">graph TD
    A[Input Features]
    
    subgraph boosting [Boosting Model]
      B[Boosting Model]
      subgraph tree_outputs [Individual Trees]
        T1((T1))
        T2((T2))
        T3((...))
        TN((TN))
      end
    end
    
    subgraph neural [Neural Network]
      L1[First Layers]
      subgraph neuron_activations [Last Layer Neurons]
        N1((N1))
        N2((N2))
        N3((...))
        NM((NM))
      end
    end

    A --&gt; B
    B --&gt; T1
    B --&gt; T2
    B --&gt; T3
    B --&gt; TN

    A --&gt; L1
    L1 --&gt; N1
    L1 --&gt; N2
    L1 --&gt; N3
    L1 --&gt; NM
    
    ML[Meta-Learner]

    T1 --&gt; ML
    T2 --&gt; ML
    T3 --&gt; ML
    TN --&gt; ML

    N1 --&gt; ML
    N2 --&gt; ML
    N3 --&gt; ML
    NM --&gt; ML

    ML --&gt; FP[Prediction]
</code></pre> <p>While stacking models added complexity to the codebase, training them was extremely quick and straightforward. Both forms of stacking showed uplift compared to our standalone MLP trained in our first experimental round, which was somewhat expected. However, their individual performance was quite surprising. On the one hand, the simple two-input stacking scheme showed a 3.8% uplift against our flagship boosting model. On the other hand, the deep stacking scheme, although passing substantially more information to the meta-learner, underperformed by 10.8% against the same baseline.</p> <table> <thead> <tr> <th style="text-align: left">Model</th> <th style="text-align: left">Performance vs. Boosting</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Standalone MLP</td> <td style="text-align: left">-18.0%</td> </tr> <tr> <td style="text-align: left">Stacking (Simple)</td> <td style="text-align: left">+3.8%</td> </tr> <tr> <td style="text-align: left">Stacking (Deep)</td> <td style="text-align: left">-10.8%</td> </tr> </tbody> </table> <p>These results were encouraging since stacking provided direct uplift compared to boosting. Furthermore, there were several unexplored approaches that could provide future performance gains on the neural network side: we had done no feature engineering, hyperparameter tuning was kept minimal, and the network architecture was extremely simple. Given these prospective improvements, we prioritized exploratory work on the transition of the model in production through stacking as it provided a great migration strategy: it gave immediate incremental gains without directly replacing the boosting models and offered a safety net while the neural network matured.</p> <p>However, we remained cautious due to the numerous unknowns regarding its deployability and latency in the live payment flow.</p> <h3 id="live-validation">Live Validation</h3> <p>The next essential step of the migration was validating whether the performance gains obtained through stacking offline could be achieved in production. More specifically, we wanted to ensure that our stacking model could correctly score transactions in production while respecting our strict latency requirements.</p> <p>The simple stacking scheme we settled on made it trivial to combine the boosting model and neural network to fulfill the two model interfaces that would be called in production: <code class="language-plaintext highlighter-rouge">score</code>, which requests a probability of fraud for a transaction, and <code class="language-plaintext highlighter-rouge">explain</code>, which generates merchant-facing signals explaining the model’s decision.</p> <pre><code class="language-mermaid">graph LR
  Score(Score)
  Explain(Explain)

  subgraph "Stacking Predictor"
      subgraph "Torch Predictor"
    NNS(Torch Neural Network)
    NNE(Captum Explanation Module)
  end
  subgraph "Boosting Predictor"
    BMS(LightGBM Booster)
    BME(LightGBM Explanation Module)
  end
    MMS(Ridge Classifier)
    MME(Combine Explanations)
  end

  Score --&gt; NNS
  Score --&gt; BMS
  Explain --&gt; NNE
  Explain --&gt; BME

  NNS --&gt; MMS
  BMS --&gt; MMS
  MMS --&gt; FS(Model Score)

  BME --&gt; MME
  NNE --&gt; MME
  MME --&gt; FE(Model Explanations)
</code></pre> <p>We initially deployed the stacking model on our datacenters in a <em>ghost</em> state, in which they scored duplicated versions of live payments without influencing their outcomes, allowing us to monitor model behavior with minimal risk. The average latency of the stacking model was around 12 ms (versus 5 ms for the boosting model), which was significantly under our latency requirements. These results showed that CPU are more than sufficient for real-time inference, which validate one of the core hypothesis of this work, that NNs/DL doesn’t need GPUs at serving-time.</p> <p>We then gradually rolled out the stacking model to influence payments, with a stacking setup still dominated by our boosting model.</p> <h3 id="scaling-and-transition">Scaling and Transition</h3> <p>With a strong stacking model online, we had time to invest in neural network-specific model improvements with the aim of matching or outperforming stacking to complete the migration. To gauge progress, we tracked the relative importance of boosting and the neural network within the ensemble over time.</p> <p>We ran a large number of parallel experiments to iteratively improve the model: feature encoding, feature engineering, loss function, and training stability were all revisited and meaningfully changed. These incremental changes increased the importance of the neural network within the stacking model, reducing its dependence on trees.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-from-ml-to-dl/figure1-480.webp 480w,/2026/assets/img/2026-04-27-from-ml-to-dl/figure1-800.webp 800w,/2026/assets/img/2026-04-27-from-ml-to-dl/figure1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-from-ml-to-dl/figure1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Finally, we fully switched to the standalone neural network once we observed that it consistently matched or outperformed the stacking model. The stacking strategy we adopted made the entire transition seamless, as the model could just be deployed to production while a stacking fallback was still available.</p> <h2 id="learnings">Learnings</h2> <p>Our migration from a mature boosting model to a pure deep learning model yielded a number of lessons that we believe generalize beyond our specific domain.</p> <p><strong>Start with the simplest possible uplift</strong> The goal of the first experiments was not to “beat” boosting with a sophisticated neural network architecture but to obtain <strong>any credible uplift</strong> in a way that built trust for the rest of the migration. A very simple MLP, combined with a simple two-input stacking scheme, was enough to show measurable gains over the flagship boosting model. This uplift unlocked further investment and bought time for the subsequent transition phases.</p> <p><strong>Use ensembling as a bridge, not a destination</strong> Ensembling was invaluable as a <strong>migration tool</strong>. It allowed us to gradually and safely introduce deep learning into a critical production system while improving performance and maintaining a reliable fallback. At the same time, we treated stacking as a bridge rather than a permanent architecture. Once the neural network consistently dominated the ensemble, the additional complexity of stacking no longer justified itself.</p> <p><strong>Separate feasibility from full productionisation</strong> We deliberately separated the question “Can this model run in production at all?” from “Can it replace the existing model everywhere?”. The initial production deployment focused on <strong>feasibility</strong> by ensuring correctness and speed in the payment flow. Only once these constraints were verified did we invest in making the DL model a first-class citizen in the main training and deployment workflows.</p> <p><strong>Invest in tooling and speed early</strong> Many of the most impactful investments were not in model architecture but in <strong>tooling</strong> such as workflows with fast feedback, efficient data loaders and GPU-aware training pipelines. Shortening our experimentation loop made it easier to iterate on architectures and features. Without this tooling, it would have been tempting to over-index on one-off model tweaks rather than systematic improvement.</p> <p><strong>Do not underestimate simple architectures at scale</strong> On our million-scale tabular problem, a ResNet-style MLP ultimately outperformed more complex tabular DL architectures from the literature. These more complex models tended to be harder to train, slower, and offered little upside once we accounted for our data scale and operational constraints. This shows that at our scale a <strong>simple and robust</strong> architecture was a better fit than a complex and novel one.</p> <h2 id="future-work">Future Work</h2> <p>While the migration is a success in itself that already brought performance improvements in production, we still believe that most of the uplift that neural networks can bring in our domain is yet to be realized. Among these future avenues, we identify:</p> <ul> <li> <p><strong>Multi-task and multi-objective modelling.</strong> Now that the core risk model of Company is powered by a neural network, it becomes easier to consider integrating other domain-specific risk models (e.g. card testing or abusive refunds detection) in a multi-task risk model or using multi-objective losses that balance fraud, customer experience, and operational costs. This could leverage the specificities of these multiple models in a large unified network, and greatly reduce operational workloads.</p> </li> <li> <p><strong>Closer integration with foundation models.</strong> Company has ongoing work on foundation models for payments. A deep learning-based fraud model provides a natural anchor for integrating such models, whether through shared embeddings, pretraining on large-scale transaction data, or joint training for related tasks.</p> </li> <li> <p><strong>Better use of structured and semi-structured fields.</strong> We have only started to exploit the potential of structured fields like anonymised emails or sequence-based velocity features in FDM’s neural network. There is ample room for better architectures and regularisation schemes tailored to these modalities.</p> </li> <li> <p><strong>Benchmarks that reflect production constraints.</strong><br> Our experience suggests that small academic tabular benchmarks do not fully capture the realities of large-scale production systems where latency, explainability, and stability matter as much as raw performance metrics. Designing benchmarks that incorporate these dimensions could lead to architectures and training schemes that transfer more directly to real-world deployments.</p> </li> <li> <p><strong>Characterising when NNs should replace trees.</strong><br> Finally, we would like to better understand, in a more principled way, when deep learning is likely to dominate gradient-boosted trees in tabular settings. Factors such as data size, feature types, temporal structure, and available infrastructure all play a role. Formalising these trade-offs could help teams decide when to invest in a migration like the one described here.</p> </li> </ul> <p>We hope that this case study, and the concrete migration pattern we followed – from offline experiments, to stacking, to a full deep learning model – can serve as a practical blueprint for teams considering a migration path from ML to DL in production — not by replacing everything at once, but by moving <em>one reversible step at a time</em>.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-from-ml-to-dl.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-long-context/">Text-as-Image, A Visual Encoding Approach for Long-Context Understanding</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/using-large-language-models-to-simulate-and-predict-human-decision-making/">Using Large Language Models to Simulate and Predict Human Decision-Making</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/useful-calibrated-uncertainties/">What (and What Not) are Calibrated Uncertainties Actually Useful for?</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>