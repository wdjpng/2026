<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Performative Prediction made practical | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Performative Prediction studies settings where deploying a model induces a distribution shift in the data with the aim of building robust and good-peforming models under these post-deployment effects. Most existing work in this area is theoretical and relies on strict assumptions to converge to those models, which makes the resulting techniques difficult to apply in practice and limits their accessibility to the broader Machine Learning (ML) community. In this blog post, we use visualization techniques 1) to provide an intuitive explanation of Performative Prediction and 2) to extract practical insights for studying convergence when theoretical assumptions do not hold."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/performative-prediction/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Performative Prediction made practical",
            "description": "Performative Prediction studies settings where deploying a model induces a distribution shift in the data with the aim of building robust and good-peforming models under these post-deployment effects. Most existing work in this area is theoretical and relies on strict assumptions to converge to those models, which makes the resulting techniques difficult to apply in practice and limits their accessibility to the broader Machine Learning (ML) community. In this blog post, we use visualization techniques 1) to provide an intuitive explanation of Performative Prediction and 2) to extract practical insights for studying convergence when theoretical assumptions do not hold.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Performative Prediction made practical</h1> <p>Performative Prediction studies settings where deploying a model induces a distribution shift in the data with the aim of building robust and good-peforming models under these post-deployment effects. Most existing work in this area is theoretical and relies on strict assumptions to converge to those models, which makes the resulting techniques difficult to apply in practice and limits their accessibility to the broader Machine Learning (ML) community. In this blog post, we use visualization techniques 1) to provide an intuitive explanation of Performative Prediction and 2) to extract practical insights for studying convergence when theoretical assumptions do not hold.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#deployed-model-happy-ml-engineer">Deployed model = Happy ML engineer?</a> </div> <div> <a href="#introduction-performative-prediction-is-a-two-step-iterative-process">Introduction - Performative Prediction is a two-step iterative process</a> </div> <ul> <li> <a href="#how-to-measure-performance-of-the-model-then">How to measure performance of the model then?</a> </li> <li> <a href="#visualizing-the-decoupled-risk">Visualizing the decoupled risk</a> </li> </ul> <div> <a href="#interest-points-of-performative-prediction">Interest points of Performative Prediction</a> </div> <ul> <li> <a href="#the-new-visualization-inspires-a-redefinition-of-the-interest-points">The new visualization inspires a redefinition of the interest points</a> </li> </ul> <div> <a href="#how-to-reach-them-optimization-in-performative-prediction">How to reach them? Optimization in Performative Prediction</a> </div> <ul> <li> <a href="#algorithms-that-converge-to-the-stable-point">Algorithms that converge to the stable point</a> </li> <li> <a href="#algorithms-that-converge-to-the-optimal-point">Algorithms that converge to the optimal point</a> </li> <li> <a href="#visualization-of-the-algorithms-in-the-decoupled-risk-landscape">Visualization of the algorithms in the decoupled risk landscape</a> </li> </ul> <div> <a href="#practical-insights-in-real-world-experiments">Practical insights in real-world experiments</a> </div> <div> <a href="#to-open-a-discussion-on-future-directions-of-the-field">To open a discussion on future directions of the field</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="deployed-model--happy-ml-engineer">Deployed model = Happy ML engineer?</h2> <p>You are a ML engineer working at a bank. You build a model to grant loans by predicting whether an applicant is likely to default. The model performs extremely well on your test set. The company deploys it, and in the first weeks everything looks great — the model performs just as expected. You receive congratulations, and you feel proud of a well-done job.</p> <p>But… a few months later, performance starts to drop. Repeat applicants have begun adjusting their financial profiles just enough to get approved. They have learned how to game your model. As a result, the reapplicants your system sees no longer resemble those in your training data. Simply by deploying your model, you have changed the data distribution. In other words, <strong>deploying your model has triggered a distribution shift</strong>.</p> <p>Once a ML model is deployed, it undoubtedly has an effect in the real world. Yet, this effect has been largely overlooked in ML, as deployment is often the <em>final</em> step of the ML pipeline. If we want models to perform reliably in practice, those post-deployment effects must be taken into account.</p> <p>This scenario has been formalized in the field of <em>Performative Prediction</em>, <d-cite key="perdomo2020performative"></d-cite> which studies situations when deploying a learn model $\theta$<d-footnote>Throughout the text, we use $\theta$ to refer both to the model parameters and to the model itself.</d-footnote> changes the data distribution. In Performative Prediction, these distribution changes have been formalized with a <em>distribution map</em> $\mathcal{D}(\theta)$, a function from the parameter space to the set of probability distributions $\mathcal{P}$ over data $\mathcal{X} \times \mathcal{Y}$.</p> \[\begin{aligned} \mathcal{D} :\; &amp; \Theta \longrightarrow \mathcal{P} (\mathcal{X} \times \mathcal{Y}) \\ &amp; \: \theta \longmapsto \mathcal{D}(\theta) \end{aligned}\] <p>This formulation contrasts with the classical ML setup, where one assumes a fixed data distribution $\mathcal{Q}$. In Performative Prediction, however, each deployment of the model induces a shift in the data distribution. After this shift occurs, the model must be retrained, but the newly trained model, once deployed, triggers yet another distribution change. This creates an iterative feedback loop between model training and distribution shift, which is the defining characteristic of the performative setting.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-performative-prediction/ML%20vs%20PP.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-performative-prediction/ML%20vs%20PP.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Whereas the main objective in classical ML is to find a model that minimizes the risk under a fixed data distribution, Performative Prediction presents a more intricate challenge because the model itself affects the distribution. In this setting, one can seek a <em>stable</em> model, that has the same risk before and after deployment, i.e., is robust to the distribution shift. One may also seek <em>optimal</em> model, which is the model that produces the lower risk after deployment. We will formalize this notions later.</p> <p>Published works in Performative Prediction have mainly adopted a theoretical perspective, relying on strict assumptions to ensure guarantees for the convergence of optimization algorithms to stable or optimal points. However, these assumptions significantly narrow the scope of problems that can be solved. This can be surprising, since the motivation of the field is intrinsically practical — it concerns what happens after a model is put into production. Another issue of this is that although Performative Prediction is now a well-recognized framework for studying the effects of deployment, its technical details remain relatively unknown to the broader ML community.</p> <p>Through visualization, this blog post aims to make the technical details of performative prediction more accessible to the broader ML community and to provide insights to support more practical research that emerge directly from these visualizations. Specifically, our goals are:</p> <ol> <li> <p>To introduce Performative Prediction as a two-step iterative process, an intuitive interpretation that naturally reveals the structure of the optimization landscape, and to explain how this landscape can be visualized (Section <a href="#introduction-performative-prediction-is-a-two-step-iterative-process">Introduction</a>).</p> </li> <li> <p>To use the visualization to describe the two interest points of Performative Prediction (stable and optimal points) and reformulate them (Section <a href="#interest-points-of-performative-prediction">Interest Points</a>), and to visualize the trajectories of different optimization algorithms, developing intuition for why they converge (Section <a href="#how-to-reach-them-optimization-in-performative-prediction">Optimization in Performative Prediction</a>).</p> </li> <li> <p>To show how this reformulation of the interest points can be practically used in experimental settings that fall outside the usual theoretical assumptions, illustrating their value in more realistic scenarios (Section <a href="#practical-insights-in-real-world-experiments">Practical insights in experiments</a>). As part of pushing Performative Prediction forward, we also reflect on the future of its practical research, hoping to spark a discussion within the community about its current limitations and emerging opportunities (Section <a href="##to-open-a-discussion-on-future-directions-of-the-field">Future directions of the field</a>).</p> </li> </ol> <p>To support practical research, we also make available <a href="https://anonymous.4open.science/r/performativeGYM" rel="external nofollow noopener" target="_blank"><em>PerformativeGym</em></a>, a small library that implements the main algorithms used in Performative Prediction.</p> <h2 id="introduction-performative-prediction-is-a-two-step-iterative-process">Introduction: Performative Prediction is a two-step iterative process</h2> <p>In order to characterize the feedback loop between the model and the data distribution, Performative Prediction can be described as an iterative process. Before the process begins, we have an initial data distribution, which we use to train the first model $\theta^{(1)}$. Some time after deployment, the world will react to the model causing a distribution shift. The data will then follow a new probability distribution given by $\mathcal{D}(\theta^{(1)})$. In each subsequent iteration, we <span style="color:#8DA0CB"> 1) obtain a model \(\theta^{(t+1)}\) trained on the data distribution \(\mathcal{D}(\theta^{(t)})\)</span> and <span style="color:#E5C494">2) deploy it, causing the distribution to shift to \(\mathcal{D}(\theta^{(t+1)})\)</span>. This distribution shift is inherent to model deployment, i.e., it happens because of the mere deployment of the model and is unavoidable.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-performative-prediction/Iteration.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-performative-prediction/Iteration.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Classical ML reports the performance of the model after training. But this is not enough in Performative Prediction due to the distribution shift. The fact that the environment reacts to the model makes it difficult to evaluate its performance.</p> <h3 id="how-to-measure-performance-of-the-model-then">How to measure performance of the model then?</h3> <p>In Performative Prediction, we can to consider two risks when evaluating a model $\theta^{(t+1)}$: the risk of the model under the learn distribution $\mathcal{D}(\theta^{(t)})$, which reflects its training performance; and the risk of the same model under the new distribution $\mathcal{D}(\theta^{(t+1)})$, which captures its performance after the distribution shift and is ultimately the more informative quantity. The <em>decoupled performative risk</em> $\mathcal{DPR}(\cdot, \cdot)$ allows us to measure both. For clarity, we will introduce the notation \(\theta_M\) to refer to the parameters of the model we want to evaluate and \(\theta_D\) to refer to the parameters of the model that defines the data distribution $\mathcal{D}(\theta_D)$.</p> \[\mathcal{DPR}(\theta_D, \theta_M) := \mathbb{E}_{(x,y) \sim \mathcal{D}(\theta_D)} \big[\ell(\theta_M; x, y)\big],\] <p>where \(\ell(\theta_M; x, y)\) is the loss function of the model \(\theta_M\) on the data sample \((x,y)\).</p> <p>Ultimately, we are, however, mostly interested in the risk of the model on the distribution <em>induced by its own parameters</em>, since this captures its actual post-deployment performance. This risk is referred to as the <em>performative risk</em> and is defined as follows:</p> \[\mathcal{PR}(\theta) := \mathcal{DPR}(\theta, \theta) = \mathbb{E}_{(x,y) \sim \mathcal{D}(\theta)} [\ell(\theta; x, y)].\] <div class="l-page-outset" style="display: flex; justify-content: center; align-items: center; padding: 40px 0; min-height: 105vh;"> <div style="background: #f2f2f2ff; border-radius: 40px; padding: 30px; width: 90%; height: 90%; display: flex; align-items: center; justify-content: center; box-shadow: 0 4px 20px rgba(0,0,0,0.1);"> <iframe src="/2026/assets/html/2026-04-27-performative-prediction/risk_iteration_scroll.html" frameborder="0" scrolling="yes" width="100%" height="100%" style="border: none; border-radius: 30px; background: transparent;"> </iframe> </div> </div> <h3 id="visualizing-the-decoupled-risk">Visualizing the decoupled risk</h3> <p>All published works center their analysis on the performative risk, as it measures the performance after the shift. However, as we saw in the last section, the performative risk cannot measure performance right after training. To have full understanding of the optimization dynamics, we need to take then a look beyond the performative risk.</p> <p>To do this, we can visualize the decoupled performative risk. Throughout this blog post, we use this visualization to gain insights into questions like: what are the practical differences between an stable and an optimal point? How do certain algorithm converge? Or how can we have a sense of convergence if theoretical assumptions are not met?</p> <p>Before introducing the visualization, let us present another example: now imagine that you are a ML engineer in a retailer company in charge of training a model that sets the price of the products. The price of the products will undoubtedly affect the demand of the customers to buy the product. Thus, we can formalize this problem as Performative Prediction. We consider a <em>simplified</em> version of this setup introduced by Izzo et al. <d-cite key="izzo2021learn"></d-cite>: let $\theta \in \mathbb{R}^d$ be the price of $d$ products and $z \in \mathbb{R}^d$ be the demand of customers for buying the products. The price of the product affects the demand of that product by a Gaussian distribution $\mathcal{D}(\theta) = \mathcal{N}(\mu_o - \varepsilon\theta; \Sigma)$, i.e., the demand linearly decreases as the price increases. The retail company is interested maximizing their total revenue, the performative risk is:</p> \[\mathcal{PR}_{pricing}(\theta) = \mathbb{E}_{z \sim \mathcal{D}(\theta)}[-\theta^T z]\] <p>Note that in this example, the model $\theta$ is the price of the products and the output of the model is the total revenue ($\theta^T z$). As you want to maximize it, then the loss function is simply $\ell(z;\theta) = -\theta^T z$, there is no need to have labels.</p> <p><strong>With the decoupled risk visualization, it is easier to understand the optimization dynamics!</strong></p> <p>Consider the case where your retailer company only sells one product, $z,\theta \in \mathbb{R}$. <span style="color:#8DA0CB">You train an initial model $\theta^{(1)}$ on the initial distribution $\mathcal{D}(\theta^{(0)})$. Your risk is then $\mathcal{DPR}(\theta^{(0)}, \theta^{(1)})$.</span> <span style="color:#E5C494">After deployment, the distribution changes to $\mathcal{D}(\theta^{(1)})$. Your risk is then $\mathcal{PR}(\theta^{(1)}) = \mathcal{DPR}(\theta^{(1)}, \theta^{(1)})$.</span> This pull towards the diagonal cross-section, i.e., the performative risk as $\mathcal{DPR}(\theta, \theta) = \mathcal{PR}(\theta)$, happens again at every step, creating a repeated cycle of updating the model and observing the induced distribution shift.</p> <div class="l-screen" style="min-height: 105vh;"> <iframe src="/2026/assets/html/2026-04-27-performative-prediction/decoupled_risk_landscape.html?v=3" frameborder="0" scrolling="no" width="100%" height="100%" style="border: none; background: transparent; min-height: 90vh;"> </iframe> </div> <p>During the <span style="color:#8DA0CB">optimization step</span>, the data distribution does not change. Therefore, the optimization happens in a vertical section of the plane, where $\theta_D$ is fixed. We will call this section of the plane the <strong>fixed-distribution cross-section</strong>. Its corresponding risk is:</p> \[\mathcal{DPR}(\theta_D, \theta_M)\big|_{\theta_D = \theta^{\prime}_D} = \mathcal{R}_{\theta^{\prime}_D} (\theta_M) = \mathbb{E}_{(x,y) \sim \mathcal{D}(\theta^{\prime}_D)} \big[\ell(\theta_M; x, y)\big] .\] <p>This can be seen as a marginalization of the decoupled performative risk. Thus, $\theta^{\prime}_D$ is a fixed value. Note that in classical ML, you operate within just one single fixed-distribution cross-section, without accounting for the richer space in which the system actually evolves after model deployment.</p> <h2 id="interest-points-of-performative-prediction">Interest points of Performative Prediction</h2> <p>One natural solution to the problem of Performative Prediction is deploying a model that — after it has affected the data distribution — does not require retraining, i.e., a model robust to the distribution shift. This model will be optimal for the distribution induced by itself. This point is called performatively stable:</p> \[\theta_{ST} = \operatorname*{argmin}_{\theta} \mathbb{E}_{(x,y)\sim \mathcal{D}(\theta_{ST})}[\ell(\theta;x, y)]\] <p>and it can be seen as the fixed point solution of the function \(\theta^*_M(\theta_D) := \operatorname*{argmin}_{\theta_M} \mathcal{DPR}(\theta_M, \theta_D)\).</p> <p>However, this solution is not optimal for the closed-loop interaction between the data and the model, i.e., it is not the minimum of the performative risk. The minimum of the performative risk is called the performative optimum:</p> \[\theta_{OP} = \operatorname*{argmin}_{\theta} \mathcal{PR}(\theta) = \operatorname*{argmin}_{\theta} \mathbb{E}_{(x,y)\sim \mathcal{D}(\theta)}[\ell(\theta;x, y)]\] <h3 id="the-new-visualization-inspires-a-redefinition-of-the-interest-points">The new visualization inspires a redefinition of the interest points</h3> <p>The new visualization allows visualizing the difference between robustness (stable point) and optimality (optimum point) more clearly. Keep in mind they are interest points of the performative risk, and therefore, they lie in the diagonal section of the plane, where $\mathcal{DPR}(\theta, \theta) = \mathcal{PR} (\theta)$.</p> <p>If the minimum in one fixed-distribution cross-section \(\theta^* = \operatorname*{argmin}_{\theta_M} \mathcal{R}_{\theta_i}(\theta_M)\) is the same model that induced the distribution ($\theta^* =\theta_i$), then $\theta_i$ is stable.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-performative-prediction/Stable_pricing-480.webp 480w,/2026/assets/img/2026-04-27-performative-prediction/Stable_pricing-800.webp 800w,/2026/assets/img/2026-04-27-performative-prediction/Stable_pricing-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-performative-prediction/Stable_pricing.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>In the pricing example, $\theta_1$ is stable because the fixed-distribution risk \(\mathcal{R}_{\theta_1}(\theta_M)\) is flat; consequently, every $\theta_M \in \Theta$ is a minimizer of $\mathcal{R}_{\theta_1}(\theta_M)$, including $\theta_1$. On the other hand, $\theta_2$ is not a stable point because \(\theta_2 \ne \operatorname*{argmin} \mathcal{R}_{\theta_2}(\theta_M)\).</p> <p>Therefore, the gradient in the direction of the fixed-distribution cross-section has to be zero for the point to be stable. We abuse notation and use $\nabla_M$, $\nabla_D$ to refer to $\nabla_{\theta_M}$, $\nabla_{\theta_D}$, respectively.</p> <p><strong>Proposition 1.</strong> <em>If</em> $\theta_{ST}$ <em>is a stable point of the performative risk</em> $\mathcal{PR}(\theta)$, <em>then</em></p> \[\nabla_M\,\mathcal{DPR}(\theta_\mathit{ST}, \theta_\mathit{ST}) = 0~.\] <p>On the other hand, the optimal point of the performative risk is the minimum of the performative risk, i.e., the diagonal section.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-performative-prediction/Optimal_pricing-480.webp 480w,/2026/assets/img/2026-04-27-performative-prediction/Optimal_pricing-800.webp 800w,/2026/assets/img/2026-04-27-performative-prediction/Optimal_pricing-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-performative-prediction/Optimal_pricing.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Thus, the gradient in the diagonal direction has to be zero.</p> <p><strong>Proposition 2.</strong> <em>If</em> $\mathcal{PR}(\theta)$ <em>is strictly convex, then</em> $\theta_{OP}$ <em>is an optimal point of the performative risk</em> $\mathcal{PR}(\theta)$ <em>if and only if</em></p> \[\nabla_M\,\mathcal{DPR}(\theta_\mathit{OP}, \theta_\mathit{OP}) + \nabla_D\,\mathcal{DPR}(\theta_\mathit{OP}, \theta_\mathit{OP}) = 0~.\] <p>In Section <a href="#practical-insights-in-real-world-experiments">Practical insights in experiments</a>, we will show how these reformulations can be used to get practical insights about convergence in experiments where strict theoretical assumptions do not hold.</p> <h2 id="how-to-reach-them-optimization-in-performative-prediction">How to reach them? Optimization in Performative Prediction</h2> <h3 id="algorithms-that-converge-to-the-stable-point">Algorithms that converge to the stable point</h3> <p>The Performative Prediction literature started out by focusing on how to find the stable solution, as it is a more tractable optimization problem. The first algorithms were based on <em>just</em> retraining the model after sampling from the new distribution. The process of these algorithms can be summarized as:</p> <ol> <li>Get the data samples of the distribution induced by \(\theta^{(t)}\): \((x,y)\sim \mathcal{D}(\theta^{(t)})\)</li> <li>Train the model on those data samples considering the distribution fixed: \(\theta^{(t)} \rightarrow \theta^{(t+1)}\)</li> <li>Deploy the model, causing a new distribution shift: \(\mathcal{D}(\theta^{(t)}) \rightarrow \mathcal{D}(\theta^{(t+1)})\)</li> </ol> <p>If the model is fully optimized at each step, the algorithm is called <em>Repeated Risk Minimization</em> (RRM). Whereas if only one gradient descent step is performed, we call it <em>Repeated Gradient Descent</em> (RGD).</p> <p>In the founding paper of Performative Prediction, Perdomo et al.,<d-cite key="perdomo2020performative"></d-cite> provide convergence guarantees of these algorithms to a stable point. Their proof relies on three key assumptions. The loss function $\ell(\theta; x, y)$ must be convex with respect to the model parameters $\theta$ and jointly smooth in $(x,y)$ and $\theta$. And the distribution map $\mathcal{D}(\theta)$ must be $\varepsilon$-sensitive to changes in $\theta$; that is, small variations in the parameters should produce only small variations in the induced data distribution. Formally, this is captured by the condition</p> \[\mathcal{W}(\mathcal{D}(\theta_1), \mathcal{D}(\theta_2)) \le \varepsilon \| \theta_1 - \theta_2\|^2 ,\] <p>where $\mathcal{W}(\cdot, \cdot)$ is the Wasserstein distance of the two distributions. $\varepsilon$ can be understood as the performative effect. It is a measure of how much performativity there is in the example. When $\varepsilon=0$, then $\mathcal{D}(\theta_1) = \mathcal{D}(\theta_2)$ and we are in the classical ML setup.</p> <p>With our visualization, it is quite intuitive to see why the algorithms converge. Recall that a fixed-distribution cross-section has a static distribution, $\mathcal{D}(\theta^{\prime}_D)$, where $\theta^{\prime}_D$ is the model that generates the fixed distribution. Therefore, if $\ell(\theta_M; x,y)$ is strongly convex in $\theta_M$, then the fixed-distribution risk \(\mathcal{R}_{\theta^{\prime}_D}(\theta_M) = \mathbb{E}_{\mathcal{D}((x,y) \sim \theta^{\prime}_D)}[\ell(\theta_M;x,y)]\) is also strongly convex since the expectation will preserve convexity.<d-footnote>This will not happen in the performative risk $\mathcal{PR}(\theta) = \mathbb{E}_{\mathcal{D}(\theta) \sim (x,y)}[\ell(\theta;x,y)]$ because the expectation is dependent on $\theta$.</d-footnote> Moreover, under the $\varepsilon$-sensitivity on the distribution map (which can be seen as a regularity assumption), nearby values of $\theta_D$ induce similar distributions. This means that the fixed-distribution cross-sections for nearby $\theta_D$ will also look similar.</p> <p>Consequently, each cross-section is strongly convex (i.e., they have a global minimum), consecutive cross-sections vary smoothly and the optimizer moves across them. Because the diagonal covers all the domain of $\theta$, eventually, during this process, the minimizer of some fixed-distribution cross-section will be within the diagonal—i.e., it will coincide with the model that generates that very distribution. At that point, the update becomes self-consistent, and the algorithm reaches a stable point.</p> <p>Note that these algorithms do not use the information of the distribution map when training the model. They just use the data sampled from the shifted distribution. This distribution is considered to be static. Although it is very easy to apply them in practice (wait until the distribution shifts, observe new data samples and retrain), they do not find the optimal solution, as they do not use explicit information of the distribution map while retraining the model. In these initial algorithms, the only guarantee to optimality is that the stable point might lie close to the optimal point under certain conditions, which are even more strict than the convergence to the stable point.</p> <h3 id="algorithms-that-converge-to-the-optimal-point">Algorithms that converge to the optimal point</h3> <p>Later, the literature started focusing on how to find the optimal solution directly. This point is more interesting, as it is the minimum of the performative risk. The most immediate idea is to apply gradient descent to the performative risk, this algorithm is called Performative Gradient Descent (PerfGD). The key step here is to calculate the performative gradient:</p> \[\nabla_{\theta} \mathcal{PR}(\theta) = \nabla_{\theta} \mathbb{E}_{(x,y) \sim \mathcal{D}(\theta)} [\ell(\theta; x, y)]~.\] <p>This gradient is difficult to calculate due to the dependency of the data distribution on the model parameters in the expectation.<d-footnote>When finding the stable point, one need only to calculate $$\mathbb{E}_{(x,y) \sim \mathcal{D}(\theta)} [ \nabla_{\theta} \ell(\theta; x, y)]$$ because the distribution is considered static; that is why it is more mathematically tractable.</d-footnote> Two approaches have been proposed:<d-footnote>As both $x$, $y$ can be changed due to the distribution map, we consider the notation $z=(x,y)$ from now on.</d-footnote></p> <ol> <li>REINFORCE<d-cite key="izzo2021learn"></d-cite>: uses the fact that the gradient of the likelihood of a random variable is the same as the likelihood times the gradient of the log likelihood $\nabla_{\theta} p_{\mathcal{D}(\theta)}(z) = p_{\mathcal{D}(\theta)}(z)\nabla_{\theta}\log p_{\mathcal{D}(\theta)}(z)$: \(\begin{align} \nabla_\theta \mathcal{PR}(\theta) &amp;= \nabla_\theta \int \ell(\theta;z) p_{\mathcal{D}(\theta)}(z) dz \nonumber \\ &amp;= \int \frac{\partial \ell(\theta;z)}{\partial \theta} p_{\mathcal{D}(\theta)}(z)dz + \int \ell(\theta;z) \frac{\partial p_{\mathcal{D}(\theta)}(z)}{\partial \theta}dz\nonumber\\ &amp; = \int \frac{\partial \ell(\theta;z)}{\partial \theta} p_{\mathcal{D}(\theta)}(z)dz + \int \ell(\theta;z) \frac{\partial \log p_{\mathcal{D}(\theta)}(z)}{\partial \theta} p_{\mathcal{D}(\theta)}(z)dz \nonumber \\ &amp;=\mathbb{E}_{z \sim \mathcal{D}(\theta)} \left[\frac{\partial \ell(\theta;z)}{\partial \theta} \right] + \mathbb{E}_{z \sim \mathcal{D}(\theta)} \left[ \ell(\theta;z)\frac{\partial \log p_{\mathcal{D}(\theta)}(z)}{\partial \theta} \right] \nonumber\\ &amp;=\mathbb{E}_{z \sim \mathcal{D}(\theta)} \left[\frac{\partial \ell(\theta;z)}{\partial \theta} + \ell(\theta;z)\frac{\partial \log p_{\mathcal{D}(\theta)}(z)}{\partial \theta} \right] \end{align}\)</li> <li>Reparametrization trick<d-cite key="cyffers2024optimal"></d-cite>: uses a deterministic function that is dependent in a base distribution and encodes the transformation caused by a parameter. Therefore, the expectation depends on the base-distribution only. In the case of Performative Prediction, this is achievable by defining a base distribution $\mathcal{D}_o$ that captures the samples before performativity and a push-forward model that defines the transformation of each sample due to performativity $z = \varphi(z_o, \theta)$. We can then use the multivariate chain rule to calculate the performative gradient: \(\begin{align} \nabla_\theta \mathcal{PR}(\theta) &amp;= \nabla_\theta \mathbb{E}_{z \sim \mathcal{D}(\theta)} \big[ \ell(\theta;z)\big] \nonumber \\ &amp; = \nabla_\theta \mathbb{E}_{z_o \sim \mathcal{D}_o} \big[ \ell(\theta; \varphi(z_o, \theta))\big] \nonumber \\ &amp;= \mathbb{E}_{z_o \sim \mathcal{D}_o} \Big[ \nabla_\theta \ell(\theta; \varphi(z_o, \theta))\Big] \nonumber \\ &amp;= \mathbb{E}_{z_0 \sim \mathcal{D}_0} \left[ \left.\frac{\partial \ell(\theta;z)}{\partial z}\right|_{z=\varphi(z_0; \theta)} \frac{\partial \varphi(z_0;\theta)}{\partial \theta} + \left. \frac{\partial \ell(\theta;z)}{\partial \theta}\right|_{z=\varphi(z_0; \theta)} \right] \end{align}\)</li> </ol> <p>The proof of convergence of PerfGD is more complex; it requires to study the convexity of $\mathcal{PR}(\theta)$. Due to the dependency on $\theta$ of the expectation, it is not enough that $\ell(\theta;z)$ is convex. Miller et al. <d-cite key="miller2021outside"></d-cite> explore the conditions under which the performative risk is strongly-convex.</p> <p>Note that this algorithm still does not move directly along $\mathcal{PR}(\theta)$. Instead, it still takes a step in the fixed-distribution direction first (<span style="color:#8DA0CB">in the optimization step</span>) and only afterwards returns to the diagonal (<span style="color:#E5C494">when the distribution shift happens</span>). What does change is that the optimization step now incorporates information about how the distribution responds to $\theta$. In fact, the gradient decomposes as:</p> \[\nabla \mathcal{PR}(\theta)= \nabla_D \mathcal{DPR}(\theta_D, \theta_M) + \nabla_M \mathcal{DPR}(\theta_D, \theta_M),\] <p>so the update reflects both the direct model gradient and the distribution-induced component.</p> <h3 id="visualization-of-the-algorithms-in-the-decoupled-risk-landscape">Visualization of the algorithms in the decoupled risk landscape</h3> <p>The figure illustrates the convergence behavior of the different algorithms for the pricing example in the decoupled risk landscape. We keep the same color scheme from previous visualizations to facilitate comparison and interpretation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-performative-prediction/viz_pricing.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-performative-prediction/viz_pricing.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>RGD converges to the stable point, whereas RRM does not converge at all. The reason is that the loss $\ell(\theta;z)$ is convex but not strongly convex (see Proposition 3.6 of Perdomo et al. <d-cite key="perdomo2020performative"></d-cite>, which characterizes when RRM fails to converge). PerfGD converges to the optimal point quicker when using the reparameterization trick rather than REINFORCE.</p> <h2 id="practical-insights-in-real-world-experiments">Practical insights in real-world experiments</h2> <p>One of the restrictive assumptions for convergence of the Performative Prediction algorithms is convexity of the loss function $\ell(\theta; x,y)$ with respect to the model weights.<d-footnote>As far as we are concerned, there is only one work considering convergence to non-convex losses in Performative Prediction.<d-cite key="li2024stochastic"></d-cite> It is still a theoretical work that proves the convergence to a relaxation of our redefinition of the stable point.</d-footnote> As we saw in the previous example, the algorithms can still converge even when this assumption is not met, which often happens in more practical examples (think about state-of-the-art Neural Network architectures, whose losses are usually high-dimensional and not strongly convex at all). What can we say about convergence in practical examples then?</p> <p>In classical ML, under these conditions, the focus is typically shifted from convergence to the global minimum to find solutions heuristically that perform well. In fact, it has been widely studied in Deep Learning, that a flat minimum generalize well (not necessarily only a global minimum)<d-cite key="dinh2017sharp"></d-cite>. In Performative Prediction, we can use our Proposition 2 when $\mathcal{PR}(\theta)$ is not convex to find good performing points. Therefore, the gradients of the $\mathcal{DPR}(\theta_D,\theta_M)$ can be used as a metric to have a sense of flatness and therefore, convergence.</p> <p>In this section, we use the gradients to show convergence in one experiment with high-dimensions and another one Neural Network as a model.</p> <p>Let’s extend the pricing example to $d=100$ products. Now, $\theta, z \in \mathbb{R}^d$. The stable point ($\theta_{ST} = \frac{\mu_0}{\epsilon}$) and optimal point ($\theta_{OP} = \frac{\mu_0}{2\epsilon}$) can be computed in closed-form. <d-footnote>Please refer to Izzo et al. <d-cite key="izzo2021learn"></d-cite>, where this example comes from, for full details on this derivation</d-footnote></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-performative-prediction/pricing_web.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-performative-prediction/pricing_web.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>To check how good the algorithms perform, we plot the distance to the optimal point. As expected, RGD only converges to the stable point whereas PerfGD converges to the optimal. We suspect the REINFORCE variant of PerfGD struggles in the high-dimensionality case because the variance of the REINFORCE approach is too high for it to get a good estimate of the gradient. Based on our experience with both versions of PerfGD, we recommend to use the reparametrization one whenever possible. We can see how the gradients confirm the convergence to the stable or optimal point. In RGD, only $\nabla_M \mathcal{DPR}$ goes to zero. In contrast, with PerfGD neither of $\nabla_M \mathcal{DPR}$ nor $\nabla_D \mathcal{DPR}$ go to zero, but their sum $\nabla \mathcal{PR}$ does because both of the components go in opposite directions.</p> <p>Let’s also consider the bank loan example. We use the real-world dataset <em>Give me some credit</em> <d-cite key="GiveMeSomeCredit"></d-cite>, which was established by Perdomo et al. <d-cite key="perdomo2020performative"></d-cite> as the default dataset of Performative Prediction. This tabular dataset describes if a loan was returned ($y=0$) or defaulted ($y=1$) based on financial features of applicants ($x\in\mathbb{R}^{11}$). As standard in Performative Prediction, we equip the dataset with a transformation function $\Gamma$ based on strategic classification. Instead of using logistic regression, we use a nonlinear classifier (a 2-layer multi-layer perceptron MLP) as a model.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-performative-prediction/credit_web.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-performative-prediction/credit_web.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Inspired by classical ML, we have added momentum to GD and used Adagrad as an optimizer. These techniques work heuristically to find good minima but have never been used in Performative Prediction. By looking at the gradients, we can analyze if they also work in Performative Prediction.</p> <p>In this case, RGD converges to a stable point because only $\nabla_M \mathcal{DPR}(\theta_D, \theta_M)$ goes to zero. In contrast, all variants of PerfGD converge to a flat minimum because both $\nabla_M \mathcal{DPR}(\theta_D, \theta_M)$ and $\nabla_D \mathcal{DPR}(\theta_D, \theta_M)$ approach zero, thereby making $\nabla \mathcal{PR}(\theta)$ vanish. Using adagrad or momentum speeds up the convergence dynamics. We cannot assert that this minimum is the performatively optimal point; we can only say that it is a flat stationary point. However, as discussed, this distinction is not crucial in practice: $\mathcal{PR}(\theta)$ is not strongly convex, thus we cannot say much about the global optimum and prior results have shown that flat minima often perform well in real-world settings.<d-cite key="dinh2017sharp"></d-cite></p> <p>We believe that these new insights about the gradients of the decoupled risk are specially relevant in Performative Prediction to account for realistic scenarios with non-convex losses or no sensitivity guarantees on the distribution map.</p> <h2 id="to-open-a-discussion-on-future-directions-of-the-field">To open a discussion on future directions of the field</h2> <p>We could conclude the blog post at this point. We have clarified why Performative Prediction is a relevant problem, introduced the core ideas of the field to a broader ML audience, and proposed metrics that can be directly applied in practical scenarios. However, the blog post format gives us space to reflect more broadly. In particular, we would like to use this opportunity to discuss the challenges ahead and outline what we see as meaningful next steps for practical research on Performative Prediction within the community.</p> <p>Performative Prediction is, in many ways, a cursed field. Its motivation is fundamentally practical—models deployed in the world influence the data they will later learn from—yet practical research faces severe, structural limitations. To converge to the truly relevant performative optimum, we need information about the distribution map $\mathcal{D}(\theta)$, which governs how the environment shifts in response to a model’s predictions. Recall that Performative Prediction is a two-step iterative process: (1) train a model, (2) deploy it and observe the induced distribution shift. Most existing works focus on the first step, proposing algorithms that optimize under the assumption that either (a) the distribution map is known, or (b) it can be reliably estimated from samples. The core barrier to practical Performative Prediction is that in realistic settings, we have very limited visibility into how deployment shifts the data. Estimating $\mathcal{D}(\theta)$ from samples alone is difficult or ethically constrained. Once a model is deployed, it is typically fixed, and deploying multiple models across different populations solely to collect data for estimation would be highly unethical.</p> <p>We argue that there are two main directions for the field of <em>practical</em> Performative Prediction to advance: 1) developing techniques to estimate $\mathcal{D}(\theta)$ considering the previously stated limitations or 2) focusing on <em>outcome performativity</em>, which frames the distribution map as a function of the inputs and the model’s predictions.</p> <p>On the one hand, regarding estimation of the distribution map, Miller et al.<d-cite key="pmlr-v139-miller21a"></d-cite> take an initial step, although their analysis is restricted to linear location–scale families. More recently, Bracale et al.<d-cite key="bracale2024learning"></d-cite> approach the problem through causal tools. The central challenge remains the same: approximating complex, latent distributions under limited information. Progress on this front could unlock new, practically grounded approaches to estimating $\mathcal{D}(\theta)$.</p> <p>On the other hand, outcome performativity <d-cite key="kim2023making"></d-cite> offers a more tractable way to model the distribution map. Outcome performativity considers settings where the model’s prediction $\hat{y}$ affects only the final outcome $y$, not the input features $x$. For example, reconsider the bank-lending scenario. A model predicts a borrower’s default risk, and the bank uses this prediction to decide whether to grant the loan <em>and</em> at what interest rate. The interest rate, in turn, influences the borrower’s ability to repay; thus the outcome $y$ changes as a consequence of the prediction $\hat{y}$ even though the features $x$ remain unchanged. In this view, the distribution map can be formalized as a function that takes the features and prediction as inputs and returns the resulting outcome:</p> \[\begin{aligned} \Gamma :\; &amp; \mathcal{X} \times \mathcal{Y} \longrightarrow \mathcal{Y} \\ &amp; \: x \times \hat{y} \: \longmapsto y \end{aligned}\] <p>Crucially, it is often possible to collect data that reflects this effect, because the outcome is shaped by decision rules already in operational use. For this reason, we argue that Performative Prediction research may be more practically impactful when focused on outcome performativity. The ethical and feasibility limitations discussed above are significantly reduced, and the feedback loop can be studied in realistic, deployment-aligned scenarios.</p> <h2 id="conclusion">Conclusion</h2> <p>Now imagine you are a ML engineer working in the company of your dreams. You have been hired some time ago and you are excited because you worked on the next big thing. The soon-to-be-released model that works amazingly well in your test sets — AGI is just around the corner! But… have you considered the effects that this model will have on the data distribution? If the internet is populated with text and images created by your model… you might not be able to train the next model. <d-cite key="shumailov2024ai"></d-cite></p> <p>Are we taking enough care in post-deployment effects? It seems that it is crucial and retraining is not enough…</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-performative-prediction.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-long-context/">Text-as-Image, A Visual Encoding Approach for Long-Context Understanding</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/vis-llm-latent-geometry/">Visualizing LLM Latent Space Geometry Through Dimensionality Reduction</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/using-large-language-models-to-simulate-and-predict-human-decision-making/">Using Large Language Models to Simulate and Predict Human Decision-Making</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>