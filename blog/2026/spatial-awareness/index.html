<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Where's the Chicken? Unpacking Spatial Awareness in Vision-Language Models | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Modern vision-language models (VLMs) have achieved impressive success in recognizing and describing visual content, yet they continue to struggle with understanding spatial relationships. The limitation persists even with massive data and model scaling, suggesting that the root of the problem lies in the architecture and training objective rather than data alone. This post examines the underlying causes and discusses why recent proposed fixes, while promising, remain insufficient to achieve robust spatial reasoning."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/spatial-awareness/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Where's the Chicken? Unpacking Spatial Awareness in Vision-Language Models",
            "description": "Modern vision-language models (VLMs) have achieved impressive success in recognizing and describing visual content, yet they continue to struggle with understanding spatial relationships. The limitation persists even with massive data and model scaling, suggesting that the root of the problem lies in the architecture and training objective rather than data alone. This post examines the underlying causes and discusses why recent proposed fixes, while promising, remain insufficient to achieve robust spatial reasoning.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Where's the Chicken? Unpacking Spatial Awareness in Vision-Language Models</h1> <p>Modern vision-language models (VLMs) have achieved impressive success in recognizing and describing visual content, yet they continue to struggle with understanding spatial relationships. The limitation persists even with massive data and model scaling, suggesting that the root of the problem lies in the architecture and training objective rather than data alone. This post examines the underlying causes and discusses why recent proposed fixes, while promising, remain insufficient to achieve robust spatial reasoning.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#the-what-vs-the-where">The "What" vs. the "Where"</a> </div> <div> <a href="#why-does-the-vlm-architecture-forget-position">Why Does the VLM Architecture Forget Position?</a> </div> <ul> <li> <a href="#the-evolution-of-positional-encodings">The Evolution of Positional Encodings</a> </li> <li> <a href="#the-semantic-loudness">The "Semantic Loudness"</a> </li> </ul> <div> <a href="#do-vlms-look-at-the-right-place">Do VLMs Look at the Right Place?</a> </div> <ul> <li> <a href="#text-dominates-vision-follows">Text Dominates, Vision Follows</a> </li> <li> <a href="#misdirected-gaze">Misdirected Gaze</a> </li> </ul> <div> <a href="#what-are-vlms-designed-for">What are VLMs Designed For?</a> </div> <div> <a href="#path-towards-spatially-aware-vlms">Path Towards Spatially-Aware VLMs</a> </div> </nav> </d-contents> <h2 id="introduction-the-what-vs-the-where">Introduction: The “What” vs. the “Where”</h2> <p>To truly understand an image, we have to treat it as more than a collection of pixels. As an image is a 2D projection of a fundamentally 3D world, mentally reconstructing the scene requires at least two components: recognizing “<strong>what</strong>” is in the image and understanding “<strong>where</strong>” those things are located.</p> <p>This notion of “where” comes in two forms. One is the <em>absolute where</em>: identifying an object’s position on the image plane, often by drawing a bounding box around it. The other is the <em>relational where</em>: reasoning about how objects are situated relative to one another (e.g., “the chick is behind the cup” or “the car is to the left of the tree”). Both forms are important, but in this post we focus on the latter: how models reason about spatial relationships.</p> <p>Combing back to the problem of image understanding, we cannot reliably infer the scene behind an image without knowing both what is present and where things are in relation to each other. Let’s consider a simple scene with two chicks sitting near a cup.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-spatial-awareness/1-480.webp 480w,/2026/assets/img/2026-04-27-spatial-awareness/1-800.webp 800w,/2026/assets/img/2026-04-27-spatial-awareness/1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-spatial-awareness/1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> An image with two chicks and a cup on a wooden table. From the camera’s viewpoint, a chick with a purple ribbon is in front of a ceramic cup, while a chick with a blue bonnet is behind the cup. </div> <p>If someone asks, “<em>Grab me the chick behind the cup</em>,” the instruction only makes sense if we can correctly identify the “cup” (<em>what</em>) and accurately interpret what “behind” means in the appropriate reference frame (<em>relational where</em>). For example, if “behind” is defined relative to the camera’s viewpoint, it refers to the object that is farther from the camera than the cup. This kind of relational reasoning is fundamental to real-world systems such as autonomous vehicles and robotic arms, where understanding both the objects and their spatial relationships is critical for safe and reliable action.</p> <p>Modern vision-language models (VLMs), such as Gemini and ChatGPT, have become remarkably good at the <em>what</em>. When asked to describe an image or generate a caption, they often produce accurate and detailed responses. As these models grow larger and get trained on increasingly more datasets, their ability to recognize objects and describe their visible content continues to improve. In many evaluations<d-cite key="lin2014microsoft"></d-cite><d-cite key="antol2015vqa"></d-cite>, they already achieve near human-level performance at identifying what is present in an image.</p> <p>However, when it comes to reasoning about <em>where</em> things are relative to each other, these same models often fall short.</p> <p>This <mark><b>"what" vs. "where" paradox</b></mark> in modern VLMs becomes especially clear in combined reasoning tasks like “<em>Find the hidden object</em>.”<d-footnote>View the actual conversation <a href="https://gemini.google.com/share/f5622207a4e3" rel="external nofollow noopener" target="_blank">here</a>.</d-footnote></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-spatial-awareness/2-480.webp 480w,/2026/assets/img/2026-04-27-spatial-awareness/2-800.webp 800w,/2026/assets/img/2026-04-27-spatial-awareness/2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-spatial-awareness/2.png" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Task asking Gemini 3 Pro to identify the hidden object. </div> <p>In this example, the model (Gemini 3 Pro) correctly identifies the hidden object (the golden ring) but mislocates it. It claims that the ring is to the <em>right</em> of the red Snoopy doghouse, when in fact it is located <em>below</em> it. These mistakes may seem minor, but they expose a deeper and persistent limitation of modern VLMs: strong semantic recognition does not guarantee accurate relational spatial understanding.</p> <p>We might expect this gap to shrink as models grow larger and are trained on more data. Yet, despite massive datasets and billions of parameters in state-of-the-art systems, this spatial weakness remains. A growing body of research suggests that the issue lies in the core architecture and training objectives of VLMs. In other words, the limitation stems not only from how much data we provide, but also from how these models are built and what they are fundamentally optimized to prioritize.</p> <p>In this post, we take a closer look at the architectural roots of this spatial blindness in modern VLMs. We examine the building blocks that process visual information, the training objectives that favor certain capabilities over others, and why many recent fixes still fall short of fully solving the problem. Understanding why models struggle with this relational “where” and how we might overcome this limitation is a key step toward building vision systems that truly understand the worlds they see.</p> <h2 id="why-does-the-vlm-architecture-forget-position">Why Does the VLM Architecture Forget Position?</h2> <p>To understand why VLMs often fail to capture spatial relationships, we need to look at their architectural foundation: the <strong>Transformer</strong>. Since its introduction in <em>Attention is All You Need</em><d-cite key="vaswani2017attention"></d-cite>, the Transformer has become the backbone of modern language and vision-language models due to the power of its core building block, <strong>self-attention</strong>. In brief, self-attention measures how relevant one token (or word) is to another. For example, in the sentence, “<em>Here is a yellow chick</em>,” the method learns that “<em>yellow</em>” is closely tied to “<em>chick</em>” because it describes it.</p> <p>However, pure self-attention has a critical blind spot: it does not care about order. If we only rely on relevance between tokens, the sentences “<em>The chick is in front of the cup</em>” and “<em>The cup is in front of the chick</em>” look identical, even though they describe entirely different spatial situations. Without any notion of token position, the model collapses the sentence into something like a bag-of-words representation, where only co-occurrence matters, not who is in front of whom.</p> <p>When we move from text to images, the problem intensifies. A typical VLM first chops an image into a grid of patches (for example, 16x16 or 32x32), converts the patches into tokens, and then feeds the resulting sequence into the Transformer. Without positional information, the model treats the image as a bag-of-patches. It may recognize that there is a chick and a cup, but it has no built-in mechanism to tell whether the chick patch is to the left, above, or behind the cup patch.</p> <h3 id="the-evolution-of-positional-encodings">The Evolution of Positional Encodings</h3> <p>To fix this, researchers developed a series of positional encodings that tag tokens or patches with location information. Many of these methods were originally developed to handle word order in sentences, but here we focus on what happens when they are applied to images.</p> <p>The original Transformer paper introduces <strong>absolute position encoding (APE)</strong>, which assigns a unique fixed vector to every patch. This approach is fine when all input images share the same size, but it fails once the resolution changes. If a model is trained on 224x224 images, it learns positional vectors tied to that exact grid. At test time, when we give a larger image, the model has no natural way to represent the extra patches. A common workaround is to stretch or interpolate the learned embeddings to a new size, but this distorts the spatial relationships. The model essentially overfits to the specific absolute locations, which hurts its ability to generalize to new resolutions or exhibit translation-invariant behavior.</p> <p>To move beyond fixed grid sizes, later work introduced <strong>relative position encoding (RPE)</strong><d-cite key="shaw2018rpe"></d-cite>. Instead of defining where a patch is globally, RPE says where it is <em>relative to other patches</em> (for example, “The chick patch is two steps away from the cup patch”). As self-attention already operates on pairwise relationships, this feels like a natural fit, and the model can handle variable-length inputs and different layouts more gracefully. But for 2D images, this comes with a trade-off. RPE largely discards absolute coordinates, which are crucial for localization-heavy tasks like object detection. If you care about <em>exactly</em> where a pixel or patch is on the image, a purely relative scheme can degrade performance on tasks that demand precise spatial grounding<d-cite key="wu2021rethinking"></d-cite>. In addition, many RPE implementations do not play well with key–value (KV) caching, since relative biases depend on the current sequence length, which forces the model to recompute more attention scores and leads to higher memory and computation costs for long contexts.</p> <p>Today, the de facto standard for large language models and many VLMs is <strong>rotary positional encoding (RoPE)</strong><d-cite key="su2024roformer"></d-cite>. RoPE aims to get the best of both APE and RPE: it preserves absolute position information while using rotation operations to naturally represent relative distances. This design tends to generalize better to longer sequences and has proven robust in text generation. But this method is built for 1D token sequences. When we apply it to images, we need to flatten the 2D grid into a 1D line, which breaks spatial locality: patches that are neighbors vertically might end up far apart in the flattened sequence. To address this, models like <strong>Qwen2-VL</strong><d-cite key="wang2024qwen2"></d-cite> introduce a multimodal variant (<strong>M-RoPE</strong>) that splits the rotary embedding into temporal, height, and width components, applying separate rotations to each. This better preserves the 2D (and temporal) structure of visual data. Still, these encodings are fundamentally 2D: they do not natively capture depth or full 3D spatial relationships without extra geometric signals.</p> <h3 id="the-semantic-loudness">The “Semantic Loudness”</h3> <p>Let’s imagine there exists a positional encoding scheme that perfectly preserves 3D structure. Even with such an ideal encoding, a growing line of work suggests that spatial reasoning would still remain a persistent weakness. The problem is not only <em>how good</em> our positional encodings are, but <em>how the Transformer processes them internally</em>. Recent work points to a deeper issue inside the Transformer itself: a phenomenon often referred to as <strong>embedding norm suppression</strong><d-cite key="qi2025beyondsemantics"></d-cite>. Transformers excel at building high-dimensional vectors that encode <strong>what</strong> is in each patch (e.g., “a yellow chick,” “a ceramic cup”). These semantic embeddings tend to have large vector magnitudes; in other words, they are <em>loud</em> in the embedding space. Positional encodings, by contrast, are comparatively <em>quiet</em>, contributing only a small magnitude to the final representation. During self-attention, the “loud” semantic content dominates the computation, effectively drowning out the “quieter” positional signals. The model often behaves as if position barely matters, even though we have carefully encoded it.</p> <p>This effect becomes evident in a simple permutation test. Take an image, break it into patches as usual, but then randomly shuffle the order of the visual tokens so that the original spatial layout is destroyed. Many VLMs show only a negligible drop in performance on standard benchmarks. For instance, consider the following captioning example:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-spatial-awareness/3-480.webp 480w,/2026/assets/img/2026-04-27-spatial-awareness/3-800.webp 800w,/2026/assets/img/2026-04-27-spatial-awareness/3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-spatial-awareness/3.png" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Result of the permutation test with Qwen3-VL. Even after the shuffling the visual tokens, the model produces similar captions that are nearly identical to the original, preserving key features (highlighted in brown, yellow, and blue). </div> <p>Even after random shuffling, the generated captions remain nearly identical. The model still identifies the objects (e.g., “chick” and “blue bonnet”) and produces a reasonable description of the scene. This suggests that, despite mechanisms like M-RoPE, the model effectively treats the image as a bag-of-semantic-features, with positional encodings contributing little to the final decision.</p> <p>To counteract this, recent work proposes <strong>embedding norm normalization</strong><d-cite key="qi2025beyondsemantics"></d-cite>. The idea is simple: rescale the magnitudes of the visual feature embeddings so that the semantic content and positional signals are on a comparable scale. This encourages the model to pay more attention to positional cues, improving spatial reasoning without sacrificing overall semantic understanding.</p> <p>However, balancing signal magnitudes is only a partial fix. It reveals that positional information is “quiet” relative to semantics, but even once that is corrected, deeper issues remain. Spatial reasoning goes beyond simply knowing coordinates. Encodings like APE, RPE, and RoPE, even when combined with embedding norm normalization, can indicate where tokens are and how far apart they are, but they do not by themselves capture the <em>structure</em> of the scene. Many spatial questions depend on richer relations such as containment (“Is the water <em>in</em> the cup?”), occlusion (“Is the chick <em>behind</em> the cup?”), and relative depth. These concepts require structured spatial reasoning and domain-specific visual priors, not just louder positional coordinates. In other words, making position information “loud enough to hear” is necessary, but not sufficient for VLMs to reason about space in the way humans do.</p> <h2 id="attention-allocation-problem-do-vlms-look-at-the-right-place">Attention Allocation Problem: Do VLMs Look at the Right Place?</h2> <p>Beyond how models encode position, another line of work asks a different question: <em>where</em> do models actually focus their attention? These studies suggest that many spatial errors arise not because positional information is missing, but because the model looks at the wrong place. In VLMs, this happens at two levels: <strong>between modalities</strong> (text vs. image) and <strong>within the image itself</strong> (relevant vs. irrelevant regions).</p> <h3 id="text-dominates-vision-follows">Text Dominates, Vision Follows</h3> <p>VLMs often process two sources of input: the image (e.g., a chick and a cup on a table), and the text (e.g., “Describe the image” or “Where is the chick relative to the cup?”). Architecturally, this is implemented with a visual backbone that encodes the image into patch tokens and a language backbone (usually a strong LLM) that encodes the text. These two pieces of information are then fused so that the model can reason over both modalities jointly.</p> <p>However, prior works<d-cite key="zhou2024analyzing"></d-cite><d-cite key="sun2024aligning"></d-cite> repeatedly finds that these two inputs do not contribute equally. As LLM backbones are typically much more heavily trained and semantically richer than the visual encoder, the combined model often exhibits a textual bias. Chen et al. (2025)<d-cite key="chen2025why"></d-cite>, for example, quantify this imbalance: roughly 90% of the attention flows to textual tokens, with only about 10% going to the visual tokens. The model leans heavily on the text prompt and its internal language statistics, and only lightly consults the image.</p> <p>This leads to a familiar failure mode in spatial reasoning. The model hallucinates what “should” be instead of varying what “is” there. If the textual prior suggests that “clouds are usually above the grass,” the model may confidently assert the relation even if, in the actual image, the scene is upside down and the clouds appear below. The model often sees what it expects from language, not what the pixels actually show. Spatial errors, in this view, are often less about raw capability and more about imbalances between modalities. VLMs tend to trust text more than vision.</p> <p>A naive reaction is to try to increase attention to the visual tokens. But this alone is not sufficient. If the model focuses more on the image but does not look at the right parts, spatial reasoning still fails.</p> <h3 id="misdirected-gaze">Misdirected Gaze</h3> <p>Even if we successfully encourage a VLM to rely more on visual input, a second failure mode appears: the model may simply look in the wrong place.</p> <p>Consider asking the question, “Is the chick in front of or behind the cup?” Ideally, the model should focus on the patches containing the chick and the cup. However, empirical analysis<d-cite key="chen2025why"></d-cite> shows that VLMs frequently scatter their attention across irrelevant regions, such as the table surface, the background wall, or other high-contrast noises, while paying relatively little attention to the actual objects mentioned in the question. In such cases, the model is technically “looking at the image,” but not at the evidence needed to answer the question.</p> <p>To address this, Chen et al. proposed AdaptVis<d-cite key="chen2025why"></d-cite>, a training-free method that redirects attention at inference time. The key idea is to use the model’s own prediction confidence to adapt its visual attention pattern dynamically. When the model is confident in its prediction, AdaptVis will sharpen the attention distribution, narrowing the focus to the regions the model already considers relevant and suppressing background noise. When the model is uncertain, AdaptVis broadens the attention window, encouraging it to explore a wider area of the image and potentially discover objects and relationships it initially overlooked. This confidence-guided adjustment has shown strong empirical gains. It suggests that many spatial failures do not stem from the lack of capability to reason about space; instead, the model often has the necessary information but fails to direct its focus to where it matters most.</p> <p>Overall, methods like AdaptVis highlight that attention allocation is a valuable piece in spatial reasoning. They show that VLMs often underuse the visual information they already encode, and that steering attention can meaningfully improve behavior.</p> <p>However, AdaptVis is not a complete solution. As the method operates purely at inference time, it cannot change the model’s internal representation or how the spatial relations are encoded. If the model has never formed a robust notion of concepts like “in front of” vs. “behind,” simply pointing its attention more precisely will not fully resolve those gaps. Attention steering can only help the model use what it knows more effectively. By itself, it cannot guarantee the richer, human-like spatial reasoning that many applications demand.</p> <h2 id="from-global-glance-to-localized-training-what-are-vlms-designed-for">From Global Glance to Localized Training: What are VLMs Designed For?</h2> <p>The methods introduced so far (norm normalization and AdaptVis) can be viewed as post-hoc fixes. That is, they operate on top of an already trained architecture, leaving both the original model design and its pretraining objectives unchanged. While these approaches did show some improvements, an increasing number of studies argue that true spatial awareness cannot simply be derived by adding an auxiliary mechanism. Instead, the root of the problem lies in how these models are trained from the beginning.</p> <p>The dominant paradigm for training large-scale VLMs is Contrative Language-Image Pretraining (CLIP)<d-cite key="radford2021learning"></d-cite>. In this framework, a model is trained on extensive collections of image-caption pairs (e.g., COCO) to match each image to its corresponding text description. This training signal is fundamentally global: the model aligns the entire image with the entire caption, and as a result, it is strongly encouraged to encode the overall meaning and semantic themes of a scene. This design choice gives modern VLMs their impressive ability to describe images fluently and accurately. However, it also comes with a built-in limitation: the training objective rewards learning what is in the image, but does not penalize for ignoring where those entities are located. Spatial structure is, at most, learned only indirectly.</p> <p>To address this limitation at its source, Chen et al. (2024) propose Contrastive Localized Language-Image Pretraining (CLOC)<d-cite key="chen2025contrastive"></d-cite>, which is a framework that explicitly incorporates spatial grounding into the pretraining objective. Under CLOC, the model is no longer trained solely to align the whole image with the whole caption. Instead, it is encouraged to learn both the object identity and spatial localization to produce representations that are grounded at a finer, region-aware level. In principle, this enables the visual backbone to encode spatial information directly within its representation, rather than relying on later-stage corrections.</p> <p>However, this approach comes with a substantial cost. Adopting localized pretraining typically requires retraining the visual backbone from scratch; this process is computationally expensive and time-consuming, given that many state-of-the-art VLMs result from years of training. This motivated a parallel line of research focusing on modular interventions: methods that aim to inject spatial awareness into existing, frozen models without requiring full retraining. Dorkenwald et al. (2024) introduce Positional Insert (PIN)<d-cite key="dorkenwald2024pin"></d-cite>, an input-agnostic, learnable spatial prompt with a small number of parameters that can be inserted into a frozen VLM to unlock object localization capabilities. Bhowmik et al. (2025) propose Twist &amp; Scout<d-cite key="bhowmik2025twist"></d-cite>, which modifies the language model’s decoder through a dual mixture-of-experts (MoE) design: one expert remains frozen to preserve the original CLIP-style semantics, while a second expert is specialized for location grounding. At inference time, the model dynamically switches between these experts depending on the task.</p> <p>While these methods represent an essential first step towards models that are natively capable of object localization, several fundamental challenges remain. For unified models (such as CLOC), in addition to the cost of retraining, a central issue is how much spatial information can be injected without degrading global semantic understanding. It is still unclear how to balance learning what and learning where within a single system: how much representational capacity and attention should be allocated to semantics versus spatial structure, and how this balance should adapt across different levels of visual complexity. In modular designs, preserving strong semantic experts while adding a dedicated spatial expert substantially increases computational overhead and memory footprint. Moreover, as this approach relies on frozen backbones, it limits how deeply spatial information can be integrated into the visual feature hierarchy. As a result, spatial cues are often introduced only at later stages (e.g., at the decoder level), constraining the model’s ability to form truly spatially grounded representations. Taken together, these challenges suggest that while localized and modular interventions offer promising directions, they are unlikely to fully resolve spatial reasoning on their own. Overcoming the “what” vs. “where” divide will likely require coordinated advances in model architecture, training objectives, supervision signals, and various other aspects of the learning pipeline, rather than isolated fixes applied at a single point.</p> <h2 id="conclusion-path-towards-spatially-aware-vlms">Conclusion: Path Towards Spatially-Aware VLMs</h2> <p>Stepping back, a consistent picture emerges: today’s VLMs excel at answering “<em>What’s in the image?</em>” but are less reliable at “<em>Where is it?</em>” This weakness does not disappear with more data or larger models. Instead, it traces back to how these systems are built and trained: Transformers that are naturally position-agnostic, positional encodings that are often drowned by rich semantic features, and training pipelines that prioritize general gist over spatial precision.</p> <p>Throughout the post, we have seen several promising attempts to patch this gap. More expressive positional encodings (APE to RPE to RoPE and M-RoPE) better preserve the 2D structure of images, and norm normalization helps positional information to compete with loud semantic embeddings. Attention-steering methods such as AdaptVis show that models often can reason spatially when they are guided to look at the right image regions. Localized and modular training strategies push the model to encode not only object identity but also their location in the scene.</p> <p>However, none of these by themselves fully solves spatial reasoning. Spatial awareness does not arise from plugging in a single module; it is a property that emerges from the interaction among architecture, attention, and the training signal. Moving towards a spatially-aware VLM will likely require these strands to work together: positional schemes that respect 2D (and 3D) structure, mechanisms that can reliably guide attention to the right evidence, and objectives that demand spatial precision. To build agents that can truly act in the physical world, we must treat “<strong>where</strong>” not as a byproduct of “<strong>what</strong>”, but as a fundamental component of vision.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-spatial-awareness.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-long-context/">Text-as-Image, A Visual Encoding Approach for Long-Context Understanding</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/vis-llm-latent-geometry/">Visualizing LLM Latent Space Geometry Through Dimensionality Reduction</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>