<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Neural audio codecs: how to get audio into LLMs | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="A look at why audio is harder to model than text and how we can make it easier with neural audio codecs. With a codec, we can turn audio into larger discrete tokens, train models to predict continuations for these tokens, and then decode those back into audio."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/neural-audio-codecs/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.audio-sample{width:100%}.bg-black{background-color:black}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Neural audio codecs: how to get audio into LLMs",
            "description": "A look at why audio is harder to model than text and how we can make it easier with neural audio codecs. With a codec, we can turn audio into larger discrete tokens, train models to predict continuations for these tokens, and then decode those back into audio.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Neural audio codecs: how to get audio into LLMs</h1> <p>A look at why audio is harder to model than text and how we can make it easier with neural audio codecs. With a codec, we can turn audio into larger discrete tokens, train models to predict continuations for these tokens, and then decode those back into audio.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#text-is-easy">Text is easy</a> </div> <div> <a href="#sample-by-sample">Sample by sample</a> </div> <div> <a href="#autoencoders-with-vector-quantization-vq-vae">Autoencoders with vector quantization (VQ-VAE)</a> </div> <div> <a href="#residual-vector-quantization">Residual vector quantization</a> </div> <div> <a href="#now-let-s-tokenize-audio">Now let‚Äôs tokenize audio</a> </div> <div> <a href="#why-care-about-audio">Why care about audio</a> </div> <div> <a href="#dealing-with-multiple-levels">Dealing with multiple levels</a> </div> <div> <a href="#finally-let-s-train">Finally, let‚Äôs train</a> </div> <div> <a href="#how-far-can-a-codec-get-us">How far can a codec get us?</a> </div> <div> <a href="#semantic-tokens">Semantic tokens</a> </div> <div> <a href="#making-poetry-semantic">Making poetry semantic</a> </div> <div> <a href="#semantic-acoustic-tradeoff">Semantic‚Äìacoustic tradeoff</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <figure> <video src="/2026/assets/img/2026-04-27-neural-audio-codecs/codec-intro.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> <div class="caption"> The plan: sandwich a language model in an audio encoder/decoder pair (=neural audio codec), allowing it to predict audio continuations. </div> <p>As of October 2025, speech LLMs suck. Many LLMs have voice interfaces, but they usually work by transcribing your speech, generating the answer in text, and using text-to-speech to read the response out loud. That‚Äôs perfectly fine in many cases, but it‚Äôs a wrapper, not <em>real</em> speech understanding. The model can‚Äôt hear the frustration in your voice and respond with empathy, it can‚Äôt emphasize important words in its answer, it cannot sense sarcasm, and so on.</p> <p>Yes, there <em>are</em> LLMs (Gemini 2.5 <d-cite key="DBLP:journals/corr/abs-2507-06261"></d-cite> <d-cite key="gemini_2_5_native_audio"></d-cite>, ChatGPT‚Äôs Advanced Voice Mode <d-cite key="DBLP:journals/corr/abs-2410-21276"></d-cite>, Qwen <d-cite key="DBLP:journals/corr/abs-2509-17765"></d-cite>, Moshi <d-cite key="DBLP:journals/corr/abs-2410-00037"></d-cite>) that understand and generate speech natively. But in practice, they‚Äôre either not as smart, or they behave like text model wrappers. Try asking any of them ‚ÄúAm I speaking in a low voice or a high voice?‚Äù in a high-pitched voice, and they won‚Äôt be able to tell you.</p> <p>Clearly, speech LLMs lag behind text LLMs. But why? For text, we found out a few years ago that if you take a lot of text data, a big Transformer, and a lot of GPUs, you‚Äôll get some pretty damn good text continuation models. Why can‚Äôt we just replace text with audio and get pretty damn good speech continuation models?</p> <p>As a teaser, here‚Äôs what happens when you try to do that naively (warning, loud):</p> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/nightmare-fuel-20lufs.wav" class="audio-sample" controls=""></audio> </figure> <p>We‚Äôll see how to make much better audio models using neural audio codecs, the de-facto standard way of getting audio into and out of LLMs. With a codec, we can turn audio into larger discrete <em>tokens</em>, train models to predict continuations for these tokens, and then decode those back into audio: see animation above.</p> <p>We‚Äôll start from the basics and build up all the way to Mimi, a modern neural audio codec originally developed for Moshi <d-cite key="DBLP:journals/corr/abs-2410-00037"></d-cite> and later adopted by others for their models, notably Sesame‚Äôs CSM <d-cite key="sesame_uncanny_valley_voice"></d-cite>.</p> <h2 id="text-is-easy">Text is easy</h2> <p>To tokenize text, everybody uses a technique called byte-pair encoding and rarely changes the tokenizer: OpenAI has been using <a href="https://github.com/openai/tiktoken/blob/2ab6d3706d557b560b200be48e6a32324682c9a3/tiktoken/model.py#L8-L16C17" rel="external nofollow noopener" target="_blank">the same tokenizer</a> since GPT-4o <d-cite key="DBLP:journals/corr/abs-2410-21276"></d-cite>, an ancient model if you count in LLM years.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-neural-audio-codecs/image-480.webp 480w,/2026/assets/img/2026-04-27-neural-audio-codecs/image-800.webp 800w,/2026/assets/img/2026-04-27-neural-audio-codecs/image-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-neural-audio-codecs/image.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> A random text from Wikipedia tokenized via the GPT-4o tokenizer </div> <p>You can even get decent results without tokenizing text at all, just predicting individual characters. One of the first posts that got me excited about machine learning was Andrej Karpathy‚Äôs RNN effectiveness blog post <d-cite key="unreasonable_rnns"></d-cite> from 2015. Karpathy trains a three-layer LSTM on a single GPU and gets it to generate decent-looking code and LaTeX:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-neural-audio-codecs/rnns-code-480.webp 480w,/2026/assets/img/2026-04-27-neural-audio-codecs/rnns-code-800.webp 800w,/2026/assets/img/2026-04-27-neural-audio-codecs/rnns-code-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-neural-audio-codecs/rnns-code.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-neural-audio-codecs/rnns-latex-480.webp 480w,/2026/assets/img/2026-04-27-neural-audio-codecs/rnns-latex-800.webp 800w,/2026/assets/img/2026-04-27-neural-audio-codecs/rnns-latex-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-neural-audio-codecs/rnns-latex.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Remember this was ten years ago, back when we didn‚Äôt even know that attention is all we need <d-cite key="DBLP:journals/corr/VaswaniSPUJGKP17"></d-cite>. Now compare Karpathy‚Äôs results to a sample from WaveNet <d-cite key="DBLP:conf/ssw/OordDZSVGKSK16"></d-cite>, a model DeepMind published a year later:</p> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/speaker-1.wav" class="audio-sample" controls=""></audio> </figure> <p>Purely acoustically, the audio sounds good, but it rarely even manages to produce a single correct English word. We can‚Äôt be too hard on WaveNet, though. The samples from Karpathy‚Äôs RNNs are only a few thousand characters long, but this 10-second audio consists of 160k audio samples, and WaveNet creates it by painstakingly predicting sample-by-sample.</p> <figure> <video src="/2026/assets/img/2026-04-27-neural-audio-codecs/wavenet-audio.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> <div class="caption"> A single second of audio consists of tens of thousands of samples, although it corresponds to just a few words. Animation from the WaveNet blog post <d-cite key="wavenet_blog"></d-cite> </div> <p>It‚Äôs difficult to build models that are coherent over time scales this long, and the model also takes very long to run for so many steps.</p> <p>So instead of running the model to predict the samples one-by-one directly, we‚Äôd like to train a model to compress the audio into a more manageable size. We could compress the audio, use an LLM to predict a continuation in the compressed representation, and then decompress the result.</p> <h2 id="sample-by-sample">Sample by sample</h2> <p>But first, let‚Äôs get a baseline model by generating audio sample by sample, like WaveNet does. The code for all of these experiments is open-source! [Link to code omitted for anonymization, reading the code is not necessary for understanding the blog post.] I forked Andrej Karpathy‚Äôs <a href="https://github.com/karpathy/nanoGPT" rel="external nofollow noopener" target="_blank">nanoGPT</a> repo, a simple implementation of GPT-2.</p> <p>Text and audio are kind of the same from the perspective of the language model: it‚Äôs just tokens in, tokens out. The only thing we need to do is to quantize the continuous values of the samples into discrete buckets. Like WaveNet, we‚Äôll use the ‚ÄúŒº-law algorithm‚Äù <d-cite key="ITU-G711"></d-cite> to get 256 buckets. We‚Äôll treat those as 256 possible tokens.</p> <p>Let‚Äôs train a language model on audio tokenized like this. For the dataset, we‚Äôll use the Libri-Light dataset <d-cite key="DBLP:conf/icassp/KahnRZKXMKLCFLS20"></d-cite>, following AudioLM <d-cite key="DBLP:journals/taslp/BorsosMVKPSRTGTZ23"></d-cite>. Its train split contains 50k hours in total, but we‚Äôll go with 1000 hours for this experiment. With this sample-by-sample tokenization, we end up with a dataset of 53 GB.</p> <p>We train a small-ish transformer of 151.28M parameters, about the size of the smallest GPT-2 variant <d-cite key="radford2019language"></d-cite>. When we sample from the model, it makes babbling sounds (warning, loud at times!):</p> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/20250925_140600_3.wav" class="audio-sample" controls=""></audio> </figure> <p>Often, it goes into a ‚Äúcrackling mode‚Äù that it can‚Äôt seem to get out of:</p> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/20250925_140600_4.wav" class="audio-sample" controls=""></audio> </figure> <p>I also trained a smaller model, which I teased at the beginning. It‚Äôs prone to generate nightmare fuel screeches (loud!):</p> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/nightmare-fuel-2-20lufs.wav" class="audio-sample" controls=""></audio> </figure> <p>As you can tell, we‚Äôre not AGI yet. It sounds speech-like, but you can‚Äôt make out a single word and the voice keeps changing. No wonder: the context size of the model is 2048, which, for 16 kHz audio, translates to 128ms, not even the length of one word. Also, these 10-second examples took 30 minutes to generate on an H100, so we‚Äôre a few orders of magnitude away from being real-time.</p> <p>So let‚Äôs build a neural audio codec to compress the audio. The hope is that if we reduce the sampling rate 100x, the model will also become ‚Äú100x more coherent‚Äù. An old idea in machine learning is to do this using an <em>autoencoder:</em> a model that takes an input, compresses it into a smaller ‚Äúlatent space‚Äù, and then tries to reconstruct the original input.</p> <p>In our case, we‚Äôll want an autoencoder whose latent space is quantized so that we can feed the latents into a language model and produce continuations. (You <em>can</em> generate continuations with unquantized latents, but it‚Äôs trickier <d-cite key="DBLP:journals/corr/abs-2508-19205"></d-cite> <d-cite key="DBLP:journals/corr/abs-2509-06926"></d-cite>.)</p> <h2 id="autoencoders-with-vector-quantization-vq-vae">Autoencoders with vector quantization (VQ-VAE)</h2> <p>Bear with me, because we‚Äôll take a detour from audio: let‚Äôs build a quantized autoencoder on images from Fashion-MNIST <d-cite key="DBLP:journals/corr/abs-1708-07747"></d-cite>. We‚Äôll take a subset with the first three classes: t-shirt/top, trouser, and pullover.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-neural-audio-codecs/fashion-mnist-3-480.webp 480w,/2026/assets/img/2026-04-27-neural-audio-codecs/fashion-mnist-3-800.webp 800w,/2026/assets/img/2026-04-27-neural-audio-codecs/fashion-mnist-3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-neural-audio-codecs/fashion-mnist-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> image source: <d-cite key="DBLP:journals/information/NikfamCMM023"></d-cite> </div> <p>First, let‚Äôs train a regular autoencoder to encode the images into two-dimensional space:</p> <figure> <video src="/2026/assets/img/2026-04-27-neural-audio-codecs/vq_images_unquantized_v2.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> <div class="caption"> Training a regular autoencoder on Fashion-MNIST </div> <p>Each frame shows one batch of training, with some batches skipped. The little images are the autoencoder‚Äôs reconstructions for the images in the batch. I‚Äôve added colors for the three classes (t-shirt/top=blue trousers=yellow, pullover=purple), but the autoencoder doesn‚Äôt get a class as input ‚Äì the space just naturally clusters by class. Let‚Äôs zoom in on a few reconstructions:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-neural-audio-codecs/rvq-without-quantization-v4-480.webp 480w,/2026/assets/img/2026-04-27-neural-audio-codecs/rvq-without-quantization-v4-800.webp 800w,/2026/assets/img/2026-04-27-neural-audio-codecs/rvq-without-quantization-v4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-neural-audio-codecs/rvq-without-quantization-v4.png" class="img-fluid rounded z-depth-1 bg-black" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Original images (top) and their reconstructed versions (bottom) </div> <p>As you can tell, the reconstruction quality is not great. The images are blurry and the first two images are reconstructed to nearly the same thing. But we used a tiny network (4 fully connected layers for the encoder and decoder each) and projected into a mere two dimensions, so we can‚Äôt expect too much of our model.</p> <p>Now let‚Äôs quantize these embeddings using a clustering. We‚Äôll do something like k-means: we‚Äôll maintain a list of the positions of the cluster centers. We initialize the positions randomly. For each training batch, we look at which embeddings would go to each cluster. (We don‚Äôt modify the embeddings, we just look at the assignment). Then we‚Äôll nudge each cluster center towards the average position of these embeddings.</p> <p>Also, if a center is unused for a while, we teleport it to a random embedding from the batch, because otherwise it has no way to get unstuck from its current position.</p> <figure> <video src="/2026/assets/img/2026-04-27-neural-audio-codecs/vq_images_unquantized_with_clustering_v2.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> <div class="caption"> Quantizing by fitting a clustering on top of the autoencoder </div> <p>You can see the reconstructions of the cluster centers getting refined over time.</p> <p>Next, we‚Äôll make the encoder and decoder themselves better at handling quantized embeddings during training, because currently, we‚Äôre just fitting the clustering on top of an autoencoder that is not ‚Äúaware‚Äù it‚Äôs being quantized. We‚Äôd like the autoencoder to adapt to the quantization as we train it. Currently, we‚Äôre doing this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="nf">get_batch</span><span class="p">()</span>
<span class="n">z</span> <span class="o">=</span> <span class="nf">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">x_reconstructed</span> <span class="o">=</span> <span class="nf">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="nf">reconstruction_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_reconstructed</span><span class="p">)</span>
</code></pre></div></div> <p>Instead of feeding the unquantized embedding into the decoder, we‚Äôll first move it to the closest cluster:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="nf">get_batch</span><span class="p">()</span>
<span class="n">z</span> <span class="o">=</span> <span class="nf">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">z_quantized</span> <span class="o">=</span> <span class="nf">to_nearest_cluster</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>     <span class="c1"># üëà
</span><span class="n">x_reconstructed</span> <span class="o">=</span> <span class="nf">decoder</span><span class="p">(</span><span class="n">z_quantized</span><span class="p">)</span>  <span class="c1"># üëà
</span>
<span class="n">loss</span> <span class="o">=</span> <span class="nf">reconstruction_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_reconstructed</span><span class="p">)</span>
</code></pre></div></div> <p>There is a snag: if we do this, we won‚Äôt be able to train the autoencoder any more, because the quantization operation is not differentiable, meaning there is no gradient flowing from the loss to the weights of the encoder. Essentially, we‚Äôre no longer able to answer the question: ‚Äúif I want the loss to decrease a bit, in which direction should I nudge the encoder‚Äôs weights?‚Äù</p> <p>We‚Äôll fix this problem by pretending it doesn‚Äôt exist. Yes, really. We‚Äôll think of <code class="language-plaintext highlighter-rouge">z_quantized</code> as <code class="language-plaintext highlighter-rouge">z</code> moved by an arbitrary vector that doesn‚Äôt affect the gradient. That will make the gradient of <code class="language-plaintext highlighter-rouge">z</code> equal to that of <code class="language-plaintext highlighter-rouge">z_quantized</code>, which is why this is also known as the <em>straight-through estimator</em> of the gradient.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="nf">get_batch</span><span class="p">()</span>
<span class="n">z</span> <span class="o">=</span> <span class="nf">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">residual</span> <span class="o">=</span> <span class="n">z</span> <span class="o">-</span> <span class="nf">to_nearest_cluster</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="c1"># .detach() means "forget that this needs a gradient"
</span><span class="n">z_quantized</span> <span class="o">=</span> <span class="n">z</span> <span class="o">-</span> <span class="n">residual</span><span class="p">.</span><span class="nf">detach</span><span class="p">()</span>
<span class="n">x_reconstructed</span> <span class="o">=</span> <span class="nf">decoder</span><span class="p">(</span><span class="n">z_quantized</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="nf">reconstruction_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_reconstructed</span><span class="p">)</span>
</code></pre></div></div> <p>In the forward pass, <code class="language-plaintext highlighter-rouge">z_quantized</code> is set to the same value as before, but importantly, the gradient of <code class="language-plaintext highlighter-rouge">z</code> is now equal to that of <code class="language-plaintext highlighter-rouge">z_quantized</code> rather than just being 0 because of the non-differentiable <code class="language-plaintext highlighter-rouge">to_nearest_cluster(z)</code> operation.</p> <p>There is a price to pay for this lie. When training, the encoder‚Äôs weights will be updated to improve the reconstruction loss, but they‚Äôre updated as if the quantization didn‚Äôt happen, so they won‚Äôt move in the optimal direction. But as long as the embeddings stick close to their cluster centers, the gradient direction will still be mostly correct.</p> <p>We can actually encourage the encoder to make embeddings that are easily quantizable by adding a <em>commitment loss</em>: a penalty for each point based on how far it is from its cluster center. The gradient of this loss will push the points closer to their cluster centers.</p> <p>By quantizing at training time and adding a commitment loss, it‚Äôs no longer just a clustering being fit on top of the embeddings. The model itself is trained to be good for quantization.</p> <figure> <video src="/2026/assets/img/2026-04-27-neural-audio-codecs/vq_images_balanced_v2.mp4" class="img-fluid rounded z-depth-1 w-full max-w-72" width="auto" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> <div class="caption"> An autoencoder trained explicitly to be easy to quantize </div> <p>You‚Äôll notice that the training dynamics look different: the commitment loss adds a certain ‚Äústiffness‚Äù that doesn‚Äôt allow the embeddings to move around as easily.</p> <p>Here‚Äôs what the reconstructions look like when we use the quantized representations:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-neural-audio-codecs/rvq-1-level-v4-480.webp 480w,/2026/assets/img/2026-04-27-neural-audio-codecs/rvq-1-level-v4-800.webp 800w,/2026/assets/img/2026-04-27-neural-audio-codecs/rvq-1-level-v4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-neural-audio-codecs/rvq-1-level-v4.png" class="img-fluid rounded z-depth-1 bg-black" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Notice how the first two images are reconstructed to <em>exactly</em> the same image. That‚Äôs simply because their embeddings got assigned to the same cluster and therefore quantized to the same value.</p> <p>The model described here is known as a VQ-VAE <d-cite key="DBLP:journals/corr/abs-1711-00937"></d-cite>: a vector-quantized variational autoencoder. The word ‚Äúvariational‚Äù here is just a vestigial leftover that doesn‚Äôt mean anything anymore <d-cite key="dieleman2025latents"></d-cite>.</p> <h2 id="residual-vector-quantization">Residual vector quantization</h2> <p>To improve the reconstruction fidelity, we can just increase the number of cluster centers. But keeping track of too many centers can get prohibitively expensive in terms of compute and memory required, so we‚Äôll do a clever trick: if we want $2^{20}$ (~1M) possible values, we won‚Äôt create $2^{20}$ clusters directly. Instead, we‚Äôll use two separate quantizers with $2^{10}=1024$ clusters and combine their result. Each embedding will then be quantized to a tuple of two integers in [0..1023], yielding $2^{20}$ possible combinations.</p> <p>Ok, but how? Well, recall the <code class="language-plaintext highlighter-rouge">residual</code> variable we used in the straight-through estimator, defined as <code class="language-plaintext highlighter-rouge">z - to_nearest_cluster(z)</code> the shift from the quantized embedding to the unquantized one. It represents the part of the original vector <code class="language-plaintext highlighter-rouge">z</code> that we didn‚Äôt manage to take into account when quantizing to <code class="language-plaintext highlighter-rouge">to_nearest_cluster(z)</code>.</p> <p>So for each embedding in the batch, we have a corresponding residual vector. The solution is obvious: we‚Äôll quantize these residuals exactly the same way we did with the original embeddings, by training another vector quantizer.</p> <p>This time, the 2D positions for a single quantizer don‚Äôt define images because we need to combine the two quantizers, so we‚Äôll just visualize everything as dots:</p> <figure> <video src="/2026/assets/img/2026-04-27-neural-audio-codecs/rvq_fmnist.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> <div class="caption"> Two-level quantization by fitting a quantizer on top of the ‚Äúresiduals‚Äù, aka the errors of the first quantizer </div> <p>Each image is then represented as the index of the cluster of the embedding and that of the residual. Let‚Äôs try to reconstruct a few images with this two-level quantizer:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-neural-audio-codecs/rvq-2-level-v4-480.webp 480w,/2026/assets/img/2026-04-27-neural-audio-codecs/rvq-2-level-v4-800.webp 800w,/2026/assets/img/2026-04-27-neural-audio-codecs/rvq-2-level-v4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-neural-audio-codecs/rvq-2-level-v4.png" class="img-fluid rounded z-depth-1 bg-black" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Original images (top), one-level reconstruction (middle), two-level reconstruction (bottom). These images are encoded as (4, 3), (4, 5), (16, 21), and (30, 3). </div> <p>The reconstructions of the first two images are similar, but no longer the exact same: the first image is represented as (4, 3) and the second as (4, 5). In other words, they share the same token for the first level, but differ in how the residual is quantized. The differences are quite subtle, so here‚Äôs a comparison between the one-level and two-level reconstructions:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-neural-audio-codecs/rvq-2-level-diff-v3-480.webp 480w,/2026/assets/img/2026-04-27-neural-audio-codecs/rvq-2-level-diff-v3-800.webp 800w,/2026/assets/img/2026-04-27-neural-audio-codecs/rvq-2-level-diff-v3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-neural-audio-codecs/rvq-2-level-diff-v3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Difference between one-level and two-level reconstructions </div> <p>I‚Äôd like to emphasize that the second quantization level makes modifications to the embedding, not the output pixels directly. This can be seen by the fact that the leftmost and rightmost image are encoded as (4, 3) and (30, 3) respectively. So they have the same residual code, 3, but it modifies the two reconstructed images in different ways.</p> <p>Clearly, the reconstructions are still not very accurate. The upper bound on the quality is the reconstruction from unquantized embeddings, so if your autoencoder is bad (and ours is), improving the quantization won‚Äôt save you.</p> <p>We‚Äôll stop here, but a natural extension to this idea is to go beyond two levels. Just take the residuals of the two-level reconstruction and quantize those, and so on. This generalized Residual Vector Quantization algorithm looks like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">rvq_quantize</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">z</span>
    <span class="n">codes</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">level</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">levels</span><span class="p">):</span>
        <span class="n">quantized</span><span class="p">,</span> <span class="n">cluster_i</span> <span class="o">=</span> <span class="nf">to_nearest_cluster</span><span class="p">(</span><span class="n">level</span><span class="p">,</span> <span class="n">residual</span><span class="p">)</span>
        <span class="n">residual</span> <span class="o">-=</span> <span class="n">quantized</span>
        <span class="n">codes</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">cluster_i</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">codes</span>
</code></pre></div></div> <p>Residual vector quantization was first applied to neural audio codecs in SoundStream <d-cite key="DBLP:journals/taslp/ZeghidourLOST22"></d-cite>, but the idea has been around since the 80s <d-cite key="multiplestage82"></d-cite>.</p> <h2 id="now-lets-tokenize-audio">Now let‚Äôs tokenize audio</h2> <p>Applying RVQ to audio is fairly straightforward. As our autoencoder, we‚Äôll use a convolutional neural network (CNN) similar to <a href="https://github.com/openai/jukebox/blob/08efbbc1d4ed1a3cef96e08a931944c8b4d63bb3/jukebox/vqvae/encdec.py" rel="external nofollow noopener" target="_blank">what Jukebox uses</a> <d-cite key="DBLP:journals/corr/abs-2005-00341"></d-cite>. The details of the architecture aren‚Äôt too important here. What‚Äôs important is that it‚Äôs a network that takes an audio with $t$ samples and converts it to a vector of shape $(\frac{t}{128}, 32)$. In other words, it downsamples by a factor of 128 and gives us 32-dimensional float representations. The decoder then takes the $(\frac{t}{128}, 32)$ embeddings and decodes them back into $t$ samples.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">audio</span> <span class="o">=</span> <span class="nf">get_batch</span><span class="p">()</span>               <span class="c1"># shape: [B, T]
</span><span class="n">z</span> <span class="o">=</span> <span class="nf">encoder</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>                <span class="c1"># shape: [B, T/128, 32]
</span><span class="n">audio_reconstructed</span> <span class="o">=</span> <span class="nf">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>  <span class="c1"># shape: [B, T]
</span></code></pre></div></div> <p>As before, we‚Äôll add an RVQ after the encoder. The only difference from the image case is that for each audio sample, we have $\frac{t}{128}$ embedding vectors, not just a single one as we did for images. We just quantize these independently (even though the encoder ‚Äúsees‚Äù more audio than what corresponds to that one vector). During training, we also have a batch dimension, so our model now looks like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">audio</span> <span class="o">=</span> <span class="nf">get_batch</span><span class="p">()</span>                         <span class="c1"># [B, T]
</span><span class="n">z</span> <span class="o">=</span> <span class="nf">encoder</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>                          <span class="c1"># [B, T/128, 32]
</span>
<span class="c1"># Combine the batch and time dimensions
</span><span class="n">z</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span>                              <span class="c1"># [B*T/128, 32]
</span>    <span class="n">z</span><span class="p">,</span> <span class="sh">"</span><span class="s">b t_emb d -&gt; (b t_emb) d</span><span class="sh">"</span>
<span class="p">)</span>

<span class="n">codes</span> <span class="o">=</span> <span class="nf">rvq_quantize</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>           <span class="c1"># integers, [B*T/128, levels]
</span><span class="n">z_quantized</span> <span class="o">=</span> <span class="nf">codes_to_embeddings</span><span class="p">(</span><span class="n">codes</span><span class="p">)</span>    <span class="c1"># [B*T/128, 32]
</span><span class="n">z_quantized</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span>                    <span class="c1"># [B, T/128, 32]
</span>    <span class="n">z_quantized</span><span class="p">,</span> <span class="sh">"</span><span class="s">(b t_emb) d -&gt; b t_emb d</span><span class="sh">"</span>
<span class="p">)</span>

<span class="n">audio_reconstructed</span> <span class="o">=</span> <span class="nf">decoder</span><span class="p">(</span><span class="n">z_quantized</span><span class="p">)</span>  <span class="c1"># [B, T]
</span></code></pre></div></div> <figure> <video src="/2026/assets/img/2026-04-27-neural-audio-codecs/codec-with-rvq.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> <p>The last missing piece before we can train our first neural audio codec is a loss function. There‚Äôs a whole rabbit hole we could go into about which one to choose, but we‚Äôll avoid it and just use a very simple one. We‚Äôll compute the log amplitude spectrogram of the original and reconstructed audio, and take their difference. The loss is the mean square of this difference between spectrograms.</p> <p>To make it harder for the model to overfit to this loss, we take the spectrogram with three different parameters for the short-time Fourier transform, and let our loss be the mean between the three sub-losses. This is called the <em>multi-scale spectral loss</em>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-neural-audio-codecs/image-2-480.webp 480w,/2026/assets/img/2026-04-27-neural-audio-codecs/image-2-800.webp 800w,/2026/assets/img/2026-04-27-neural-audio-codecs/image-2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-neural-audio-codecs/image-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Image from Evan Radkoff‚Äôs excellent blog post <d-cite key="loss_functions_audio_ml"></d-cite> about loss functions in audio ML. Check it out if you want to go down the loss function rabbit hole. </div> <p>Finally, let‚Äôs train some codecs! We‚Äôll look at how varying the number of RVQ levels affects the reconstruction quality. As we expected, increasing the number of levels helps, decreasing the spectral loss:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-neural-audio-codecs/image-3-480.webp 480w,/2026/assets/img/2026-04-27-neural-audio-codecs/image-3-800.webp 800w,/2026/assets/img/2026-04-27-neural-audio-codecs/image-3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-neural-audio-codecs/image-3.png" class="img-fluid rounded z-depth-1 max-w-104" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Let‚Äôs hear what the codecs sound like. We‚Äôll use the three codecs to reconstruct this audio from the Expresso dataset <d-cite key="DBLP:journals/corr/abs-2308-05725"></d-cite>:</p> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/orig_audio.wav" class="audio-sample" controls=""></audio> </figure> <p>And the reconstructions:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <div class="font-weight-semibold mb-1">4 RVQ levels</div> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/recon_4_rvq.wav" class="audio-sample" controls=""></audio> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <div class="font-weight-semibold mb-1">8 RVQ levels</div> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/recon_8_rvq.wav" class="audio-sample" controls=""></audio> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <div class="font-weight-semibold mb-1">16 RVQ levels</div> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/recon_16_rvq.wav" class="audio-sample" controls=""></audio> </figure> </div> </div> <p>Clearly, the audio gets better as we add more RVQ levels.</p> <p>Even with 16 levels, there is some crackling, the audio sounds muffled, and there is a constant high-pitched noise. Later we‚Äôll discuss how we could improve the codec further, but for demonstration purposes, this will do.</p> <h2 id="why-care-about-audio">Why care about audio</h2> <p>So now we have a neural audio codec: we can turn audio into LLM-friendly tokens and back. Codec just means a tokenizer for audio, but we say <em>codec</em> because that‚Äôs the term used for classic compression like MP3. I‚Äôll be using codec and tokenizer interchangeably.</p> <p>Let‚Äôs come back to what we wanted to do in the first place: modeling audio. Specifically, we‚Äôll make a model that can take an audio prefix and generate a plausible continuation for it.</p> <p>Just as a reminder, we want to train good audio LLMs so that we have models that understand and produce speech natively, understanding emotion, emphasis, and so on. They could also be fine-tuned into text-to-speech, speech-to-text, or speech translation models <d-cite key="DBLP:journals/corr/abs-2502-03382"></d-cite>, among others.</p> <p>So now that you‚Äôre convinced that audio LLMs are the path to AGI, let‚Äôs train a few.</p> <p>For our dataset, we‚Äôll use Libri-Light <d-cite key="DBLP:conf/icassp/KahnRZKXMKLCFLS20"></d-cite>, like we did for our sample-by-sample model earlier. This time we‚Äôll use 10000h of audio instead of 1000h. It‚Äôs a dataset of public-domain audiobooks, so if we have a good model for it, maybe we‚Äôll be able to generate more stories. (Don‚Äôt get your hopes up too much.) All we need to do is to convert the audio dataset into a sequence of discrete tokens so that we can feed it into an LLM.</p> <h2 id="dealing-with-multiple-levels">Dealing with multiple levels</h2> <p>We‚Äôll do that using our 8-level RVQ codec. From an audio with $t$ samples, we‚Äôll get an array of tokens of shape $(\frac{t}{128}, 8)$. But now there‚Äôs an issue: how to deal with the fact that for each time step, there‚Äôs not one but eight tokens? This is not a problem we have to deal with in text LLMs, where we have a single sequence of tokens.</p> <p>We‚Äôll do the simplest thing possible and just flatten the array into 1D of shape $(\frac{t}{128} \cdot 8)$, and have our LLM predict the eight levels in separate time steps.</p> <figure> <video src="/2026/assets/img/2026-04-27-neural-audio-codecs/flatten-rvq.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> <div class="caption"> Flattening a three-level RVQ to allow it to be fed into a language model </div> <p>The big disadvantage is that we lose some of our temporal compression. We downsampled the audio 128x, but now we‚Äôre inflating it 8x again by flattening the levels. That makes inference less efficient, and possibly worse quality because the effective context size decreases. We‚Äôll be using the 8 RVQ codec rather than the 16 RVQ one to avoid making the compression even worse.</p> <p>You could also predict all RVQ levels for a single step at once (‚Äùparallel pattern‚Äù), but it also makes things harder for the model because it has to decide on all levels at once. There are a bunch of other schemes people have tried to balance compression and quality. Here are a few tried out in MusicGen:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-neural-audio-codecs/image-4-480.webp 480w,/2026/assets/img/2026-04-27-neural-audio-codecs/image-4-800.webp 800w,/2026/assets/img/2026-04-27-neural-audio-codecs/image-4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-neural-audio-codecs/image-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure taken from the MusicGen paper <d-cite key="DBLP:journals/corr/abs-2306-05284"></d-cite>. </div> <p>Interestingly, as of 2025, there is no single solution that ‚Äúwon‚Äù: every paper does something different, and the schemes can get quite involved. Just look at this diagram from MiMo-Audio <d-cite key="mimoaudio"></d-cite>, a model released in September 2025:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-neural-audio-codecs/image-5-480.webp 480w,/2026/assets/img/2026-04-27-neural-audio-codecs/image-5-800.webp 800w,/2026/assets/img/2026-04-27-neural-audio-codecs/image-5-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-neural-audio-codecs/image-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Ways to deal with multiple RVQ levels can get quite involved </div> <h2 id="finally-lets-train">Finally, let‚Äôs train</h2> <p>Time to finally train a codec-wrapped language model! As I‚Äôve mentioned, our code is based on Andrej Karpathy‚Äôs <a href="https://github.com/karpathy/nanoGPT" rel="external nofollow noopener" target="_blank">nanoGPT codebase</a> for training text LLMs. We just need to modify it to accept audio as input. But that‚Äôs easy, because LLMs don‚Äôt care about what kind of tokens you‚Äôre feeding in ‚Äì it‚Äôs all just numbers. Once we‚Äôve tokenized the dataset and flattened it into a 1D sequence, we‚Äôre good to go. Tokenized this way, our 10000 hours of audio take up 134 GB. For comparison, storing this much data as uncompressed audio would take over 1 TB.</p> <p>We‚Äôre going to use the exact same model architecture and hyperparameters as for the sample-by-sample model: the only difference is in the tokenization. We also have a 10x bigger dataset, but the sample-by-sample model can‚Äôt even fit the dataset with 1k hours, so more data wouldn‚Äôt save it.</p> <p>I trained the model on 8 H100s for about 5 days. To get some samples, I decided to prompt the model with a sample of Libri-Light reading of two lines from Michael Field‚Äôs poem July <d-cite key="michael_field"></d-cite>. (As I learned when working on this, Michael Field is a pen name of Katherine Harris and Edith Emma Cooper.) Let‚Äôs see what kind of poetry we can get from our model:</p> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/20251002_123351_4.wav" class="audio-sample" controls=""></audio> </figure> <p>There are some signs of life, but we don‚Äôt have a poet yet. It sounds like somebody speaking behind a curtain. You can‚Äôt really make out what it‚Äôs saying, but the intonation is there: it sounds like somebody reading from a book, which is indeed what the model was trained on.</p> <p>It also maintains a coherent voice, until it decides for the last few seconds to switch to a different one. That is also consistent with the data: we sample the training data from a concatenation of all the audiobooks chopped up into segments and mixed together, so the model does encounter boundaries between different speakers.</p> <h2 id="how-far-can-a-codec-get-us">How far can a codec get us?</h2> <p>Our codec was deliberately simplistic, which explains why the results aren‚Äôt great‚Äîbut there‚Äôs been a good amount of research on neural audio codecs in the last four years that we could leverage. We won‚Äôt implement all the improvements here, but instead we‚Äôll look at what happens when we use <a href="https://huggingface.co/kyutai/mimi" rel="external nofollow noopener" target="_blank">Mimi</a> as the tokenizer.</p> <p>Mimi is a modern neural audio codec built for the audio language model Moshi <d-cite key="DBLP:journals/corr/abs-2410-00037"></d-cite>. It‚Äôs since been used as the tokenizer for other models as well, like Sesame CSM <d-cite key="sesame_uncanny_valley_voice"></d-cite>, VoXtream <d-cite key="DBLP:journals/corr/abs-2509-15969"></d-cite>, and LFM2-Audio <d-cite key="lfm2_audio"></d-cite> (<a href="https://github.com/Liquid4All/liquid-audio" rel="external nofollow noopener" target="_blank">GitHub</a>).</p> <p>Unsurprisingly, Mimi sounds a lot better than the homemade codec we trained earlier.</p> <p>Instead of the multi-scale spectral loss, Mimi uses an adversarial loss, like a GAN. There‚Äôs a discriminator network that tries to classify audios as being original or reconstructed by the codec, and the goal of the codec is to fool this discriminator.</p> <p>Another improvement Mimi adds is using RVQ dropout: it uses 32 RVQ levels but during training, the reconstruction is sometimes randomly truncated to a lower number of levels. That allows us to run Mimi for a lower number of RVQ levels at inference time and still get decent results, because it doesn‚Äôt rely on all levels being present. For our codec, we had to train separately.</p> <p>Let‚Äôs hear our example audio reconstructed with Mimi:</p> <p>Original</p> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/original_24kHz.wav" class="audio-sample" controls=""></audio> </figure> <div class="row mt-3"> <div class="col-sm w-100"> 4 RVQ levels <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/mimi_4_rvq_recon.wav" class="audio-sample" controls=""></audio> </figure> </div> <div class="col-sm w-100"> 8 RVQ levels <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/mimi_8_rvq_24kHz.wav" class="audio-sample" controls=""></audio> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <div>16 RVQ levels</div> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/mimi_16_rvq_recon.wav" class="audio-sample" controls=""></audio> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <div>32 RVQ levels</div> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/mimi_24kHz.wav" class="audio-sample" controls=""></audio> </figure> </div> </div> <p>For our purposes, a variant with fewer levels might have the advantage of being easier to model because it‚Äôs more compressed. Let‚Äôs train models with 8- and 32-level Mimi and compare the results.</p> <p>I trained the exact same model architecture as before, the only thing that changes is the tokenizer. It‚Äôs 10k hours from Libri-Light as the dataset, just like when we used our simple codec. Mimi has a sample rate of 24 kHz but Libri-Light uses 16 kHz, which puts a cap on how good it can sound, since we lose the higher frequencies of the audio.</p> <p>Mimi downsamples the audio a lot more aggressively, too: its sample rate is 12.5 frames per second, whereas we used 125 frames per second for our codec ‚Äì 10x higher! This means the dataset is also smaller on disk. With our codec, it took 134 GB, but for Mimi it‚Äôs ‚Äújust‚Äù 54 GB.</p> <p>Here‚Äôs a poem generated with the model trained on Mimi-tokenized data. I prompted it with two lines from the poem, as before:</p> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/20251002_115006_2.wav" class="audio-sample" controls=""></audio> </figure> <p>Here is my best attempt at a transcription:</p> <blockquote> <p><em>When the grass is gone<br> And corn still grassy;</em><br> Illness worried in the fur<br> this and pelan in stones<br> during the turan‚Äôs ciscerey<br> headforths nepet Paul Twain.<br> He <em>sees</em> zin in them.<br></p> </blockquote> <p>A tad too surrealist for my taste, but maybe Lewis Carroll would like it.</p> <h2 id="semantic-tokens">Semantic tokens</h2> <p>I have a confession to make: I lied to you just now. But just a bit, and for didactic purposes. In fact, the model above was trained on audio from a 31-level Mimi, where I omitted the very first level, which contains the ‚Äúsemantic token‚Äù.</p> <p>The role of this token is to represent semantic information of the audio, without necessarily aiding reconstruction. I won‚Äôt go into how these work, but in one sentence, Mimi‚Äôs semantic tokens are distilled from WavLM <d-cite key="DBLP:journals/corr/abs-2110-13900"></d-cite>, which you can think of as a BERT <d-cite key="DBLP:journals/corr/abs-1810-04805"></d-cite> for speech.</p> <p>To get a feeling for what information semantic tokens encode, let‚Äôs take this example audio, passed through Mimi:</p> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/original.wav" class="audio-sample" controls=""></audio> </figure> <p>Now let‚Äôs train a language model trained on the full Mimi, including semantic tokens. We‚Äôre going to run the model in a way where we keep the semantic tokens from the original audio but we discard the others, and let the model predict them. That means the information from the semantic tokens is fixed (‚Äùteacher-forced‚Äù), but the model is free to decide the others according to what continuations it finds plausible.</p> <figure> <video src="/2026/assets/img/2026-04-27-neural-audio-codecs/regenerate-with-semantic.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> <div class="caption"> We can get an idea of what information is contained in semantic tokens by keeping them fixed and letting the model regenerate the rest. </div> <p>Listen to two different reconstructions we obtain this way:</p> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/regenerate-1.wav" class="audio-sample" controls=""></audio> </figure> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/regenerate-2.wav" class="audio-sample" controls=""></audio> </figure> <p>The voice is completely different, but it‚Äôs saying the same thing! This means the semantic tokens encode what the person is saying, but are invariant to the voice. That‚Äôs useful because it helps the model focus on <em>what</em> to say, not <em>how</em> to say it. In that regard, they‚Äôre closer to text tokens, which also don‚Äôt contain information about the voice, intonation, timing, or emotion.</p> <h2 id="making-poetry-semantic">Making poetry semantic</h2> <p>Now let‚Äôs take the model trained on semantic Mimi and ask it to complete the poem:</p> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/20251002_115255_0.wav" class="audio-sample" controls=""></audio> </figure> <blockquote> <p><em>When grass is gone<br> and corn still grassy;</em><br> from the man was nothing moan.<br> The low death and heart<br> She came <em>fyde</em> wood.<br> A finteriest, a fall,<br> all them.<br></p> </blockquote> <p>It still makes up words and the sentences are not too coherent, but clearly, the proportion of real words is much higher; the model is ‚Äúmore semantic‚Äù. The acoustic quality is the same, which is what we‚Äôd expect.</p> <p>Let‚Äôs listen to a second poem:</p> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/20251002_115255_2.wav" class="audio-sample" controls=""></audio> </figure> <blockquote> <p><em>When grass is gone<br> and corn still grassy;</em><br> hope won and she<br> who is just a night in Tatan<br> in doe ock-ohm?<br> the whom?<br></p> </blockquote> <p>Indeed, the whom?</p> <h2 id="semanticacoustic-tradeoff">Semantic‚Äìacoustic tradeoff</h2> <p>We can sacrifice some acoustic quality to improve the semantics by reducing the number of RVQ levels. Let‚Äôs do 8. That way, we get higher audio compression, and a proportionally higher part of the loss comes from the semantic token, since now it‚Äôs $\frac{1}{8}$ tokens and not just $\frac{1}{32}$.</p> <p>One of the first things I noticed about this model is that it learned to memorize the Librivox notice, so it sometimes generates things like:</p> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/20251002_121528_3_librivox_intro_trim.wav" class="audio-sample" controls=""></audio> </figure> <blockquote> <p>Chapter 6 of The Founday, by R. Auclair.<br> This is a Librivox recording. All Librivox recordings are in the public domain. For information, or to volunteer, please visit librivox.org.<br> Reading by: Kelvert</p> </blockquote> <p>Repeating the training data is generally not what you want, but in our case it‚Äôs a great sign of life, because the previous models couldn‚Äôt even manage that. It also makes up the book, author, and reader, so there is still novelty here.</p> <p>Now let‚Äôs try to make some more poetry:</p> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/20251002_120917_0_mimi8_temp08_trim.wav" class="audio-sample" controls=""></audio> </figure> <blockquote> <p><em>When grass is gone<br> and corn still grassy;</em><br> When so we could say<br> that in fairy interesting wife<br> who lay there and gone<br> that save the rosy light of life<br> Jay Dien, the antique mollity<br> and a mollity the beast of gray failed summon<br></p> <p>end of poem.</p> <p>This recording is in the public domain.</p> <p>[different voice]<br> So we have formed a float that sent in would rattle down. The piece of opportunity reading and assimila‚Äî</p> </blockquote> <p>This is great. There are several signs of the model being better than the previous ones. I love that it makes up the word ‚Äúmollity‚Äù and then repeats it in the next line. Also, it realizes that it‚Äôs reciting a poem and ends the section with ‚Äúend of poem‚Äù. Then it decides it‚Äôs the end of the chapter/section and it ends with the ‚ÄúThis recording is in the public domain.‚Äù disclaimer. After that, it changes the voice and continues talking. That makes sense, since the clips from various audiobooks are just shuffled and concatenated during training, so here the model simulated a clip boundary.</p> <p>We might get even better results by weighing the loss of the semantic tokens higher than the acoustic tokens, to make the model focus more on the meaning than the sound ‚Äì in fact, Moshi uses a semantic loss factor of 100x! But we have to stop somewhere.</p> <h2 id="conclusion">Conclusion</h2> <p>We‚Äôve managed to use neural audio codecs to make an audio language model that generates somewhat coherent speech. Obviously, that‚Äôs not where the state of the art is in 2025 (and we‚Äôre not trying to reach it here) but keep in mind that by using the <em>exact same model</em> without neural audio codecs gives us this:</p> <figure> <audio src="/2026/assets/img/2026-04-27-neural-audio-codecs/20250925_140600_3.wav" class="audio-sample" controls=""></audio> </figure> <p>Of course, still a long way to go to match text models! Currently, there seems to be a trade-off between speech understanding and reasoning abilities. At the beginning, I mentioned that the speech-native models (Gemini 2.5 <d-cite key="DBLP:journals/corr/abs-2507-06261"></d-cite> <d-cite key="gemini_2_5_native_audio"></d-cite>, ChatGPT‚Äôs Advanced Voice Mode <d-cite key="DBLP:journals/corr/abs-2410-21276"></d-cite>, Qwen <d-cite key="DBLP:journals/corr/abs-2509-17765"></d-cite>, Moshi <d-cite key="DBLP:journals/corr/abs-2410-00037"></d-cite>) aren‚Äôt able to tell you whether you‚Äôre speaking in a high or low voice, despite the fact that they‚Äôre trained to natively understand audio. This is likely because they‚Äôre trained on a lot of data generated synthetically with text-to-speech and/or because understanding the tone of the voice (apparently) doesn‚Äôt help the models make more accurate predictions.</p> <p>Audio models still lag behind text LLMs. But why? To me, this mysterious unsolved ‚Äúmodality gap‚Äù makes audio ML an exciting field to work on.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-neural-audio-codecs.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/vis-llm-latent-geometry/">Visualizing LLM Latent Space Geometry Through Dimensionality Reduction</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/subject-invariant-eeg/">The Decoupling Hypothesis: Attempting Subject-Invariant EEG Representation Learning via Auxiliary Injection</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/style-representations/">Artistic Style and the Play of Neural Style Representations</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/spatial-awareness/">Where's the Chicken? Unpacking Spatial Awareness in Vision-Language Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/sparsity/">Don't Look Up (Every Token): Escaping Quadratic Complexity via Geometric Patterns¬†and¬†Algorithms</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>