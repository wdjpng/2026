<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From REINFORCE to Dr. GRPO: A Unified Perspective on LLM Post-Training | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Recently, many reinforcement learning (RL) algorithms have been applied to improve the post-training of large language models (LLMs). In this article, we aim to provide a unified perspective on the objectives of these RL algorithms, exploring how they relate to each other through the Policy Gradient Theorem — the fundamental theorem of policy gradient methods."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/llm-post-training/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "From REINFORCE to Dr. GRPO: A Unified Perspective on LLM Post-Training",
            "description": "Recently, many reinforcement learning (RL) algorithms have been applied to improve the post-training of large language models (LLMs). In this article, we aim to provide a unified perspective on the objectives of these RL algorithms, exploring how they relate to each other through the Policy Gradient Theorem — the fundamental theorem of policy gradient methods.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>From REINFORCE to Dr. GRPO: A Unified Perspective on LLM Post-Training</h1> <p>Recently, many reinforcement learning (RL) algorithms have been applied to improve the post-training of large language models (LLMs). In this article, we aim to provide a unified perspective on the objectives of these RL algorithms, exploring how they relate to each other through the Policy Gradient Theorem — the fundamental theorem of policy gradient methods.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#background">Background</a> </div> <div> <a href="#policy-gradient-theorem">Policy Gradient Theorem</a> </div> <div> <a href="#reinforce">REINFORCE</a> </div> <div> <a href="#remax">ReMax</a> </div> <div> <a href="#rloo">RLOO</a> </div> <div> <a href="#ppo">PPO</a> </div> <div> <a href="#grpo">GRPO</a> </div> <div> <a href="#dr-grpo">Dr. GRPO</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="background">Background</h2> <p>Let \(\Delta(X)\) be the space of all probability distributions supported over the set \(X\). Consider a Markov decision process (MDP), \(M=(\mathcal{S}, \mathcal{A}, \mathbb{P}, p_0, R, \gamma)\), where \(\mathcal{S}\) is the discrete state space, \(\mathcal{A}\) is the discrete action space, \(\mathbb{P}: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})\) is the transition probability, \(p_0 \in \Delta(\mathcal{S})\) is the initial state distribution, \(R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) is the reward function, and \(\gamma \in [0,1]\) is the discount factor.</p> <p>An agent interacts with the MDP environment based on a policy \(\pi: \mathcal{S} \rightarrow \Delta(\mathcal{A})\). Specifically, the agent starts from state \(s_0 \sim p_0(\cdot)\). At each time-step \(t\), it observes the state \(s_t \in \mathcal{S}\), takes an action \(a_t \sim \pi(\cdot \mid s_t)\), transits to the next state \(s_{t+1} \sim \mathbb{P}(\cdot \mid s_t, a_t)\), and receives a scalar reward \(r_{t+1} = R(s_t, a_t)\). A trajectory (up to time-step \(T\)) is defined as \(\tau = (s_0, a_0, r_1, s_1, \cdots, s_T)\). Define return \(G_t\) over \(\tau\) as the total (discounted) reward from time-step \(t\):</p> \[\begin{equation} G_t = \sum_{i=t}^{T-1} \gamma^{i-t} R(s_i, a_i). \end{equation}\] <p>State-value functions are defined as the expected return under policy \(\pi\),</p> \[\begin{align} V_{\pi}(s) = \mathbb{E}_{\pi}[G_t \mid s_t=s]. \end{align}\] <p>Similarly, action-value functions are defined as</p> \[\begin{align} Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t \mid s_t=s, a_t=a]. \end{align}\] <p>Furthermore, \(V_{\pi}\) and \(Q_{\pi}\) are connected with the following equations:</p> \[\begin{align} V_{\pi}(s) &amp;= \sum_{a \in \mathcal{A}} \pi(a \mid s) Q_{\pi}(s,a), \\ Q_{\pi}(s,a) &amp;= R(s,a) + \gamma \sum_{s' \in \mathcal{S}} \mathbb{P}(s' \mid s,a) V_{\pi}(s'). \end{align}\] <p>The policy is usually parameterized by a weight vector \(\theta\), i.e., \(\pi_{\theta}\). The goal of the agent is to find a policy that maximizes the expected return. Formally, we aim to find \(\theta\) that maximizes the objective:</p> \[\begin{align} J(\theta) = \sum_{s \in \mathcal{S}} p_0(s) V_{\pi_{\theta}}(s). \end{align}\] <p>Now, let’s formalize LLM post-training within the RL framework. <strong>In essence, LLM post-training is a specific type of RL task, distinguished by some unique properties.</strong> Specifically, the initial state distribution is defined over the prompt dataset \(\mathcal{Q}\), i.e., \(p_0 \in \Delta(\mathcal{Q})\). The initial state \(s_0\) corresponds to a prompt \(\mathbf{q} = [\mathbf{q}_1, \dots, \mathbf{q}_m]\) sampled from \(\mathcal{Q}\), where \(\mathbf{q}_i\) is the \(i\)-th token in the prompt and \(m = | \mathbf{q} |\) is the prompt length. At time-step \(t\), the state includes the prompt tokens \(\mathbf{q}\) and the response tokens generated so far, i.e., \(s_t = [\mathbf{q}_1, \dots, \mathbf{q}_m, \mathbf{o}_1, \dots, \mathbf{o}_{t-1}]\). The transition function is deterministic — the next state is simply the concatenation of the current state and the action, i.e., \(s_{t+1} = s_t \mid a_t\), where \(\mid\) denotes the concatenation operation. The reward function \(R\) is prompt-dependent, which can either be an outcome reward function or a process reward function. For example, for most math tasks which use outcome reward functions, the reward signals are all zeros except the final reward, which could be 1 for a correct answer and -1 for an incorrect answer. An episode ends when the token budget is exhausted or when the End-of-Sentence (EOS) token is generated.</p> <p>Considering different action granularities, we define three types of MDPs for LLM post-training:</p> <ol> <li> <p><strong>Token-level MDPs</strong>: In this case, we have the most fine-grained actions: each action \(a\) corresponds to a single token, and the action space \(\mathcal{A}\) is the set of tokens. The reward function \(R(s, a)\) is also defined at the token level.</p> </li> <li> <p><strong>Response-level MDPs</strong>: Here, an action \(a_0\) represents the entire response generated by the LLM, i.e., \(a_0 = \mathbf{o} = [\mathbf{o}_1, \dots, \mathbf{o}_T]\), where \(T\) is the response length. Response-level MDPs are essentially <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit#Contextual_bandit" rel="external nofollow noopener" target="_blank">contextual bandits</a>, where each prompt \(\mathbf{q}\) serves as a context or an initial state \(s_0\), and a response is considered an arm. The reward \(R(s_0, a_0)\) corresponds to the return.</p> </li> <li> <p><strong>Step-level MDPs</strong>: The granularity of actions in these MDPs lies between the token-level and the response-level, where each action represents an intermediate step in the response generation process, such as Chain-of-Thought reasoning <d-cite key="wei2022chain"></d-cite>.</p> </li> </ol> <p>In this article, we mainly focus on token-level MDPs.</p> <h2 id="policy-gradient-theorem">Policy Gradient Theorem</h2> <p>We now apply gradient ascent techniques to get the gradient of the objective. Since the true gradient \(\nabla_{\theta} J(\theta)\) is not typically available, we resort to Monte Carlo methods <d-cite key="mohamed2020monte"></d-cite>.</p> <p>This gradient estimation problem can be formalized as computing the unbiased gradient of an expectation of a function with respect to some parameters of a distribution. Consider a general case. Let \(p_{\theta}(x)\) be the probability distribution of \(x\) with parameters \(\theta\). Define the objective to be \(F(\theta) = \sum_{x \sim X} p_{\theta}(x) \phi(x)\). To estimate \(\nabla_{\theta} F(\theta)\), we apply the likelihood-ratio gradient estimator <d-cite key="glynn1990likelihood"></d-cite> which uses the log-derivative technique (\(\nabla \log{x} = \frac{\nabla x}{x}\)) to obtain an unbiased gradient estimation:</p> \[\begin{align*} \nabla_{\theta} F(\theta) &amp;= \sum_x \phi(x) \nabla_{\theta} p_{\theta}(x) \\ &amp;= \sum_x \phi(x) p_{\theta}(x) \nabla_{\theta} \log{p_{\theta}(x)} \\ &amp;= \mathbb{E}_{X \sim p_{\theta}}[\phi(X) \nabla_{\theta} \log{p_{\theta}(X)}]. \end{align*}\] <p>For our specific case (Equation (6)), we have</p> \[\begin{align} \nabla_{\theta} J(\theta) &amp; = \sum_{s \in \mathcal{S}, a \in \mathcal{A}} d^{\pi_{\theta}}(s) \pi_{\theta}(a \mid s) Q_{\pi_{\theta}}(s,a) \nabla_{\theta} \log{\pi_{\theta}(a \mid s)} \\ &amp; = \mathbb{E}_{\tau \sim \pi_{\theta}}[\sum_t \gamma^t Q_{\pi_{\theta}}(s_t,a_t) \nabla_{\theta} \log{\pi_{\theta}(a_t \mid s_t)}] \end{align}\] <p>where \(d^{\pi_{\theta}}(s') = \sum_{s \in \mathcal{S}} \sum_{t=0}^{\infty} \gamma^t p_0(s) p(s \to s', t, \pi_{\theta})\) is the (discounted) stationary state distribution of policy \(\pi_{\theta}\) and \(p(s \to s', t, \pi_{\theta})\) is the transition probability from \(s\) to \(s'\) with \(t\) steps under policy \(\pi_{\theta}\). For a detailed proof, please check Section 13.2 of the RL introduction book <d-cite key="sutton2011reinforcement"></d-cite> and OpenAI Spinning Up <d-cite key="SpinningUp2018"></d-cite>.</p> <p>Finally, in terms of implementation, the term \(\gamma^t\) in Equation (8) is usually ignored:</p> \[\begin{align} \nabla_{\theta} J(\theta) \approx \mathbb{E}_{\tau \sim \pi_{\theta}}[\sum_t Q_{\pi_{\theta}}(s_t,a_t) \nabla_{\theta} \log{\pi_{\theta}(a_t \mid s_t)}] \end{align}\] <p>For a more detailed discussion, please check Zhang et al. 2022 <d-cite key="zhang2022deeper"></d-cite>.</p> <h2 id="reinforce">REINFORCE</h2> <p>REINFORCE <d-cite key="williams1992simple"></d-cite> is a classic Monte Carlo policy gradient algorithm. Specifically, it approximates \(Q_{\pi_{\theta}}(s_t, a_t)\) in Equation (8) with a sampled return \(G_t\) (see Equation (3)). Formally, the gradient estimation is</p> \[\begin{equation} \nabla_{\theta} J_\text{REINFORCE}(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}}[\sum_t G_t \nabla_{\theta} \log{\pi_{\theta}(a_t \mid s_t)}]. \end{equation}\] <p>In practice, REINFORCE usually suffers from high gradient variance, making training unstable. To reduce variance, we apply the <a href="https://en.wikipedia.org/wiki/Control_variates" rel="external nofollow noopener" target="_blank">control variates</a> method by subtracting a baseline \(b_t\) from \(G_t\):</p> \[\begin{align} \nabla_{\theta} J_\text{REINFORCE with baseline}(\theta) &amp; = \mathbb{E}_{\tau \sim \pi_{\theta}}[\sum_t (G_t - b_t) \nabla_{\theta} \log{\pi_{\theta}(a_t \mid s_t)}] \\ &amp; = \mathbb{E}_{\tau \sim \pi_{\theta}}[\sum_t A_t \nabla_{\theta} \log{\pi_{\theta}(a_t \mid s_t)}], \nonumber \end{align}\] <p>where \(A_t = G_t - b_t\) is referred as the (general) advantage.</p> <p><strong>In general, the baseline can be any function as long as it is not affected by the action \(a_t\); otherwise we will have biased gradient estimation.</strong> For example, when \(b_t = b(s_t)\), a function of the state \(s_t\) only, \(\nabla_{\theta} J_\text{REINFORCE}(\theta) = \nabla_{\theta} J_\text{REINFORCE with baseline}(\theta)\) because the subtracted quantity is zero:</p> \[\begin{align*} \mathbb{E}_{\pi_{\theta}}[b(s_t) \nabla_{\theta} \log{\pi_{\theta}(a_t \mid s_t)}] &amp;= \sum_{a_t \sim \mathcal{A}} \pi_{\theta}(a_t \mid s_t) b(s_t) \nabla_{\theta} \log{\pi_{\theta}(a_t \mid s_t)} \\ &amp;= \sum_{a_t \sim \mathcal{A}} b(s_t) \nabla_{\theta} \pi_{\theta}(a_t \mid s_t) \\ &amp;= b(s_t) \nabla_{\theta} \left(\sum_{a_t \sim \mathcal{A}} \pi_{\theta}(a_t \mid s_t) \right) \\ &amp;= b(s_t) \nabla_{\theta} 1 \\ &amp;= 0. \end{align*}\] <p>In practice, a natural choice for the baseline is the state-value function \(V_{\pi_{\theta}}(s)\):</p> \[\begin{align} \nabla_{\theta} J_\text{REINFORCE with baseline}(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}}[\sum_t (G_t - V_{\pi_{\theta}}(s_t)) \nabla_{\theta} \log{\pi_{\theta}(a_t \mid s_t)}]. \end{align}\] <h2 id="remax">ReMax</h2> <p>In Equation (12), estimating the gradient requires the state-value function, which is challenging to obtain accurately in LLM post-training and is both memory-intensive to store and computationally expensive to train <d-cite key="li2024remax"></d-cite>. ReMax (a.k.a. REINFORCE with greedy rollout baseline <d-cite key="li2024remax"></d-cite> <d-cite key="kool2019attention"></d-cite>) eliminates the need for \(V_{\pi_{\theta}}(s_t)\) by replacing it with the return of the greedy trajectory sampled on the fly at each time-step \(t\).</p> <p>Formally, given the policy \(\pi_{\theta}\) and the current state \(s_t\), we sample the greedy trajectory starting from \(s_t\), i.e., \(\hat{\tau}_t = (\hat{s}_t, \hat{a}_t, \hat{r}_{t+1}, \hat{s}_{t+1}, \hat{a}_{t+1}, \hat{r}_{t+2}, \dots, \hat{s}_T)\), where \(\hat{s}_t = s_t\), \(\hat{a}_i = \arg\max_{a \in \mathcal{A}} \pi_{\theta}(a \mid \hat{s}_i)\) and \(\hat{r}_{i+1} = R(\hat{s}_i, \hat{a}_i)\). Denote \(\hat{G}_t\) as the return over trajectory \(\hat{\tau}_t\) and set \(b_t = \hat{G}_t\) in Equation (11). We then have:</p> \[\begin{align} \nabla_{\theta} J_\text{ReMax}(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}}[\sum_t (G_t - \hat{G}_t) \nabla_{\theta} \log{\pi_{\theta}(a_t \mid s_t)}]. \end{align}\] <p>Note that in this case, the baseline does not bias the gradient estimation (see Proposition 1 of Section 4.3 in <d-cite key="li2024remax"></d-cite>), but it is no longer an unbiased estimation of \(V_{\pi_{\theta}}(s_t)\).</p> <h2 id="rloo">RLOO</h2> <p>Unlike ReMax which samples a greedy trajectory to compute the baseline, REINFORCE Leave-One-Out (RLOO) <d-cite key="kool2019buy"></d-cite> <d-cite key="ahmadian2024back"></d-cite> eliminates the need for \(V_{\pi_{\theta}}(s_t)\) at each time-step \(t\) by replacing it with the expected return over multiple trajectories sampled on the fly. <strong>However, not all RL tasks allow multiple trajectory sampling from the same state \(s_t\). If an environment does not permit action resampling, RLOO cannot be applied. Luckily, in LLM post-training, the agent has significant control over transitions (i.e., \(s_{t+1} = s_t \mid a_t\)), enabling multiple trajectory sampling and making RLOO a viable approach.</strong></p> <p>Specifically, at each time-step \(t\), we sample \(K\) trajectories \(\{ \tau_{1,t}, \dots, \tau_{K,t} \}\) <strong>starting from \(s_t\)</strong>; and the corresponding returns are \(\{ G_{1,t}, \dots, G_{K,t} \}\), respectively. One may think we can simply replace \(V_{\pi_{\theta}}(s_t)\) with a baseline \(\frac{1}{K} \sum_{i=1}^{K} G_{i,t}\):</p> \[\begin{align*} \nabla_{\theta} J(\theta) \overset{?}{=} \mathbb{E}_{\{ \tau_{k,t} \}_{k=1}^K \sim \pi_{\theta}} \left[\sum_t \frac{1}{K} \sum_{k=1}^{K} \left(G_{k,t} - \frac{1}{K} \sum_{i=1}^{K} G_{i,t}\right) \nabla_{\theta} \log{\pi_{\theta}(a_{k,t} \mid s_t)}\right]. \end{align*}\] <p>However, this baseline leads to a biased gradient estimation since the baseline contains \(G_{k,t}\) which is affected by \(a_{k,t}\). Thus, to get an unbiased gradient estimation, we choose \(\frac{1}{K-1} \sum_{i \neq k} G_{i,t}\) as the baseline:</p> \[\begin{align*} \nabla_{\theta} J(\theta) = \mathbb{E}_{\{ \tau_{k,t} \}_{k=1}^K \sim \pi_{\theta}} \left[\sum_t \frac{1}{K} \sum_{k=1}^{K} \left(G_{k,t} - \frac{1}{K-1} \sum_{i \neq k} G_{i,t}\right) \nabla_{\theta} \log{\pi_{\theta}(a_{k,t} \mid s_t)}\right]. \end{align*}\] <p>Furthermore, we have</p> \[\begin{align*} G_{k,t} - \frac{1}{K-1} \sum_{i \neq k} G_{i,t} &amp;= G_{k,t} + \frac{1}{K-1} G_{k,t} - \frac{1}{K-1} \sum_{i=1}^K G_{i,t} \\ &amp;= \frac{K}{K-1} G_{k,t} - \frac{1}{K-1} \sum_{i=1}^K G_{i,t} \\ &amp;= \frac{K}{K-1} \left(G_{k,t} - \frac{1}{K} \sum_{i=1}^K G_{i,t}\right). \end{align*}\] <p>Applying the above trick, we have</p> \[\begin{align} \nabla_{\theta} J_\text{RLOO}(\theta) = \mathbb{E}_{\{ \tau_{k,t} \}_{k=1}^K \sim \pi_{\theta}} \left[\textcolor{red}{\frac{1}{K-1}} \sum_t \sum_{k=1}^{K} (G_{k,t} - \bar{V}(s_t)) \nabla_{\theta} \log{\pi_{\theta}(a_{k,t} \mid s_t)}\right], \end{align}\] <p>where \(\bar{V}(s_t) = \mathbb{E}[G_t] = \frac{1}{K} \sum_{i=1}^K G_{i,t}\) which is an unbiased estimation of \(V_{\pi_{\theta}}(s_t)\).</p> <h2 id="ppo">PPO</h2> <p>The above algorithms assume that the behavior policy (the policy that is used to generate experience) and the target policy (the policy being learned) are the same. When the behavior policy and the target policy are different, we need to correct the gradient estimation by utilizing <a href="https://en.wikipedia.org/wiki/Importance_sampling" rel="external nofollow noopener" target="_blank">importance sampling</a>. Let \(\pi_{\theta}\) be the target policy and the old policy \(\pi_{\theta_{\text{old}}}\) be the behavior policy. Formally, we aim to maximize a surrogate objective:</p> \[J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta_{\text{old}}}}[\sum_t \rho_t(\theta) Q_{\pi_{\theta_{\text{old}}}}(s_t,a_t)],\] <p>where \(\rho_t(\theta) = \frac{ \pi_{\theta}(a_t \mid s_t) }{ \pi_{\theta_{\text{old}}}(a_t \mid s_t) }\) is called the importance-sampling ratio.</p> <p>The gradient estimation is:</p> \[\begin{align*} \nabla_{\theta} J(\theta) &amp;= \nabla_{\theta} \mathbb{E}_{\tau \sim \pi_{\theta_{\text{old}}}}\left[\sum_t \frac{\pi_{\theta}(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)} Q_{\pi_{\theta_{\text{old}}}}(s_t,a_t)\right] \\ &amp;= \mathbb{E}_{\tau \sim \pi_{\theta_{\text{old}}}}\left[\sum_t \frac{\nabla_{\theta} \pi_{\theta}(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)} Q_{\pi_{\theta_{\text{old}}}}(s_t,a_t)\right] \\ &amp;= \mathbb{E}_{\tau \sim \pi_{\theta_{\text{old}}}}\left[\sum_t \frac{\pi_{\theta}(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)} Q_{\pi_{\theta_{\text{old}}}}(s_t,a_t) \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t)\right] \\ &amp;= \mathbb{E}_{\tau \sim \pi_{\theta_{\text{old}}}}\left[\sum_t \rho_t(\theta) Q_{\pi_{\theta_{\text{old}}}}(s_t,a_t) \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t)\right] \\ &amp;= \mathbb{E}_{\tau \sim \pi_{\theta_{\text{old}}}}\left[\sum_t \rho_t(\theta) G_t \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t)\right]. \end{align*}\] <p>Similarly, we can subtract a baseline \(b_t\) from the return \(G_t\) to reduce the gradient variance without adding bias:</p> \[\begin{align*} \nabla_{\theta} J(\theta) &amp;= \mathbb{E}_{\tau \sim \pi_{\theta_{\text{old}}}}\left[\sum_t \rho_t(\theta) (G_t - b_t) \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t)\right] \\ &amp;= \mathbb{E}_{\tau \sim \pi_{\theta_{\text{old}}}}\left[\sum_t \rho_t(\theta) A_t \nabla_{\theta} \log \pi_{\theta}(a_t \mid s_t)\right] \\ &amp;= \nabla_{\theta} \mathbb{E}_{\tau \sim \pi_{\theta_{\text{old}}}}[\sum_t \rho_t(\theta) A_t], \end{align*}\] <p>where \(A_t = G_t - b_t\).</p> <p>To enhance training stability, it’s crucial to prevent excessive changes to the policy in a single update step. Trust Region Policy Optimization (TRPO) <d-cite key="schulman2015trust"></d-cite> achieves this by imposing a <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="external nofollow noopener" target="_blank">KL divergence</a> constraint, ensuring controlled and gradual policy updates at each update. Proximal Policy Optimization (PPO) <d-cite key="schulman2017proximal"></d-cite> is inspired by the same goal as TRPO while being significantly simpler to implement. Specifically, PPO uses a <em>clipped surrogate objective</em> to constrain the policy update:</p> \[\begin{equation} J_{\text{PPO}}(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta_{\text{old}}}}\left[\sum_t \min(\rho_t(\theta) A_t, \operatorname{clip}(\rho_t(\theta), 1-\epsilon, 1+\epsilon) A_t)\right]. \end{equation}\] <p>Define a modified ratio <d-cite key="lan2022model"></d-cite> \(\hat{\rho}_t(\theta)\):</p> \[\begin{align*} \hat{\rho}_t(\theta) = \begin{cases} 0, &amp; \text{if } A_t &gt; 0 \text{ and } \rho_t(\theta) &gt; 1+\epsilon, \\ 0, &amp; \text{if } A_t &lt; 0 \text{ and } \rho_t(\theta) &lt; 1-\epsilon, \\ \rho_t(\theta), &amp; \text{otherwise.} \end{cases} \end{align*}\] <p>Then Equation (15) can be rewritten as:</p> \[\begin{equation} J_{\text{PPO}}(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta_{\text{old}}}}[\sum_t \hat{\rho}_t(\theta) A_t]. \end{equation}\] <p>For advantage \(A_t\), we usually use \(\lambda\)-return \(G_t^{\lambda}\) <d-cite key="sutton2011reinforcement"></d-cite> instead of \(G_t\) and set \(b_t = V_{\pi_{\theta}}(s_t)\):</p> \[\begin{equation} A_t = G_t^{\lambda} - V_{\pi_{\theta}}(s_t). \end{equation}\] <p>The above term is also known as the Generalized Advantage Estimate (GAE) <d-cite key="schulman2016high"></d-cite>.</p> <p>In practice, we may also normalize advantages to further improve training stability:</p> \[\hat{A}_t = \frac{A_t - \mathbb{E}[A_t]}{\operatorname{std}[A_t]},\] <p>where \(\mathbb{E}[A_t] = \frac{1}{T} \sum_{t=0}^{T-1} A_t\) and \(\operatorname{std}[A_t] = \sqrt{\frac{1}{T-1} \sum_{t=0}^{T-1} (A_t - \mathbb{E}[A_t])^2}\).</p> <h2 id="grpo">GRPO</h2> <p>Group Relative Policy Optimization (GRPO) <d-cite key="shao2024deepseekmath"></d-cite> basically combines PPO with the multiple sampling trick from RLOO:</p> \[\begin{align*} J_{\text{PPO with multiple sampling (version 1)}}(\theta) = \mathbb{E}_{\{ \tau_{k,t} \}_{k=1}^K \sim \pi_{\theta_{\text{old}}}}\left[\frac{1}{K-1} \sum_t \sum_{k=1}^{K} \hat{\rho}_{k,t}(\theta) A_{k,t} \right], \end{align*}\] <p>where \(A_{k,t} = G_{k,t}^{\lambda} - b_t\).</p> <p>Note that the above objective differs from the one proposed in the DeepSeekMath paper <d-cite key="shao2024deepseekmath"></d-cite>:</p> \[\begin{align} J_{\text{GRPO}}(\theta) = \mathbb{E}_{\{ \tau_{k} \}_{k=1}^K \sim \pi_{\theta_{\text{old}}}} \left[ \frac{1}{K} \sum_{k=1}^K \frac{1}{ \mid o_k \mid } \sum_{t=1}^{ \mid o_k \mid } \left(\hat{\rho}_{k,t}(\theta) \hat{A}_{k} - \beta D_{\text{KL}}(\pi_{\theta} \mid \mid \pi_{\text{ref}})\right) \right], \end{align}\] <p>where \(o_k\) is the \(k\)-the response, \(\mid o \mid\) is the response length, \(\hat{A}_{k} = \frac{r_k - \mathbb{E}[r_k]}{\operatorname{std}[r_k]}\), and \(\pi_{\text{ref}}\) is the reference policy.</p> <p>To reduce the gap, we consider the outcome reward setting with \(\lambda=\gamma=1\). In this case, we have \(G_t^{\lambda} = G_t = \sum_{i=t}^{T-1} R(s_i, a_i) = R(s_{T-1}, a_{T-1})\), where \(R(s_{T-1}, a_{T-1})\) is the outcome reward. Moreover, we do not sample multiple trajectories \(\{\tau_{k,t}\}_{k=1}^{K}\) on the fly starting from \(s_t\) at each time-step \(t\). Instead, we sample multiple trajectories \(\{\tau_{k}\}_{k=1}^{K}\) <strong>starting from the initial state \(s_0=\mathbf{q}\)</strong>; and the sampling process is only conducted once at \(t=0\) for each prompt. Set \(b_t = \mathbb{E}[r_k] = \frac{1}{K} \sum_{k=1}^K r_k\) where \(r_k\) is the outcome reward of the \(k\)-th trajectory. We then have</p> \[\begin{align} J_{\text{PPO with multiple sampling (version 2)}}(\theta) = \mathbb{E}_{\{ \tau_{k} \}_{k=1}^K \sim \pi_{\theta_{\text{old}}}} \left[\frac{1}{K-1} \sum_{k=1}^{K} \sum_{t=1}^{ \mid o_k \mid } \hat{\rho}_{k,t}(\theta) A_{k} \right], \end{align}\] <p>where \(A_{k} = r_k - \mathbb{E}[r_k]\).</p> <p>Note that here the baseline \(\mathbb{E}[r_k]\) is no longer an unbiased estimation of \(V_{\pi_{\theta}}(s_t)\), but an unbiased estimation of \(V_{\pi_{\theta}}(s_0)\). In practice, we may also normalize advantages to further improve training stability:</p> \[\begin{align} \hat{A}_{k} = \frac{A_k - \mathbb{E}[A_k]}{\operatorname{std}[A_k]} = \frac{A_k}{\operatorname{std}[A_k]} = \frac{A_k}{\operatorname{std}[r_k]} = \frac{r_k - \mathbb{E}[r_k]}{\operatorname{std}[r_k]}. \end{align}\] <p>Notice that compared with GRPO objective (Equation (18)), the KL divergence term is dropped in Equation (19). There are several reasons for doing this:</p> <ol> <li>As proposed in the PPO paper <d-cite key="schulman2017proximal"></d-cite>, the clipped objective is designed as a replacement of constraint policy optimization in form of the KL divergence term. Thus, adding a KL divergence term is not necessary theoretically.</li> <li>Removing the KL divergence term simplifies the implementation, saving memory and computation.</li> <li>In practice, some recent works (e.g., DAPO <d-cite key="yu2025dapo"></d-cite> and Dr. GRPO <d-cite key="liu2025understanding"></d-cite>) have shown that the KL divergence term is not necessary for LLM reasoning tasks.</li> </ol> <p>However, even after removing the KL divergence term, Equation (19) still differs from GRPO objective. In fact, GRPO objective is biased, as pointed out in the Dr. GRPO paper <d-cite key="liu2025understanding"></d-cite>.</p> <h2 id="dr-grpo">Dr. GRPO</h2> <p>Specifically, there are three biases in GRPO objective:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-llm-post-training/grpo-480.webp 480w,/2026/assets/img/2026-04-27-llm-post-training/grpo-800.webp 800w,/2026/assets/img/2026-04-27-llm-post-training/grpo-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-llm-post-training/grpo.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li> <p><em>Baseline bias</em>: This is caused by using a biased baseline without correcting the scaling factor (see Equation (14)). When using \(\mathbb{E}[r_k]\) as the baseline, we should use a scaling factor of \(\frac{1}{K-1}\), instead of \(\frac{1}{K}\).</p> </li> <li> <p><em>Response-level length bias</em>: This bias arises from dividing by response length \(\mid \mathbf{o} \mid\). For correct answers (with positive advantages), this bias incentivizes shorter responses; for incorrect answers (with negative advantages), this bias results in longer responses.</p> </li> <li> <p><em>Question-level difficulty bias</em>: Although advantage normalization is a common technique for stabilizing RL training, it introduces bias into the estimated gradient when the advantages are divided by the standard deviation term. In the context of LLM post-training, questions within one batch can vary significantly in type, domain, and difficulty. As a result, normalizing advantages at the question level leads to question-specific gradient estimation bias. That is, the estimated gradients for different questions are skewed in different ways, disproportionately influencing optimization across questions. Furthermore, each question effectively represents a different task with its own reward function. <strong>In essence, LLM post-training is a form of multi-task learning.</strong> From this perspective, applying advantage normalization across diverse questions can result in unintended weighting distortions in the objective. For example, ideally, optimization should prioritize learning from medium-difficulty questions: easy questions are already solved, while hard questions may be too difficult to learn from effectively. Thus, we should reduce the weighting of both easy and hard questions during policy updates. However, when advantages are divided by their standard deviations, the weighting of easy and hard questions is unintentionally increased, as these questions typically exhibit lower advantage standard deviations. This undermines the desired optimization dynamics, making the learning process less effective.</p> </li> </ol> <p>For the baseline bias, the scaling factor can be absorbed into the learning rate. Since the learning rate is usually tuned in practice, this bias does not significantly affect the training performance. For the other two biases, Group Relative Policy Optimization Done Right (Dr. GRPO) <d-cite key="liu2025understanding"></d-cite> addresses them by simply removing \(\frac{1}{| \mathbf{o} |}\) and \(\operatorname{std}[r_k]\):</p> \[\begin{align} J_{\text{Dr. GRPO}}(\theta) = \mathbb{E}_{\{ \tau_{k} \}_{k=1}^K \sim \pi_{\theta_{\text{old}}}}\left[\sum_{k=1}^{K} \sum_{t=1}^{ \mid o_k \mid } \hat{\rho}_{k,t}(\theta) (r_k - \mathbb{E}[r_k])\right], \end{align}\] <p>which is essentially equivalent to Equation (19), differing only by a factor of \(\frac{1}{K-1}\).</p> <h2 id="conclusion">Conclusion</h2> <p>In this article, we have presented a unified theoretical framework for understanding recent RL algorithms applied to LLM post-training, grounded in the Policy Gradient Theorem. By formalizing LLM post-training as a token-level MDP — a setting uniquely characterized by deterministic transitions, prompt-dependent rewards, and the ability to sample multiple trajectories from the same state — we have shown how seemingly disparate algorithms such as REINFORCE, ReMax, RLOO, PPO, GRPO, and Dr. GRPO are in fact variations of the same core principle: estimating unbiased gradients of the expected return while mitigating variance through carefully designed baselines.</p> <p>As RL continues to play a central role in LLM post-training, this unified view offers a roadmap for developing more robust, interpretable, and scalable post-training methods — grounded not in empirical tricks, but in the enduring mathematics of RL.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-llm-post-training.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/symbolic-connect/">Symbolism Outside, Connectionism Inside: The Trend of Fusing LLMs and Automatic Programs with Symbolic Intermediate Representations</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/sac-massive-sim/">Getting SAC to Work on a Massive Parallel Simulator: An RL Journey With Off-Policy Algorithms</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/pushing-meta-cl-methods/">Pushing Meta-Continual Learning Algorithms to the Limit</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/nlp-for-human-sciences/">Language as a Window Into the Mind: How NLP and LLMs Advance Human Sciences</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>