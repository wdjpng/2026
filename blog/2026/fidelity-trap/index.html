<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Coverage Boundary: Why High-Fidelity Primitives Don't Compose | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="A controlled experiment showing that adversarially trained primitives hit a glass ceiling on compositional generalization, while low-fidelity pedagogical primitives achieve perfect transfer."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/fidelity-trap/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "The Coverage Boundary: Why High-Fidelity Primitives Don't Compose",
            "description": "A controlled experiment showing that adversarially trained primitives hit a glass ceiling on compositional generalization, while low-fidelity pedagogical primitives achieve perfect transfer.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>The Coverage Boundary: Why High-Fidelity Primitives Don't Compose</h1> <p>A controlled experiment showing that adversarially trained primitives hit a glass ceiling on compositional generalization, while low-fidelity pedagogical primitives achieve perfect transfer.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#the-question">The Question</a> </div> <div> <a href="#background-coverage-and-compositionality">Background: Coverage and Compositionality</a> </div> <div> <a href="#the-experiment">The Experiment</a> </div> <div> <a href="#experimental-architecture-controls">Experimental Architecture &amp; Controls</a> </div> <div> <a href="#the-fidelity-trap">The Fidelity Trap</a> </div> <div> <a href="#the-coverage-boundary">The Coverage Boundary</a> </div> <div> <a href="#the-showdown-the-glass-ceiling-of-adversarial-training">The Showdown: The Glass Ceiling of Adversarial Training</a> </div> <div> <a href="#why-it-matters-contract-of-appearance-vs-contract-of-meaning">Why It Matters: Contract of Appearance vs. Contract of Meaning</a> </div> <div> <a href="#summary">Summary</a> </div> <div> <a href="#related-work">Related Work</a> </div> <div> <a href="#limitations-next-steps">Limitations &amp; Next Steps</a> </div> </nav> </d-contents> <h2 id="the-question">The Question</h2> <p>Can neural networks learn compositional rules that generalize beyond their training data?</p> <p>The compositional generalization literature has established that models can succeed on <em>novel combinations of known primitives</em>: a system that learns “red circle” and “blue square” can often generate “red square” <d-cite key="keysers2020cfq,park2021benchmark"></d-cite>. But this hides a critical assumption:</p> <blockquote> <p><strong>What exactly counts as a “known” primitive?</strong></p> </blockquote> <p>Consider a generative model pre-trained to produce images of the digit 7. Does high visual fidelity, the ability to render a photorealistic 7, constitute “knowing” the digit well enough to use it compositionally in relational tasks like “generate X &gt; Y”?</p> <p>Intuitively, we assume better primitives yield better composition. If a model can generate a crisp, perfect digit, it must understand that digit.</p> <p><strong>We found the opposite.</strong></p> <p>In a controlled experiment, we show that high-fidelity primitives trained adversarially (GANs) hit a <em>glass ceiling</em> of composability, while low-fidelity “blotchy” primitives trained pedagogically achieve perfect transfer.</p> <hr> <h2 id="background-coverage-and-compositionality">Background: Coverage and Compositionality</h2> <p>Recent work has clarified that compositional generalization is constrained by the <em>coverage</em> of primitives in training data. Benchmarks like SCAN and its descendants show that models struggle on held-out combinations when key primitives never appear in the right structural contexts <d-cite key="keysers2020cfq"></d-cite>. The <strong>Coverage Principle</strong> formalizes this: for pattern-matching learners, reliable generalization is only possible within the “coverage” of functionally equivalent fragments seen during training <d-cite key="chang2025characterizingpatternmatchinglimits"></d-cite>. In other words, coverage is a <strong>necessary condition</strong> for compositional generalization.</p> <p>Our experiments take this as a starting point. We instantiate the Coverage Principle in an intentionally simple generative setting and then ask a deeper question: <strong>even when coverage is satisfied, do all primitives admit compositional use?</strong></p> <hr> <h2 id="the-experiment">The Experiment</h2> <p>To investigate this boundary, we designed a deliberately simple experiment using <strong>Relational MNIST</strong>. The task: generate three-digit displays of the form <code class="language-plaintext highlighter-rouge">[X][&gt;][Y]</code> where <code class="language-plaintext highlighter-rouge">X</code> and <code class="language-plaintext highlighter-rouge">Y</code> are MNIST-style digits and <code class="language-plaintext highlighter-rouge">X &gt; Y</code> numerically.</p> <p>The simplicity is intentional. MNIST is the petri dish, not the ecology. If the coverage boundary failed to appear here, in the most controlled possible environment, it would suggest the phenomenon is an artifact of complexity. That it appears so sharply in this minimal setting implies a fundamental property of neural compositionality that scale may <em>mask</em> but cannot <em>cure</em>.</p> <p>Our approach follows <strong>pedagogical training with frozen primitives</strong>. We pre-trained a <em>single-digit weaver</em> to generate individual digits <code class="language-plaintext highlighter-rouge">[0-9]</code>, then froze it and trained only a compositional layer, the <em>latent splitter</em>, to route latent codes for generating relational displays.</p> <p>Crucially, we compared two types of teachers for the primitive generator:</p> <ol> <li> <strong>Adversarial (GAN):</strong> Optimized to fool a discriminator, producing sharp, high-fidelity digits rich in texture.</li> <li> <strong>Pedagogical (Ours):</strong> Optimized for structural reconstruction, producing abstract, low-frequency representations that preserve structure but discard texture.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-fidelity-trap/fig4_fidelity_comparison-480.webp 480w,/2026/assets/img/2026-04-27-fidelity-trap/fig4_fidelity_comparison-800.webp 800w,/2026/assets/img/2026-04-27-fidelity-trap/fig4_fidelity_comparison-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-fidelity-trap/fig4_fidelity_comparison.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 1: The Fidelity Trap. Left: Standard GAN samples, high contrast and sharp edges, but carrying pseudo-texture learned to satisfy an adversarial discriminator. Right: Our pedagogical samples, visually blotchy and diffuse, prioritizing structural clarity over textural noise. </div> <hr> <h2 id="experimental-architecture--controls">Experimental Architecture &amp; Controls</h2> <p>We separate <em>primitive competence</em> from <em>relational competence</em> by freezing the primitive generator and training only the relational layer.</p> <ul> <li> <strong>Single-Digit Weaver (Frozen).</strong> Pre-trained on digits <code class="language-plaintext highlighter-rouge">[0-9]</code> using either adversarial (GAN) or pedagogical objectives. Once trained, its weights are frozen.</li> <li> <strong>Latent Splitter (Trainable).</strong> Receives a latent code and learns to route it into <code class="language-plaintext highlighter-rouge">(X, &gt;, Y)</code> displays, implementing relational structure over the same primitives.</li> <li> <strong>Static Judge (Ground Truth Oracle).</strong> Evaluates whether <code class="language-plaintext highlighter-rouge">X &gt; Y</code> holds numerically. The judge is fixed and never trained. The judge sees only the rendered digits and not the internal latent codes, preventing any trivial leakage.</li> </ul> <p>Key controls:</p> <ul> <li>The primitive generator <strong>never</strong> sees the relational test set.</li> <li>The relational layer (latent splitter) is trained only on training relations; held-out relations are used purely for evaluation.</li> <li>After early experiments revealed collusion when the student could influence the teacher, we removed all student-to-teacher reward paths: the teacher’s objective depends solely on student performance as evaluated by the static judge.</li> <li>During all relational experiments, primitive generators are <strong>frozen</strong>, ensuring that differences in performance arise from the training objectives used to build primitives, not from additional fine-tuning.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-fidelity-trap/fig3_experimental_design-480.webp 480w,/2026/assets/img/2026-04-27-fidelity-trap/fig3_experimental_design-800.webp 800w,/2026/assets/img/2026-04-27-fidelity-trap/fig3_experimental_design-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-fidelity-trap/fig3_experimental_design.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 2: Experimental architecture. We freeze the primitive generator (whether GAN or Pedagogical) and train only the relational routing layer. </div> <hr> <h2 id="the-fidelity-trap">The Fidelity Trap</h2> <p>A surprising observation emerged during primitive training. The pedagogical teacher generated digits that were visually <em>blotchy</em>, diffuse, soft-edged, and structurally abstract.</p> <p>This apparent degradation acts as a <strong>semantic bottleneck</strong>. By discarding texture, the pedagogical objective forces the latent space to represent only the structural information required for compositional reasoning. Importantly, this does not imply that high-fidelity rendering is undesirable, only that it should be <em>decoupled</em> from structural learning. A plausible training recipe is two-stage: first learn the concept under a “Contract of Meaning” (low fidelity, high structure), then layer the “Contract of Appearance” (high fidelity) only after the compositional logic is secured.</p> <p>In the discriminative setting, Geirhos et al. famously showed that ImageNet-trained CNNs are strongly biased toward texture, and that increasing shape bias improves robustness and generalization <d-cite key="geirhos2018imagenet"></d-cite>. Our results suggest an analogous phenomenon on the generative side: adversarial objectives encourage texture-rich primitives that look good but compose poorly, whereas pedagogical objectives yield “blotchy” but primitives that compose perfectly.</p> <p>This matters methodologically: because our primitives are consistent with topology rather than texture, logical failures in the relational task cannot be attributed to pixel-level distribution shift. The model knew the abstract form of “7” perfectly. The only remaining question was whether it could <em>use</em> that knowledge compositionally.</p> <hr> <h2 id="the-coverage-boundary">The Coverage Boundary</h2> <p>We first asked whether primitives could compose <em>without</em> specific relational training coverage.</p> <ul> <li> <p><strong>Condition (Phase 1.5):</strong> Train relational displays only for digits <code class="language-plaintext highlighter-rouge">[0-4]</code> (10 valid <code class="language-plaintext highlighter-rouge">X &gt; Y</code> pairs). Test on relational displays for digits <code class="language-plaintext highlighter-rouge">[5-9]</code>, which are <strong>completely unseen</strong> in relational context.</p> </li> <li> <p><strong>Result:</strong> <strong>0% digit accuracy</strong> and ~<strong>chance-level relation accuracy</strong> on the novel digits.</p> </li> </ul> <p>The model produced recognizable digits in isolation but garbage in relational contexts. This concretely instantiates the <strong>Coverage Principle</strong> <d-cite key="chang2025characterizingpatternmatchinglimits"></d-cite> in a generative setting:</p> <blockquote> <p><strong>Primitive Competence</strong> (being able to draw a 7) <strong>does not grant</strong> <strong>Compositional License</strong> (using 7 correctly in a relation).</p> </blockquote> <p>As the Coverage Principle predicts, license is only acquired when a primitive appears in a <em>relational</em> context during training. Coverage is necessary, but, as we show next, it is not sufficient.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-fidelity-trap/fig1_coverage_boundary-480.webp 480w,/2026/assets/img/2026-04-27-fidelity-trap/fig1_coverage_boundary-800.webp 800w,/2026/assets/img/2026-04-27-fidelity-trap/fig1_coverage_boundary-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-fidelity-trap/fig1_coverage_boundary.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 3: The coverage boundary and glass ceiling. Same architecture, different training objectives, opposite outcomes. </div> <hr> <h2 id="the-showdown-the-glass-ceiling-of-adversarial-training">The Showdown: The Glass Ceiling of Adversarial Training</h2> <p>Once we established that coverage is necessary, we asked the deeper question:</p> <blockquote> <p><strong>Is coverage sufficient?</strong></p> </blockquote> <p>If we give an adversarial model every advantage, full relational coverage, identical architecture, and visually superior primitives, can it match pedagogical performance?</p> <p>We ran the experiment on <strong>Novel Combinations</strong> (Phase 1.6):</p> <ul> <li> <p><strong>Training:</strong> Digits <code class="language-plaintext highlighter-rouge">[0-9]</code>, with 41 of 45 valid <code class="language-plaintext highlighter-rouge">X &gt; Y</code> pairs. We hold out four specific relations (e.g., <code class="language-plaintext highlighter-rouge">7 &gt; 3</code>, <code class="language-plaintext highlighter-rouge">8 &gt; 2</code>, <code class="language-plaintext highlighter-rouge">9 &gt; 1</code>, <code class="language-plaintext highlighter-rouge">6 &gt; 4</code>). Every digit appears in many relational contexts during training.</p> </li> <li> <p><strong>Testing:</strong> Only the 4 held-out relations. These are <em>novel combinations of seen digits</em>.</p> </li> </ul> <p><strong>Results:</strong></p> <table> <thead> <tr> <th>Training Objective</th> <th>Primitives</th> <th>Held-out Relation Accuracy</th> </tr> </thead> <tbody> <tr> <td>Pedagogical (Ours)</td> <td>Blotchy</td> <td><strong>100.0%</strong></td> </tr> <tr> <td>Adversarial (GAN)</td> <td>Crisp</td> <td><strong>81.1%</strong></td> </tr> </tbody> </table> <p>Similar symptoms have been reported at scale in text-to-image systems: models can render individual concepts with high fidelity yet catastrophically fail on compositional prompts (negation, counting, spatial relations), even when evaluation metrics like FID remain strong <d-cite key="park2021benchmark,huang2023t2i,vatsa2025rightlookswrongreasons"></d-cite>. These works document the <strong>what</strong>. Our result isolates a candidate <strong>why</strong>: adversarial objectives encourage texture-heavy representations that cannot be perfectly recomposed, even under full relational coverage.</p> <p>The adversarial model is not “broken.” 81% is not failure, it is a <em>ceiling</em>. The model had full relational coverage. It had seen every digit in compositional context. Yet it could not fully compose. A natural question is whether this 81% ceiling would disappear at larger scales. While increasing parameters or data might push performance upward by brute-force memorization of more relational pairs, the <strong>structural tax remains</strong>. The pedagogical model reaches 100% with minimal data because its primitive representations are composition-friendly from the start. In contrast, adversarially trained primitives must continually burn capacity to maintain textural fidelity. Thus the “Glass Ceiling” should be interpreted not as an absolute limit at infinite scale, but as a measure of <strong>compositional inefficiency</strong> introduced by adversarial objectives.</p> <p>This is the <strong>Glass Ceiling of Adversarial Training</strong>. The model pays a <strong>tax on composition</strong>: capacity spent maintaining textural fidelity entangles the latent space in ways that resist perfect reassembly. No amount of additional coverage can break through, because the limitation appears structural rather than statistical.</p> <p>By contrast, our pedagogical primitives, although visually worse, compose perfectly—consistent with cleaner underlying structure.</p> <hr> <h2 id="why-it-matters-contract-of-appearance-vs-contract-of-meaning">Why It Matters: Contract of Appearance vs. Contract of Meaning</h2> <p>This experiment is a critique of how we train generative models.</p> <p>Modern practice follows a Contract of Appearance. Adversarial objectives (GANs) and preference optimization (RLHF/RLAIF) reward models for producing outputs that match surface statistics—textures, sharpness, or human-rated plausibility. Appearance is not inherently problematic; indeed, it is crucial in many applications. The difficulty emerges when appearance is optimized too early, before the underlying structure is stabilized. Premature optimization entangles texture with structure, forcing a model to satisfy a discriminator’s aesthetic constraints at the same time it is trying to learn a rule. This entanglement imposes a structural tax on composition. As our GAN results show, this produces high-fidelity primitives that look perfect to a critic but are hollow to a composer. They possess <strong>Primitive Competence</strong> but lack <strong>Compositional License</strong>. This same pattern appears in large language models trained with reinforcement learning from human feedback (RLHF/RLAIF): optimizing for human-rated plausibility can privilege surface agreement over structural understanding, with downstream costs to robustness and compositional generalization <d-cite key="vatsa2025rightlookswrongreasons"></d-cite>.</p> <p>Our pedagogical approach enforces a <strong>Contract of Meaning</strong>. By restricting the model to “blotchy,” low-frequency primitives, we create a <strong>semantic bottleneck</strong> that forces the latent space to prioritze invariant structure over texture. The model must learn the concept, the topology of the digit, because the texture is unavailable. High-fidelity appearance could be layered on <em>after</em> struture is learned, but confounding the two objectives during early training degrades compositional generalization.</p> <p>This distinction matters for safety and robustness. A model that understands meaning can be trusted to handle unseen combinations; a model trained under a Contract of Appearance may look correct while behaving unpredictably outside its training manifold.</p> <p>Although demonstrated here on MNIST for clarity, the Coverage Boundary and Glass Ceiling are <strong>architectural</strong> phenomena, not dataset quirks. Large-scale generative training (GANs, diffusion, preference tuning) may be subject to the same fidelity trap: objectives that reward appearance can actively degrade compositional reasoning, even when coverage is abundant.</p> <hr> <h2 id="summary">Summary</h2> <table> <thead> <tr> <th>Experiment</th> <th>What’s Novel</th> <th>Result</th> </tr> </thead> <tbody> <tr> <td><strong>Phase 1.5</strong></td> <td>Novel Primitives (No Relational Coverage)</td> <td> <strong>0% Transfer</strong> — The Coverage Boundary</td> </tr> <tr> <td><strong>Phase 1.6</strong></td> <td>Adversarial Primitives (Full Coverage)</td> <td> <strong>81.1% Accuracy</strong> — The Glass Ceiling</td> </tr> <tr> <td><strong>Phase 1.6</strong></td> <td>Pedagogical Primitives (Full Coverage)</td> <td> <strong>100% Accuracy</strong> — The Contract of Meaning</td> </tr> </tbody> </table> <p>The Coverage Boundary tells us <em>when</em> composition is possible. The Glass Ceiling tells us <em>whether</em> the primitives are capable of it.</p> <p>You need both: primitives shaped for meaning, and coverage that licenses their use.</p> <p>Code and experimental details will be released upon acceptance.</p> <hr> <h2 id="related-work">Related Work</h2> <p>Our setup connects to several strands of prior work. Compositional generalization benchmarks such as SCAN and its extensions highlight the importance of primitive coverage in sequence-to-sequence models <d-cite key="keysers2020cfq,friedman2022findingdatasetshortcutsgrammar"></d-cite>. The <strong>Coverage Principle</strong> of Chang et al. (2025) formalizes coverage as a necessary condition for pattern-matching learners, a condition our “Coverage Boundary” experiment instantiates in a generative regime <d-cite key="chang2025characterizingpatternmatchinglimits"></d-cite>.</p> <p>On the generative side, compositional text-to-image benchmarks repeatedly find that models with excellent perceptual quality metrics still fail on novel combinations of attributes and objects <d-cite key="park2021benchmark,huang2023t2i"></d-cite>. Vatsa et al. (2025) describe this as “right looks, wrong reasons,” emphasizing failures of compositional fidelity in modern diffusion models <d-cite key="vatsa2025rightlookswrongreasons"></d-cite>. Our <strong>Glass Ceiling</strong> result pinpoints adversarial objectives as one mechanism that can produce this pattern, even in a minimal MNIST petri dish.</p> <p>Our “blotchy but coherent” primitives resonate with work on shape vs. texture bias in CNNs <d-cite key="geirhos2018imagenet"></d-cite> and with emerging views of deep representations as learning topological manifolds amenable to symbolic or relational reuse. We view our pedagogical objective as a small, controlled example of <strong>training for meaning rather than appearance</strong>, a design choice that may scale to more realistic architectures and datasets.</p> <p>We view generative compositionality as an underexplored junction between representation learning and training objectives, and hope this minimal example encourages further mechanistic work.</p> <hr> <h2 id="limitations--next-steps">Limitations &amp; Next Steps</h2> <p><strong>Toy domain.</strong> Our experiments use MNIST to make the phenomenon as visible and controllable as possible. Real-world data are higher dimensional and noisier, but if the fidelity trap appears in this simplest setting, we expect it to persist, if hidden, at scale.</p> <p><strong>Topology.</strong> While we infer topology from our results, claiming this will require further validation.</p> <p><strong>Frozen primitives.</strong> We freeze the digit generator when training relations to cleanly separate primitive learning from relational learning. Future work could study joint training and analyze how much compositional capacity can be recovered, or destroyed, when primitives continue to adapt.</p> <p><strong>Single relation.</strong> We focus on a single relational operator (<code class="language-plaintext highlighter-rouge">&gt;</code>). Extending to multiple relations (equality, ordering, arithmetic expressions) and to symbolic domains would test whether pedagogical primitives systematically support richer compositional logics.</p> <p><strong>Beyond MNIST.</strong> The natural next step is to apply pedagogical objectives to more complex visual and language domains, and to compare them directly against adversarial or preference-based objectives used in modern AI training pipelines.</p> <p>If the fidelity trap generalizes, then <strong>training models to teach rather than to mimic</strong> may be a necessary ingredient in building systems that truly understand, and safely extend, what they learn.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-fidelity-trap.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/general-agent-evaluation/">Ready For General Agents? Let's Test It.</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-vlms-waste-their-vision/">Why vlms waste their vision</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>