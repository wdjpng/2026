<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Using Large Language Models to Simulate and Predict Human Decision-Making | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="We explore how large language models can be used to predict human decisions in language-based persuasion games, comparing direct prompting, LLM-based data generation, and hybrid methods that mix synthetic and human data."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/using-large-language-models-to-simulate-and-predict-human-decision-making/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Using Large Language Models to Simulate and Predict Human Decision-Making",
            "description": "We explore how large language models can be used to predict human decisions in language-based persuasion games, comparing direct prompting, LLM-based data generation, and hybrid methods that mix synthetic and human data.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Using Large Language Models to Simulate and Predict Human Decision-Making</h1> <p>We explore how large language models can be used to predict human decisions in language-based persuasion games, comparing direct prompting, LLM-based data generation, and hybrid methods that mix synthetic and human data.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#why-predict-human-decisions-with-llms">Why predict human decisions with LLMs?</a> </div> <div> <a href="#baseline-learning-directly-from-human-data">Baseline: learning directly from human data</a> </div> <div> <a href="#llm-as-a-classifier">LLM as a classifier</a> </div> <div> <a href="#llm-as-a-data-generator">LLM as a data generator</a> </div> <div> <a href="#incorporating-human-data-into-the-process">Incorporating human data into the process</a> </div> <div> <a href="#accuracy-vs-calibration-the-hidden-cost-of-synthetic-data">Accuracy vs. calibration: the hidden cost of synthetic data</a> </div> <div> <a href="#dual-double-use-of-human-data-for-augmented-learning">DUAL: double use of human data for augmented learning</a> </div> <div> <a href="#takeaways-and-open-questions">Takeaways and open questions</a> </div> </nav> </d-contents> <p>Can large language models (LLMs) help us predict what humans will do in strategic, language-based interactions, such as persuasion games, bargaining, or recommendation settings? In this blogpost, we explore several concrete ways to use LLMs to predict human decisions in a particular language-based economic environment. </p> <p>Our main messages are simple. First, you can indeed use LLMs to predict human decisions in this kind of game. Second, in our experiments, it is often more effective to use LLMs as data generators for small predictive models than to rely on them as direct classifiers. Third, even when human data exists, incorporating LLM-generated data into the prediction process can improve performance and, with the right method, maintain reasonable calibration. The blogpost is based on our long paper (we will cite it in the camera-ready version) and is closely related to prior work on language-based persuasion games and language-based economic environments.<d-cite key="apel2022predicting, raifer2022designing, shapira2025human, shapira2024glee"></d-cite></p> <hr> <h2 id="why-predict-human-decisions-with-llms">Why predict human decisions with LLMs?</h2> <p>Many systems need to anticipate how humans will react: will users trust a recommendation or ignore it, accept an offer or walk away, follow a suggestion or resist it? Accurate prediction of human decisions can help us design mechanisms that are more efficient or more fair, run virtual experiments without recruiting thousands of participants, and stress-test systems before deployment by simulating how people might respond in different conditions.</p> <p>From a behavioral-economics perspective, forecasting decisions is challenging because people care not only about monetary payoffs but also about fairness, reference points, and social preferences.<d-cite key="kahneman1986fairness, samuelson1988statusquo, charness2002socialprefs"></d-cite> When we build decision predictors, we would like them to capture these richer patterns rather than merely replicating simple heuristics.</p> <p>However, two practical constraints often arise. First, when designing a new system or game, we may have no human interaction data at all. Running a large-scale behavioral experiment is expensive and time-consuming. Second, even once we have run an experiment, the amount of human data is typically limited: a few hundred or a few thousand decisions, spread across many conditions and histories. In that regime, every additional bit of signal is valuable, and we want to use the available data as efficiently as possible.</p> <p>Recent progress in LLMs, from GPT-3 and GPT-4 to PaLM 2, Gemini, and Qwen-2, has shown that scaling up language models can yield strong performance on a broad range of NLP and reasoning benchmarks.<d-cite key="vaswani2017attention, brown2020gpt3, openai2023gpt4, anil2023palm2, team2024gemini, yang2024qwen2"></d-cite> A growing line of work explores using these models as proxies for human respondents in psychology, political science, and economics, asking when they reproduce human judgments and when they depart from them.<d-cite key="aher2023simulate, argyle2023outofone, horton2023llmsimulated, susnjak2023applying_lang4, acquisti2016economics"></d-cite> Our experiments live at this intersection: we use LLMs not only as agents, but as sources of training data for compact predictors of human decisions.</p> <p>LLMs offer a natural way to supplement the sparse information we typically have in behavioral datasets. They have been trained on massive text corpora, can be prompted to play the role of agents in games, and can generate synthetic data that sometimes resembles human behavior. The central question we tackle is therefore: given a specific decision-prediction problem, how should we best combine human data and LLMs to predict behavior, and what trade-offs arise between accuracy, calibration, and computational cost in that particular setting?</p> <h3 id="our-playground-language-based-persuasion-games">Our playground: language-based persuasion games</h3> <p>We study these questions in a repeated persuasion game about hotel bookings, firstly presented by <d-cite key="apel2022predicting"></d-cite>. Each interaction between an expert and a human decision-maker (DM) proceeds as follows. The expert sees a hotel described by several textual reviews, along with a numeric score (1–10) derived from those reviews. The expert chooses one review to show the DM. The DM then chooses between two actions: “Go Hotel” (accept the hotel) or “Stay Home” (reject and keep the outside option). The hotel is either high quality or low quality according to the true average score. The expert always wants the DM to go, while the DM benefits only if they go when the hotel is high quality.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-using-large-language-models-to-simulate-and-predict-human-decision-making/language_based_persuasion_game-480.webp 480w,/2026/assets/img/2026-04-27-using-large-language-models-to-simulate-and-predict-human-decision-making/language_based_persuasion_game-800.webp 800w,/2026/assets/img/2026-04-27-using-large-language-models-to-simulate-and-predict-human-decision-making/language_based_persuasion_game-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-using-large-language-models-to-simulate-and-predict-human-decision-making/language_based_persuasion_game.png" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> In our repeated hotel persuasion game, the expert (sender) sees several textual reviews and a numeric score for a hotel, chooses one review as a textual signal, and shows it to the human decision-maker (receiver). The decision-maker then chooses between “Go Hotel” and “Stay Home”. </div> <p>Persuasion games of this sort connect to a rich literature in economics on communication, reputation, and information design.<d-cite key="aumann1995repeated, farrell1996cheap, kamenica2011bayesian, arieli2024reputation, kim1996cheap, best2020persuasion"></d-cite> Our hotel game instantiates these ideas in a concrete, language-based environment where the expert’s messages are textual reviews and the decision-maker’s actions are hotel choices.</p> <p>This game is repeated across several rounds with the same expert–DM pair. Over time, the DM can learn about the expert’s behavior: if the expert tends to recommend good hotels, trust should increase; if the expert repeatedly highlights bad hotels, trust should erode. In the human experiments underlying this blogpost, participants played against several different expert strategies, such as always sending the highest-scoring review, always being honest, or adapting to the DM’s past decisions and outcomes. The resulting dataset consists of many human decisions across a variety of histories and expert behaviors.</p> <p>Our prediction task is: given the interaction history so far and the current review shown by the expert, predict whether the human will choose “Go Hotel” or “Stay Home”. All of the numbers we report in this blogpost are measured on this specific prediction task, using the particular data collection protocol and model configurations described below.</p> <hr> <h2 id="baseline-learning-directly-from-human-data">Baseline: learning directly from human data</h2> <p>We begin with the most straightforward approach: collect human data for the hotel game and train a small predictive model directly on these human decisions. To do this, we represent each decision with a set of features that encode both the current review and the interaction history. Features include information about the review itself (such as sentiment and score) and about the past rounds (such as how often the expert has previously “lied”, previous choices and payoffs, and trust-related patterns). A small model—such as an LSTM over the history , a tabular method over handcrafted features or teaxtual-tabular model over the messages and the features — is trained to predict the DM’s action.<d-cite key="hochreiter1997lstm, chen2016xgboost, arazi2025tabstar"></d-cite></p> <p><img src="/2026/assets/img/2026-04-27-using-large-language-models-to-simulate-and-predict-human-decision-making/simpleML.png" alt="Simple ML predictor trained only on human data" style="display:block;margin:0 auto;max-width:40%;height:auto;"></p> <p>For the purposes of this blogpost, we focus on a clean subset of the data collected by <d-cite key="shapira2025human"></d-cite>. Specifically, we consider the first two games that each of the 210 human DMs played against each of the six experts. We use data from 110 of these players to train the “human baseline model”, and hold out the remaining 100 players for testing. Thus, the model never encounters the test players during training, and all reported metrics are computed on these held-out players.</p> <p>In this configuration, a classifier trained only on human data achieves 77.7% accuracy on held-out players. This serves as our main reference point for the rest of the blogpost: it shows how well a small, relatively inexpensive predictor can do in this specific environment when trained on the available human data.</p> <p>We also train a language-only baseline that sees only the current review, without any history features. This model achieves 76.5% accuracy, just 1.2 percentage points below the full human baseline. At first glance, this might suggest that using history brings only a small marginal gain. However, a more detailed analysis in <d-cite key="shapira2025human"></d-cite> (in particular, Section 7.1) shows that relying solely on the current text and ignoring history leads to systematic errors: such models miss important strategic patterns in how humans adapt their behavior over time.</p> <hr> <h2 id="llm-as-a-classifier">LLM as a classifier</h2> <p>The most obvious way to use an LLM is to treat it as a direct classifier. For each decision in the dataset, we construct a natural-language prompt that describes the game, the full interaction history so far, and the current review. We then ask the LLM a question such as “Will the human decide to go to the hotel or stay home in this situation?” and map its answer to a class label.</p> <p><img src="/2026/assets/img/2026-04-27-using-large-language-models-to-simulate-and-predict-human-decision-making/simpleLLM.png" alt="Using an off-the-shelf LLM directly as a classifier" style="display:block;margin:0 auto;max-width:40%;height:auto;"></p> <p>This naive approach has clear advantages. It requires no additional training or fine-tuning, and from a software engineering perspective it is easy to deploy: one only needs to maintain the prompt and call the LLM at inference time. It is especially attractive when we have a limited number of predictions to make or when we want to quickly prototype on a new decision problem. Treating LLMs as zero-shot or few-shot classifiers in this way echoes a broader trend in NLP.<d-cite key="liu2024best_synth6, wang2023chatgptsentiment, zhang2023chatgptsummarization, brown2020gpt3"></d-cite></p> <p>At the same time, treating the LLM as a classifier has significant drawbacks. Each prediction involves running a large, multi-billion-parameter model, which makes inference expensive in terms of both compute and latency. When we need to make many predictions or operate in a real-time system, this cost can quickly become prohibitive. Furthermore, the model behaves as a black box: it is harder to inspect or compress into a smaller predictor.</p> <p>In our hotel game, the naive LLM-as-classifier approach reaches 77.5% accuracy on the held-out human players. This is slightly better than the language-only baseline (76.5%) but slightly worse than the small human-trained model that uses history (77.7%). In other words, in this specific setting, using a large LLM directly as a classifier is competitive with, but does not clearly outperform, a carefully engineered small model trained on human data, while being much more expensive to run.</p> <hr> <h2 id="llm-as-a-data-generator">LLM as a data generator</h2> <p>Instead of using the LLM as a direct predictor, we can use it to generate synthetic data and then train a small classifier on that data. Concretely, we treat the LLM as a synthetic human player in the hotel game. We prompt it as if it were a DM, allow it to play repeated games against the expert strategies, and record its decisions. Repeating this process many times yields a large synthetic dataset of “LLM players”, each with full interaction histories.<d-cite key="aher2023simulate, argyle2023outofone, horton2023llmsimulated"></d-cite></p> <p><img src="/2026/assets/img/2026-04-27-using-large-language-models-to-simulate-and-predict-human-decision-making/dataGenerator.png" alt="LLM generates synthetic players, a small model learns from them" style="display:block;margin:0 auto;max-width:40%;height:auto;"></p> <p>Once we have the synthetic dataset, we train a small model (such as an LSTM or a tabular method) on LLM-generated data only, using the same feature representation we used for human decisions. We then evaluate the resulting small predictor on the real human test players. This experiment addresses a “no human data” scenario in a controlled way: in our implementation, we still have human data for evaluation, but the model used at inference time has never seen any human decisions during training, only LLM behavior.</p> <p>In our hotel game, we find that as we increase the number of synthetic players, the performance of the small classifier improves. With 512 LLM-generated players, the small model already exceeds 78% accuracy. When we increase the synthetic sample to 4,096 LLM-generated players, the small model reaches 79.1% accuracy on the human test set. This is higher than the 77.7% human-only baseline from the same environment, despite the fact that no human decisions were used for training the predictor.</p> <p>These numbers are specific to the hotel persuasion game and our particular experimental protocol, but they illustrate an important qualitative point: in this setting, LLM-generated data can be more informative for prediction than the limited amount of human data that we can feasibly collect. At the same time, the predictor we deploy is a compact model, so inference remains cheap and low-latency.</p> <hr> <h2 id="incorporating-human-data-into-the-process">Incorporating human data into the process</h2> <p>So far, we considered two extremes within our hotel game: a small model trained only on human data, and a small model trained only on LLM-generated data. In realistic applications, we often have some human data, but not a lot. The natural question is how to best combine this human data with LLMs: should we fine-tune the LLM, train a small model on a mixture of human and synthetic data, or both?</p> <p>In this section, we discuss three methods that incorporate human data into the process in different ways. All of the numbers in this section are again specific to our hotel persuasion game and to the exact splits and model configurations described above.</p> <h3 id="fine-tune-the-llm-and-use-it-as-a-classifier">Fine-tune the LLM and use it as a classifier</h3> <p>The first method fine-tunes the LLM itself on human data and then continues to use it as a direct classifier. We start from a pre-trained LLM, fine-tune it on the training portion of the human hotel dataset (110 players), and then use the fine-tuned model to predict decisions for the held-out human players, given history and current review.</p> <p><img src="/2026/assets/img/2026-04-27-using-large-language-models-to-simulate-and-predict-human-decision-making/FineTuning.png" alt="Fine-tuning the LLM and then using it as a classifier" style="display:block;margin:0 auto;max-width:40%;height:auto;"></p> <p>In this configuration, the fine-tuned LLM achieves 79.6% accuracy on the human test set. In our experiments, this improves both over the human-only baseline (77.7%) and over the small model trained solely on LLM-generated data (79.1%). The price we pay is that inference still requires running the full LLM, so the method inherits the computational cost and latency of the naive LLM-as-classifier approach.</p> <h3 id="train-a-small-model-on-human--llm-data">Train a small model on Human + LLM data</h3> <p>The second method remains committed to using a small classifier at inference time, but trains it on a mixture of human and synthetic data. We again keep the 110 human training players, generate a large number of LLM players using the base (non-fine-tuned) LLM, and pool the resulting decisions. We then train a small model on the union of human and LLM-generated data, using the same features as before.</p> <p><img src="/2026/assets/img/2026-04-27-using-large-language-models-to-simulate-and-predict-human-decision-making/humanAndLLM.png" alt="Small model trained on combined human and LLM-generated data" style="display:block;margin:0 auto;max-width:40%;height:auto;"></p> <p>In our hotel game, this Human + LLM data approach yields 79.5% accuracy on the test humans, almost identical to the 79.6% achieved by the fine-tuned LLM classifier. The advantage is that at inference time we only use a compact model, so predictions are much cheaper to compute. Importantly, this accuracy figure again refers specifically to the hotel persuasion game; our broader lesson is that, at least in this environment, combining limited human data with synthetic LLM data can give a small model performance comparable to that of a fine-tuned LLM classifier.</p> <h3 id="fine-tune-the-llm-and-use-it-as-a-data-generator">Fine-tune the LLM and use it as a data generator</h3> <p>The third method combines fine-tuning and data generation. We first fine-tune the LLM on the human hotel data, and then use this fine-tuned model, rather than the base LLM, as the generator of synthetic players. The small classifier is trained on these fine-tuned LLM-generated players only, and then evaluated on human test players.</p> <p><img src="/2026/assets/img/2026-04-27-using-large-language-models-to-simulate-and-predict-human-decision-making/FineTuningTheGenerator.png" alt="Fine-tuned LLM used as a data generator" style="display:block;margin:0 auto;max-width:50%;height:auto;"></p> <p>In our experiments on the hotel game, this “fine-tuned generator” approach attains the highest accuracy among all methods we considered: 80.1% accuracy on held-out human players. As before, this number is specific to our data and configuration, but it shows that fine-tuning the LLM and then using it to generate synthetic players can be very powerful in this particular setting. The final model used at inference time is still a small classifier, so we benefit from both strong performance and inexpensive prediction.</p> <hr> <h2 id="accuracy-vs-calibration-the-hidden-cost-of-synthetic-data">Accuracy vs. calibration: the hidden cost of synthetic data</h2> <p>Accuracy is not the only quantity that matters when predicting human decisions. Our models also produce probability scores, such as “80% chance of Go Hotel”. We would like these probabilities to be well calibrated, meaning that among all decisions for which the model predicts an 80% probability, roughly 80% of the outcomes should indeed be Go Hotel. Poorly calibrated probabilities can lead to overconfident or misleading decisions downstream, even when accuracy is relatively high.</p> <p>We measure calibration using Expected Calibration Error (ECE), where smaller values indicate better calibration. In our hotel game, we obtain the following ECE values:</p> <p><img src="/2026/assets/img/2026-04-27-using-large-language-models-to-simulate-and-predict-human-decision-making/calibration.png" alt="Fine-tuned LLM used as a data generator" style="display:block;margin:0 auto;max-width:100%;height:auto;"></p> <p>These numbers again refer only to our hotel persuasion game. Within this environment, several patterns emerge. The human-only small model is well calibrated, with an ECE of 0.04. Methods that rely heavily on synthetic data alone, whether from the base LLM or from a fine-tuned version, show much worse calibration, with ECE around 0.13. Interestingly, training a small model on a combination of human data and base LLM-generated data yields both strong accuracy and very good calibration (ECE ≈ 0.03).</p> <p>The fine-tuned generator method, which achieves the highest accuracy in our experiments (80.1%), has ECE ≈ 0.15. In the hotel game, this means that increasing accuracy by about 0.5 percentage points beyond the fine-tuned LLM classifier comes at the cost of significantly degraded calibration. Whether this trade-off is acceptable depends on the intended use: some applications care only about the top label, while others critically depend on well-calibrated probabilities.</p> <hr> <h2 id="dual-double-use-of-human-data-for-augmented-learning">DUAL: double use of human data for augmented learning</h2> <p>To mitigate the calibration problem while preserving high accuracy in our hotel game, we propose DUAL, which stands for “Double Use of human data for Augmented Learning”. The core idea is to use the same human dataset twice in the pipeline: once to fine-tune the LLM that generates synthetic data, and once to train the final small classifier on a mixture of human and synthetic data.</p> <p><img src="/2026/assets/img/2026-04-27-using-large-language-models-to-simulate-and-predict-human-decision-making/DUAL.png" alt="DUAL: fine-tune the LLM, generate data, and train on both human + generated data" style="display:block;margin:0 auto;max-width:50%;height:auto;"></p> <p>The procedure is as follows. First, we fine-tune the LLM on the human hotel data, just as in the “fine-tuned LLM” and “fine-tuned generator” setups. Second, we use this fine-tuned LLM to generate synthetic players for the hotel game. Third, we train a small classifier on the combination of human decisions and fine-tuned LLM-generated decisions.</p> <p>In our hotel persuasion game, DUAL achieves the same accuracy as the fine-tuned generator method—80.1% on held-out human players—but with markedly better calibration. Its ECE is around 0.08, which is not as low as the 0.03 achieved by the Human + base LLM data method, but substantially better than the 0.15 observed for methods that rely solely on synthetic data from a fine-tuned generator. Thus, in this specific environment, DUAL balances high accuracy with improved calibration by reusing the human data at two stages of the pipeline.</p> <hr> <h2 id="takeaways-and-open-questions">Takeaways and open questions</h2> <p>All of the quantitative results in this blogpost are specific to a particular decision-prediction problem: a repeated hotel persuasion game with fixed expert strategies, a certain human experiment design, and specific model choices. Different tasks and setups could behave quite differently. Within this case study, however, several qualitative takeaways emerge.</p> <p>First, LLMs can indeed help predict human decisions. In our hotel game, even when the small classifier is trained only on LLM-generated players and never sees any human decisions during training, it reaches 79.1% accuracy on held-out humans, outperforming the human-only baseline. This suggests that, in at least some environments, synthetic data from LLMs can meaningfully supplement or even partially substitute for small human datasets.</p> <p>Second, using LLMs as data generators for small models can be more attractive than using them as direct classifiers, at least in the setting we studied. The naive LLM-as-classifier approach is simple but expensive and does not clearly beat a carefully designed small model trained on human data. In contrast, LLM-as-generator approaches allow us to train compact predictors that are cheap to run while still reaching strong accuracy. In our hotel game, the best-performing method (80.1% accuracy) is a small classifier trained on data from a fine-tuned LLM generator.</p> <p>Third, even after human data has been collected, incorporating LLMs into the prediction pipeline is still beneficial. Mixed methods that combine human decisions with LLM-generated decisions often improve both accuracy and calibration relative to human-only models. In our experiments, the DUAL approach—fine-tuning the LLM, generating synthetic players, and then training a small classifier on both human and synthetic data—achieves the best accuracy and significantly better calibration than methods that rely solely on fine-tuned synthetic data.</p> <p>A conceptual lesson is that history matters. In the hotel game, models that capture how humans use interaction history—how trust in the expert evolves, how past outcomes influence future choices—produce synthetic data that is more useful for prediction than models that only match the sentiment or surface form of individual reviews. Predicting human decisions in strategic environments is not just sentiment analysis; it requires representing and reasoning about how incentives, trust, and learning interact over time.</p> <p>Many open questions remain. How robust are these findings to other types of games or tasks, such as bargaining, negotiation, or recommendation dialogues?<d-cite key="shapira2024glee"></d-cite> How sensitive are the results to the choice of LLM, the details of the prompt, the fine-tuning procedure, or the feature representation used in the small classifier? And beyond technical performance, what ethical and societal issues arise when we deploy systems that can anticipate and potentially shape human decisions at scale?<d-cite key="acquisti2016economics, kahneman1986fairness, susnjak2023applying_lang4"></d-cite></p> <p>We hope this case study illustrates that LLMs are not only tools for generating text or acting as autonomous agents. In suitable settings, they can also serve as powerful data generators that, when combined with human data and calibrated carefully, help us better understand and predict human decision-making in language-based environments.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-using-large-language-models-to-simulate-and-predict-human-decision-making.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/useful-calibrated-uncertainties/">What (and What Not) are Calibrated Uncertainties Actually Useful for?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/unlearning-or-untraining/">Is your algorithm Unlearning or Untraining?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/tracing-principles-behind-modern-diffusion-models/">Tracing the Principles Behind Modern Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/symbolic-connect/">From Dense Monoliths to Modular Minds: The Rise of Symbolic Routing in LLMs</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>