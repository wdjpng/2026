<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Sparsity | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Large Language Models (LLMs) have brought about a significant change in the field of artificial intelligence, where they have transitioned in scope from being specialized research tools to common resources that drive the next generation of software. With increasing model parameters and training data, LLMs demonstrate new abilities in reasoning, code generation, and solving complex problems that were once considered unattainable. However, scaling these models effectively for long-context applications uniquely poses a challenge. This is primarily due to the inherent limitations of the self-attention mechanism, which has time complexity $\mathcal{O}\left(n^2\right)$. This quadratic bottleneck hinders applications for long documents, high-resolution images, and large codebases, among others. However, what is interesting to observe is that effectively only a few parameters are used in token computation, and most calculations are sparse. Hence, Sparsity emerges as an effective solution to this problem. Rather than relying on the $N \times N$ attention matrix, one can utilize an approximate or “sparse” version of attention to achieve almost the same results much faster. The backbone of this approach is the idea that tokens do not require the entire context; they only need local context, and thus, most of the computation carried out is wasteful. In this blog, we analyze the types of attention patterns that emerge and how to use them to our advantage for faster and efficient LLMs."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/sparsity/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Sparsity",
            "description": "Large Language Models (LLMs) have brought about a significant change in the field of artificial intelligence, where they have transitioned in scope from being specialized research tools to common resources that drive the next generation of software. With increasing model parameters and training data, LLMs demonstrate new abilities in reasoning, code generation, and solving complex problems that were once considered unattainable. However, scaling these models effectively for long-context applications uniquely poses a challenge. This is primarily due to the inherent limitations of the self-attention mechanism, which has time complexity  $\mathcal{O}\left(n^2\right)$. This quadratic bottleneck hinders applications for long documents, high-resolution images, and large codebases, among others. However, what is interesting to observe is that effectively only a few parameters are used in token computation, and most calculations are sparse. Hence, Sparsity emerges as an effective solution to this problem. Rather than relying on the $N \times N$ attention matrix, one can utilize an approximate or “sparse” version of attention to achieve almost the same results much faster. The backbone of this approach is the idea that tokens do not require the entire context; they only need local context, and thus, most of the computation carried out is wasteful. In this blog, we analyze the types of attention patterns that emerge and how to use them to our advantage for faster and efficient LLMs.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Sparsity</h1> <p>Large Language Models (LLMs) have brought about a significant change in the field of artificial intelligence, where they have transitioned in scope from being specialized research tools to common resources that drive the next generation of software. With increasing model parameters and training data, LLMs demonstrate new abilities in reasoning, code generation, and solving complex problems that were once considered unattainable. However, scaling these models effectively for long-context applications uniquely poses a challenge. This is primarily due to the inherent limitations of the self-attention mechanism, which has time complexity $\mathcal{O}\left(n^2\right)$. This quadratic bottleneck hinders applications for long documents, high-resolution images, and large codebases, among others. However, what is interesting to observe is that effectively only a few parameters are used in token computation, and most calculations are sparse. Hence, Sparsity emerges as an effective solution to this problem. Rather than relying on the $N \times N$ attention matrix, one can utilize an approximate or “sparse” version of attention to achieve almost the same results much faster. The backbone of this approach is the idea that tokens do not require the entire context; they only need local context, and thus, most of the computation carried out is wasteful. In this blog, we analyze the types of attention patterns that emerge and how to use them to our advantage for faster and efficient LLMs.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#preliminaries">Preliminaries</a> </div> <ul> <li> <a href="#self-attention">Self-Attention</a> </li> </ul> <div> <a href="#attention-sinks">Attention Sinks</a> </div> <ul> <li> <a href="#the-problem">The Problem</a> </li> <li> <a href="#how-it-works">How it works</a> </li> <li> <a href="#implications-on-sparse-attention">Implications on Sparse Attention</a> </li> <li> <a href="#beyond-softmax">Beyond Softmax</a> </li> </ul> <div> <a href="#static-patterns-of-attention">Static Patterns of Attention</a> </div> <ul> <li> <a href="#minference">MInference</a> </li> <li> <a href="#xattention">XAttention</a> </li> <li> <a href="#validation-via-learned-gates">Validation via Learned Gates</a> </li> <li> <a href="#sliding-window-attention">Sliding Window Attention</a> </li> </ul> <div> <a href="#dynamic-sparsity">Dynamic Sparsity</a> </div> <ul> <li> <a href="#tidal-decode">Tidal Decode</a> </li> <li> <a href="#the-heavy-hitter-hypothesis">The Heavy Hitter Hypothesis</a> </li> </ul> <div> <a href="#dynamic-kv-cache-management">Dynamic KV Cache Management</a> </div> <ul> <li> <a href="#kv-caching">KV Caching</a> </li> <li> <a href="#optimizing-kv-cache">Optimizing KV Cache</a> </li> <li> <a href="#squeezed-attention">Squeezed Attention</a> </li> <li> <a href="#epicache">Epicache</a> </li> </ul> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>The foundation of modern Large Language Models is the Self-Attention mechanism, first introduced in <em>Attention Is All You Need</em> (Vaswani et al., 2017). This process enables the model to assess the relative importance of different tokens with respect to each other. It achieves this by mapping inputs to Query ($Q$), Key ($K$), and Value ($V$) matrices to compute a weighted sum. This mechanism, while revolutionary, also serves as a fundamental barrier to long-context Transformer inference, in the self-attention layer, where the latency scales quadratically, $\mathcal{O}\left(n^2\right)$. This is because every token must attend to every other token; the complexity grows with the sequence length.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sparsity/1-480.webp 480w,/2026/assets/img/2026-04-27-sparsity/1-800.webp 800w,/2026/assets/img/2026-04-27-sparsity/1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-sparsity/1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Figure 1: Attention heatmaps showing naturally sparse patterns in dense models. Even with a full attention budget, models learn to concentrate attention on specific tokens rather than distributing it uniformly.</strong></p> <p>However, recent observations suggest that token generation relies on a sparse subset of effective interactions rather than the whole parameter space. Consequently, sparsity presents a robust solution to the scaling bottleneck. By substituting the exhaustive $N \times N$ attention matrix with sparse approximations, one can maintain performance fidelity while significantly reducing latency. We can verify this hypothesis by examining the token heatmaps. When we visualize the attention layers of standard, dense models, they often reveal a fascinating insight: even when given the budget to look everywhere, these models learn naturally sparse patterns. Attention is not uniformly distributed, but rather concentrated on a few specific tokens. This finding serves as the foundation for the efficacy of sparse attention methods. If regular self-attention is inherently sparse in long contexts, then methods that rely on sparse attention serve as a more efficient architecture that relies on established patterns.</p> <hr> <h2 id="preliminaries">Preliminaries</h2> <h3 id="self-attention">Self-Attention</h3> <p>Simple attention (i.e. a single attention layer) processes the entire sequence simultaneously, such that every element attends to every other element. For a sequence of length $L$ and embedding dimension $d$, the input is represented as a matrix $X \in \mathbb{R}^{L \times d}$. The mechanism uses learnable weight matrices $W_Q, W_K, W_V \in \mathbb{R}^{d \times d_{model}}$ to project the input. The attention matrix is computed globally for the full sequence:</p> \[\text{Attention}(Q, K) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right)\] <p>The self-attention output Z is a weighted sum of the value vectors, where weights are determined by the compatibility of the query and key.</p> \[Z = \text{Attention}(Q, K) \cdot V\] <p>For individual tokens, the attention score $a_{m,n}$ represents the relevance of token $n$ (key) to token $m$ (query):</p> \[a_{m,n} = \frac{\exp(q_m \cdot k_n / \sqrt{d_k})}{\sum_{j=1}^{L} \exp(q_m \cdot k_j / \sqrt{d_k})}\] <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sparsity/3-480.webp 480w,/2026/assets/img/2026-04-27-sparsity/3-800.webp 800w,/2026/assets/img/2026-04-27-sparsity/3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-sparsity/3.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Figure 2: Showing the quadratic computational cost as the context length increases</strong></p> <p>This mechanism has a computational complexity of $O(L^2)$, as it computes interactions between all $L \times L$ pairs. This initially involves calculating the scaled dot products of the Query and Key matrices. However, for autoregressive tasks where the model must predict the next token based only on past tokens, a Masked Self-Attention mechanism is required. To enforce this causality, a Look-Ahead Mask is added to the scaled dot products before the softmax function is applied. This ensures that the resulting Self-Attention Matrix, $A \in \mathbb{R}^{L \times L}$, assigns zero probability to future tokens while maintaining a valid probability distribution over past tokens.</p> <p>The mask $M$ is defined as:</p> \[M_{ij} = \begin{cases} 0 &amp; \text{if } i \geq j \\ -\infty &amp; \text{if } i &lt; j \end{cases}\] <p>The masked attention is then computed as:</p> \[\text{Attention}_{\text{causal}}(Q, K, V) = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} + M \right) V\] <p>This mask zeroes out the upper triangular portion of the attention matrix. Consequently, 50% of the possible connections in the matrix are empty excluding the diagonal. This means we are performing $O(L^2)$ computations but discarding half of the results. The information propagates through the depth of the network: a token at layer l aggregates information from layer $l−1$, which in turn aggregated from $l−2$.</p> <hr> <h2 id="attention-sinks">Attention Sinks</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sparsity/4-480.webp 480w,/2026/assets/img/2026-04-27-sparsity/4-800.webp 800w,/2026/assets/img/2026-04-27-sparsity/4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-sparsity/4.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Figure 3: Graph showing the unusually high attention score for the <code class="language-plaintext highlighter-rouge">&lt;bos&gt;</code> token as compared to the other tokens, the attention scores are in log scale</strong></p> <p>From the figure above, we can observe that a large amount of attention score is allocated to the beginning tokens, especially the <code class="language-plaintext highlighter-rouge">&lt;bos&gt;</code> token. Upon experiments, it was further observed that this pattern sustains regardless of the relevance of the tokens to the language modeling task. These tokens were thereby termed as “attention sinks”. At first, this seems like an incredible inefficiency, almost as if the model is wasting its computation. However, recent contributions and publications argue that this tendency is not a flaw but rather a way to overcome a fundamental flaw in an LLM’s architecture, and this behaviour is what is termed as attention sinks.</p> <h3 id="the-problem">The Problem</h3> <p>The core issue addressed by attention sinks is “over-mixing”. A Transformer operates on the principle of repeatedly mixing information between tokens at every layer. While this is a necessary principle for understanding context, it becomes a problem in very deep models or when processing long sequences or a large context. After too many layers of mixing, the unique information for each token can become “smoothened out”, and all the tokens start to look the same to the model. This exhibits the phenomena of <strong>Rank collapse</strong> (or <strong>Over-smoothing</strong>, in case of GNNs), i.e., representations of all tokens in a sequence become too similar to each other as they pass through the model’s layers. When this information blur happens, the model can no longer distinguish between tokens effectively, which harms its ability to make accurate predictions. Attention Sinks provide a simple yet effective solution to this; they act as a “do-nothing” option. The followings points show how the attention sink work step-by-step:</p> <ol> <li> <strong>The Sink as a neutral target:</strong> The model learns that the first token is a reliable, ever-present, and neutral target.</li> <li> <strong>Low-Information value:</strong> The value vector associated with this token is often learned to have a very small norm, indicating that it contains very little information.</li> <li> <strong>Skipping the update:</strong> When an attention head wants to avoid mixing more information into a token, it simply directs all its attention to the sink. It picks up the low-information value, and when that is added back to the token’s representation, it changes it very little (it’s like adding zero). The token effectively skips the mixing step in that layer, preserving its distinct information. This allows the model to dynamically control how much mixing occurs at each layer for each token, preventing the representations from becoming a blurry mess.</li> </ol> <h3 id="implications-on-sparse-attention">Implications on Sparse Attention</h3> <p>Attention Sinks directly constrain how we design sparse masks, as they are the most fundamental pattern to be handled. Many sparse methods like pure sliding windows or aggressive early-token eviction remove the first few tokens as the sequence grows. This unintentionally deletes the “do-nothing” path attention heads rely on to avoid over‑mixing. When the BOS/initial tokens vanish, heads are forced to mix into non‑sink positions, accelerating rank collapse and causing quality drops.</p> <p>As illustrated by StreamingLLM, robust sparse attention requires a hybrid approach: retaining a static set of initial tokens while the rest of the window slides. Maintaining this global anchor ensures that ‘attention sinks’ remain accessible, thereby stabilizing representations and preventing performance degradation. In the attention matrix, this manifests as the classic A-shape (A local diagonal tethered to the sequence’s start).</p> <h3 id="beyond-softmax">Beyond Softmax</h3> <p>Softmax is traditionally used in Transformers to turn scores into a probability distribution, offering stability via dense gradients and a bounded norm. However, its sum-to-one normalization encourages “attention sinks” and can amplify hidden-state outliers, complicating low‑precision quantization. Primitive alternatives include: removing normalization with sigmoid (sinks return once normalization is reintroduced), adding trainable biases to Q/K/V (benefits fade in deeper models), or tweaking softmax with custom optimizers to tame outliers. One of the more effective solutions is Softpick. It aims to preserve softmax’s training benefits while permitting “null attention.” The first variant rectifies reduced exponentials:</p> \[\text{Softmax}(\mathbf{x})_i = \frac{\text{ReLU}\left(e^{x_i} - 1\right)}{\sum_{j=1}^{N} \text{ReLU}\left(e^{x_j} - 1\right)}\] <p>This led to “dying” heads (too many zeros) and still did not remove sinks; negatives had no gradient. The final Softpick version uses an absolute-valued denominator:</p> \[\text{Softpick}(\mathbf{x})_i = \frac{\text{ReLU}(e^{x_i} - 1)}{\sum_{j=1}^{N} |e^{x_j} - 1|}\] <p>This allows gradient flow for negative inputs and breaks the sum-to-one constraint, mitigating sinks. But this introduces the problem of “underscoring” in long contexts: many negative tokens inflate the denominator and shrink scores, hurting retrieval.</p> <hr> <h2 id="static-patterns-of-attention">Static Patterns of Attention</h2> <p>It may seem as if each input should have it’s unique attention pattern, however empirical evidence suggests that models consistently converge on a limited set of geometric archetypes. These recurring structures manifest primarily as diagonals, vertical lines, and dense blocks. By exploiting these predictable motifs, frameworks like MInference can approximate the full attention matrix, enabling significant computational pruning without degrading model performance.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sparsity/2-480.webp 480w,/2026/assets/img/2026-04-27-sparsity/2-800.webp 800w,/2026/assets/img/2026-04-27-sparsity/2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-sparsity/2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Figure 4: Common sparsity patterns observed in attention matrices: A-shape (combining local and global attention), Block-sparse (sliding window with global blocks), and Vertical-slash (fixed-stride or dilated attention).</strong></p> <p><strong>A-shape</strong></p> <p>The “A-shape” pattern is a method for selecting which token interactions are most important. It gets its name from the shape it forms when you visualize the sparse attention matrix. This structure often combines local and global information. For example, tokens may focus completely on their nearby neighbors, creating the wide base of the “A,” while also linking to a few carefully chosen “summary” tokens or positions earlier in the context, forming the peak. This “A” structure effectively connects detailed local context with a sparse but important long-range signal, without requiring a full quadratic computation.</p> <p><strong>Vertical-slash</strong></p> <p>The “vertical-slash” pattern offers another useful method. It gets its name from the way it looks in the attention matrix. This pattern often represents a “fixed-stride” or “dilated” attention mechanism. Instead of focusing on every previous token, which is too costly, or just a local window, which misses the bigger picture, a vertical-slash pattern may have each token attend to every 100th token. This approach lets the model sample the entire context, regardless of its length. It creates a basic structure of long-range dependencies that is very efficient to compute.</p> <p><strong>Block-sparse</strong></p> <p>Perhaps the most intuitive of these patterns is block-sparse attention. Imagine the full, N×N attention matrix as a massive grid. Instead of calculating the entire grid, block-sparse attention only computes small, specific blocks within it. Most commonly, this includes a sliding window which consists of the blocks along the main diagonal. In this setup, each token only looks at its immediate neighbors. This approach is great for capturing local context. To avoid losing sight of the long-range view, this pattern is often improved by computing a few fixed global blocks. These blocks allow certain groups of tokens to communicate even if they are far apart in the sequence.</p> <p>These pattern observations further allow us to refine our approaches in optimizing sparse attention methods:</p> <h3 id="minference">MInference</h3> <p>Having identified that attention heads exhibit distinct sparsity patterns (A-shape, Vertical-Slash, and Block-Sparse), the challenge lies in effectively utilizing them. The first step is to assign the correct sparsity pattern to each attention head. MInference performs this via an offline “Kernel-Aware Optimal Sparse Pattern Search”. The process involves constructing a search space where each candidate configuration (e.g., specific number of vertical lines or block sizes) meets a target FLOPs budget. The algorithm then evaluates these candidates against a reference example, selecting the pattern that minimizes the difference between the sparse and dense attention outputs.</p> <hr> <p><strong>Algorithm: Kernel-Aware Sparse Pattern Search</strong></p> <p><strong>Input:</strong> $Q, K, V \in \mathbb{R}^{S \times d_h}$, patterns $p$, search space $\sigma$, target FLOPs $t$<br> <strong>Output:</strong> Optimal pattern $p_{best}$</p> <ol> <li><strong>Build kernel-aware search space</strong></li> <li> <strong>For</strong> $i \leftarrow 1$ to $\lvert p \rvert$: <ul> <li>$t_i \leftarrow \texttt{FLOPs-in-kernel}(\sigma_i)$</li> <li> <strong>While</strong> $\lvert t_i - t \rvert &gt; \epsilon$: <ul> <li>$\sigma_i \leftarrow \texttt{ChangeSpace}(\sigma_i, p_i)$</li> <li>$t_i \leftarrow \texttt{FLOPs-in-kernel}(\sigma_i)$</li> </ul> </li> <li>$\rho \leftarrow \rho \cup \sigma_i$</li> </ul> </li> <li><strong>Search for optimal head pattern</strong></li> <li>$p_{best} \leftarrow \emptyset$</li> <li>$y \leftarrow \text{Softmax}(QK^\top / \sqrt{d})V$</li> <li> <strong>For</strong> $i \leftarrow 1$ to $\lvert p \rvert$: <ul> <li>$y_i \leftarrow \text{SparseAttention}(QK^\top / \sqrt{d}, \rho_i)$</li> <li>$p_{\text{best}} \leftarrow \text{argmin}(\lvert y - y_i \rvert, p_{\text{best}})$</li> </ul> </li> <li> <strong>Return</strong> $p_{\text{best}}$</li> </ol> <hr> <p>Once patterns are assigned, MInference distinguishes between static and dynamic execution strategies.</p> <p><strong>Offline (A-Shape)</strong>: The A-shape pattern relies on local windows (neighboring tokens) and initial tokens (attention sinks). Since these indices depend only on relative positions and fixed starting points, the sparse mask is static. Therefore, no online approximation is needed; the mask is pre-determined offline, incurring zero overhead during inference.</p> <p><strong>Online (Vertical-Slash &amp; Block-Sparse)</strong>: In contrast, the Vertical-Slash and Block-Sparse patterns are highly dynamic. A token might attend to specific “heavy hitter” tokens (vertical) or semantic clusters (blocks) that vary significantly depending on the input prompt. Consequently, MInference must approximate these indices on the fly (online) for every new input.</p> <p>For heads exhibiting the Vertical-Slash pattern, attention is concentrated on specific vertical lines (tokens attended by many others) and slash lines (periodic attention). To find these dynamically, MInference uses the continuity of these lines to its advantage. Instead of calculating the full matrix, it computes a small estimate using only the last few query vectors ($Q_{[-\text{lastQ}:]}$) and the full Key matrix. The indices with the highest summed weights in this estimate determine the vertical and slash lines for the entire matrix.</p> <hr> <p><strong>Algorithm: Vertical-Slash Head Approximation</strong></p> <p><strong>Input:</strong> $Q, K, V \in \mathbb{R}^{S \times d_h}$, $k_v, k_s \in \mathbb{N}$<br> <strong>Output:</strong> Sparse attention output $y$</p> <ol> <li> <strong>Approximate vertical and slash pattern</strong> (<code class="language-plaintext highlighter-rouge">last_q</code> = 64) <ul> <li>$\hat{A} \leftarrow \text{softmax}(Q_{[-\text{lastQ}:]}K^\top / \sqrt{d} + m_{\text{causal}})$</li> </ul> </li> <li> <strong>Indices of top $k_v$ vertical lines</strong> (sum in vertical) <ul> <li>$i_v \leftarrow \text{argtopk}(\text{sum}_v(\hat{A}), k_v)$</li> </ul> </li> <li> <strong>Indices of top $k_s$ slash lines</strong> (sum in slash) <ul> <li>$i_s \leftarrow \text{argtopk}(\text{sum}_s(\hat{A}), k_s)$</li> </ul> </li> <li> <strong>Build sparse attention index</strong> <ul> <li>$i_{vs} \leftarrow \text{sparseformat}(i_v, i_s)$</li> </ul> </li> <li> <strong>Final dynamic sparse attention scores</strong> (only index block) <ul> <li>$A \leftarrow \text{softmax}(\text{sparse}(QK^\top, i_{vs}) / \sqrt{d})$</li> </ul> </li> <li> <strong>Sparse mixed scores and values</strong> <ul> <li>$y \leftarrow \text{sparse}(AV, i_{vs})$</li> </ul> </li> <li> <strong>Return</strong> $y$</li> </ol> <hr> <p>For the Block-Sparse pattern, where attention is spatially clustered, MInference uses a “low-resolution” view of the attention matrix. It applies Mean Pooling to compress the Query and Key matrices into smaller blocks (e.g., 64×64). It then multiplies these compressed matrices to estimate block-level attention weights. This efficiently highlights the most important regions (blocks), allowing the model to compute full attention only for those specific high-value blocks.</p> <hr> <p><strong>Algorithm: Block-Sparse Head Approximation</strong></p> <p><strong>Input:</strong> $Q, K, V \in \mathbb{R}^{S \times d_h}$, $k_b \in \mathbb{N}$<br> <strong>Output:</strong> Sparse attention output $y$</p> <ol> <li> <strong>Approximate block-sparse pattern</strong> (<code class="language-plaintext highlighter-rouge">block_size</code> = 64) <ul> <li>$\hat{Q} \leftarrow \texttt{MeanPooling}(Q, \text{blockSize})$</li> <li>$\hat{K} \leftarrow \texttt{MeanPooling}(K, \text{blockSize})$</li> <li>$\hat{A} \leftarrow \text{softmax}(\hat{Q}\hat{K}^\top / \sqrt{d} + m_{\text{causal}})$</li> </ul> </li> <li> <strong>Indices of top $k_b$ blocks</strong> <ul> <li>$i_b \leftarrow \text{argtopk}(\hat{A}, k_b)$</li> </ul> </li> <li> <strong>Build sparse attention index</strong> <ul> <li>$i_b \leftarrow \text{expand}(i_b)$</li> </ul> </li> <li> <strong>Final dynamic sparse attention scores</strong> (only index block) <ul> <li>$A \leftarrow \text{softmax}(\text{sparse}(QK^\top, i_b) / \sqrt{d})$</li> </ul> </li> <li> <strong>Sparse mixed scores and values</strong> <ul> <li>$y \leftarrow \text{sparse}(AV, i_b)$</li> </ul> </li> <li> <strong>Return</strong> $y$</li> </ol> <hr> <h3 id="xattention">XAttention</h3> <p>Instead of treating these patterns as distinct categories, XAttention uses a geometric heuristic called Antidiagonal Scoring- where the attention matrix is divided into blocks and scored by summing values along their antidiagonals (lines from bottom-left to top-right). This is effective because antidiagonals geometrically intersect both vertical lines (commonly found in A-Shape and Vertical-Slash patterns) and diagonal bands (commonly found in local windows). This ensures that blocks containing these high-value structures receive high scores and are preserved during pruning. For the Block-Sparse pattern, which consists of isolated clusters far from the diagonal, XAttention’s dynamic thresholding naturally adapts. It calculates scores globally across all blocks rather than just local neighborhoods and thus identifies and retains any off-diagonal block with a high density of relevant interactions. This allows XAttention to act as a “one-size-fits-all” solution.</p> <h3 id="validation-via-learned-gates">Validation via Learned Gates</h3> <p>Unlike methods that enforce a shape from the start, SeerAttention trains a gate to autonomously discover the optimal block-level sparsity mask for a given input. Crucially, visualizations of SeerAttention’s learned outputs reveal that the model spontaneously “rediscovers” these exact geometric structures. Without being explicitly programmed to do so, the learned gates frequently converge to A-shape, Vertical-Slash, and Diagonal patterns. This confirms that the static patterns observed in frameworks like MInference accurately reflect the ground-truth information flow within Large Language Models.</p> <h3 id="sliding-window-attention">Sliding Window Attention</h3> <p>The most intuitive example of this class is Sliding Window Attention (SWA). Here, the attention matrix is restricted to a band along the diagonal: each query token $q_i$ only attends to a fixed-size window of key tokens $k_j$ where $\lvert i-j \rvert \le w$. This effectively relies on the “locality” heuristic that a token’s most relevant information is its immediate neighbors.</p> <p>Methods like SWA, dilated attention, and fixed-block patterns share a key characteristic: <strong>Regularity</strong>. Because the sparsity pattern is known ahead of time (e.g., $i \pm w$), indices can be pre-computed and memory access can be coalesced. This allows for efficient implementation using custom GPU kernels, minimizing the overhead of indirect addressing that plagues unstructured sparsity. However, strict locality introduces a challenge. In SWA, direct interaction between unadjacent tokens (where $\lvert i-j \rvert \gg w$) is prohibited within a single layer. Information must “hop” from block to block across multiple layers to travel the length of the sequence. This multi-hop path causes signal attenuation and hinders the model’s ability to capture long-range dependencies effectively.</p> <p><strong>Star Attention</strong>: A method that mitigates this propagation issue while maintaining the efficiency of neighborhood-based methods. It adopts a query-centric global topology, assuming that while most information is local, specific “anchor” tokens can act as global summarizers. A query token computes attention over two sets:</p> <ol> <li> <strong>Local Block:</strong> Its immediate neighbors (Standard SWA).</li> <li> <strong>Anchor Block:</strong> A shared global summarizer (e.g., the first block) that acts as a hub in a star-graph topology, also handles the problem of attention sinks.</li> </ol> <p>This two-phase approach allows any query to access a summarized version of global information immediately, effectively eliminating the need for multi-hop propagation.</p> <h2 id="dynamic-sparsity">Dynamic Sparsity</h2> <p>The limitation of the methods above is that they are structurally fixed. They apply the same geometric rule regardless of the text. So we look at <strong>Dynamic Sparsity</strong>, content‑aware masks that reveal patterns less geometric and more temporal. Think of it as rhythms over shapes, the propagation of attention across layers follows consistent trends, even as the mask adapts to the input. Formally, the attention pattern $S$ becomes a function of the input: $S=f(Q,K,V)$, letting the model allocate computation to the most relevant interactions for that specific sequence.</p> <h3 id="tidal-decode">Tidal Decode</h3> <p>An implemented example of this is Tidal Decode. It works by dynamically adjusting the budget of tokens being attended to. This is implemented in the form of a learned threshold or gating mechanism applied to the attention score matrix $A=QK^T$. After computing the full (or a subset of) scores, the model prunes any $A_{ij}$ score that falls below $\tau$, which is a dynamically computed, sequence-specific threshold. This effectively focuses computation on only the $q_i-k_j$ pairs that have the highest relevance for that specific input.</p> <p>Through attention heatmaps like fig. 1, we observe that high attention scores are usually localized in two regions: the beginning and in this case, the middle. So, token selection layers are placed in these regions to sample the top-k most significant tokens. This significantly optimizes not only the the attention computation but also the sampling process.</p> <h3 id="the-heavy-hitter-hypothesis">The Heavy Hitter Hypothesis</h3> <p>Attention scores follow a Power-Law distribution, meaning a small subset of “Heavy Hitter” tokens contribute the vast majority of value. Identifying and retaining only these critical tokens allows for significant cache reduction (up to 20x) without performance degradation.</p> <p><strong>Eviction Policy:</strong> The retention strategy is formulated as a dynamic submodular problem. At each decoding step $i$, the algorithm calculates a normalized score based on the current cache:</p> \[o_i := D_{i-1} \cdot \exp(Q_{i,:} (KS_{i,:})^\top)\] <p>The algorithm applies a greedy policy without accessing future tokens:</p> <ul> <li> <strong>Cache Not Full:</strong> The current token is simply added to the set.</li> <li> <strong>Cache Full:</strong> The algorithm evicts the token $u$ with the lowest accumulated attention score to make room.</li> </ul> <p><strong>Theoretical Guarantees:</strong> This method is backed by a theoretical bound proving it is not just a heuristic. It guarantees performance relative to an optimal policy (one with infinite resources):</p> \[f(S_i) \geq (1-\alpha)(1-1/e) \cdot opt_i - \beta\] <p>This indicates the policy achieves at least $\sim 63\%$ (i.e., $1-1/e$) of the optimal value, adjusted for the accumulated error between decoding steps ($1-\alpha$).</p> <hr> <h2 id="dynamic-kv-cache-management">Dynamic KV Cache Management</h2> <h3 id="kv-caching">KV Caching</h3> <p>In normal autoregressive generation (predicting one token at a time), re-computing the Key and Value matrices for the entire history at every step is inefficient. The most common method to reduce compute overhead is by introducing KV Caching. By storing the $K$ and $V$ states of previous tokens in GPU memory, the model only needs to compute attention for the <em>current</em> token thereby speeding up inference. While KV caching avoids re-computation, the memory and operations required for the attention mechanism still scale poorly. Concretely, at decoding step $t$ we compute the current query $q_t$ and reuse the cached keys/values from prior steps, $K_{1:t}$ and $V_{1:t}$. The attention output is</p> \[y_t = \operatorname{softmax}\left( \frac{q_t K_{1:t}^\top}{\sqrt{d}} + m_\text{causal} \right) V_{1:t},\] <p>where $d$ is the head dimension and $m_\text{causal}$ enforces causality. KV caching removes the need to recompute $K,V$ for tokens $1\ldots t-1$, but the cache itself grows linearly with $t$ across heads and layers. The memory footprint is roughly</p> \[\mathcal{O}(t \cdot n_\text{layers} \cdot n_\text{heads} \cdot d)\quad\text{(for keys and values)},\] <p>which becomes a bottleneck for long contexts. This is why later sections focus on pruning, clustering, or episodic reuse of cached states rather than storing everything.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sparsity/5-480.webp 480w,/2026/assets/img/2026-04-27-sparsity/5-800.webp 800w,/2026/assets/img/2026-04-27-sparsity/5-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-sparsity/5.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Figure 5: A comparison of attention computation showing how KV Caching (bottom) improves efficiency by storing previous Key and Value states (highlighted in dark pink) for reuse. Unlike the standard approach (top) which re-calculates all interactions, the cached approach allows the model to only process the newest query (Query Token 4) against the stored history to generate the next token.</strong></p> <h3 id="optimizing-kv-cache">Optimizing KV Cache</h3> <p>In classical Transformers, while computing self-attention scores during the prefilling stage, the norm is to store the KV cache to enable efficient inference, i.e, reducing redundant calculations. However, in cases of long, multiturn conversations, this turns into a significant bottleneck due to the following reasons:</p> <ol> <li>The cache grows linearly with context length. In the above case, the cache grows significantly, leading to OOMs.</li> <li>As originally observed, in any context, the attention pattern is sparse, leading us to conclude that for a particular query, parts of the context are entirely irrelevant.</li> </ol> <p>This implies that an Optimal Policy (or Oracle) exists that would retain only this tiny, critical fraction of the context and discard the rest without losing accuracy. Since we cannot predict the future to know exactly which tokens the “Optimal Policy” would choose, methods like H2O (Heavy-Hitter Oracle) replicate this behavior by using past attention as a proxy for future importance. The intuition is that tokens which are important once tend to remain important. H2O dynamically retains tokens that have accumulated high attention scores over time (the “Heavy Hitters”) and evicts those that do not.</p> <p>These types of dynamic approaches ensure that while the total memory usage remains constant (avoiding OOMs), the cache effectively “evolves” to contain the most relevant information for the ongoing conversation.</p> <h3 id="squeezed-attention">Squeezed Attention</h3> <p>This method targets fixed-document contexts with user queries. Its key idea is offline optimization: run K‑means on key vectors to form semantically grouped clusters, each represented by a “key centroid.” At inference, compare the query to centroids instead of all keys to quickly select relevant clusters and retrieve only important keys. To handle skewed attention distributions (where top‑k fails), clusters are scored with a softmax‑based weighting that supports thresholding:</p> \[S_i^{(1)} = \frac{\exp\left(q \; C_i^{(1)\top}\right)}{\sum_j N_j^{(1)} \cdot \exp\left(q \; C_j^{(1)\top}\right)}\] <p>Because centroid lookups add overhead, retrieval scales via hierarchical clustering: first form fine‑grained clusters (Level 2), then cluster those into coarse groups (Level 1). At inference, score Level 1, select a subset to meet a target sparsity (e.g., 90%), then refine within Level 2 to pick final keys.</p> \[S_l^{(2)} = \frac{\exp \left(q C_l^{(2)\top}\right)}{\sum_m N_m^{(2)} \cdot \exp \left(q C_m^{(2)\top}\right)}\] <p>However, the squeezed attention method again introduces reliance on hyperparameter tuning, which is dependent on the input context and varies across any dataset with multiple samples. A further optimisation that could be done is to automate the hyperparameter tuning process. Additionally, the clustering process is very slow, rendering the method ineffective when context is provided online in real-time. Further work can also be done to address this limitation, thereby removing the fixed context requirement.</p> <h3 id="epicache">Epicache</h3> <p>While Squeezed Attention offers an effective solution for static, long-context documents, it runs into a critical limitation: it relies on “post-prefill” processing. The model must first load the entire context into memory to cluster the keys, leading to unbounded peak memory usage. Which proves to be problematic. Epicache utilises a block-wise prefill, where less important tokens in each block are pruned using a patched prompt; this is what enables the fixed-budget operation. Their core insight is that long conversations are not a single topic but are composed of various “episodes.”For example, a discussion about video games, followed by planning a movie, followed by an aside about the weather, and this has been termed as “episodes” in the paper.</p> <h4 id="methodology">Methodology</h4> <ol> <li>Partition the conversation $H={u_1,\dots,u_{N_u}}$ into $K$ segments of size $w_{embed}$:</li> </ol> \[K = \left\lceil \frac{N_u}{w_{embed}} \right\rceil\] <p>Encode segments to embeddings ${e_k}$ and cluster into $E$ topical episodes:</p> \[C(\{e_k\}_{k=1}^K) \rightarrow \{\mathcal{E}_1, \dots, \mathcal{E}_E\}\] <p>Use the medoid of each episode (closest to its centroid) as a patched prompt for scoring.</p> <ol> <li>Prefill block‑wise per episode: process blocks of size $M$, append the patched prompt, and score tokens by the maximum attention they receive from prompt queries:</li> </ol> \[s_i^{max} = \max_{t \in [n+1, n+p]} Attn(x_t \to x_i)\] <p>Allocate the global budget by layer using sensitivity $\sigma_l$ (cosine similarity between full‑context and block‑prefill keys). With $s_l = 1 - \sigma_l$ and sharpness $\alpha$:</p> \[M_l^{alloc} = \frac{s_l^\alpha}{\sum_{l'} s_{l'}^\alpha} (L \cdot M), \qquad \sigma_l = \frac{1}{H N} \sum_{h=1}^{H} \sum_{i=1}^{N} \cos \big( \mathbf{k}_{full,i}^{(l,h)}, \mathbf{k}_{block,i}^{(l,h)} \big)\] <p>This prioritizes layers most perturbed by compression; $\alpha$ controls how sharply budget concentrates (e.g., 1.1 for Llama, 1.3 for Qwen).</p> <ol> <li>During decoding, the query is embedded into the same vector space as the clusters, where a similarity search is performed to identify the most relevant episodes. The system then retrieves the pre-computed episodic caches, which are used to generate the output.</li> </ol> <hr> <h2 id="conclusion">Conclusion</h2> <p>Transformers underperform not because they can’t reason, but because dense attention scales quadratically. The way through isn’t “more compute”, it’s embracing the fact that models rarely need to attend everywhere at once. Sparsity turns this observation into a principle: spend compute where it matters, skip where it doesn’t. Across the post, three threads converge:</p> <ul> <li>Mathematical Reformulation: Rethink normalization (e.g., Softpick) to reduce sink-driven outliers while keeping gradients healthy.</li> <li>Geometric Patterns: Lean on recurrent shapes (A‑shape, verticals, blocks) to prune decisively with minimal loss.</li> <li>Dynamic Memory: Treat KV as a living library where we prioritize heavy hitters, cluster contexts, and cache episodes, not raw logs.</li> </ul> <p>Put together, these ideas sketch a practical blueprint for long‑context systems:</p> <ul> <li>Keep anchors: Always retain initial tokens/sinks to preserve skip‑mix stability.</li> <li>Mix local + global: Pair diagonal windows with sparse global links (A‑shape/star) to avoid multi‑hop dilution.</li> <li>Select, don’t store: Use thresholds, clustering, and episodic caches to bound memory while preserving recall.</li> </ul> <p>The outcome is a transformer that thinks in long horizons without drowning in its own history. As tooling and kernels improve, expect “sparse‑first” stacks where attention masks, cache policies, and retrieval all cooperate in a fast, stable, and precise manner at scale.</p> <hr> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-sparsity.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-long-context/">Text-as-Image, A Visual Encoding Approach for Long-Context Understanding</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/vis-llm-latent-geometry/">Visualizing LLM Latent Space Geometry Through Dimensionality Reduction</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/unigramlm-manual/">UnigramLM - An Attempt at Writing the Missing Manual</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/symbolic-connect/">From Dense Monoliths to Modular Minds: The Rise of Symbolic Routing in LLMs</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>