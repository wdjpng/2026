<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> (LLM-)Judges on autopilot | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="How do you evaluate Large Language Model (LLM)-based systems in production at scale? Most teams turn to an LLM-as-a-judge: an approach that grasps the nuances of natural language where classical metrics fall short. But these judge models have their own “will”: sometimes they follow instructions precisely, sometimes they don't. To address this inconsistency, the judge prompt is &lt;i&gt;calibrated&lt;/i&gt; to align with known, trusted cases. The problem? Manual calibration is time-consuming and error-prone. In this blog post, we explore auto-calibration techniques inspired by recent prompt-optimization research. We tackle &lt;i&gt;context collapse&lt;/i&gt; by iteratively processing data in batches, similarly to a machine learning training pipeline. Along the way, we share some surprising findings about what works and what doesn't—including cases where simpler approaches outperform more sophisticated ones."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/auto-calibration/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "(LLM-)Judges on autopilot",
            "description": "How do you evaluate Large Language Model (LLM)-based systems in production at scale? Most teams turn to an LLM-as-a-judge: an approach that grasps the nuances of natural language where classical metrics fall short. But these judge models have their own “will”: sometimes they follow instructions precisely, sometimes they don't. To address this inconsistency, the judge prompt is <i>calibrated</i> to align with known, trusted cases. The problem? Manual calibration is time-consuming and error-prone. In this blog post, we explore auto-calibration techniques inspired by recent prompt-optimization research. We tackle <i>context collapse</i> by iteratively processing data in batches, similarly to a machine learning training pipeline. Along the way, we share some surprising findings about what works and what doesn't—including cases where simpler approaches outperform more sophisticated ones.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>(LLM-)Judges on autopilot</h1> <p>How do you evaluate Large Language Model (LLM)-based systems in production at scale? Most teams turn to an LLM-as-a-judge: an approach that grasps the nuances of natural language where classical metrics fall short. But these judge models have their own “will”: sometimes they follow instructions precisely, sometimes they don't. To address this inconsistency, the judge prompt is <i>calibrated</i> to align with known, trusted cases. The problem? Manual calibration is time-consuming and error-prone. In this blog post, we explore auto-calibration techniques inspired by recent prompt-optimization research. We tackle <i>context collapse</i> by iteratively processing data in batches, similarly to a machine learning training pipeline. Along the way, we share some surprising findings about what works and what doesn't—including cases where simpler approaches outperform more sophisticated ones.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#what-is-an-llm-as-a-judge">What is an LLM-as-a-judge?</a> </div> <div> <a href="#what-are-we-evaluating">What are we evaluating?</a> </div> <div> <a href="#what-about-the-data">What about the data?</a> </div> <ul> <li> <a href="#the-challenge-of-high-quality-ground-truth">The challenge of high-quality ground truth</a> </li> </ul> <div> <a href="#who-are-you-to-judge-me">Who are you to judge me?</a> </div> <ul> <li> <a href="#our-first-attempt-manual-calibration">Our first attempt: manual calibration</a> </li> <li> <a href="#take-your-cheatsheet-out-the-game-begins">Take your cheatsheet out, the game begins</a> </li> <li> <a href="#ace-up-your-sleeve-agentic-context-engineering">ACE Up Your Sleeve: Agentic Context Engineering</a> </li> <li> <a href="#any-baci-please">Any BACI, please?</a> </li> </ul> <div> <a href="#what-did-we-learn">What did we learn?</a> </div> <ul> <li> <a href="#starting-with-the-basics-when-less-is-more">Starting with the basics: when less is more</a> </li> <li> <a href="#learning-in-batches-the-missing-ingredient">Learning in batches: the missing ingredient</a> </li> <li> <a href="#the-final-verdict">The final verdict</a> </li> </ul> <div> <a href="#key-takeaways">Key Takeaways</a> </div> </nav> </d-contents> <div class="row mt-4"> <div class="col text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-auto-calibration/judge_funny-480.webp 480w,/2026/assets/img/2026-04-27-auto-calibration/judge_funny-800.webp 800w,/2026/assets/img/2026-04-27-auto-calibration/judge_funny-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-auto-calibration/judge_funny.png" class="img-fluid d-block mx-auto" width="100%" height="auto" style=" max-width: 90%; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="what-is-an-llm-as-a-judge">What is an LLM-as-a-judge?</h2> <p>Modern evaluation pipelines increasingly rely on LLM-as-a-judge <d-cite key="zheng2023llmjudge"></d-cite> to assess the quality of AI-generated responses. This approach uses an LLM to act as a judge, assessing the quality of the generated outputs against specific criteria. The judge can be either a different model or the same one that generated the response. An example prompt for the <i>answer relevance</i> metric is shown below <d-cite key="opik_answer_relevance"></d-cite>.</p> <blockquote> <p>You are an expert in NLP evaluation metrics, specifically trained to assess the relevance of answers. Your role is to evaluate the relevance of a given answer based on the user’s input. Follow these steps to complete the evaluation. […]</p> </blockquote> <p>However, this approach is inherently self-referential: an LLM produces the answer and another LLM evaluates it. What assures us that the judge is trustworthy? The primary way to establish trust is by verifying its alignment with known cases. For example, one could ask a pool of experts to rate a small subset of responses; if the LLM-as-a-judge and the experts agree on those evaluations, it’s reasonable to assume the automated judge will also perform reliably on the remaining examples.</p> <p>Typically, adjusting the LLM-as-a-judge prompt to align with human feedback is cumbersome and largely manual: ground-truth collection requires human experts, and prompt calibration requires engineering expertise. This reliance on human judgment makes the process less reproducible and prone to inconsistency.</p> <p>In this article, we investigate how to automate LLM-as-a-judge prompt calibration. Recent work introduced the use of a “cheatsheet”, a set of reference instructions that guides the LLM’s reasoning. Methods such as Dynamic Cheatsheet (DC) <d-cite key="suzgun2025dynamiccheatsheet"></d-cite> and Agentic Context Engineering (ACE) <d-cite key="zhang2025agenticcontextengineering"></d-cite> iteratively refine the cheatsheet based on observed examples, treating prompt optimization as an online learning problem.</p> <p>However, when processing samples one at a time and updating the cheatsheet sequentially, important details are lost and performance degrades over time—a phenomenon known as <i>context collapse</i> <d-cite key="zhang2025agenticcontextengineering"></d-cite>. We observe this issue in our experimental evaluation, and address it with a batch-based approach inspired by classical machine learning training pipelines. Before diving into the methods, let’s first discuss what we’re evaluating and the data used.</p> <h2 id="what-are-we-evaluating">What are we evaluating?</h2> <p>Our reference use case is assessing <i>answer relevance</i> for responses generated by an internal AI assistant for network troubleshooting. Answer relevance is a metric that evaluates how pertinent an AI system’s response is to the user’s query <d-cite key="opik_answer_relevance"></d-cite>. To measure answer relevance, we provide the LLM-as-a-judge with an input query and the corresponding output response. The judge produces a score between 0 and 1, along with a justification for its rating. Example (anonymized):</p> <table style="border: none; border-collapse: collapse;"> <tr style="border: none;"> <td style="border: none; vertical-align: top; padding: 8px;"><strong>user</strong></td> <td style="border: none; padding: 8px;">"Show me the latest issues"</td> </tr> <tr style="border: none;"> <td style="border: none; vertical-align: top; padding: 8px;"><strong>system</strong></td> <td style="border: none; padding: 8px;">"Fetching issues from the past 24 hours…We detected 5 alerts in the last 24 hours, 5 of which are major. The latest major alert was 'Packet Loss Internet - Alert - XYZ' applied on the target 'xyz'. The alert started at Dec 1, 2025 05:36:00 and lasted for 145 seconds impacting 1 test and 1 agent across the following 1 location: XYZ. The alert triggered because the packet loss, 15.0%, was outside of the threshold of 3%. We also detected 4 application outages. The latest app outage started at November 30, 2025 21:15:02 &amp; lasted for 180 seconds impacting 5 servers &amp; 2 locations. The outage occurred in these locations: London, England, UK; New York, New York, US."</td> </tr> <tr style="border: none;"> <td style="border: none; vertical-align: top; padding: 8px;"><strong>score</strong></td> <td style="border: none; padding: 8px;">1.0</td> </tr> <tr style="border: none;"> <td style="border: none; vertical-align: top; padding: 8px;"><strong>reason</strong></td> <td style="border: none; padding: 8px;">"The answer addresses the question exhaustively and clearly."</td> </tr> </table> <blockquote> <p><b><i>Note:</i></b> Unlike traditional metrics with clear mathematical definitions, this score represents a qualitative judgment guided by high-level criteria.</p> </blockquote> <h2 id="what-about-the-data">What about the data?</h2> <p>Let’s establish some notation that we’ll use throughout this article. Given a dataset of input/output pairs with associated scores and reasons, for a sample $i$ we denote:</p> <table> <thead> <tr> <th>Symbol</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>$x_i$</td> <td>User input query</td> </tr> <tr> <td>$y_i$</td> <td>System output</td> </tr> <tr> <td>$s_i$</td> <td>LLM-as-a-judge generated score</td> </tr> <tr> <td>$r_i$</td> <td>LLM-as-a-judge generated reason</td> </tr> <tr> <td>$\tilde{s}_i$</td> <td>Ground-truth score (human-annotated)</td> </tr> <tr> <td>$\tilde{r}_i$</td> <td>Ground-truth reason (human-annotated)</td> </tr> </tbody> </table> <p>Later in this article, we will denote the cheatsheet at iteration $t$ as $M_t$ (where $M$ stands for memory). In addition, when discussing batch-based approaches to prompt calibration, we will use $B$ to denote the set of sample indices belonging to a batch.</p> <h3 id="the-challenge-of-high-quality-ground-truth">The challenge of high-quality ground truth</h3> <p>To assess the performance of LLM-as-a-judge metrics, we need ground-truth scores and reasons. Obtaining reliable scores is far from trivial. For our purposes, we relied on human annotations: experts followed rating guidelines (e.g., “subtract 0.1 to 0.3 for unnecessary verbosity or repetition”) to produce scores with quantitative justifications. In other words, for each input/output pair $(x_i, y_i)$, a human annotator provided a ground-truth score and reason, which we denote as $\tilde{s}_i$ and $\tilde{r}_i$.</p> <p>However, upon careful review, we found that many scores didn’t align with expected ratings. Subjectivity is inherent when evaluating LLM-generated content. To reduce this bias, we had multiple annotators perform ratings independently and retained only those records where they showed strong agreement.</p> <p>Another challenge is sample diversity. LLMs can produce multiple valid responses for a given input, making random train/test splits potentially problematic. To better represent a realistic production scenario, we split the training and test sets based on a temporal cutoff: samples before a certain date were used for training, and those after for testing.</p> <h2 id="who-are-you-to-judge-me">Who are you to judge me?</h2> <p>Now that we have established our evaluation metric and collected ground truth annotations, the key question becomes: how do we calibrate the judge prompt to align its evaluations with human judgments? In this section, we explore different approaches to calibration.</p> <h3 id="our-first-attempt-manual-calibration">Our first attempt: manual calibration</h3> <p>Traditional calibration relies on manually crafted prompts. The workflow depends on human annotation samples and uses techniques such as few-shot prompting to address problematic queries. This process is typically iterative:</p> <ol> <li>Collect ground-truth scores from human annotators</li> <li>Compute alignment between human annotations and LLM-as-a-judge metrics</li> <li>Identify discordant samples where human and model scores diverge</li> <li>Refine the judge prompt to minimize score discrepancy</li> </ol> <p>Engineering teams perform multiple rounds of manual calibration over time to ensure that human annotators and the LLM-as-a-judge metrics converge. Beyond being time-consuming and error-prone, this approach raises several technical concerns: (a) step 4 typically involves adding few-shot examples, which results in extremely long and over-engineered prompts; (b) ensuring that existing functionalities are preserved at each calibration round requires careful selection and analysis of reference samples. This process requires an expert engineer and can take several days. These limitations motivated us to explore more automated approaches.</p> <h3 id="take-your-cheatsheet-out-the-game-begins">Take your cheatsheet out, the game begins</h3> <p>Dynamic Cheatsheet Cumulative (DC-Cu) <d-cite key="suzgun2025dynamiccheatsheet"></d-cite> was introduced in April 2025 by researchers at <i>Stanford University</i> and <i>Together AI</i>. The key idea is to treat prompt optimization as an online learning problem where an LLM processes samples sequentially, updating the cheatsheet after each one.</p> <p>DC-Cu distinguishes between two roles: a <i>Generator</i> LLM and a memory <i>Curator</i> LLM. The Generator takes the input query $x_i$ and the current cheatsheet $M_i$ to produce an output $y_i$:</p> \[y_i = \text{Generator}(x_i, M_i)\] <p>The Curator then evaluates the output and refines the cheatsheet, keeping only the most useful and generalizable strategies:</p> \[M_{i+1} = \text{Curator}(M_i, x_i, y_i)\] <p>Since no ground truth is available, the Curator itself judges response quality. In practice, this self-verification approach is achievable for math questions and puzzles where the correctness of the solution can be verified using tools such as a calculator or code-execution environment.</p> <blockquote> <p><b><i>Note:</i></b> Unlike math questions and puzzles, LLM-as-a-judge evaluations are not self-verifiable. There is no objective tool to verify whether a relevance score is correct. Therefore, we need ground-truth human annotations.</p> </blockquote> <p>We adapted DC-Cu for supervised auto-calibration by providing the Memory Curator with ground truth scores $\tilde{s}_i$ and reasoning $\tilde{r}_i$ alongside the input/output pairs $(x_i, y_i)$ (see Fig. 1). We also modified the prompts to ensure these human annotations guide the cheatsheet generation process.</p> <div class="row mt-4"> <div class="col text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-auto-calibration/2025_12_01_DC.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-auto-calibration/2025_12_01_DC.svg" class="img-fluid d-block mx-auto" width="100%" height="auto" style=" max-width: 90%; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-left"> Fig. 1: Overview of the DC‑Cu method adapted to Judge auto‑calibration. Input–output pairs $(x_i, y_i)$ are retrieved from the database and evaluated by the Judge, which assigns a relevance score and reason $(s_i, r_i)$. The predicted score–reason pair is concatenated with the original input–output pair and the ground‑truth score–reason pair $(\tilde{s}_i, \tilde{r}_i)$, and passed to the Curator, which updates the cheatsheet $M_i$ to align the Judge's scoring behavior with the ground truth. </div> <p>In our adapted DC-Cu framework, the process alternates between two phases. In the <strong>Judgment Phase</strong>, the Judge evaluates the input/output pair using the current cheatsheet $M_i$ to produce a score and reasoning:</p> \[s_i, r_i = \text{Judge}(x_i, y_i, M_i)\] <p>In the <strong>Curation Phase</strong>, the Curator updates the cheatsheet by comparing the Judge’s predictions with the ground truth annotations ($\tilde{s}_i$, $\tilde{r}_i$):</p> \[M_{i+1} = \text{Curator}(M_i, x_i, y_i, \tilde{s}_i, \tilde{r}_i, s_i, r_i)\] <p>The Curator uses the discrepancy between predicted and ground truth values to refine the evaluation criteria stored in the cheatsheet.</p> <h3 id="ace-up-your-sleeve-agentic-context-engineering">ACE Up Your Sleeve: Agentic Context Engineering</h3> <p>Agentic Context Engineering (ACE) <d-cite key="zhang2025agenticcontextengineering"></d-cite> builds upon DC-Cu by further refining its architectural structure. In ACE, the Memory Curator role is decomposed into two specialized components: a <i>Reflector</i>, which synthesizes insights from both correct and erroneous outputs, and a <i>Curator</i>, which integrates these insights into context updates. This separation prevents overburdening a single agent with the dual responsibilities of quality assessment and cheatsheet evolution. Additionally, ACE introduces a grow-and-refine mechanism that implements incremental updates to avoid full cheatsheet rewrites, pruning redundant entries through semantic analysis to ensure the cheatsheet remains both comprehensive and concise.</p> <p>Inspired by ACE, we developed a customized implementation for the LLM-as-a-Judge supervised use case by extending DC-Cu with three key enhancements: (i) incorporating ground-truth scores and reasons during training, (ii) introducing a Reflector LLM, and (iii) enabling batching and epoch-based training. Further details are provided in the following section.</p> <h3 id="any-baci-please">Any BACI, please?</h3> <p>In this section, we introduce BACI (Batching Agentic Context Iteratively), our proposed strategy for automated judge calibration. The overall architecture is illustrated in Fig. 2.</p> <div class="row mt-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-auto-calibration/2025_12_01_BACI.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-auto-calibration/2025_12_01_BACI.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption text-left"> Fig. 2: Overview of the BACI method. A batch $B$ of annotated samples is retrieved from the database, where each sample contains an input–output pair $(x_i, y_i)$ and the corresponding ground‑truth score–reason $(\tilde{s}_i, \tilde{r}_i)$. The Judge uses the cheatsheet rules to evaluate each input–output pair $(x_i, y_i)$ individually, producing a predicted score and reason $(s_i, r_i)$. Each prediction is then concatenated with its input-output pair and ground-truth, and the batch is forwarded to the Reflector. The Reflector compares ground-truth and predicted scores along with their reasons, identifying relationships among samples to extract insights $I_t$ about the Judge's errors. The Curator incorporates these insights into the cheatsheet $M_t$, aligning the Judge's scores with the ground truth. This process repeats for $k$ epochs to progressively optimize the cheatsheet. </div> <p>BACI incorporates <strong>batching</strong> as a core component to <strong>iteratively</strong> optimize the <strong>agentic context</strong>. During training, the Judge individually evaluates each sample $i$ within a batch $B$ using the current cheatsheet $M_t$ (where $t$ denotes the iteration number):</p> \[s_i, r_i = \text{Judge}(x_i, y_i, M_t) \quad \forall i \in B\] <p>The batch is then passed to the Reflector, which extracts insights $I_t$ by comparing the Judge’s predictions with the human-provided ground truth across all samples in the batch:</p> \[I_{t} = \text{Reflector}(\{(x_i, y_i, \tilde{s}_i, \tilde{r}_i, s_i, r_i)\}_{i \in B})\] <p>These insights are fed to the Curator, which updates the cheatsheet accordingly:</p> \[M_{t+1} = \text{Curator}(M_t, I_t)\] <p>In subsequent iterations, the Judge uses the updated cheatsheet to generate new scores and reasons. This iterative process is repeated for all batches, with the cheatsheet being continuously refined at each step. The entire cycle is run for $k$ epochs, like a standard machine learning pipeline but employing gradient-free optimization.</p> <p>At test time, we provide the Judge with the final version of the cheatsheet, refined during training. The Judge uses this cheatsheet to evaluate new, unseen samples by generating scores and reasons based on the accumulated knowledge. The final cheatsheet serves as a distilled summary of the most relevant evaluation patterns learned during training, guiding the Judge’s evaluations in the test phase.</p> <p>Compared to ACE, our Curator is responsible for both adding new evaluation instructions and de-duplicating entries. This design makes our method more lightweight than the original ACE approach, which maintains embeddings for each instruction in the cheatsheet.</p> <p>In BACI, the Reflector extracts insights, and the Curator is instructed to perform updates by adding instructions (i) only if they are sufficiently different from existing ones, (ii) refining entries that lack important aspects, and (iii) discarding items that are similar to those already present. The combination of batching and the Reflector-Curator architecture helps us avoid context collapse and redundancy of instructions. In particular, the batch size plays a crucial role in this process — as we will demonstrate in the following section.</p> <blockquote> <p><b><i>Note:</i></b> we use Claude Sonnet 4.5, which has a nearly unlimited context window (200K base, can be extended up to 1 million tokens) for the LLM Judge, Reflector, and Curator. When using a model with a smaller context window, a trade-off in the batch size might be needed.</p> </blockquote> <h2 id="what-did-we-learn">What did we learn?</h2> <h3 id="starting-with-the-basics-when-less-is-more">Starting with the basics: when less is more</h3> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-auto-calibration/score_comparison_other_methods.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <div class="caption text-left"> Fig. 3: Score comparison on test set for the baseline methods. The score for each sample has been computed as the average across 10 predictions. The standard deviaton for each score can be seen by hovering over the point. </div> <p>We compared the score distributions across our test set for different baselines (see Fig. 3). We first evaluated what happens when the Judge receives no special instructions, using only the basic definition of Answer Relevance, with no training, evolution, or use case specific guidelines. Surprisingly, on average, this “empty cheatsheet” approach performs on par with our manually calibrated prompt. This is remarkable given that an empty cheatsheet contains no domain-specific details.</p> <p>Even more unexpectedly, the DC-Cu method performs worse than the previous baselines. As noted in <d-cite key="zhang2025agenticcontextengineering"></d-cite>, the main issue is context collapse: over time, the LLM Curator tends to generate shorter, less informative summaries, leading to a sharp decline in performance.</p> <p>These observations lead to our first key insight:</p> <blockquote> <p>It is better to provide no instructions in the cheatsheet than to include suboptimal instructions that may cause confusion or conflicting behavior.</p> </blockquote> <p>This also explains why the carefully calibrated prompt did not outperform the baselines on the test set, despite meticulous fine-tuning, adjusted dataset scoring, and multiple few-shot examples.</p> <h3 id="learning-in-batches-the-missing-ingredient">Learning in batches: the missing ingredient</h3> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-auto-calibration/score_comparison_BACI.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <div class="caption text-left"> Fig. 4: Score comparison on test set for BACI with different batch sizes and DC-Cu. The score for each sample has been computed as the average across 10 predictions. The standard deviaton for each score can be seen by hovering over the point. </div> <p>We evaluated two BACI configurations: BACI-1 uses a batch size of one with a single training epoch, while BACI-32 uses a batch size of 32 and trains for five epochs. Figure 4 compares their performance against ground truth and DC-Cu. Here are the key observations:</p> <ul> <li> <p>BACI-1 significantly outperforms DC-Cu. This improvement stems primarily from the separation of concerns between the Curator and Reflector components. As noted in the ACE work, this architectural separation — where insight extraction (Reflector) and cheatsheet updating (Curator) are distinct processes — helps mitigate context collapse across iterations.</p> </li> <li> <p>BACI-32 outperforms BACI-1. The larger batch size and multiple training epochs enable the system to observe the entire training dataset repeatedly, refining the cheatsheet iteratively. Crucially, processing samples in larger batches allows the Reflector to identify more generalizable patterns rather than overfitting individual examples.</p> </li> </ul> <h3 id="the-final-verdict">The final verdict</h3> <p>Bringing it all together: Table 1 summarizes all experimental results, showing average Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) <d-cite key="bishop2006pattern"></d-cite> across 10 test runs. BACI-32 (bold) clearly outperforms all competing methods on our dataset.</p> <div align="center"> <table> <thead> <tr> <th>Method</th> <th>MAE</th> <th>RMSE</th> </tr> </thead> <tbody> <tr> <td>Empty Cheatsheet</td> <td>0.134 ± 0.002</td> <td>0.201 ± 0.004</td> </tr> <tr> <td>Manually calibrated</td> <td>0.139 ± 0.010</td> <td>0.219 ± 0.016</td> </tr> <tr> <td>DC-Cu</td> <td>0.272 ± 0.006</td> <td>0.308 ± 0.007</td> </tr> <tr> <td>BACI-1</td> <td>0.147 ± 0.003</td> <td>0.212 ± 0.002</td> </tr> <tr> <td>BACI-32</td> <td> <strong>0.111</strong> ± 0.003</td> <td> <strong>0.198</strong> ± 0.004</td> </tr> </tbody> </table> </div> <div class="caption text-left"> Table 1: Experimental results comparing all methods. MAE and RMSE averaged across 10 test runs. </div> <p>Statistical validation using the Wilcoxon signed-rank test <d-cite key="wilcoxon1945individual"></d-cite> confirms: (1) manual calibration provided no significant improvement over an empty cheatsheet ($p = 0.62$); (2) BACI-32 significantly outperforms both BACI-1 and the empty cheatsheet ($p &lt; 0.01$).</p> <h2 id="key-takeaways">Key Takeaways</h2> <p><strong>Context collapse is real.</strong> Our empirical analysis confirms that iterative, sample-by-sample approaches to prompt calibration suffer from a critical issue: context collapse. The solution lies in applying proper machine learning strategies—processing samples in batches and tuning hyperparameters such as batch size and number of epochs.</p> <p><strong>Manual calibration faces similar challenges.</strong> Even manually crafted prompts can suffer from analogous issues, as they’re typically adjusted iteratively on a static set of examples. Moreover, human bandwidth limits the number of samples that can be examined, making the process tedious, error-prone, and nearly impossible to scale.</p> <p><strong>Sometimes, less is more.</strong> Perhaps our most surprising finding: the strong performance of the empty cheatsheet baseline. This serves as a clear warning that wrong guidance can be worse than no guidance at all. When a simple solution works, there’s no need to overcomplicate it.</p> <p><strong>Data quality matters — a lot.</strong> The quality of training data is just as important as the calibration method itself. While this principle applies to all machine learning, it is especially critical for generative AI: we cannot expect an LLM to generate meaningful insights from inconsistent or low-quality input data. In our work, we observed a substantial subjectivity bias in human annotations. Data cleaning was crucial and required time and resources. Despite the impressive capabilities of modern LLMs, human judgment remains indispensable—at least for now, AI cannot fully replace expert reviewers.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-auto-calibration.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/vis-llm-latent-geometry/">Visualizing LLM Latent Space Geometry Through Dimensionality Reduction</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/symbolic-connect/">From Dense Monoliths to Modular Minds: The Rise of Symbolic Routing in LLMs</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/subject-invariant-eeg/">The Decoupling Hypothesis: Attempting Subject-Invariant EEG Representation Learning via Auxiliary Injection</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/style-representations/">Artistic Style and the Play of Neural Style Representations</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>