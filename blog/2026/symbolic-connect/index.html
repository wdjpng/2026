<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From Dense Monoliths to Modular Minds: The Rise of Symbolic Routing in LLMs | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="The history of Artificial Intelligence (AI) has largely been defined by a dichotomy: the flexible, probabilistic learning of Connectionism versus the rigorous, compositional logic of Symbolism. However, the emergence of Large Language Models (LLMs) is fostering a synthesis of these paradigms through a fundamental architectural shift: the move from Dense Monoliths to Modular, Routed Systems. This shift is fractal. At the Macro level, LLMs function as central planners, using symbolic protocols to orchestrate external tools and specialized neural agents. Simultaneously, at the Micro level, the models themselves are evolving into sparse, modular structures (such as Mixture-of-Experts) governed by internal routing mechanisms. In this post, we explore this transition toward Symbolic Routing. We discuss how this paradigm enables us to build societies of neural agents, discover latent modularity within dense networks, thus enabling composable, verifiable, interpretable and continually learnable AI system. And we also discuss how to leverage these structures to synthesize training data and formally verify AI reasoning."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/symbolic-connect/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "From Dense Monoliths to Modular Minds: The Rise of Symbolic Routing in LLMs",
            "description": "The history of Artificial Intelligence (AI) has largely been defined by a dichotomy: the flexible, probabilistic learning of Connectionism versus the rigorous, compositional logic of Symbolism. However, the emergence of Large Language Models (LLMs) is fostering a synthesis of these paradigms through a fundamental architectural shift: the move from Dense Monoliths to Modular, Routed Systems. This shift is fractal. At the Macro level, LLMs function as central planners, using symbolic protocols to orchestrate external tools and specialized neural agents. Simultaneously, at the Micro level, the models themselves are evolving into sparse, modular structures (such as Mixture-of-Experts) governed by internal routing mechanisms. In this post, we explore this transition toward Symbolic Routing. We discuss how this paradigm enables us to build societies of neural agents, discover latent modularity within dense networks, thus enabling composable, verifiable, interpretable and continually learnable AI system. And we also discuss how to leverage these structures to synthesize training data and formally verify AI reasoning.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>From Dense Monoliths to Modular Minds: The Rise of Symbolic Routing in LLMs</h1> <p>The history of Artificial Intelligence (AI) has largely been defined by a dichotomy: the flexible, probabilistic learning of Connectionism versus the rigorous, compositional logic of Symbolism. However, the emergence of Large Language Models (LLMs) is fostering a synthesis of these paradigms through a fundamental architectural shift: the move from Dense Monoliths to Modular, Routed Systems. This shift is fractal. At the Macro level, LLMs function as central planners, using symbolic protocols to orchestrate external tools and specialized neural agents. Simultaneously, at the Micro level, the models themselves are evolving into sparse, modular structures (such as Mixture-of-Experts) governed by internal routing mechanisms. In this post, we explore this transition toward Symbolic Routing. We discuss how this paradigm enables us to build societies of neural agents, discover latent modularity within dense networks, thus enabling composable, verifiable, interpretable and continually learnable AI system. And we also discuss how to leverage these structures to synthesize training data and formally verify AI reasoning.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction-the-neuro-symbolic-renaissance">Introduction: The Neuro-Symbolic Renaissance</a> </div> <div> <a href="#macro-symbolism-the-planner-executor-paradigm">Macro-Symbolism: The Planner-Executor Paradigm</a> </div> <ul> <li> <a href="#the-core-mechanism-the-probabilistic-deterministic-loop">The Core Mechanism: The Probabilistic–Deterministic Loop</a> </li> <li> <a href="#scaling-to-neural-modules-the-agentic-workflow">Scaling to Neural Modules: The Agentic Workflow</a> </li> <li> <a href="#the-future-the-rise-of-the-llm-os">The Future: The Rise of the LLM-OS</a> </li> </ul> <div> <a href="#micro-symbolism-the-internal-routing-paradigm">Micro-Symbolism: The Internal Routing Paradigm</a> </div> <ul> <li> <a href="#the-explicit-router-mixture-of-experts-moe">The Explicit Router: Mixture-of-Experts (MoE)</a> </li> <li> <a href="#the-implicit-router-discovering-latent-modularity">The Implicit Router: Discovering Latent Modularity</a> </li> <li> <a href="#the-future-post-hoc-modularization-and-structured-control">The Future: Post-Hoc Modularization and Structured Control</a> </li> </ul> <div> <a href="#automatic-data-synthesis-and-formal-verification">Automatic Data Synthesis and Formal Verification</a> </div> <ul> <li> <a href="#program-aided-data-synthesis">Program-Aided Data Synthesis</a> </li> <li> <a href="#verified-inference-the-logic-of-truth">Verified Inference: The Logic of Truth</a> </li> </ul> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="introduction-the-neuro-symbolic-renaissance">Introduction: The Neuro-Symbolic Renaissance</h2> <p>Artificial Intelligence (AI) has long swung between two poles. On one side is <em>Symbolism</em>, the tradition of explicit rules, logic, and step-by-step reasoning. On the other is <em>Connectionism</em>, the belief that intelligence emerges from pattern recognition in large neural networks. This divide mirrors an old philosophical tension between rationalism and empiricism <d-cite key="goel2021looking,ciatto2024symbolic"></d-cite>. While the rise of modern LLMs might look like a decisive victory for connectionism, the reality is more interesting: today’s models increasingly blend the strengths of both worlds.</p> <p><strong>Symbolic AI</strong>, which dominated from the 1950s to the 1990s, is rooted in the explicit manipulation of human-readable symbols according to logical rules <d-cite key="bhuyan2024neuro,WikipediaSymbolicAI"></d-cite>. Its primary virtues are <em>transparency and verifiability</em>; reasoning can be audited step-by-step. However, symbolic systems are notoriously brittle, struggling with the ambiguity of the real world and the “knowledge acquisition bottleneck” <d-cite key="Pangakis_Wolken_2025,NEURIPS2024_bf236666"></d-cite>.</p> <p><strong>Connectionist AI</strong>, inspired by the biological brain, posits that intelligence emerges from vast networks of simple units (neurons) learning from data <d-cite key="Shoup2023LLMsResurrect,Garson2018Connectionism"></d-cite>. Its strength lies in <em>flexibility and robustness</em>, excelling at unstructured data like images and text. Yet, they face the <em>“black box” problem</em> <d-cite key="singh2024rethinking"></d-cite>. Knowledge is diffused across billions of opaque weights, making reasoning difficult to trace and prone to “hallucinations” <d-cite key="huang2025survey"></d-cite>.</p> <p>Today, we are seeing a third phase emerge: <strong>the rise of the router</strong>. Modern LLMs possess an emergent mastery of both distributional representations and discrete tokens (e.g., code, SQL, JSON) <d-cite key="lee2025symba,hu-etal-2025-os"></d-cite>. This allows them to function as <em>semantic translators</em>, converting fuzzy human intent into precise intermediate symbolic protocols for execution as shown in Figure 1. Crucially, this translation mechanism is reshaping AI architecture at two distinct scales:</p> <ol> <li> <strong>Macro-Symbolism (System Level):</strong> The LLM becomes a <em>Planner</em>, deciding when and how to call a database, a coder interpreter, a JSON API request, or which specialized model to invoke. In effect, it routes tasks across a society of tools and agents.</li> <li> <strong>Micro-Symbolism (Model Level):</strong> Inside the LLM itself, we see a shift from dense monoliths to modular, sparse structures. Mixture-of-Experts (MoE) architectures introduce explicit routers that choose which internal “experts” to activate, while mechanistic interpretability reveals latent circuits that already behave like implicit modules.</li> </ol> <figure style="text-align: center; width: 100%;"> <img src="/2026/assets/img/2026-04-27-symbolic-connect/SymbolicExamples.png" style="width: 95%;"> <figcaption style="font-size: 1em;">Figure 1: The LLM as a semantic translator. It converts natural language requests into symbolic forms (e.g., SQL, Python, JSON) that deterministic tools can execute. The verified outputs are then folded back into the final answer.</figcaption> </figure> <p>In this post, we explore how this shift—from dense models to routed, modular minds—is reshaping both AI systems and the models that power them. We start with <em>Macro-Symbolism</em>: the probabilistic-deterministic loop that lets LLMs ground their answers in external tools and orchestrate specialized neural agents. Then we zoom in to <em>Micro-Symbolism</em>: how routing and modularity are emerging inside the model itself, from explicit MoE experts to latent circuits discovered via interpretability. We discuss how these two kinds of symbolism benefit future LLM system. Finally, we discuss how these routed architectures enable two critical capabilities for the next generation of AI systems: <em>automatic data synthesis</em> and <em>verified reasoning</em>.</p> <h2 id="macro-symbolism-the-planner-executor-paradigm">Macro-Symbolism: The Planner-Executor Paradigm</h2> <p>The first major shift toward a modular AI architecture is happening at the system level. We call this <em>Macro-Symbolism</em>. In this paradigm, the LLM stops acting as a solitary “Oracle” and instead becomes a <em>Router</em>: a central planner that orchestrates external modules to solve problems.</p> <p>This architecture is built on a simple division of labor. The connectionist “Brain” (the LLM) handles ambiguity, planning, and language. The specialized “Hands” (external modules) handle concrete execution. They talk to each other through <em>structured protocols</em>: explicit symbolic languages such as SQL, JSON, or Python code. In practice, this routing happens in two main ways:</p> <ol> <li> <strong>Routing to deterministic tools:</strong> “Glass box” systems like calculators, databases, search engines, and code interpreters, whose behavior is transparent and verifiable.</li> <li> <strong>Routing to neural specialists:</strong> “Black box” systems such as vision models or other LLMs, which act as experts for perception, generation, or domain-specific reasoning.</li> </ol> <h3 id="the-core-mechanism-the-probabilisticdeterministic-loop">The Core Mechanism: The Probabilistic–Deterministic Loop</h3> <p>Despite their linguistic prowess, standalone LLMs remain limited: their knowledge is frozen at training time, they are prone to confident hallucinations, and they are effectively “brains in a jar”, disconnected from the external world. The hybrid neural–symbolic paradigm addresses these issues by pairing the flexibility of LLMs with the rigor of deterministic programs <d-cite key="xu2024symbol,de2025tool,schick2023toolformer"></d-cite>.</p> <p>In this architecture as shown in Figure 2, the LLM serves as an intuitive <em>semantic interface</em>, while the external program (a search engine, database, Python interpreter, or theorem prover) plays the role of <em>verifiable executor</em>. The key step is translation: the LLM converts user intent into a precise, logically interpretable symbolic intermediate representation (IR).</p> <p><strong>The Four-Stage Cycle: Input, Translation, Execution, Grounding.</strong> Most tool-augmented systems follow the same four-stage loop:</p> <ol> <li> <strong>User input (fuzzy intent).</strong> A person describes a goal in natural language, such as “How did our European sales do last quarter?” or “Help me clean up my hard drive.”</li> <li> <strong>Translation (symbolic bridge).</strong> The LLM acts as a natural-to-formal compiler, turning this fuzzy request into an unambiguous IR: a SQL query, a Python script, or a JSON API call.</li> <li> <strong>Execution (glass box).</strong> IR is passed to a deterministic program that executes it faithfully. Unlike the neural network, this component is a “glass box” with transparent and predictable behavior.</li> <li> <strong>Grounding (factual synthesis).</strong> The results (e.g., table, calculation, search snippet, or proof state) are fed back into the LLM, which synthesizes a fluent answer grounded in these verifiable outputs <d-cite key="LabelStudioExternalKnowledge,Jones2025DeepResearch"></d-cite>.</li> </ol> <p><strong>Verifiability and control.</strong> This loop introduces a natural <em>firewall</em> between the probabilistic model and the real world. The LLM never executes actions directly; it proposes a plan in the form of symbolic code. That code can be logged, inspected by humans, analyzed by static tools, or rejected before it is run. This kind of auditing and intervention is difficult to achieve in end-to-end neural systems.</p> <p><strong>Computational integrity.</strong> Connectionist models excel at pattern recognition but struggle with tasks that demand exact arithmetic or strict logical rules. Rather than memorizing multiplication tables, a tool-augmented LLM can write Python or formulate an optimization problem, then delegate the actual computation to a solver. This separates <em>reasoning about the problem</em> (neural) from <em>computing the solution</em> (symbolic), combining linguistic fluency with mathematical rigor.</p> <p><strong>Dynamic extensibility.</strong> Finally, symbolic routing breaks the “parametric knowledge boundary”. Instead of retraining the model every time the world changes, we can hook it up to new tools by defining new schemas and APIs. Adding a live stock feed, a proprietary enterprise database, or a theorem prover becomes a matter of describing the interface, not touching the weights. The LLM evolves from a static text generator into an agentic controller of external systems.</p> <figure style="text-align: center; width: 100%;"> <img src="/2026/assets/img/2026-04-27-symbolic-connect/ToolUse.png" style="width: 60%;"> <figcaption style="font-size: 1em;">Figure 2: The tool-use paradigm. The LLM translates user requests into symbolic codes (JSON, Python, Shell), which are executed by deterministic programs. Their outputs are then folded back into the model's response, grounding it in verifiable computation.</figcaption> </figure> <p><strong>Applications: From Fuzzy Language to Interpretable Actions</strong>. Although the tools differ, the same probabilistic–deterministic loop appears across many domains:</p> <ul> <li> <strong>Information access.</strong> The model converts questions into search queries or Text-to-SQL statements, then grounds its answers in retrieved web pages or database rows <d-cite key="Survey-LLM-Text-to-SQL,hong2025next"></d-cite>.</li> <li> <strong>Code and data analysis.</strong> Instead of doing arithmetic in its head, the LLM writes and runs Python in a sandbox, using the results to answer questions about files, logs, or datasets <d-cite key="OpenAICookbookCodeInterpreter"></d-cite>.</li> <li> <strong>System and service control.</strong> Natural language instructions are translated into shell commands, GUI scripts, or JSON API calls that interact with servers, legacy software, or cloud services <d-cite key="schick2023toolformer,nguyen2024gui,xu2024symbol,AssistGUI,hu-etal-2025-os"></d-cite>.</li> <li> <strong>Formal reasoning and robotics.</strong> High-level intents become Lean tactics for a theorem prover <d-cite key="de2015lean,wang2025malot"></d-cite> or control commands for a robot, with the symbolic engine (kernel or controller) enforcing correctness and safety <d-cite key="zeng2023large"></d-cite>.</li> </ul> <p>Across all of these, the pattern is the same: the LLM transform fuzzy human language into executable symbolic languages for an automatic program to interpret or execute.</p> <h3 id="scaling-to-neural-modules-the-agentic-workflow">Scaling to Neural Modules: The Agentic Workflow</h3> <p>So far we have focused on tools like databases, search engines, and interpreters. The next step is to treat <em>other neural networks</em> as tools as well. Instead of building a single, monolithic model that tries to do everything, we can compose smaller experts behind symbolic interfaces. This mirrors the evolution of software from monoliths to microservices <d-cite key="Blueprint"></d-cite>.</p> <p><strong>Wrapping Neural Networks in Symbolic Interfaces</strong>. Any system that accepts structured input and produces predictable output can be wrapped in an API definition. This lets a central planner treat highly specialized models as if they were ordinary Python functions <d-cite key="WhiteLLMFunctionCalling,shen2023hugginggpt,shen2024small"></d-cite>. Examples include:</p> <ul> <li> <strong>Perception.</strong> Vision and audio models (CNNs, ViTs, speech recognizers) perform OCR, object detection, or transcription more efficiently than a general-purpose multimodal LLM <d-cite key="he2016deep,dosovitskiy2020image,radford2023robust"></d-cite>.</li> <li> <strong>Generative media.</strong> Diffusion models act as the system’s “imagination”, turning text prompts into high-fidelity images or videos <d-cite key="ramesh2021zero,rombach2022high"></d-cite>.</li> <li> <strong>Domain experts.</strong> Models such as small, fine-tuned LLMs specialize in particular scientific or professional domains, from protein folding to legal analysis <d-cite key="AlphaFold,shen2024small"></d-cite>.</li> </ul> <p>From the planner’s perspective, these are all just callable tools: each has a name, an input schema, and an output schema.</p> <p><strong>The Orchestration Workflow</strong>. Consider a user who uploads a quarterly earnings PDF and asks: <em>“Analyze this report, identify the main revenue drivers, plot them, and draft a press release.”</em> A planner LLM can handle this without doing every step itself:</p> <ol> <li> <strong>Decompose the task.</strong> The planner breaks the request into subtasks: extract text from the PDF, analyze the financial data, generate a plot, and write the press release <d-cite key="Huang_Lipovetzky_Cohn_2025,SaMSolutionsAgentic"></d-cite>.</li> <li> <strong>Call the right experts.</strong> It routes the document to an OCR or document-understanding model, passes the extracted tables to a financial-analysis agent or code interpreter, and uses a plotting tool to generate visuals <d-cite key="AnalyticsVidhyaFunctionCalling,xu2024symbol,kim2024llm"></d-cite>.</li> <li> <strong>Synthesize the answer.</strong> Finally, it folds the analysis and the chart back into a coherent narrative, written in the user’s preferred tone.</li> </ol> <p>Throughout this process, the planner does not need to know how OCR, financial modeling, or plotting work internally. It only needs to understand how to speak the right symbolic language to each expert and how to route information between them.</p> <p><strong>Why Modular AI Wins</strong>. Shifting from a monolithic “God Model” to a modular system of agents offers profound engineering advantages, validating the macro-symbolic approach <d-cite key="KumarMultiAgent,WhiteLLMFunctionCalling"></d-cite>:</p> <ul> <li> <strong>Performance via specialization.</strong> Dedicated perception or domain models typically outperform generalist LLMs on their home tasks. Divide-and-conquer yields higher quality.</li> <li> <strong>Efficiency and cost.</strong> There is no need to invoke a trillion-parameter model to perform simple OCR or schema extraction. Routing lightweight tasks to small experts reduces latency and compute.</li> <li> <strong>Maintainability.</strong> Components can be upgraded or replaced independently. Swapping in a better OCR model or a new diffusion model does not require retraining the planner.</li> </ul> <p>This compositional view suggests that future AI systems may look less like a single all-knowing agent and more like a robust <em>society of models</em>, coordinated through structured protocols.</p> <h3 id="the-future-the-rise-of-the-llm-os">The Future: The Rise of the LLM-OS</h3> <p>As these planner–executor patterns mature, a natural analogy emerges: the <em>LLM-as-operating-system</em> (LLM-OS) <d-cite key="hu-etal-2025-os,karpathy2023llmos"></d-cite>. Here, the LLM acts as a cognitive kernel that manages:</p> <ul> <li> <strong>Memory,</strong> via context windows, external vector stores, and retrieval mechanisms.</li> <li> <strong>Processes,</strong> by scheduling and coordinating multi-step agentic workflows.</li> <li> <strong>I/O,</strong> through drivers that expose tools, APIs, and other models as callable resources.</li> </ul> <p>Two developments seem especially important. First, we are moving toward standardized <em>agent interfaces</em> that let diverse tools and models discover and call one another with minimal glue code <d-cite key="mo2025livemcpbench"></d-cite>. Second, planners are becoming capable of writing and executing their own tools on the fly, dynamically compiling new “drivers” for novel tasks <d-cite key="wang2023voyager"></d-cite>.</p> <p>At the system level, then, modularity and routing are already reshaping how we build AI applications. Yet the core model—the neural kernel itself—remains largely opaque. In the next section, we turn this lens inward and ask: <em>can we apply the same modular logic inside the model, not just around it?</em></p> <h2 id="micro-symbolism-the-internal-routing-paradigm">Micro-Symbolism: The Internal Routing Paradigm</h2> <p>Macro-Symbolism shows how LLMs route between tools and agents outside the model. A parallel transformation is beginning to happen <em>inside</em> the network itself. Traditional deep learning has relied on dense, monolithic architectures where every parameter is active for every token. These models work astonishingly well, but their internal logic is heavily entangled: it is hard to tell whether a model is genuinely reasoning or simply exploiting “shortcuts”: superficial correlations in the training data that bypass causal understanding <d-cite key="bengio2013representation,geirhos2020shortcut,chi2025chimera"></d-cite>.</p> <p>We refer to the emerging alternative as <strong>Micro-Symbolism</strong>. The architectural logic of the planner–executor pattern is internalized: the dense block of weights is factored into distinct functional components, and information flows through them via routing mechanisms. The goal is to move from opaque pattern matching toward systems that solve problems by composing disentangled skills.</p> <h3 id="the-explicit-router-mixture-of-experts-moe">The Explicit Router: Mixture-of-Experts (MoE)</h3> <p>The most concrete realization of micro-symbolism is the Mixture-of-Experts (MoE) architecture. Instead of activating the entire network for every token, MoE introduce <em>sparsity</em>: only a small subset of parameters is used for each input <d-cite key="shazeer2017outrageously,fedus2022switch,jiang2024mixtral"></d-cite>.</p> <p>In an MoE Transformer, the standard feed-forward layer is replaced by a collection of parallel “expert” networks as shown in Figure 3. A trainable <em>gating network</em> (or router) sits in front of them, inspects the current token, and makes a discrete decision such as: <em>send this token to Expert 3 and Expert 7</em>. This is a microscopic analogue of the system-level planner. Just as an LLM routes a math question to a calculator, the MoE router can route numerically heavy tokens to a math-specialized MLP. Note that considering Attention heads are responsible for different functions, they also can be routed and sparsely activated during inference as shown in Figure 3.</p> <p>These routing decisions create a quasi-symbolic bottleneck inside the network: each token is explicitly assigned to a small set of experts. Different experts can specialize in different sub-functions (e.g., syntax, factual or procedural knowledge), while the router learns to compose them on the fly. Rather than learning every new task from scratch, the model can solve novel problems by recombining pre-learned functions, much like assembling Lego blocks <d-cite key="hahn2023theory,li2024what,chen2024skills"></d-cite>. This structural disentanglement brings the model’s internal behavior closer to the compositional way humans reuse skills.</p> <figure style="text-align: center; width: 100%;"> <img src="/2026/assets/img/2026-04-27-symbolic-connect/InterSymbols.png" style="width: 60%;"> <figcaption style="font-size: 1em;">Figure 3: <strong>Micro-symbolism</strong>. LLMs use routers to disentangle and call different modules to clearly process different functions.</figcaption> </figure> <h3 id="the-implicit-router-discovering-latent-modularity">The Implicit Router: Discovering Latent Modularity</h3> <p>Most current LLMs, however, are still dense transformers with no explicit MoE layers. At first glance, they look like undifferentiated blocks where every unit talks to every other. Yet mechanistic interpretability work suggests that even these dense models spontaneously develop a <em>latent modular structure</em> <d-cite key="elhage2021mathematical,hou2022has,wang2023interpretability,qiu2024unlocking"></d-cite>. The challenge of implicit micro-symbolism is to uncover and shape this hidden structure.</p> <p><strong>The Cost of Entanglement: Shortcut Learning</strong>. Without clear internal boundaries, dense models often learn “shortcuts”: heuristics that work on the training distribution but fail under shift. Consider multimodal models analyzing charts. When shown a scatter plot of population data, a model might confidently call it a “line graph” simply because the caption mentions “population” and the points trend upward.</p> <p>We can view this as a <strong>routing failure</strong>. The model likely contains a perceptual circuit capable of distinguishing dots from lines, but the internal controller does not reliably route the signal through it. Instead, the model takes an easier path: a <em>linguistic shortcut</em> (“population” $\Rightarrow$ line graph) or a <em>prior-knowledge shortcut</em> (populations usually grow). Because the “seeing” circuit and the “guessing” circuit are entangled, the stronger heuristic overrides perception.</p> <p><strong>Uncovering the Latent Router.</strong> These failures do not mean that dense models are structureless. They indicate that the structure is <em>latent</em> and poorly controlled. Careful probing shows that transformers already organize themselves in modular ways:</p> <ul> <li> <strong>Layer-wise specialization.</strong> Early layers often behave like syntactic parsers, tracking word order and surface form, while deeper layers encode more abstract semantics and factual knowledge <d-cite key="tenney2019bert,hou2021bird,meng2022locating"></d-cite>.</li> <li> <strong>Procedural traces.</strong> In multi-step reasoning tasks, specific attention heads in LLMs track particular stages of inference, effectively acting as registers for intermediate variables <d-cite key="olsson2022context,hou2023towards"></d-cite>.</li> </ul> <p>Viewed this way, the attention mechanism itself functions as a soft, continuous <em>implicit router</em>. By choosing where to attend in the residual stream, attention heads route information between different subspaces—syntactic, semantic, factual, or task-specific.</p> <p><strong>From Entanglement to Circuit Discovery.</strong> Micro-symbolism in dense models is therefore an analytical project. By applying tools from mechanistic interpretability, we can “symbolize” parts of the network: map directions in activation space to human-interpretable concepts (a gender direction, a previous-token head, a negation circuit) <d-cite key="elhage2021mathematical,conmy2023automated,wang2023interpretability"></d-cite>. This turns the model from a pure black box into a “grey box” with identifiable components and interfaces.</p> <p>Identifying these latent modules is the first step toward a more ambitious goal: <em>post-hoc modularization</em>, where we turn discovered circuits into explicit, controllable building blocks.</p> <h3 id="the-future-post-hoc-modularization-and-structured-control">The Future: Post-Hoc Modularization and Structured Control</h3> <p>If dense models already approximate modularity internally, a natural next step is to make that structure explicit. <strong>Post-hoc modularization</strong> imagines taking a pretrained model and refactoring it into a transparent, composable cognitive system.</p> <p><strong>Refactoring the Monolith</strong>. Standard training optimizes for end-to-end loss, often at the expense of clean internal structure. Post-hoc modularization reverses this: using interpretability tools, we identify circuits for specific capabilities: arithmetic <d-cite key="stolfo2023mechanistic"></d-cite>, visual binding <d-cite key="rajaram2024automatic,saravanan2025investigating"></d-cite>, or factual recall <d-cite key="meng2022locating"></d-cite>, and encapsulate them as separate modules.</p> <p>This process turns the “art of alchemy” into something closer to software engineering. Once a capability is disentangled, it becomes:</p> <ul> <li> <strong>Inspectable:</strong> we can test and verify the module in isolation.</li> <li> <strong>Repairable:</strong> if a circuit encodes a bias or persistent error, we can patch it without retraining the whole model <d-cite key="wu2024continual"></d-cite>.</li> <li> <strong>Composable:</strong> robust modules can be reused across tasks or even modalities—for example, reusing a math circuit for text and vision. This aligns with ideas in model merging and adapters <d-cite key="zhou-etal-2025-mergeme,hu2022lora"></d-cite>, but with a more interpretable notion of what is being combined.</li> </ul> <p><strong>Structured Reasoning Controllers</strong>. To make these modules work together, we need internal controllers that play a role analogous to the planner at the system level. A <em>structured reasoning controller</em> would guide the flow of information between modules, enforcing <strong>process over output</strong> <d-cite key="lightman2023lets,uesato2022solving"></d-cite>.</p> <p>Instead of letting information diffuse across all layers, the controller would explicitly route data from a perception module (to bind entities) to a logic module (to infer relationships), and only then to a language module (to verbalize the conclusion) <d-cite key="andreas2016neural,hou2024vision"></d-cite>. This reduces the temptation to rely on shortcuts or label priors and aligns the model’s internal computation with the stepwise structure of the task.</p> <p>If Macro-Symbolism systems are for orchestrate tools and agents, Micro-Symbolism aims for models whose <em>internal</em> operations follow similarly modular, interpretable patterns. They point toward AI systems that do not just imitate correct answers, but earn them through structured reasoning.</p> <h2 id="automatic-data-synthesis-and-formal-verification">Automatic Data Synthesis and Formal Verification</h2> <p>The move from purely connectionist black boxes to neuro-symbolic systems is not just an architectural shift; it changes <em>how</em> models learn and <em>how</em> we trust them. Whether we look at <em>Macro-Symbolism</em> (agents and tools) or <em>Micro-Symbolism</em> (MoE and circuits), two basic questions remain: <em>where does the data come from</em>, and <em>how do we know the answer is actually correct?</em></p> <p>The structured protocols that we have discussed above could do more than improve performance. They provide a foundation for two complementary capabilities: <em>automatic data synthesis</em>, where models generate data via formal structure and automatic programs, and <em>formal verification</em>, where we check their reasoning using logical proofs.</p> <h3 id="program-aided-data-synthesis">Program-Aided Data Synthesis</h3> <p>High-quality human text is finite, and much of it has already been scraped. Synthetic data is the natural next step, but naive approaches—“ask an LLM to write more text like the internet”—risk <em>model collapse</em>, where models amplify their own artifacts and drift away from reality <d-cite key="Nadas2025SyntheticData,BerdanierSyntheticData,su2025scaling"></d-cite>.</p> <p>The planner–executor architecture in previous section suggests a different strategy. Instead of treating the model as a storyteller, we treat it as a <em>generator of programs and simulations</em>. Rather than hallucinating a fact, the LLM writes code or constructs an API call that <em>derives</em> that fact from an external system <d-cite key="long-etal-2024-llms,Huang_Lipovetzky_Cohn_2025"></d-cite>. Ground truth comes from execution, not from the model’s own weights.</p> <p><strong>The Mechanism: From Text to Trajectories</strong>. This perspective turns training data from static text into <em>causal trajectories</em>: records of successful interactions with the world. The pattern recurs across domains:</p> <ul> <li> <strong>Digital APIs and tools.</strong> To create fine-tuning data for a new API, the model can generate a user query, write the corresponding code or JSON call, execute it, and record the result. Each example is a clean <code class="language-plaintext highlighter-rouge">(Prompt, Code, Answer)</code> triplet where the answer is guaranteed by the tool, not by the model’s guess <d-cite key="long-etal-2024-llms,SuperAnnotateFinetuning"></d-cite>.</li> <li> <strong>Simulated physical worlds.</strong> Acting as a “director” for physics engines such as Unity or Blender, an LLM can script scenes, vary lighting, pose, and texture, and automatically collect perfectly labeled image–annotation pairs. This allows us to target rare or dangerous scenarios that are hard to observe in the real world <d-cite key="wang2024survey,chen2025symbolic"></d-cite>.</li> <li> <strong>Agent trajectories.</strong> In interactive environments, the model can act, observe the outcomes, and record (<em>State, Action, Reward</em>) traces. Successful runs can then be distilled into training data for downstream RL agents, giving them a strong warm start without long periods of random exploration <d-cite key="goldie2025synthetic,zhou2025anyprefer"></d-cite>.</li> </ul> <p>In all three cases, we are no longer training on what people happened to write. We are training on what <em>worked</em>: trajectories where a plan, encoded as symbolic code, succeeded when executed.</p> <p><strong>The Curriculum: Agentic Continual Pre-training.</strong> Once we can generate large numbers of automatic trajectories, a natural next step is to use them to continually refine the base model itself. <em>Agentic Continual Pre-training (Agentic CPT)</em> <d-cite key="su2025scaling"></d-cite> immerses an LLM in synthetic experiences that reflect the full agent loop: planning, acting, observing, and correcting. Instead of optimizing only for next-token prediction, the model is trained to internalize the <em>agentic workflow</em>:</p> <ol> <li> <strong>Multi-turn tool use.</strong> Learning that the output of Tool A (e.g., search) should be fed into Tool B (e.g., code or analysis), and that actions have consequences over multiple steps.</li> <li> <strong>Reflection and correction.</strong> When a tool call fails, the training data includes the model’s debugging and self-correction process, teaching it how to recover from mistakes <d-cite key="novikov2025alphaevolve"></d-cite>.</li> <li> <strong>Goal clarification.</strong> In ambiguous situations, successful trajectories may include the model asking clarifying questions rather than acting prematurely.</li> </ol> <p>This shifts the training signal from <em>what people say</em> to <em>what successful agents do</em>. Automatic data synthesis turns symbolic routing into a data engine.</p> <h3 id="verified-inference-the-logic-of-truth">Verified Inference: The Logic of Truth</h3> <p>Even with better data, there is a second problem at inference time: the model’s internal logic remains probabilistic. A large LLM predicts the next token, not the next true statement <d-cite key="sistla2025towards,miranda2025veribench"></d-cite>. Chain-of-Thought prompting helps us see its reasoning, but it does not guarantee that the reasoning is valid. The model can produce beautiful, step-by-step arguments that are subtly wrong. In high-stakes settings (e.g., medicine, law, mathematics), these “logical hallucinations” are unacceptable <d-cite key="huang2025survey,zhang2025siren"></d-cite>.</p> <p><strong>Theoretical Foundations: From Natural Language to Formal Logic</strong>. The final step in the neuro-symbolic story is to apply the same translation machinery used for tools to the model’s <em>reasoning itself</em>.</p> <p>The core idea is simple: let the LLM reason in natural language, but verify its reasoning in a formal system as shown in Figure 4. Concretely, we translate the model’s explanations into a symbolic language such as first-order logic or the tactic language of a proof assistant like Lean <d-cite key="Yang2023LeanDojo,lee2025symba,pan2023logic"></d-cite>.</p> <p>This idea has deep roots. Richard Montague’s work in the 1970s argued that natural language could, in principle, be given a model-theoretic semantics as rigorous as that of programming languages <d-cite key="kim2020montague,SasagawaMontagueGrammar,Janssen2017MontagueSemantics"></d-cite>. For decades, this was more philosophy than practice. Modern LLMs, however, provide the missing bridge: models such as LogicLLaMA can map messy, ambiguous English sentences into the rigid world of formal logic well enough to support automated reasoning <d-cite key="yang-etal-2024-harnessing,Brunello2024TranslatingNL,quan-etal-2024-verification"></d-cite>.</p> <figure style="text-align: center; width: 100%;"> <img src="/2026/assets/img/2026-04-27-symbolic-connect/AutoMatic.png" style="width: 70%;"> <figcaption style="font-size: 1em;">Figure 4: The verification loop. LLMs propose a natural language argument. A formalizer translates it into logic, which is then checked by a theorem prover. Feedback from the prover guides correction.</figcaption> </figure> <p><strong>The Verification Workflow.</strong> Putting this into practice suggests a verification loop:</p> <ol> <li> <strong>Generation (conjecture).</strong> The LLM solves a problem and outputs its reasoning in natural language. At this point, the explanation is treated as a <em>proposal</em>, not a certified proof.</li> <li> <strong>Formalization (translation).</strong> A specialized “formalizer” model parses the explanation and translates each step into a formal claim, such as a Lean proposition or a first-order logic formula <d-cite key="wang2025malot,quan-etal-2024-verification,toroghi-etal-2024-verifiable"></d-cite>.</li> <li> <strong>Verification (proof).</strong> These claims are handed to a theorem prover or model checker, which attempts to show that each step follows from the previous ones. The prover’s kernel acts as a mathematical gatekeeper <d-cite key="QuantumZeitgeistVerification,VeriPlan"></d-cite>.</li> <li> <strong>Feedback (correction).</strong> If a step fails, the verifier returns a concrete error (for instance, a missing assumption or a contradiction). This feedback is fed back to the LLM, which can revise its reasoning and try again <d-cite key="MoreFormalizingReasoning,toroghi-etal-2024-verifiable"></d-cite>.</li> </ol> <p>The same pattern extends beyond pure mathematics. In fact-checking, natural language claims can be translated into queries over structured knowledge bases, with symbolic systems checking consistency against trusted sources <d-cite key="ou2025holmes,COLINGLOKI"></d-cite>. In all cases, the key shift is the same: we no longer trust an answer because it <em>sounds</em> confident, but because its reasoning has been checked against a formal standard.</p> <p>Automatic data synthesis and verified inference close the loop on neuro-symbolic AI. The former turns tools and simulations into a source of reliable training data; the latter turns logic and proofs into a way to certify reasoning at test time. Together with macro- and micro-symbolism, they sketch a path toward AI systems that are not only powerful, but also grounded and trustworthy.</p> <h2 id="conclusion">Conclusion</h2> <p>The integration of LLMs with structured protocols marks a turning point in how we design intelligent systems. Instead of treating models as dense, all-purpose monoliths, we are moving toward <em>modular, routed architectures</em>—systems that plan, delegate, and compose.</p> <p>At the <strong>Macro</strong> level, this means LLMs acting as planners: translating human intent into symbolic representations, coordinating tools, and orchestrating specialized neural agents. At the <strong>Micro</strong> level, the same logic appears within the models themselves, from explicit MoE routers to the latent circuits uncovered through interpretability.</p> <p>Together, these trends open the door to two capabilities that dense models struggled to provide: <em>automatic data synthesis</em>, where data is generated through code and simulation rather than speculation, and <em>verified inference</em>, where reasoning can be checked against formal logic.</p> <p>The path forward is not about choosing between neurons and symbols. It is about building the interfaces that let them work together—systems that can plan, reason, verify, and learn in structured, interpretable ways. As routing becomes a central organizing principle, AI moves closer to something genuinely trustworthy: intelligence that can explain how it works, not just what it outputs.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-symbolic-connect.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-long-context/">Text-as-Image, A Visual Encoding Approach for Long-Context Understanding</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/using-large-language-models-to-simulate-and-predict-human-decision-making/">Using Large Language Models to Simulate and Predict Human Decision-Making</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/useful-calibrated-uncertainties/">What (and What Not) are Calibrated Uncertainties Actually Useful for?</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>