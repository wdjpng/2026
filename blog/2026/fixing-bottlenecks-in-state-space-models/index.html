<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding and Fixing Bottlenecks in State Space Models: What Recency and Over-Smoothing Tell Us | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="This work analyzes how recency bias and hidden-state over-smoothing emerge in modern State Space Models, revealing the bottlenecks that limit their ability to capture long-range dependencies."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/fixing-bottlenecks-in-state-space-models/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.theorem-box{border-right:4px solid #3b75ff;background:rgba(59,117,255,0.08);padding:1rem 1.25rem;margin:1.5rem 0;border-radius:6px}.theorem-title{font-weight:600;color:#1a4dcc;margin-bottom:.5rem;font-size:1.02rem}figure.side-by-side{display:flex;flex-direction:row;align-items:flex-start;gap:2rem;margin:2rem 0}figure.side-by-side .side-image{flex:0 0 45%}figure.side-by-side .side-image img{width:100%;border-radius:6px}figure.side-by-side figcaption{margin-top:.5rem;font-size:.9rem;color:#555}figure.side-by-side .side-text{flex:1;font-size:.95rem;line-height:1.5}@media(max-width:768px){figure.side-by-side{flex-direction:column}figure.side-by-side .side-image,figure.side-by-side .side-text{width:100%}}.highlight-block-1{background:linear-gradient(90deg,#fff9d9 0%,#fff 100%);padding:14px 18px;border-left:4px solid #e4c000;border-radius:5px;margin:18px 0;font-weight:500}.box-important-yellow{background:#fff7c2;padding:14px 18px;border-left:5px solid #f1c40f;border-radius:6px;margin:20px 0;font-weight:500}.box-important-purple{background:#f7e8ff;padding:14px 18px;border-right:5px solid #b37bd6;border-radius:6px;margin:20px 0;font-weight:500}.box-important-green{background:#e8f7e8;padding:14px 18px;border-left:5px solid #27ae60;border-radius:6px;margin:20px 0;font-weight:500}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Understanding and Fixing Bottlenecks in State Space Models: What Recency and Over-Smoothing Tell Us",
            "description": "This work analyzes how recency bias and hidden-state over-smoothing emerge in modern State Space Models, revealing the bottlenecks that limit their ability to capture long-range dependencies.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Understanding and Fixing Bottlenecks in State Space Models: What Recency and Over-Smoothing Tell Us</h1> <p>This work analyzes how recency bias and hidden-state over-smoothing emerge in modern State Space Models, revealing the bottlenecks that limit their ability to capture long-range dependencies.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#the-promise-of-ssms-long-range-memory-efficiency">The Promise of SSMs: Long-Range Memory &amp; Efficiency</a> </div> <ul> <li> <a href="#matrix-representation-of-state-space-models">Matrix Representation of State Space Models</a> </li> <li> <a href="#selection-mechanisms">Selection Mechanisms</a> </li> <li> <a href="#selection-as-a-means-of-gating">Selection as a means of Gating</a> </li> <li> <a href="#hippo-long-range-modeling-claims">Hippo:Long-Range Modeling Claims</a> </li> </ul> <div> <a href="#reality-check-ssms-are-recency-biased">Reality Check: SSMs are Recency-Biased</a> </div> <ul> <li> <a href="#hidden-problem-ssms-are-recency-biased">Hidden Problem: SSMs are Recency-Biased</a> </li> <li> <a href="#why-ssms-forget-long-range-context">Why SSMs forget Long-Range Context?</a> </li> <li> <a href="#a-natural-question-arises">A natural question arises</a> </li> </ul> <div> <a href="#evidence-ssms-fail-on-long-range-retrieval">Evidence: SSMs fail on Long-Range Retrieval</a> </div> <div> <a href="#challenges-to-model-robustness">Challenges to Model Robustness</a> </div> <div> <a href="#depth-scaling-and-its-limits">Depth Scaling and its Limits</a> </div> <div> <a href="#why-deep-ssms-start-to-fail-over-smoothing">Why Deep SSMs Start to Fail: Over-Smoothing</a> </div> </nav> </d-contents> <h2 id="the-promise-of-ssms-long-range-memory--efficiency">The Promise of SSMs: Long-Range Memory &amp; Efficiency</h2> <p><strong>Structured State Space Sequence Models (S4, DSS, S4D)</strong> represent a modern class of deep learning sequence models that share conceptual similarities with RNNs, CNNs, and classical state space models. In control systems, state-space models (SSMs) represent a system where the relationship between inputs and outputs is defined through state variables (or simply states), with the system’s behavior described by first-order differential equations governing these states <d-cite key="xiao2023introductiontransformersnlpperspective"></d-cite>.</p> <p>These models are motivated by a continuous-time system that maps a one-dimensional input sequence $x(t) \in \mathbb{R}$ to an output sequence $y(t) \in \mathbb{R}$ through an implicit latent state $h(t) \in \mathbb{R}^N$. S4 models are defined by four primary parameters $(\Delta, A, B, C)$, which govern the sequence-to-sequence transformation through two stages.</p> <p>The first stage involves the <strong>state update</strong>, where the continuous-time hidden state evolves as:</p> \[h'(t) = A h(t) + B x(t), \qquad y(t) = C h(t)\] <p>In the discretized form of the time-invariant ODE, the hidden state ( h_t ) at each time step is updated as:</p> \[h_t = \overline{A} h_{t-1} + \overline{B} x_t\] <p>The second stage reformulates the system using a <strong>convolutional form</strong>, where the system’s dynamics can be captured by a convolution kernel:</p> \[\overline{K} = \left( C \overline{B},\; C \overline{A} \overline{B},\; \dots,\; C \overline{A}^k \overline{B},\; \dots \right)\] <p>The output sequence ( y ) is then computed by convolving the input sequence ( x ) with this kernel:</p> \[y = x * \overline{K}\] <p>This convolutional formulation allows S4 models to efficiently capture long-range dependencies while maintaining computational efficiency through the structured representation of the system’s dynamics.</p> <p>In SSMs, the parameter $A \in \mathbb{R}^{N \times N}$ extracts information from the previous state $h_{t-1}$, while<br> $B : \mathbb{R} \to \mathbb{R}^N$ projects input tokens to the hidden space, and<br> $C : \mathbb{R}^N \to \mathbb{R}$ decodes the hidden state at time $t$ to produce the final output.<br> The scalar parameter $\Delta \in \mathbb{R}$ controls how much information from the new token is fused into the hidden memory.</p> <p>The first stage converts the continuous parameters $(\Delta, A, B)$ into discrete parameters $(\overline{A}, \overline{B})$ using fixed formulas:</p> \[\overline{A} = f_A(\Delta, A), \qquad \overline{B} = f_B(\Delta, A, B)\] <p>The pair $(f_A, f_B)$ is known as a <strong>discretization rule</strong>.<br> A common discretization method is the zero-order hold (ZOH) <d-cite key="10.1093/imamci/dnac005"></d-cite>, defined as:</p> <p>\(\overline{A} = \exp(\Delta A), \qquad \overline{B} = (\Delta A)^{-1}(\exp(\Delta A) - I) \cdot \Delta B\)<br></p> <p><strong>SSMs with Selection (Mamba).</strong> An effective way to incorporate a selection mechanism in models is to make parameters—such as RNN dynamics or CNN kernels—dependent on the input, allowing adaptive interactions along the sequence. The key difference is making the parameters $\Delta$, $B$, and $C$ input-dependent, with corresponding changes to tensor shapes.</p> <p>The input-dependent parameters are defined using linear transformations. Specifically, $s_B(x)$ and $s_C(x)$ are linear projections that map the input $x$ to an output space of dimension $N$. For example, we can write $s_B(x) = \mathrm{Linear}_N(x)$ and $s_C(x) = \mathrm{Linear}_N(x)$.</p> <p>For the parameter $s_{\Delta}(x)$, a two-step transformation is applied:</p> \[s_{\Delta}(x) = \mathrm{Linear}_D(\mathrm{Linear}_1(x))\] <p>where the input first passes through an intermediate linear layer before being projected to a $D$-dimensional space.</p> <p>$s_{\Delta}(x) = \mathrm{Linear}_D(\mathrm{Linear}_1(x))$</p> <p>where the input first passes through an intermediate linear layer before being projected to a $D$-dimensional space.</p> <p>To retain data dependencies without sacrificing parallelism, Mamba leverages a hardware-efficient associative scan–based algorithm that enables linear-time training while effectively modeling long-range dependencies. Notably, these parameters now have a length dimension $T$, shifting the model from time-invariant to time-varying.</p> <h3 id="matrix-representation-of-state-space-models">Matrix Representation of State Space Models</h3> <p>State Space Models (SSMs) and self-attention based mechanisms, despite their differing implementations, can be unified under a shared theoretical framework. Both classes of models can be interpreted as operating through <em>structured semiseparable matrices or matrix mixers</em>, which undergo efficient decompositions and enable long-range sequence modeling. Gu and Dao <d-cite key="dao2024transformersssmsgeneralizedmodels"></d-cite> leveraged this connection to propose the <strong>State Space Duality (SSD)</strong> framework, which formalizes the equivalence between recurrent and global (attention-like) views of sequence transformations.</p> <p>Mamba-2 establishes a connection between State Space Models (SSMs) and Transformers by demonstrating that certain SSMs can be interpreted as a specific instance of causal linear attention <d-cite key="katharopoulos2020transformersrnnsfastautoregressive"></d-cite>. This equivalence serves as a foundational component of the proposed framework, enabling a reinterpretation of various SSM computations as structured matrix multiplication algorithms. Leveraging this connection, the framework introduces new properties and more efficient algorithms for SSMs.</p> <p>In simple terms, the way selective SSMs like Mamba process input sequences—by mapping $X \in \mathbb{R}^{T \times D}$ to $Y \in \mathbb{R}^{T \times D}$—can be naturally expressed using a matrix mixer framework. Starting from the initial hidden state $h_0 = B_0 x_0$, the hidden state at each time step $h_t$ is computed recursively as a composition of input projections and recurrent $A$ matrices:</p> \[h_t = \sum_{s=0}^t A_{t:s}^\times B_s x_s\] <p>The output is then given by</p> \[y_t = \sum_{s=0}^t C_t^\top A_{t:s}^\times B_s x_s\] <p>where $C_t^\top$ is the time-dependent output projection. Collecting the outputs across all time steps yields the matrix form</p> \[y = \mathrm{SSM}(A, B, C)(x) = M(x) \cdot x\] <p>where $M$ is the mixer matrix and is represented as</p> \[M_{ji} := C_j^\top A_j \cdots A_{i+1} B_i\] <p>For scalar SSMs, the matrices $A_t$ reduce to scalars, which can be factored out of the entries:</p> \[C_t^\top A_{t:s}^\times B_s = A_{t:s}^\times \cdot (C_t^\top B_s)\] <h3 id="selection-mechanisms">Selection Mechanisms</h3> <h4 id="mamba-1">Mamba-1</h4> <p>Mamba makes the discrete parameters in the S4 algorithm vary based on the input:</p> \[h_t = s_{\overline{A}}(x_t) h_{t-1} + s_{\overline{B}}(x_t) x_t \\ y_t = s_C(x_t) h_t\] <p>The core challenge in sequence modeling lies in compressing context into a smaller state. Attention mechanisms are effective but inefficient, as they avoid compression entirely—requiring storage of the full context (e.g., KV cache), leading to quadratic training and linear inference costs. Recurrent models, while efficient with constant-time inference and linear-time training, struggle with compressing context effectively into a finite state. Static recurrent updates also fail to adapt their hidden state dynamically based on input, while fixed convolution kernels struggle with tasks requiring context awareness.</p> <p>This tradeoff underscores the need for sequence models to balance efficiency (small states) with effectiveness (context-aware states). A key principle for achieving this is <em>selectivity</em>: the ability to focus on relevant inputs while filtering out irrelevant ones during state propagation. The selection mechanism in Mamba makes parameters that influence sequence interactions dependent on the input, modifying the structure to adapt dynamically to context across a sequence.</p> <p>This mechanism can be compared to the update gate in GRUs, balancing the influence of past hidden states and the current input <d-cite key="de2024griffinmixinggatedlinear"></d-cite>. By selectively retaining or discarding information, it enables the hidden state to reset when necessary—similar to the forget gate in LSTMs—thereby enhancing the model’s ability to manage long-range dependencies efficiently.</p> <h4 id="mamba-2">Mamba-2</h4> <p>In Mamba-2, the SSD (Selective SSM) layer introduces a further simplification by constraining the matrix $A$ to a scaled identity form. That is, all diagonal entries of $A$ are required to be equal at each time step, such that $A_t$ becomes a scalar multiple of the identity matrix. Consequently, $A$ can be compactly represented as a tensor of shape $(T)$, with each element $a_t$ denoting the scalar coefficient at time step $t$. This structural constraint reduces both the number of parameters and the computational overhead, while retaining the expressive power needed for sequence modeling.</p> <p>Gu and Dao <d-cite key="dao2024transformersssmsgeneralizedmodels"></d-cite> defined a matrix $M$ to encode a sequence-to-sequence transformation mapping where a one-dimensional input vector $x \in \mathbb{R}^T$ is transformed into a one-dimensional output vector $y \in \mathbb{R}^T$ through matrix multiplication $y = Mx$. Here, $T$ denotes the sequence length, and $M = L \circ C B^\top \in \mathbb{R}^{T \times T}$.</p> <p>To ensure computational efficiency, <em>structured SSMs</em> impose specific constraints on the transition matrix $A$. A common and effective structure is to assume $A$ is diagonal <d-cite key="gu2022efficientlymodelinglongsequences,gupta2022diagonalstatespaceseffective,gupta2023simplifyingunderstandingstatespace,smith2023simplifiedstatespacelayers"></d-cite>, in which case only the diagonal entries of each $N \times N$ matrix are stored. This reduces the shape of $A$ to $(T, N)$, significantly lowering both memory and compute requirements, where $T$ is the sequence or time dimension.</p> <p>Let the input sequence be $Z \in \mathbb{R}^{T \times D}$, where $T$ is the number of tokens and $D$ is the feature (or channel) dimension. The model produces an output $O \in \mathbb{R}^{T \times D}$ while maintaining an internal hidden state of size $H$.</p> <p>To begin, the model computes three intermediate projections $B, C, V \in \mathbb{R}^{T \times H}$ using functions $g_K$, $g_Q$, and $g_U$, which apply input-dependent transformations. These typically consist of linear projections along the feature dimension, short convolutions across the sequence axis, and nonlinear activations such as Swish <d-cite key="ramachandran2017searchingactivationfunctions"></d-cite>.</p> <p>These matrices can be interpreted as analogs to the key, query, and value components used in attention <d-cite key="vaswani2023attentionneed"></d-cite>. For each output channel $j$, the model maintains a hidden state vector $s^j_t$ and updates it at each time step via the recurrence:</p> \[s^j_t = \alpha_t \cdot s^j_{t-1} + \beta_t \cdot B_t \cdot v^j_t\] <p>where $v^j = V_{:,j} \in \mathbb{R}^H$ and $v^j_t = v^j[t]$. Let $B_t = B[t, :]$ and $C_t = C[t, :]$ denote the $t$-th rows of $B$ and $C$, respectively. The output is then computed as:</p> \[o^j_t = C_t^\top s^j_t, \quad o^j_t = O[t, j]\] <p>In models such as Linear Attention <d-cite key="katharopoulos2020transformersrnnsfastautoregressive"></d-cite>, the coefficients are typically fixed with $\alpha_t = 1$ and $\beta_t = 1$. RetNet <d-cite key="sun2023retentivenetworksuccessortransformer"></d-cite> introduces a learned decay parameter $\gamma$ to modulate the hidden state updates, using $\alpha_t = \gamma$ and $\beta_t = 1$.</p> <p>Mamba-2 extends this approach by making both coefficients data-dependent through a selectivity matrix $\delta = g_\delta(Z) \in \mathbb{R}^T$, allowing the model to dynamically control the flow of information. Specifically, the coefficients are defined as $\alpha_t = \exp(-\delta_t)$ and $\beta_t = \delta_t$, enabling context-aware adaptation of the update and retention behavior for each token. This gating mechanism dynamically adjusts the balance between information retention and update based on context, enabling the model to prioritize important tokens and effectively capture long-range dependencies.</p> <h3 id="selection-as-a-means-of-gating">Selection as a means of Gating</h3> <p>The most important connection is that the classical gating mechanism of RNNs is an instance of the selection mechanism used in SSMs. Originally, gating mechanisms were introduced in recurrent neural networks (RNNs), such as LSTMs and GRUs <d-cite key="chung2014empiricalevaluationgatedrecurrent"></d-cite>, to regulate the flow of information into the hidden state. These mechanisms controlled how signals propagated over time, enabling the model to capture sequential dependencies.</p> <p>However, the modern interpretation of gating has broadened to encompass any multiplicative interaction, often modulated by an activation function. Formally, the gating function is defined as</p> \[g_t = \sigma(\mathrm{Linear}(x_t)),\] <p>which governs the hidden state update:</p> \[h_t = (1 - g_t) h_{t-1} + g_t x_t.\] <p>This formulation enables the input $x_t$ to dynamically control the trade-off between integrating new information and preserving past context through the gating parameter $g_t$. Because $g_t$ is constrained between 0 and 1, the model can suppress irrelevant context when necessary, improving the efficiency of information propagation. This selective updating enhances the model’s ability to extract meaningful dependencies over long sequences while reducing the impact of extraneous information. As a result, gating provides a context-aware alternative to sparsified graph attention, retaining only the most essential dependencies across long-range contexts.</p> <p>The selection mechanism introduced in Mamba SSMs <d-cite key="gu2024mambalineartimesequencemodeling"></d-cite> functions similarly to the update gate in GRUs, blending the previous hidden state with the current input $x_t$. This allows the model to reset its hidden state and discard outdated information when necessary—akin to the forget gate in LSTMs—while maintaining computational efficiency and long-range modeling capability.</p> <div class="theorem-box"> However, <span class="theorem-title">Theorem 3.1</span> from <d-cite key="wang2025understandingmitigatingbottlenecksstate"></d-cite> shows that despite incorporating advanced selection mechanisms, SSMs such as Mamba still suffer from a strong recency bias. Moreover, <span class="theorem-title">Theorem 4.2</span> in the same work extends directly to Mamba, indicating that its “selective” state-space updates do not make it more expressive at signal filtering than the classical linear S4 model. In practice, both architectures behave predominantly as low-pass filters. We will examine both theorems in detail in a later section. </div> <h3 id="hippolong-range-modeling-claims">Hippo:Long-Range Modeling Claims</h3> <p>State Space Models (SSMs) were originally introduced as a principled framework for modeling long-range dependencies in sequential data. Their modern formulation is grounded in the <strong>HiPPO (High-Order Polynomial Projection Operators)</strong> framework introduced by Gu et al. <d-cite key="gu2020hipporecurrentmemoryoptimal"></d-cite>, which shows that a simple first-order ODE can maintain a compressed and continuously updated representation of the entire input history.</p> <p>HiPPO defines a hidden state $h(t)$ evolving under the linear differential equation:</p> \[\frac{d}{dt} h(t) = A\,h(t) + B\,x(t),\] <p>where $x(t)$ is the incoming signal and $A$ is the HiPPO matrix that governs memory dynamics. The key insight is that $h(t)$ serves as the coefficients of an <em>online polynomial projection</em> of the past signal. Formally, HiPPO computes:</p> \[h(t) \approx \arg\min_{y \in \mathrm{Poly}} \| x_{(-\infty,\, t]} - y \|_{L^2(\omega_t)},\] <p>for a time-varying weighting measure $\omega_t$, allowing the model to maintain the best $L^2$ approximation of the input history. This enables efficient long-term memory representation.</p> <p>The original HiPPO matrix, such as HiPPO-LegS, is dense and upper triangular:</p> \[A_{ij} = \begin{cases} 2i + 1, &amp; i = j, \\ -(2i + 1), &amp; i &gt; j, \\ 0, &amp; i &lt; j, \end{cases}\] <p>which is theoretically elegant but computationally expensive for deep learning applications.</p> <p>Subsequent works <d-cite key="gu2021combiningrecurrentconvolutionalcontinuoustime,gu2022parameterizationinitializationdiagonalstate,gupta2022diagonalstatespaceseffective"></d-cite> provided major simplifications by showing that these dense HiPPO matrices can be approximated using <em>diagonal</em> or <em>diagonal-plus-low-rank</em> structures:</p> \[A \approx D + L,\] <p>where $D$ is diagonal and $L$ is low rank. This dramatically improves computational efficiency while preserving the ability to model long-range dependencies.</p> <p>These insights directly enabled the development of modern SSM-based architectures such as <strong>S4</strong> <d-cite key="gu2022efficientlymodelinglongsequences"></d-cite> and <strong>Mamba</strong> <d-cite key="gu2024mambalineartimesequencemodeling"></d-cite>, which use HiPPO-inspired parameterizations to construct fast, expressive sequence models. By leveraging diagonal or structured versions of the HiPPO matrix, these models achieve scalable and effective context filtering, often outperforming attention-based approaches on tasks requiring long-range memory.</p> <h2 id="reality-check-ssms-are-recency-biased">Reality Check: SSMs are Recency-Biased</h2> <h3 id="hidden-problem-ssms-are-recency-biased">Hidden Problem: SSMs are Recency-Biased</h3> <p>In their paper <em>Understanding and Mitigating Bottlenecks of State Space Models Through the Lens of Recency and Over-Smoothing</em> <d-cite key="wang2025understandingmitigatingbottlenecksstate"></d-cite>, the authors argue that although Structured State Space Models (SSMs) have been widely promoted as strong alternatives to Transformers—especially for long-sequence modeling—their behavior is more limited than commonly assumed. They show that SSMs exhibit a pronounced <em>recency bias</em>, meaning they rely heavily on the most recent tokens and rapidly lose access to information from farther in the past. According to their experiments, this bias not only weakens long-range recall but also creates potential robustness issues, since local perturbations can disproportionately affect the output.</p> <p>To investigate whether this limitation can be reduced through scaling, the authors explore deeper SSM architectures. They find that increasing depth does help the model access longer contexts, but only up to a point. Beyond that, deeper SSMs begin to suffer from <em>over-smoothing</em>, where token representations become increasingly similar as they propagate through more layers. This causes the model to lose discriminative power, offsetting the benefits of added depth.</p> <p>Overall, the paper highlights a fundamental trade-off: shallow SSMs tend to forget long-range information due to recency bias, while deeper models become over-smoothed and struggle to maintain meaningful token distinctions.</p> <h3 id="why-ssms-forget-long-range-context">Why SSMs forget Long-Range Context?</h3> <p>Although transformers may appear better suited for long-range tasks, the authors show theoretically that an SSM layer is inherently biased toward recent tokens and loses long-term memory exponentially. They also provide empirical validation that SSMs struggle to retrieve information from distant context. Furthermore, they show that strong local bias can negatively impact robustness, since perturbations in recent tokens disproportionately affect the output.</p> <p>To analyze how information propagates through State Space Models (SSMs) and how these models capture long-range dependencies, the authors examine how the output at time step $t$ depends on an earlier input token at time $s \le t$. They quantify this via the influence score:</p> \[\left|\frac{\partial y_t}{\partial x_s}\right|,\] <p>which measures the impact of the $s$-th input token on the $t$-th output token. A larger value indicates stronger influence; a smaller value indicates weaker contribution.</p> <p>The authors present a theorem (Theorem 3.1) demonstrating that State Space Models—including S4 and Mamba—exhibit an inherent <strong>recency bias</strong>. The theorem assumes that the SSM parameters are continuous and differentiable, and that each state transition matrix $A_t$ has diagonal entries strictly between $0$ and $1$, a condition satisfied by many modern SSMs.</p> <p>Under these assumptions, the influence between two tokens in an SSM decays <em>exponentially</em> with their relative distance $(t - s)$. The decay rate is controlled by the largest diagonal entry among the state matrices:</p> \[A_{\max} = \max_{t \in [T], n \in [N]} (A_t)_{n,n}.\] <p>Formally, let the SSM be parameterized by the sequence ${(A_t, b_t, c_t, \Delta_t)}_{t \in [T]}$, and assume:</p> <ol> <li>the input space $X \subset \mathbb{R}^T$ is compact,</li> <li>the parameters are continuous and continuously differentiable,</li> <li>each $A_t \in (0,1)^{N \times N}$ is diagonal.</li> </ol> <p>Then for any $x \in X$ and indices $s &lt; t$, the influence score satisfies:</p> \[\left|\frac{\partial y_t}{\partial x_s}\right| = O\!\left(\exp(-\kappa (t - s))\right), \qquad \kappa = \Theta\!\left(\log(A_{\max}^{-1})\right).\] <p>The expression above implies that the influence of the input token at position $s$ on the output at position $t$ decreases exponentially as the distance between them grows. If the transition matrices $A_t$ are “small” (all entries $&lt; 1$), then the hidden state is repeatedly multiplied by values less than 1. Consequently, the influence of early inputs decays exponentially and is rapidly forgotten.</p> <p>Consider a simple SSM recurrence:</p> \[h_t = 0.8\, h_{t-1} + b x_t, \qquad y_t = c h_t.\] <p>Here the transition coefficient is $0.8$. The influence of an early input $x_s$ on the output at time $t$ is:</p> \[\frac{\partial y_t}{\partial x_s} \approx 0.8^{\, (t - s)}.\] <p><strong>Example</strong><br> Let $s = 1$. How does $x_1$ influence $y_t$?</p> \[\begin{aligned} t = 2: &amp;\quad 0.8^{1} = 0.8, \\ t = 4: &amp;\quad 0.8^{3} = 0.512, \\ t = 8: &amp;\quad 0.8^{7} \approx 0.21, \\ t = 20: &amp;\quad 0.8^{19} \approx 0.014. \end{aligned}\] <p>Even with a relatively large retention factor (0.8), the influence from early inputs decays to nearly zero after only a few dozen steps—illustrating the inherent recency bias of SSMs.</p> <p><strong>Interpretation</strong><br> At $t = 8$, most of the influence is already gone.<br> At $t = 20$, the influence of the earlier input is essentially zero.</p> <p>This matches the exponential decay form:</p> \[\exp(-\kappa (t - s)),\] <p>where</p> \[\kappa = -\log(0.8) \approx 0.22.\] <figure class="side-by-side"> <div class="side-image"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-fixing-bottlenecks-in-state-space-models/figure1-480.webp 480w,/2026/assets/img/2026-04-27-fixing-bottlenecks-in-state-space-models/figure1-800.webp 800w,/2026/assets/img/2026-04-27-fixing-bottlenecks-in-state-space-models/figure1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-fixing-bottlenecks-in-state-space-models/figure1.png" class="img-fluid rounded" width="100%" height="auto" alt="Logarithmic influence scores" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption> Figure 1: Logarithmic influence scores plotted against relative token distance. The linear decay illustrates the inherent recency bias in SSMs <d-cite key="wang2025understandingmitigatingbottlenecksstate"></d-cite>. </figcaption> </div> <div class="side-text"> <p> The authors further validate their theory by plotting the logarithm of influence scores against the relative distance between tokens. Across a wide range of model sizes, Mamba consistently exhibits a linear decay—both at initialization and during training. </p> <p> This pattern shows that the recency behavior is not merely learned from data statistics; it reflects an intrinsic architectural bias predicted by the theorem above. </p> <p> Moreover, commonly used initialization schemes amplify this locality bias, causing distant tokens to have even less effect on the output. </p> </div> </figure> <h3 id="a-natural-question-arises">A natural question arises</h3> <p><strong>Is the built-in decay of long-range dependencies in SSMs a desirable property, or simply an artifact of the model design that may limit performance in certain settings?</strong></p> <p>A key insight from the authors’ analysis is that the way modern State Space Models (SSMs) parameterize their transition matrices $A_t$ has major consequences. By constraining each element of $A_t$ to lie in $(0,1)$, these models enforce a built-in decay of information over time: the influence of past tokens automatically weakens as their distance from the current position increases. This design choice is intentional. Many recent SSM architectures—such as Mamba and its variants—deliberately adopt this parameterization because it provides stable dynamics, efficient long-sequence processing, and controlled memory behavior. This built-in decay serves several purposes:</p> <p><strong>Why does the interval $(0,1)$ cause decay?</strong><br> If each entry of the transition matrix $A_t$ lies in the interval $(0,1)$, then the hidden state update</p> \[h_t = A_t h_{t-1} + b_t x_t\] <p>induces an exponentially vanishing influence from earlier tokens. Repeated application of the recurrence gives</p> \[h_t \approx A_t A_{t-1} \cdots A_s \, h_s,\] <p>and since multiplying numbers less than $1$ causes the result to shrink, the influence of earlier tokens decays exponentially with their distance.</p> <p><strong>Example</strong><br> Consider a simple scalar case where $A_t \approx 0.9$. Then the influence decays as follows:</p> <ul> <li>After 1 step: $0.9$</li> <li>After 5 steps: $0.9^5 = 0.59$</li> <li>After 20 steps: $0.9^{20} \approx 0.12$</li> </ul> <p>Thus, as tokens become more distant, their contribution to the model’s hidden state diminishes rapidly.</p> <h2 id="evidence-ssms-fail-on-long-range-retrieval">Evidence: SSMs fail on Long-Range Retrieval</h2> <p>To assess the ability of SSMs to capture long-context information, the authors evaluate open-source SSM models using the <strong>Needle in a Haystack</strong> benchmark and compare their performance to transformer-based variants.</p> <p><strong>Goal</strong></p> <p>Measure how strongly large language models (LLMs) rely on positional cues in context and whether they truly use information presented in the context window (rather than memorized facts). The benchmark embeds a short, intentionally false factual statement into a long document at different positions and tests whether an LLM can retrieve that statement when asked. In simple words: in this benchmark, a short statement is hidden inside the middle of a long document, and the AI model is asked to find and use that information.</p> <p>The goal is to test whether the model actually reads and understands the text, rather than relying on information it memorized during training. The hidden statement is designed to look natural but contains a deliberate factual error. This forces the model to depend only on what appears in the document itself. If the model answers correctly, it shows that it truly located and used the information from the text.</p> <p>To better understand how the model handles long content, the position of the hidden sentence is changed across different experiments. Sometimes it appears near the beginning of the document, sometimes in the middle, and sometimes toward the end. The model’s accuracy at each position is then measured. This process reveals whether the model has a positional bias — for example, whether it pays more attention to the start of a document than to the end.</p> <p>Strong performance across all positions indicates that the model can effectively use long-range context. Authors show that <strong>Mistral-7B</strong> (Transformer-based), performs consistently no matter where the hidden sentence appears in the document. This means it can pay attention to information at the beginning, middle, or end equally well. Further, authors point out that <strong>Mamba-Codestral-7B</strong> (based on State Space Models / SSMs) behaves differently. It performs better when the hidden sentence is closer to the end of the document and worse when the sentence appears near the beginning.</p> <p><strong>This pattern shows that the SSM-based model has a positional bias, meaning it naturally focuses more on recent or nearby tokens rather than information that appeared far earlier in the text. In other words, it <em>remembers</em> newer information more strongly than older information.</strong></p> <p>The figure <d-fig key="figure2">2</d-fig> below compares how two different types of language models retrieve information from very long documents.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-fixing-bottlenecks-in-state-space-models/figure2-480.webp 480w,/2026/assets/img/2026-04-27-fixing-bottlenecks-in-state-space-models/figure2-800.webp 800w,/2026/assets/img/2026-04-27-fixing-bottlenecks-in-state-space-models/figure2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-fixing-bottlenecks-in-state-space-models/figure2.png" width="100%" height="auto" alt="Influence heatmaps" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 2: Results from Needle in a Haystack Experiment <d-cite key="wang2025understandingmitigatingbottlenecksstate"></d-cite>.</figcaption> </figure> <p>The left heatmap represents an SSM-based model (Mamba-Codestral-7B). Warmer colors (red/orange) indicate lower accuracy, while greener colors indicate higher accuracy. A clear pattern appears: when the hidden sentence is placed near the beginning of the document, the model performs poorly. As the sentence moves closer to the end of the document, the model becomes much more accurate. This shows that the SSM model is biased toward recent information and struggles to recall content from far earlier in the text.</p> <p>The right heatmap represents a Transformer-based model (Mistral-7B). Unlike the SSM model, the colors remain mostly uniform across the entire heatmap. This means the model’s accuracy stays consistent no matter where the hidden sentence is placed. In other words, the Transformer does not show a strong positional bias and can retrieve information equally well from the beginning, middle, or end of the document.</p> <h2 id="challenges-to-model-robustness">Challenges to Model Robustness</h2> <p>The authors also investigated potential robustness issues arising from recency bias in State Space Models (SSMs) by evaluating them on an image classification task designed in an unconventional sequence-based setting. Instead of treating images like 2D grids, they flattened each image into a long sequence of pixel values—like turning a picture into a long list of numbers. This allows them to use sequence models (which are usually used for text or time-series data) to process images. They tested several popular SSM-based models such as H3, RWKV, and Mamba, and compared their performance with a Transformer model (the standard architecture used in models like GPT). To make these models work for classification, they added a special learnable <em>class token</em> at the very end of the pixel sequence. This token acts like a summary of everything the model has seen. After the model processes the full sequence, it looks at the final state of this class token and passes it through a classifier head to produce the final prediction (for example, deciding which CIFAR-10 class the image belongs to).</p> <p>The goal of this experiment is to check whether State Space Models (SSMs) are more sensitive to noise at the end of a sequence than at the beginning. The authors took images from the CIFAR-10 dataset and converted each image into a sequence of 1,024 tokens (by flattening the pixels). Then, they deliberately added random noise to different parts of the sequence to see how much the model’s accuracy dropped. The authors evaluate positional sensitivity by introducing controlled corruption to different regions of the input sequence.</p> <p><strong>Trailing corruption</strong> involves adding random noise to the last segment of the sequence, located near the appended class token, while <strong>leading corruption</strong> involves perturbing the initial segment of the sequence.</p> <p>The experimental setup is summarized as follows:</p> <ul> <li>Two corruption levels are evaluated: a mild setting with 32 out of 1024 tokens corrupted, and a more aggressive setting with 96 out of 1024 tokens corrupted.</li> <li>Random noise is injected into either the leading (beginning) or trailing (end) segments of the input sequence.</li> <li>These experiments are designed to measure whether the models depend more strongly on information located near the end of the sequence than at the beginning.</li> </ul> <div class="box-important-yellow"> These results demonstrate that state space models (SSMs) such as H3, RWKV, and Mamba exhibit a strong recency bias, relying heavily on the most recent tokens in the input sequence. In contrast, transformer-based models show more balanced sensitivity to perturbations across the full sequence. </div> <div class="box-important-purple"> The experiments show that corrupting the trailing tokens harms State Space Models (SSMs) much more than corrupting the leading tokens, revealing a strong recency bias. Mamba exhibits the most extreme behavior: corrupting the last 32 of 1024 tokens causes an 81.24% accuracy drop, while corrupting the first 32 tokens reduces accuracy by only 2.30%. In contrast, Transformers are less sensitive to trailing corruption and appear to rely more on early-sequence information, consistent with prior findings. </div> <div class="box-important-green"> Targeted attacks reveal a serious weakness in State Space Models (SSMs). When the last part of an input sequence is replaced with tokens from a target class, SSMs are easily fooled into misclassifying the input. This happens because these models rely heavily on recent tokens. In contrast, Transformer models are more robust and are not disproportionately affected by attacks on the end of the sequence. </div> <h2 id="depth-scaling-and-its-limits">Depth Scaling and its Limits</h2> <p>State Space Models (SSMs), including Mamba, exhibit exponentially decaying dependencies with respect to token distance, effectively behaving as localized operators with finite receptive fields, analogous to convolutional and graph-based architectures. To study whether architectural depth compensates for this locality, models were pretrained under varying context lengths (2048 and 8192 tokens) and depths (16–72 layers). The results indicate that increasing depth improves performance under longer context settings, suggesting an expansion of the effective receptive field.</p> <p>However, these gains saturate beyond intermediate depths (approximately 32–48 layers), after which further increases lead to deteriorating performance, as evidenced by rising validation perplexity. Short-context models exhibit sharper degradation at high depth, whereas long-context models demonstrate greater tolerance to deeper architectures. Overall, these findings suggest that increased depth partially alleviates, but does not eliminate, the limitations imposed by the inherently local dynamics of SSMs.</p> <h2 id="why-deep-ssms-start-to-fail-over-smoothing">Why Deep SSMs Start to Fail: Over-Smoothing</h2> <p>The paper further investigates the depth-scaling limitations of State Space Models (SSMs) by analyzing the evolution of hidden states and token features across layers. A central finding is that deep SSMs exhibit over-smoothing, a phenomenon where token embeddings become increasingly similar and lose discriminative power. The analysis begins with the continuous-time S4 model, which is formulated as a linear dynamical system governed by ordinary differential equations. Using the known equivalence between S4 and convolutional operators <d-cite key="gu2021combiningrecurrentconvolutionalcontinuoustime"></d-cite>, the authors show that S4 behaves as a low-pass filter in the frequency domain when the system matrix has negative diagonal entries. This implies that high-frequency (sharp, local) signal components are systematically attenuated at each layer, independent of how the parameters are trained.</p> <p>The degree of over-smoothing is shown to depend on both context length and the smallest state transition coefficients. Longer contexts require more steps for information to mix across positions, while transition parameters approaching one cause the model to behave like a uniform averaging operator over the sequence. This aligns with an intuitive interpretation of SSMs as performing a form of running average over token embeddings.</p> <p><strong>The authors provide empirical validation using a 1.4B-parameter Mamba model. They quantify representation sharpness via pairwise distances between token embeddings and observe that sharpness consistently decreases across layers. Compared with Transformers of comparable size, SSMs exhibit a much faster decay of feature diversity, although Transformers are also theoretically susceptible to over-smoothing.</strong></p> <p><strong>Intuition Behind Theorem 4.2 (Over-Smoothing in SSMs): A simple way to understand the over-smoothing effect in SSMs is to view each layer as a contractive update. If the recurrent coefficient satisfies $A_t \leq 1$, then differences between hidden states shrink over time. For example, when $A_t = 0.9$ and the input sequence is short, even inputs that differ significantly (e.g., by 2 units) produce hidden states whose differences are tightly bounded (e.g., $\approx 0.54$). As the sequence length increases, this contraction becomes stronger, forcing token representations to become increasingly similar. This explains why stacking many SSM layers causes the model to behave like a running low-pass filter, progressively removing high-frequency (sharp) features and leading to over-smoothing.</strong></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-fixing-bottlenecks-in-state-space-models.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-vlms-waste-their-vision/">Why vlms waste their vision</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>