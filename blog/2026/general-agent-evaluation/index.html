<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Ready For General Agents? Let's Test It. | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="General-purpose agents are emerging, but current evaluation tools cannot yet measure how well they adapt to unfamiliar environments or protocols; we outline the gaps and a path to a protocol-agnostic framework."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/general-agent-evaluation/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Ready For General Agents? Let's Test It.",
            "description": "General-purpose agents are emerging, but current evaluation tools cannot yet measure how well they adapt to unfamiliar environments or protocols; we outline the gaps and a path to a protocol-agnostic framework.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Ready For General Agents? Let's Test It.</h1> <p>General-purpose agents are emerging, but current evaluation tools cannot yet measure how well they adapt to unfamiliar environments or protocols; we outline the gaps and a path to a protocol-agnostic framework.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#abstract">Abstract</a> </div> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#preliminary">Preliminary</a> </div> <div> <a href="#the-shift-to-general-agents">The Shift to General Agents</a> </div> <div> <a href="#the-promise-of-general-agents">The Promise of General Agents</a> </div> <div> <a href="#state-of-general-agent-evaluation">State of General Agent Evaluation</a> </div> <div> <a href="#challenges-in-general-agent-evaluation">Challenges in General Agent Evaluation</a> </div> <div> <a href="#existing-agent-environment-protocols">Existing Agent-Environment Protocols</a> </div> <div> <a href="#general-agent-evaluation-framework">General Agent Evaluation Framework</a> </div> <div> <a href="#conclusions">Conclusions</a> </div> </nav> </d-contents> <h2 id="abstract">Abstract</h2> <p>Recent progress in LLMs has pushed the field from domain-specific systems toward increasingly general-purpose models. A similar shift is emerging for AI agents: domain agents share reusable components and can already operate across multiple domains with minimal adaptation. This ability to integrate into new environments and solve entirely new classes of tasks gives general agents the potential for effectively unbounded real-world value. Yet current evaluation tools cannot measure this core capability. We organize existing work into a five-level taxonomy and identify the missing fifth level: general agent evaluation, which must assess how well an agent operates across many unfamiliar environments. We outline the challenges that prevent such evaluation today and propose the requirements for a protocol-agnostic framework that can reliably measure the generality and adaptability of emerging agent systems.</p> <h2 id="introduction">Introduction</h2> <p>Over the past few years, the NLP community has shifted from building domain-specific systems such as standalone summarization or translation models toward developing general-purpose language models <d-cite key="brown2020languagemodelsfewshotlearners"></d-cite><d-cite key="bommasani2022opportunitiesrisksfoundationmodels"></d-cite>. Many see this trend as a contemporary example of Richard Sutton’s bitter lesson <d-cite key="sutton2019bitter"></d-cite>: “The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.”</p> <p>This transition was not abrupt. It emerged from a long sequence of incremental advances that gradually expanded the scope and capability of models. Along the way, domain-specific solutions and increasingly general methods coexisted, each informing and accelerating the other.</p> <p>In this blog post, we argue that a similar shift is now unfolding in the field of AI agents: the field is moving from domain-specialized agents toward increasingly general-purpose ones. Agents that can address diverse types of multi-step tasks across different target domains and previously unseen environments.</p> <p>This development highlights the need for a unified evaluation framework for general-purpose agents that assesses their abilities across environments and compares different architectures. Such a framework is crucial for tracking progress, identifying gaps, and guiding the development of next-generation general agents. It is also essential for evaluating the core of generality itself: an agent’s ability to integrate into new environments and perform successfully.</p> <p>We begin by introducing a shared terminology for discussing domain and general agents, their evaluation, and the agent-to-environment communication protocol. Next, we describe how today’s domain agents are evolving toward greater generality and outline the effect we expect it to have on general agents. We then explore the benefits of general-purpose agents and illustrate their advantages through two representative use cases. This motivates the need for evaluation solutions that can assess general agent capabilities. For that end, we survey the current landscape of agent evaluation, presenting a five-level taxonomy, and detail the limitations that make existing approaches insufficient for easily evaluating general agents. We then assess whether existing agentic protocols can address these limitations. Finally, we outline key requirements that a general agent solution needs to fulfill.</p> <p>We hope this blog will help clarify what general agents are, increase awareness of their emergence from domain-specific agents, and highlight the gaps in their evaluation. More broadly, we aim for it to serve as a call to action that inspires a community-wide effort to develop rigorous, scalable, and actionable evaluation frameworks. We believe that as agents become more general and autonomous, such frameworks are essential to guide their progress.</p> <h2 id="preliminary">Preliminary</h2> <p>AI agents are autonomous, goal-driven systems that perform multi-step tasks by interacting with their environment <d-cite key="bandi2025rise"></d-cite>. The environment is the “world” the agent is situated in. The agent can observe and interact with the environment, obtaining observations according to its internal mechanism <d-cite key="cheng2024exploringlargelanguagemodel"></d-cite>. An agent deployed in the environment can interact with it to achieve its goal. Many of those terms were adopted and adjusted from the domain of reinforcement learning to the field of LLM and AI agents.</p> <p>AI agents are systems composed of interacting algorithmic components for reasoning, planning, memory preservation, code execution, and more. The orchestration of these components, often referred to as the agent’s architecture or scaffold, collectively determines the agent’s behavior. A large language model typically serves as the central computational element, providing core capabilities for perception, reasoning, and generation. The agent can also have access to external capabilities like code execution and search.</p> <p>Another important concept is the agent-to-environment communication protocol. This protocol interface describes the way the agent interacts with its environment. Main examples are web browsing, terminal, MCP, and tool schemas.</p> <p>Most current agents are being developed with a specific domain in mind <d-cite key="Wang_2024"></d-cite><d-cite key="yehudai2025surveyevaluationllmbasedagents"></d-cite>. Common examples are web agents and software engineering agents (SWE agents). In such cases, the agent is restricted to a relevant set of components and tools, and the environment is tailored to the target domain.</p> <p>To evaluate the capabilities of different domain agents, researchers defined domain-specific benchmarks. Such benchmarks require an environment in which the agent can operate, a set of tasks that describe the agent’s goal, and a metric that measures whether the agent has achieved its task. These benchmarks enable the assessment of the efficacy of domain agents and the comparison of different backbone LLMs.</p> <p>In contrast to these types of agents, general-purpose agents are designed to handle a diverse set of tasks across different environments. This requires them to be adaptable to different kinds of previously unseen environments, each with its own tools, requirements, and specific setup.</p> <p>In essence, general agents are defined by their capacity to integrate into new problem spaces, absorb their domain knowledge, refine their behavior through interaction, and ultimately master the tasks they encounter.</p> <h2 id="the-shift-to-general-agents">The Shift to General Agents</h2> <p>Large language models are general-purpose systems; they are designed to handle diverse tasks. Yet they have a fixed static knowledge of the world and can only interact with it by producing text. To overcome this challenge, researchers advise utilizing tools that allow LLMs to interact with the world (for example searching the web or running code). To further advance this ability, researchers also develop designed patterns such as ReAct <d-cite key="yao2023reactsynergizingreasoningacting"></d-cite> and CodeAct <d-cite key="wang2024executablecodeactionselicit"></d-cite> that facilitate a loop of interaction between the LLM and the environment. Such simple agents can be deployed in any environment to achieve multi-step tasks, making them early versions of general agents.</p> <p>Although these agents are simple, equipping them with the right tools can provide an effective solution. For example, many providers now recommend loop-based agents or agent frameworks as a standard pattern for application development. Even massively used LLM interfaces for both consumer and developer have quietly evolved from a conversation with an LLM to an interaction with an agent equipped with tools for coding and searching.</p> <p>On the other hand, such agents fall short when compared to more complex and specialized agents on target domains. Domain agents utilize more structure; they tend to have components for planning, memory, state tracking, tool use, and error handling to support reliable, iterative interaction with the domain environment. They top domain-specific leaderboards and provide real-world value. For example, SWE agents are already solving millions of GitHub issues without intervention <d-cite key="PRArena"></d-cite>, and deep research agents are being deployed to millions of users <d-cite key="McKay2025_OpenAIDeepResearch"></d-cite>.</p> <p>While different domain agents are by design different from one another, they share similar components. If we look at different SWE agents, such as Claude Code, Codex CLI, and different deep research agents such as OpenAI and Perplexity ones, they all share similar algorithmic components <d-cite key="bgauryy_open-docs_2025"></d-cite><d-cite key="langchain_ai_open_deep_research_2025"></d-cite>. Moreover, each of those components is not specific to its domain but is a general component that could be used for any target domain. As a result, such domain agents that rely on general components can be easily adopted to other domains and tasks. Anthropic wrote: “Over the past several months, Claude Code has become far more than a coding tool. At Anthropic, we’ve been using it for deep research, video creation, and note-taking, among countless other non-coding applications. In fact, it has begun to power almost all of our major agent loops.” Additionally, they released their Claude Agent SDK to serve as building blocks for developing other agents<sup id="fnref:claude-sdk"><a href="#fn:claude-sdk" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. This demonstrates that domain-specific agents are becoming more proficient and general, allowing them to target a wider set of tasks. It also shows the new type of general agents, composed of plug-and-play general components that can be applied to a diverse set of domains across different environments.</p> <h2 id="the-promise-of-general-agents">The Promise of General Agents</h2> <p>In all machine learning tasks, simple and effective solutions are better than their specialized counterparts. Such solutions generalize better, are more robust, and are less prone to overfitting <d-cite key="shalev2014understanding"></d-cite><d-cite key="haussler1990andrzej"></d-cite>. Unlike domain agents that can be over-specialized and tailored to a specific task or domain, general agents work with different environments, forcing them to generalize. They receive diverse signals from a wide range of environments, requiring them to be robust. As a result, they tend to be more cost-effective.</p> <p>We examine two concrete examples of SWE and scientific agents that demonstrate that even simple versions of general agents have a lower cost of development and are more cost-effective. To quantify these qualities, we measure lines of code (LOC) and agent average cost per task.</p> <h3 id="case-1-scientific-agents">Case 1: Scientific Agents</h3> <p>ASTA Bench <d-cite key="bragg2025astabenchrigorousbenchmarkingai"></d-cite>, an effort towards benchmarking deep scientific research agents, provides a clear test. The specialized ASTA-v0 system scores 55% at $3.40 per task and contains subsystems exceeding 13,000 lines of code (LOC)<sup id="fnref:asta-loc"><a href="#fn:asta-loc" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. Yet the second-best system is a 300-line ReAct general agent scoring 41% at just $0.31. On the literature-understanding subtask, ReAct scores 53%; while still below the Asta agent (62%) it outperforms both the specialized ASTA Paper Finder (21%) and OpenAI Deep Research (19%).</p> <table> <thead> <tr> <th>Agent</th> <th>LLMs used</th> <th>ASTA score</th> <th>Cost per task</th> <th>LOC</th> </tr> </thead> <tbody> <tr> <td>ASTA-v0</td> <td>Claude 4 Sonnet, Gemini 2.5 Flash, O3, GPT 4.1, GPT-4o</td> <td>53%</td> <td>$3.40</td> <td>&gt;13,768<sup id="fnref:asta-loc:1"><a href="#fn:asta-loc" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> </td> </tr> <tr> <td>ReAct</td> <td>GPT-5</td> <td>44%</td> <td>$0.31</td> <td>358<sup id="fnref:react-code"><a href="#fn:react-code" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> </td> </tr> </tbody> </table> <h3 id="case-2-swe-agents">Case 2: SWE Agents</h3> <p>In SWE-Bench <d-cite key="yang2025swesmith"></d-cite>, the specialized SWE-Agent scores 67%, but the tiny, domain-agnostic Mini SWE-Agent scores 65% while being roughly 30 times smaller and about 7 times cheaper per run.</p> <table> <thead> <tr> <th>Agent</th> <th>LLM</th> <th>SWE-Bench score</th> <th>Cost per task</th> <th>LOC</th> </tr> </thead> <tbody> <tr> <td>SWE-Agent</td> <td>Claude 4 Sonnet</td> <td>67%</td> <td>~$2.50</td> <td>4,161</td> </tr> <tr> <td>Mini SWE-Agent</td> <td>Claude 4 Sonnet</td> <td>65%</td> <td>$0.37</td> <td>131</td> </tr> </tbody> </table> <h3 id="the-promise">The Promise</h3> <p>As general agents mature, they will become more complex. Yet they hold great promise. Building a general agent can provide a singular solution applied to a wide range of cases. This can make such an effort much more impactful, similar to how OpenAI was able to surpass many task-specific solutions by providing a general-purpose LLM. It also provides a lower cost of development compared to building many domain-specific ones. Additionally, it reduces the development time required for building domain-specific agents, as they can start from a general agent and further adapt it to their target domain, similar to fine-tuning a general-purpose LLM for a specific task.</p> <h2 id="state-of-general-agent-evaluation">State of General Agent Evaluation</h2> <p>There is a shift in the field of AI agents from domain-specific agents to general-purpose ones, and a general agent can be easier to develop compared to many domain-specific agents, more robust, and more cost-effective. This shift raises the need for evaluation solutions that are suitable for evaluating a general agent. This requires a framework that enables running the same agent across different benchmarks and environments to evaluate adaptability. Yet there is no such solution.</p> <p>To address this gap, we organize the agent evaluation solutions space into five levels, starting from the specific to the more general. This organization helps outline the missing level for evaluating any general agent across environments.</p> <h3 id="level-1-agentic-skills-evaluation">Level 1: Agentic Skills Evaluation</h3> <p>The first level focuses on evaluating LLMs on agentic skills such as reasoning, planning, and tool calling without embedding them in a dynamic environment. Benchmarks provide a textual prompt and expect a textual response. The model’s response is assessed independently of any interaction loop or adaptive environment. These evaluations measure whether a model can demonstrate an agentic capability in principle, but they do not assess whether the model can deploy that capability reliably in realistic long-horizon tasks. Nonetheless, they can provide insight into the ability of a model to succeed in certain components.</p> <p>Representative benchmarks include GSM8K <d-cite key="cobbe2021trainingverifierssolvemath"></d-cite>, which evaluates step-by-step mathematical reasoning; HotPotQA <d-cite key="yang2018hotpotqadatasetdiverseexplainable"></d-cite>, which tests multi-hop question answering; and BFCL <d-cite key="patil2023gorilla"></d-cite>, which measures tool-use capabilities.</p> <h3 id="level-2-domain-agent-evaluation">Level 2: Domain-Agent Evaluation</h3> <p>The second level uses interactive environments, such as web browsers, applications, or terminal interfaces, with a set of tools that allow the agent to interact with them. The agent gets tasks requiring multiple steps that it needs to perform by interacting with the environment. The agent sequence of LLM and tool calls, named the agent trajectory, is then evaluated by an environment-specific metric that assesses whether the agent task was achieved.</p> <p>These benchmarks provide high value for assessing domain-specific agent capabilities. However, each benchmark often defines its own custom setup, leading to agent logic that is tightly coupled to each environment. As a result, some benchmarks are used to assess LLMs in agentic environments, while others require agents tailored to the specific benchmark, making it hard to compare the same agent across benchmarks.</p> <p>Representative examples include Tau-Bench <d-cite key="yao2024tau"></d-cite><d-cite key="barres2025tau2"></d-cite> for customer-service scenarios, AppWorld <d-cite key="trivedi-etal-2024-appworld"></d-cite> for multi-application tasks, WebArena <d-cite key="zhou2024webarenarealisticwebenvironment"></d-cite> for browser interactions, TerminalBench <d-cite key="tbench_2025"></d-cite> for Linux command-line tasks, and SWE-Bench <d-cite key="yang2025swesmith"></d-cite> for solving GitHub issues.</p> <h3 id="level-3-agentic-cross-model-evaluation">Level 3: Agentic Cross-Model Evaluation</h3> <p>The third level focuses on providing a standardized evaluation harness for reproducible agent evaluations across various domain-specific benchmarks. It can be used for comparing LLMs as the backbone of different agents, or to compare domain-specific agents on a single benchmark. Yet their standardization does not allow running the same agent across different benchmarks.</p> <p>A representative example is HAL <d-cite key="hal"></d-cite>, which compiles nine benchmarks from the second level. Each environment in HAL comes with its own fixed agent setup, and users can easily change the backbone model of an agent or add support to a new domain-specific agent. Hence, this level still does not support comparing different agent architectures across environments as needed for general agent assessment.</p> <h3 id="level-4-protocol-centric-agent-evaluation">Level 4: Protocol-Centric Agent Evaluation</h3> <p>The fourth level moves beyond fixed agent setups by defining a standardized interaction protocol, such as a unified browser API or terminal interface, that any agent can implement. Instead of each environment defining its own custom agent-to-environment communication protocol, this standardization forces the agent to follow a specific protocol to be consistently evaluated across many environments. This enables comparisons not just across models, but also across different agent architectures.</p> <p>However, protocol-centric frameworks still impose a specific mode of interaction. Agents built around fundamentally different communication protocols, such as those using the Model Context Protocol (MCP), cannot be evaluated in their native form. They must be forced through the protocol’s interface, which can obscure their design and distort performance. For example, Harbor <d-cite key="tbench_2025"></d-cite> evaluates agents through a command-line protocol, preventing MCP-based agents like Claude Code from being tested as intended.</p> <p>Representative examples include BrowserGym <d-cite key="chezelles2025browsergym"></d-cite>, which standardizes browser interaction across diverse web tasks, and Harbor <d-cite key="tbench_2025"></d-cite>, which provides a unified terminal protocol.</p> <h3 id="level-5-general-agent-evaluation">Level 5: General Agent Evaluation</h3> <p>A fifth level, still missing today, would provide a framework for general agent evaluation. It would enable the evaluation of the same agent across environments without being forced to use a specific communication protocol. It would reveal how an agent actually performs in realistic settings, how design choices influence outcomes, and how well agents generalize across diverse tasks and environments. Achieving this level of flexibility and fairness remains an open challenge, and the lack of a unified, protocol-agnostic evaluation paradigm is a central barrier to progress toward genuinely general-purpose agents.</p> <table> <thead> <tr> <th>Level</th> <th>Cross-model</th> <th>Agentic environment interaction</th> <th>Cross-environment</th> <th>Cross-agent</th> <th>Protocol-agnostic</th> <th>Examples</th> </tr> </thead> <tbody> <tr> <td>1: Agentic model skills</td> <td>Yes</td> <td>No</td> <td>No</td> <td>No</td> <td>No</td> <td>BFCL, GSM8K, HotPotQA</td> </tr> <tr> <td>2: Interactive agentic model</td> <td>Yes</td> <td>Yes</td> <td>No</td> <td>No</td> <td>No</td> <td>Tau-Bench, AppWorld, WebArena, TerminalBench</td> </tr> <tr> <td>3: Cross-model harness</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>No</td> <td>No</td> <td>HAL</td> </tr> <tr> <td>4: Protocol-centric</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>No</td> <td>BrowserGym, Harbor</td> </tr> <tr> <td>5: General agent evaluation</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>(missing)</td> </tr> </tbody> </table> <h2 id="challenges-in-general-agent-evaluation">Challenges in General Agent Evaluation</h2> <p>Benchmarking general agents in different environments is challenging. Existing benchmarks were typically designed with a specific agent domain and goal in mind, such as a user-facing conversational agent, a computer-using autonomous coder, or a web-navigation agent. These design choices led to incompatible setups, preventing the development of a single, unified evaluation protocol that any agent can be seamlessly integrated into. We outline key challenges that must be resolved to provide a unified standard for general agent evaluation.</p> <h3 id="lack-of-standardized-agent-interface">Lack of Standardized Agent Interface</h3> <p>Many benchmarks implicitly assume the tested agent possesses certain built-in, domain-specific capabilities.</p> <ul> <li>Tau-Bench assumes an agent that can inherently message or converse with a user <d-cite key="yao2024tau"></d-cite>:</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BaseAgent</span><span class="p">(</span><span class="n">ABC</span><span class="p">,</span> <span class="n">Generic</span><span class="p">[</span><span class="n">AgentState</span><span class="p">]):</span>
    <span class="sh">"""</span><span class="s">Base agent class that defines the common interface for agents.</span><span class="sh">"""</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">generate_next_message</span><span class="p">(...,</span> <span class="n">message</span><span class="p">:</span> <span class="n">UserMessage</span> <span class="o">|</span> <span class="n">ToolMessage</span> <span class="o">|</span> <span class="n">MultiToolMessage</span><span class="p">..)</span> <span class="o">-&gt;</span> <span class="p">...</span> <span class="n">AssistanceMessage</span><span class="p">:</span>
        <span class="bp">...</span>
</code></pre></div></div> <ul> <li>WebArena assumes an agent whose entire perceptual and action space is mediated through a browser interface controlled by predefined actions <d-cite key="zhou2024webarenarealisticwebenvironment"></d-cite>:</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Base class for the agent</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">next_action</span><span class="p">(...,</span> <span class="n">trajectory</span><span class="p">:</span> <span class="n">Trajectory</span><span class="p">,...)</span> <span class="o">-&gt;</span> <span class="n">Action</span><span class="p">:</span>
        <span class="c1"># ActionType: NONE, SCROLL, KEY_PRESS, MOUSE_CLICK, KEYBOARD_TYPE, MOUSE_HOVER, CLICK, TYPE, HOVER,
</span>        <span class="c1"># PAGE_FOCUS, NEW_TAB, GO_BACK, GO_FORWARD, GOTO_URL, PAGE_CLOSE, ...
</span>        <span class="bp">...</span>
</code></pre></div></div> <ul> <li>TerminalBench assumes an agent whose interaction interface is a computer with a command line <d-cite key="tbench_2025"></d-cite>:</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BaseAgent</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">run</span><span class="p">(...,</span> <span class="n">environment</span><span class="p">:</span> <span class="n">BaseEnvironment</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">...</span>

<span class="k">class</span> <span class="nc">BaseEnvironment</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">exec</span><span class="p">(...,</span> <span class="n">command</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="p">...)</span> <span class="o">-&gt;</span> <span class="n">ExecResult</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">Executes a command in the environment...</span><span class="sh">"""</span>
</code></pre></div></div> <p>These assumptions are mutually incompatible. A web-browsing agent cannot converse with a user, while a chat-oriented agent cannot click on a web element. Such rigid, benchmark-specific communication protocols make cross-environment evaluation impossible without substantial ad hoc engineering.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-general-agent-evaluation/benchmark_agent_cross-480.webp 480w,/2026/assets/img/2026-04-27-general-agent-evaluation/benchmark_agent_cross-800.webp 800w,/2026/assets/img/2026-04-27-general-agent-evaluation/benchmark_agent_cross-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-general-agent-evaluation/benchmark_agent_cross.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="lack-of-standardized-environment-interfaces">Lack of Standardized Environment Interfaces</h3> <p>Benchmarks rarely specify, in an agent-agnostic way, what task the agent must perform, what information it should receive, or what actions it can take and how those actions affect the environment. SWE-Bench defines issues the agent should solve but does not supply standard instructions on how the issues should be solved, how the solution will be validated, or how the submitted solution should be structured. Without an explicit environment interface, every user must design their own, creating different setups for different agents.</p> <p>Other benchmarks communicate environment semantics only in a form tailored to a specific agent architecture. Tau-Bench provides important aspects of the task mixed with assumptions specific to a conversational LLM agent:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">AGENT_INSTRUCTION</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
You are a customer service agent that helps the user according to the &lt;policy&gt; provided below.
In each turn you can either:
- Send a message to the user.
- Make a tool call.
You cannot do both at the same time.

Try to be helpful and always follow the policy. Always make sure you generate valid JSON only.
</span><span class="sh">"""</span>
</code></pre></div></div> <p>Such constraints make sense for a conversational LLM but do not generalize to other types of agents, such as code-act agents. The lack of standardization prevents testing from scaling to many environments, which is essential for evaluating an agent’s ability to adapt.</p> <h3 id="lack-of-a-standardized-researcher-interface">Lack of a Standardized Researcher Interface</h3> <p>To support seamless experimentation without hours spent integrating each new environment, researcher-facing interfaces must also be standardized and simplified. Today, every environment demands its own setup, scripts, and output formats. Benchmarks report outcomes in different formats, store them in different locations, and follow different conventions. Even the basic metrics differ across representative benchmarks-not only in which quantities are tracked, but also in terminology and aggregation standards.</p> <table> <thead> <tr> <th>Metric type</th> <th>Tau-Bench</th> <th>AppWorld</th> <th>SWE-Bench</th> </tr> </thead> <tbody> <tr> <td>Success (bool)</td> <td>Reward = 1</td> <td>success</td> <td>Resolved</td> </tr> <tr> <td>Score (float)</td> <td>Reward</td> <td>Score</td> <td>-</td> </tr> <tr> <td>Termination</td> <td>Termination reason</td> <td>-</td> <td>-</td> </tr> <tr> <td>Duration</td> <td>Duration</td> <td>-</td> <td>-</td> </tr> <tr> <td>Number of interactions</td> <td>Num messages</td> <td>Steps</td> <td>-</td> </tr> <tr> <td>Agent cost</td> <td>Agent cost</td> <td>-</td> <td>-</td> </tr> <tr> <td>Environment cost</td> <td>User cost</td> <td>-</td> <td>-</td> </tr> <tr> <td>Task ID</td> <td>Task ID</td> <td>Task ID</td> <td>Instance ID</td> </tr> <tr> <td>Logs</td> <td>Message log</td> <td>Task logs</td> <td>Test logs</td> </tr> <tr> <td>Success rate (benchmark)</td> <td>Avg reward</td> <td>Task goal completion</td> <td>Resolved counts</td> </tr> <tr> <td>Cost aggregate (benchmark)</td> <td>Avg agent cost</td> <td>-</td> <td>-</td> </tr> </tbody> </table> <p>These gaps underscore the need for consolidation and standardization to make large-scale evaluation of general agents feasible.</p> <h2 id="existing-agent-environment-protocols">Existing Agent-Environment Protocols</h2> <p>Several protocols have recently emerged to standardize interaction, discovery, and data exchange in agentic systems. Some of these can, in principle, support evaluation. A2A provides a standardized way for agents to discover each other’s capabilities, negotiate interaction modes, and coordinate on collaborative tasks, which could inform how benchmarks deliver tasks to agents <d-cite key="a2a_protocol"></d-cite>. MCP defines a structured way for agents to access external tools, data resources, and prompt templates <d-cite key="anthropic_mcp_2024"></d-cite>. In an evaluation setting, a benchmark could expose its environment via an MCP server.</p> <p>We ask two questions:</p> <ol> <li>Should the research community adopt a single protocol for general-agent evaluation?</li> <li>Do current protocols satisfy the practical needs of evaluation today?</li> </ol> <p>Regarding the first question, we caution against prematurely standardizing evaluation processes on a single protocol. Lock-in at this stage risks constraining innovation as agent capabilities and demands still evolve. More fundamentally, evaluation must remain capable of assessing everything-including the protocol itself-making early commitment counterproductive.</p> <p>To address the second question, we use MCP as a case study for whether existing protocols can support end-to-end evaluation workflows. MCP defines three core primitives: tools (invocable operations), resources (exposed data/content), and prompts (parameterized templates), along with mechanisms for streaming events and updates. While MCP provides a promising foundation for unifying agent-environment interaction, several gaps prevent it from serving as a complete solution for general-agent evaluation:</p> <ul> <li>Missing support for benchmark task semantics: benchmarks center around tasks, but MCP does not offer a built-in way to represent or communicate them.</li> <li>Missing support for evaluation workflows: evaluation depends on standardized metrics reporting, aggregation, logging, experiment tracking, and reproducibility. MCP is intentionally agnostic to these needs.</li> <li>Inconsistent ecosystem adoption: many frameworks implement tool calling but not resources or prompts, resulting in inconsistent capabilities and substantial integration overhead.</li> </ul> <table> <thead> <tr> <th>Agent framework</th> <th>MCP tools</th> <th>MCP resources</th> <th>MCP prompts</th> </tr> </thead> <tbody> <tr> <td>Smolagents</td> <td>Yes</td> <td>No</td> <td>No</td> </tr> <tr> <td>Llama Stack</td> <td>Yes</td> <td>No</td> <td>No</td> </tr> <tr> <td>OpenAI Agents SDK</td> <td>Yes</td> <td>No</td> <td>Yes</td> </tr> <tr> <td>Codex CLI</td> <td>Yes</td> <td>No</td> <td>No</td> </tr> <tr> <td>Claude Code</td> <td>Yes</td> <td>Yes</td> <td>No</td> </tr> </tbody> </table> <p>These limitations indicate that while protocols like MCP and A2A lay important groundwork, they do not yet meet the full requirements of standardized general-agent evaluation.</p> <h2 id="general-agent-evaluation-framework">General Agent Evaluation Framework</h2> <p>Advancing general-purpose agents requires an evaluation framework that is itself general: capable of supporting different environments, diverse agent architectures, and multiple communication protocols. Standardizing such evaluation is intrinsically difficult. Environments vary widely, implicit assumptions fragment the space, and existing protocols address only parts of the challenge. Even promising standards such as MCP provide useful building blocks but remain incomplete. These pressures motivate the need for a unifying layer that can support evaluating any agent on any benchmark without restriction to a specific communication protocol.</p> <h3 id="a-meta-protocol-for-general-evaluation">A Meta-Protocol for General Evaluation</h3> <p>At the core of such a framework is a meta-protocol: an abstract, protocol-agnostic layer that defines the semantics of evaluation independently of any concrete agent-environment protocol. Current benchmarks implicitly bind evaluation to a specific communication protocol, thereby entangling agent performance with protocol-specific setup.</p> <p>The meta-protocol needs to specify how tasks, actions, observations, documentation, and termination conditions are represented while allowing different communication protocols. This abstraction allows interfaces such as web browsing, terminal, MCP, and tool schemas without altering the evaluation semantics. By enabling protocol modularity rather than protocol uniformity, a meta-protocol allows agents to be evaluated in their native interaction modes and makes cross-protocol comparisons meaningful.</p> <h3 id="core-characteristics-of-a-general-evaluation-framework">Core Characteristics of a General Evaluation Framework</h3> <ul> <li>Environment-agnostic agent interface. The agent-facing interface should function across any environment. Environments expose a minimal, standardized schema for actions, observations, tasks, and documentation, with all assumptions explicit and discoverable rather than embedded in reference agents.</li> <li>Agent-agnostic environment interface. Environments should not assume a specific agent architecture, reasoning style, or protocol. Simple ReAct agents, memory-augmented agents, MCP-based agents, browser agents, and command-line agents should all integrate without modification.</li> <li>Plug-and-play modularity. Models, agent architectures, interaction protocols, and environments should be swappable without altering the evaluation protocol. This enables controlled comparisons across dimensions within a unified setup.</li> <li>Standardized reporting and transparency. The framework should define consistent reporting conventions, including metrics for success, robustness, interaction efficiency, and cost. Observability support for logging and full trajectory tracking is essential.</li> </ul> <p>Such a framework would let us measure the core property that makes general agents uniquely valuable: adaptability to new environments with minimal re-engineering.</p> <h2 id="conclusions">Conclusions</h2> <p>General-purpose agents are arriving faster than our ability to measure them. Simple, modular systems already challenge specialized stacks, but fragmented benchmarks hide which ideas truly matter. A protocol-agnostic evaluation layer paired with clear environment contracts and consistent reporting would give the field a common yardstick for progress and illuminate how design choices transfer across domains.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:claude-sdk"> <p>https://www.anthropic.com/engineering/building-agents-with-the-claude-agent-sdk <a href="#fnref:claude-sdk" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:asta-loc"> <p>Based on the python files in https://github.com/allenai/asta-paper-finder/tree/main/agents/mabool/api/mabool. <a href="#fnref:asta-loc" class="reversefootnote" role="doc-backlink">↩</a> <a href="#fnref:asta-loc:1" class="reversefootnote" role="doc-backlink">↩<sup>2</sup></a></p> </li> <li id="fn:react-code"> <p>Source: https://github.com/allenai/asta-bench/blob/f7e25392f4dda167f4e6d46b8c7c080eeeb4cc35/astabench/solvers/react/basic_agent.py. <a href="#fnref:react-code" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-general-agent-evaluation.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-vlms-waste-their-vision/">Why vlms waste their vision</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>