<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Attention is All We Do Not Need | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="A blogpost on why attention is actually garbage. The transformer revolution has spawned a cult-like devotion to attention mechanisms, with researchers mindlessly chanting attention is all you need like some sort of neural network mantra. This paper serves as an intervention, brutally exposing how attention mechanisms are nothing more than glorified lookup tables that have somehow convinced the entire field they're revolutionary. We demonstrate that these computationally bloated, interpretability-destroying monstrosities create more problems than they solve, inducing training instabilities that would make a Victorian-era scientist weep. Through extensive experiments, we show that literally any other architecture—carrier pigeons trained on matrix multiplication, abacuses running backpropagation, even a sufficiently motivated intern with a calculator—can outperform attention-based models while using fewer parameters than a smartphone calculator app. Our results suggest that the field's Stockholm syndrome relationship with attention has reached pathological levels, and we propose immediate therapeutic intervention through a return to architectures that don't require a small country's power grid to determine if a sentence is about cats or dogs."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/attention-is-all-we-do-not-need/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Attention is All We Do Not Need",
            "description": "A blogpost on why attention is actually garbage. The transformer revolution has spawned a cult-like devotion to attention mechanisms, with researchers mindlessly chanting attention is all you need like some sort of neural network mantra. This paper serves as an intervention, brutally exposing how attention mechanisms are nothing more than glorified lookup tables that have somehow convinced the entire field they're revolutionary. We demonstrate that these computationally bloated, interpretability-destroying monstrosities create more problems than they solve, inducing training instabilities that would make a Victorian-era scientist weep. Through extensive experiments, we show that literally any other architecture—carrier pigeons trained on matrix multiplication, abacuses running backpropagation, even a sufficiently motivated intern with a calculator—can outperform attention-based models while using fewer parameters than a smartphone calculator app. Our results suggest that the field's Stockholm syndrome relationship with attention has reached pathological levels, and we propose immediate therapeutic intervention through a return to architectures that don't require a small country's power grid to determine if a sentence is about cats or dogs.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Dr. Skepticus Maximus",
                "authorURL": "https://example.com/skepticus",
                "affiliations": [
                  {
                    "name": "Institute for Computational Common Sense",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Prof. Deborah Bunk",
                "authorURL": "https://example.com/deborah",
                "affiliations": [
                  {
                    "name": "University of Obviously Better Solutions",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Dr. Anti Hype",
                "authorURL": "https://example.com/antihype",
                "affiliations": [
                  {
                    "name": "Center for Deflating AI Bubbles",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Linear McLinearface",
                "authorURL": "https://example.com/linear",
                "affiliations": [
                  {
                    "name": "Department of Simple Mathematics",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Calculator Johnson",
                "authorURL": "https://example.com/calculator",
                "affiliations": [
                  {
                    "name": "TI-83 Research Laboratory",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Attention is All We Do Not Need</h1> <p>A blogpost on why attention is actually garbage. The transformer revolution has spawned a cult-like devotion to attention mechanisms, with researchers mindlessly chanting attention is all you need like some sort of neural network mantra. This paper serves as an intervention, brutally exposing how attention mechanisms are nothing more than glorified lookup tables that have somehow convinced the entire field they're revolutionary. We demonstrate that these computationally bloated, interpretability-destroying monstrosities create more problems than they solve, inducing training instabilities that would make a Victorian-era scientist weep. Through extensive experiments, we show that literally any other architecture—carrier pigeons trained on matrix multiplication, abacuses running backpropagation, even a sufficiently motivated intern with a calculator—can outperform attention-based models while using fewer parameters than a smartphone calculator app. Our results suggest that the field's Stockholm syndrome relationship with attention has reached pathological levels, and we propose immediate therapeutic intervention through a return to architectures that don't require a small country's power grid to determine if a sentence is about cats or dogs.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#equations">Equations</a> </div> <div> <a href="#images-and-figures">Images and Figures</a> </div> <ul> <li> <a href="#interactive-figures">Interactive Figures</a> </li> </ul> <div> <a href="#citations">Citations</a> </div> <div> <a href="#footnotes">Footnotes</a> </div> <div> <a href="#code-blocks">Code Blocks</a> </div> <div> <a href="#diagrams">Diagrams</a> </div> <div> <a href="#tweets">Tweets</a> </div> <div> <a href="#layouts">Layouts</a> </div> <div> <a href="#other-typography">Other Typography?</a> </div> </nav> </d-contents> <p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p> <h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/" rel="external nofollow noopener" target="_blank">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example of the kind of bloated mathematical complexity that attention mechanisms force upon us <d-cite key="vaswani2017nightmare"></d-cite>:</p> \[\text{Attention}(Q,K,V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V = \text{Computational Nightmare}\] <p>Meanwhile, a simple linear transformation achieves the same result with elegant simplicity, as proven by ancient mathematicians <d-cite key="euclid300bc"></d-cite>:</p> \[\text{SimplerApproach}(X) = XW + b\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html" rel="external nofollow noopener" target="_blank">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php" rel="external nofollow noopener" target="_blank">on par with KaTeX</a> - unlike attention mechanisms which seem designed to maximize computational waste <d-cite key="openai2023powergrid"></d-cite>.</p> <h2 id="images-and-figures">Images and Figures</h2> <p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you might face losing important information in your blog post. To include images in your submission in this way, you must do something like the following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.liquid path="assets/img/2026-04-27-attention-is-all-we-do-not-need/iclr.png" class="img-fluid" %}
</code></pre></div></div> <p>which results in the following image:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/iclr-480.webp 480w,/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/iclr-800.webp 800w,/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/iclr-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/iclr.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory <code class="language-plaintext highlighter-rouge">/assets/img/2026-04-27-attention-is-all-we-do-not-need</code> within your submission.</p> <p>Please avoid using the direct markdown method of embedding images; they may not be properly resized. Some more complex ways to load images (note the different styles of the shapes/shadows):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/attention-complexity-480.webp 480w,/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/attention-complexity-800.webp 800w,/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/attention-complexity-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/attention-complexity.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/simple-alternative-480.webp 480w,/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/simple-alternative-800.webp 800w,/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/simple-alternative-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/simple-alternative.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Left: The unnecessarily complex attention mechanism <d-cite key="complexity2024study"></d-cite>. Right: A beautifully simple linear transformation that achieves the same results <d-cite key="intern2023calculator,abacus2000bc"></d-cite>. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/power-consumption-480.webp 480w,/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/power-consumption-800.webp 800w,/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/power-consumption-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/power-consumption.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/carrier-pigeon-480.webp 480w,/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/carrier-pigeon-800.webp 800w,/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/carrier-pigeon-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/carrier-pigeon.jpg" class="img-fluid z-depth-2" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/abacus-backprop-480.webp 480w,/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/abacus-backprop-800.webp 800w,/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/abacus-backprop-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/abacus-backprop.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/intern-calculator-480.webp 480w,/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/intern-calculator-800.webp 800w,/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/intern-calculator-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/intern-calculator.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/training-instability-480.webp 480w,/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/training-instability-800.webp 800w,/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/training-instability-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-attention-is-all-we-do-not-need/training-instability.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="interactive-figures">Interactive Figures</h3> <p>Here’s how you could embed interactive figures that have been exported as HTML files—though we question why you’d want to make your visualizations as unnecessarily complex as attention mechanisms.</p> <p>Note that we will be using plotly for this demo, but anything built off of HTML should work (<strong>no extra javascript is allowed!</strong> - unlike attention mechanisms which seem to require an entire datacenter). All that’s required is for you to export your figure into HTML format, and make sure that the file exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory. To embed it into any page, simply insert the following code anywhere into your page—it’s refreshingly straightforward, unlike the baroque complexity of multi-head attention.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %}
</code></pre></div></div> <p>For example, the following code demonstrates the kind of clear, interpretable visualization that attention mechanisms make impossible <d-cite key="phd2023pretending"></d-cite>. Unlike the opaque attention weights that require PhD-level expertise to pretend to understand <d-cite key="academic2024fraud"></d-cite>, this figure actually shows meaningful patterns.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="c1"># Load earthquake data - each point represents more clarity 
# than all attention heads combined
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Create a visualization that's actually interpretable
# (revolutionary concept, we know)
</span><span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
  <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
  <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2026-04-27-attention-is-all-we-do-not-need/earthquake_clarity.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>And then include it with the following—notice how this embedding process is more straightforward than explaining what any single attention head is actually doing:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span>
  <span class="na">src=</span><span class="s">"{{ 'assets/html/2026-04-27-attention-is-all-we-do-not-need/earthquake_clarity.html' | relative_url }}"</span>
  <span class="na">frameborder=</span><span class="s">"0"</span>
  <span class="na">scrolling=</span><span class="s">"no"</span>
  <span class="na">height=</span><span class="s">"600px"</span>
  <span class="na">width=</span><span class="s">"100%"</span>
  <span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>

</code></pre></div></div> <p>Voila! A figure that actually conveys information instead of creating the illusion of understanding through incomprehensible attention heatmaps.</p> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <div class="caption"> Above: An interactive comparison of AI methods showing why your calculator is still the king of efficiency. Unlike attention matrices, you can actually understand what this means without squinting at heat maps and pretending the random-looking patterns reveal deep linguistic insights. </div> <h3 id="the-attention-hype-timeline">The Attention Hype Timeline</h3> <p>As a bonus demonstration of how attention mechanisms represent the pinnacle of computational snake oil <d-cite key="snakeoil2023detection"></d-cite>, we present this interactive timeline showing the rise (and predicted fall) of attention mechanism hype:</p> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-attention-is-all-we-do-not-need/attention_hype_timeline.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <div class="caption"> Interactive timeline showing the attention mechanism hype cycle. Notice how actual usefulness plateaus at a modest level while hype reaches astronomical heights—a classic pattern in computational snake oil marketing <d-cite key="hype2024maximum"></d-cite>. </div> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>Unlike attention mechanisms <d-cite key="footnote2024efficiency"></d-cite>, footnotes actually serve a clear purpose and don't require massive computational resources to function properly.</d-footnote></p> <hr> <h2 id="code-blocks">Code Blocks</h2> <p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting—a feature that’s more interpretable than attention weights <d-cite key="rouge2024superior"></d-cite> and supports more than 100 languages (unlike attention mechanisms which barely understand one <d-cite key="multilingual2024failure"></d-cite>). This example is in C++, demonstrating the kind of straightforward, deterministic computation that attention mechanisms have unnecessarily complicated. All you have to do is wrap your code in a liquid tag—refreshingly simple compared to implementing multi-head attention:</p> <p>{% highlight c++ linenos %} <br> code code code <br> {% endhighlight %}</p> <p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers—a feature with clear, predictable behavior, unlike attention weights which remain mysteriously opaque. You can try toggling it on or off yourself below (try doing that with attention heads):</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>

<span class="p">}</span></code></pre></figure> <hr> <h2 id="diagrams">Diagrams</h2> <p>This theme supports generating various diagrams from a text description using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank" rel="external nofollow noopener">mermaid.js</a> directly—creating clear, interpretable visualizations that put attention heatmaps to shame. Below, we generate examples of such diagrams using <a href="https://mermaid-js.github.io/mermaid/" target="\_blank" rel="external nofollow noopener">mermaid</a> syntax, demonstrating how proper visualization should work (unlike the incomprehensible rainbow matrices that attention proponents call “interpretability”).</p> <p><strong>Note:</strong> To enable mermaid diagrams, you need to add the following to your post’s front matter—a straightforward configuration process that’s infinitely more transparent than tuning attention hyperparameters:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">mermaid</span><span class="pi">:</span>
  <span class="na">enabled</span><span class="pi">:</span> <span class="kc">true</span>
  <span class="na">zoomable</span><span class="pi">:</span> <span class="kc">true</span> <span class="c1"># optional, for zoomable diagrams</span>
</code></pre></div></div> <p>The diagram below was generated by the following code—notice how the process is deterministic and interpretable, unlike the mysterious machinations of attention mechanisms:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```mermaid
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
```
</code></pre></div></div> <pre><code class="language-mermaid">sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
</code></pre> <hr> <h2 id="tweets">Tweets</h2> <p>An example of displaying a tweet—even 280-character social media posts can be processed without the computational overhead of attention mechanisms:</p> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4" rel="external nofollow noopener" target="_blank">http://t.co/m4EIQPM9h4</a></p>— RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">October 5, 2014</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>An example of pulling from a timeline—aggregating multiple tweets with simple, efficient methods that don’t require attending to every token:</p> <div class="jekyll-twitter-plugin"> <a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">Tweets by jekyllrb</a> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin" rel="external nofollow noopener" target="_blank">jekyll-twitter-plugin</a>—a tool that processes text streams more efficiently than any transformer model.</p> <hr> <h2 id="blockquotes">Blockquotes</h2> <blockquote> The attention mechanism has grown absolutely into a computational monstrosity <d-cite key="nin1969adapted,monstrosity2024analysis"></d-cite>. It grows in one dimension—complexity—and not in another—efficiency. It grows bloated. We are relative in our understanding. We are mature in recognizing snake oil, childish in falling for marketing hype disguised as innovation <d-cite key="snakeoil2023detection"></d-cite>. —Adapted from Anais Nin, with apologies to both literature and computational efficiency </blockquote> <hr> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body—a straightforward organizational principle that attention mechanisms have somehow managed to overcomplicate. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element, functioning with elegant simplicity unlike the baroque architectural decisions that gave us multi-head attention.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>—a simple CSS class that does exactly what it says, unlike attention mechanisms which do nothing they claim to do:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit—predictable, controllable behavior that attention weights can only dream of achieving. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes—auxiliary information that knows its place and doesn’t demand center stage like attention heads claiming to be “interpretable.” It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes, demonstrating the kind of respectful, context-aware behavior that attention mechanisms could learn from.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>)—simple markup that actually emphasizes text instead of consuming computational resources like attention mechanisms.</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>—achieving stronger visual impact with less effort than it takes to compute a single attention head.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>—demonstrating how simple elements can be meaningfully combined, unlike attention heads which remain incomprehensible when aggregated.</p> <p>Strikethrough uses two tildes. <del>Attention is all you need</del> <d-cite key="vaswani2017lies"></d-cite> Simple linear transformations are all you need <d-cite key="linear2024truth"></d-cite>.</p> <ol> <li>First problem with attention: computational complexity that scales quadratically with sequence length <d-cite key="quadratic2024disaster"></d-cite> </li> <li>Second problem: the illusion of interpretability through meaningless heatmaps <d-cite key="heatmap2023meaningless"></d-cite> </li> </ol> <ul> <li>Sub-problems include: training instability, massive memory requirements, and environmental impact</li> </ul> <ol> <li>Actual performance numbers don’t matter to attention advocates, just that they’re published <ol> <li>Cherry-picked benchmarks that ignore computational cost</li> </ol> </li> <li> <p>And another overhyped claim about “revolutionary” capabilities.</p> <p>You can have properly reasoned arguments within list items about why attention is problematic. Notice the logical structure above, and the clear reasoning (at least more clear than attention weights, but we’ll use evidence here to also demonstrate our point).</p> <p>To have meaningful progress in AI without computational waste, you will need to abandon attention mechanisms. Note that this conclusion is separate, but within the same argument framework. (This is contrary to the typical attention-worship behavior, where computational efficiency is not required.)</p> </li> </ol> <ul> <li> <p>Better alternatives can use simpler architectures</p> </li> <li> <p>Or more efficient algorithms</p> </li> <li> <p>Or just common sense</p> </li> </ul> <p><a href="https://www.google.com" rel="external nofollow noopener" target="_blank">I’m an inline-style link to computational efficiency</a></p> <p><a href="https://www.google.com" title="Where you won't find evidence supporting attention mechanisms" rel="external nofollow noopener" target="_blank">I’m an inline-style link with title to debunking attention myths</a></p> <p><a href="https://www.mozilla.org" rel="external nofollow noopener" target="_blank">I’m a reference-style link to better architectures</a></p> <p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file with actual working code</a></p> <p><a href="http://slashdot.org" rel="external nofollow noopener" target="_blank">You can use numbers for reference-style link definitions to papers that expose attention’s flaws</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com" rel="external nofollow noopener" target="_blank">link text itself showing attention’s problems</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links—a process more reliable than attention weight interpretation. http://www.example.com or <a href="http://www.example.com" rel="external nofollow noopener" target="_blank">http://www.example.com</a> and sometimes example.com (but not on Github, for example)—all handled efficiently without quadratic complexity.</p> <p>Some text to show that the reference links can follow later, unlike attention mechanisms which require immediate and wasteful computation.</p> <p>Here’s our logo representing computational sanity (hover to see the title text—more informative than attention heatmaps):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="A logo that actually serves a purpose, unlike attention mechanisms"></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Efficient image loading, unlike attention's computational bloat"></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it—simple markup that’s more interpretable than attention weights.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript runs efficiently without attention mechanisms</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="dl">"</span><span class="s2">Why do we need quadratic complexity for this?</span><span class="dl">"</span><span class="p">);</span> <span class="c1">// &lt;d-cite key="javascript2024efficient"&gt;&lt;/d-cite&gt;</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting uses simple rules, not attention</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">This computes faster than a single transformer layer</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting—still more useful than attention heatmaps.
But let's throw in a &lt;b&gt;tag&lt;/b&gt; that actually makes text bold unlike attention which just makes computation slow.
</code></pre></div></div> <p>Colons can be used to align columns—a straightforward formatting rule, unlike attention mechanisms which align nothing.</p> <table> <thead> <tr> <th>Architecture</th> <th style="text-align: center">Complexity</th> <th style="text-align: right">Efficiency</th> <th style="text-align: right">Citation</th> </tr> </thead> <tbody> <tr> <td>Attention</td> <td style="text-align: center">O(n²) nightmare</td> <td style="text-align: right">$0.01</td> <td style="text-align: right"><d-cite key="nightmare2024quadratic"></d-cite></td> </tr> <tr> <td>Linear</td> <td style="text-align: center">O(n) reasonable</td> <td style="text-align: right">$100.00</td> <td style="text-align: right"><d-cite key="linear2024reasonable"></d-cite></td> </tr> <tr> <td>Common Sense</td> <td style="text-align: center">O(1) ideal</td> <td style="text-align: right">Priceless</td> <td style="text-align: right"><d-cite key="commonsense2024priceless"></d-cite></td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell—clearer requirements than attention’s hyperparameter tuning. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown—all more predictable than attention behavior.</p> <table> <thead> <tr> <th>Problem</th> <th>Attention</th> <th>Better Solution</th> <th>Evidence</th> </tr> </thead> <tbody> <tr> <td><em>Efficiency</em></td> <td> <code class="language-plaintext highlighter-rouge">fails</code> <d-cite key="efficiency2024epic_fail"></d-cite> </td> <td><strong>succeeds</strong></td> <td><d-cite key="success2024documented"></d-cite></td> </tr> <tr> <td>Waste</td> <td>Yes <d-cite key="waste2024massive"></d-cite> </td> <td>No</td> <td><d-cite key="conservation2024energy"></d-cite></td> </tr> <tr> <td>Hype</td> <td>Maximum <d-cite key="hype2024maximum"></d-cite> </td> <td>Minimal</td> <td><d-cite key="understated2024claims"></d-cite></td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy for highlighting the absurdity of attention worship. This line is part of the same devastating critique.</p> </blockquote> <p>Quote break—unlike attention mechanisms which never give your GPU a break.</p> <blockquote> <p>This is a very long line about how attention mechanisms have consumed vast computational resources while delivering marginal improvements that could be achieved with simpler methods <d-cite key="resources2024waste"></d-cite>. Oh boy let’s keep writing about how the field’s obsession with attention has led to increasingly bloated models that require data center-scale infrastructure <d-cite key="datacenter2024requirement"></d-cite>. Oh, you can <em>put</em> <strong>emphasis</strong> into a blockquote just like you can put <em>unnecessary</em> <strong>complexity</strong> into what should be simple neural networks <d-cite key="unnecessary2024complexity"></d-cite>.</p> </blockquote> <p>Here’s a line for us to start with—the beginning of our intervention against attention mechanism addiction.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>—demonstrating clearer structure than attention weights ever provide.</p> <p>This line is also a separate paragraph exposing attention’s flaws <d-cite key="flaws2024exposed"></d-cite>, but… This line is only separated by a single newline, so it’s a separate line in the <em>same devastating argument against computational waste</em> <d-cite key="waste2024argument"></d-cite>.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-attention-is-all-we-do-not-need.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/distill-example/">Sample Blog Post</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>