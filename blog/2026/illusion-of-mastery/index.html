<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>我看别人的figure路径和格式是这样写得，请你按他们的格式修改我的</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-sparsity/1-480.webp 480w,/2026/assets/img/2026-04-27-sparsity/1-800.webp 800w,/2026/assets/img/2026-04-27-sparsity/1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-sparsity/1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 1: Attention heatmaps showing naturally sparse patterns in dense models. Even with a full attention budget, models learn to concentrate attention on specific tokens rather than distributing it uniformly.</figcaption> </figure> <hr> <p>layout: distill title: “The illusion of mastery: Breaking the Cycle of Benchmark Memorization with Generative Evaluation” description: Static evaluation traps LLMs in a cycle of overfitting, leading to inflated benchmark scores but fragile real-world performance. This post argues for a paradigm shift to Generative Evaluation, a dynamic engine that creates infinite novel tasks. By targeting unseen reasoning patterns and high-impact corner cases, it moves beyond memorization to genuinely measure and incentivize the generalizable intelligence required for AGI. date: 2026-12-04 future: true htmlwidgets: true</p> <h1 id="anonymize-when-submitting">anonymize when submitting</h1> <p>authors:</p> <ul> <li>name: Anonymous</li> </ul> <h1 id="do-not-fill-this-in-until-your-post-is-accepted-and-youre-publishing-your-camera-ready-post">do not fill this in until your post is accepted and you’re publishing your camera-ready post!</h1> <h1 id="authors">authors:</h1> <h1 id="--name-albert-einstein">- name: Albert Einstein</h1> <h1 id="url-httpsenwikipediaorgwikialbert_einstein">url: “https://en.wikipedia.org/wiki/Albert_Einstein”</h1> <h1 id="affiliations">affiliations:</h1> <h1 id="name-ias-princeton">name: IAS, Princeton</h1> <h1 id="--name-boris-podolsky">- name: Boris Podolsky</h1> <h1 id="url-httpsenwikipediaorgwikiboris_podolsky">url: “https://en.wikipedia.org/wiki/Boris_Podolsky”</h1> <h1 id="affiliations-1">affiliations:</h1> <h1 id="name-ias-princeton-1">name: IAS, Princeton</h1> <h1 id="--name-nathan-rosen">- name: Nathan Rosen</h1> <h1 id="url-httpsenwikipediaorgwikinathan_rosen">url: “https://en.wikipedia.org/wiki/Nathan_Rosen”</h1> <h1 id="affiliations-2">affiliations:</h1> <h1 id="name-ias-princeton-2">name: IAS, Princeton</h1> <h1 id="must-be-the-exact-same-name-as-your-blogpost">must be the exact same name as your blogpost</h1> <p>bibliography: 2026-04-27-illusion-of-mastery.bib</p> <h1 id="add-a-table-of-contents-to-your-post">Add a table of contents to your post.</h1> <h1 id="--make-sure-that-toc-names-match-the-actual-section-names">- make sure that TOC names match the actual section names</h1> <h1 id="for-hyperlinks-within-the-post-to-work-correctly">for hyperlinks within the post to work correctly.</h1> <p>toc:</p> <ul> <li>name: Introduction</li> <li>name: “The Problem: The Failures of Static Evaluation” subsections: <ul> <li>name: The Contamination Illusion</li> <li>name: The Stagnant 80% Crisis</li> <li>name: Ignoring High-Impact Corner Cases</li> <li>name: High Cost and Inefficiency</li> <li>name: The Mismatch on the Path to AGI</li> </ul> </li> <li>name: The Blueprint of Generative Evaluation subsections: <ul> <li>name: Why Generative Evaluation Solves the Problems?</li> <li>name: Generating Diverse, Contamination-Resistant Tasks</li> <li>name: Discovering Novel Reasoning Patterns</li> <li>name: Ensuring Reliability in the Generative Pipeline</li> </ul> </li> <li>name: Discussion subsections: <ul> <li>name: Managing Error in Generative Frameworks</li> <li>name: Potential Influence on Society</li> <li> <h2 id="name-limitations">name: Limitations</h2> </li> </ul> </li> </ul> <h2 id="1-introduction">1. Introduction</h2> <p>The development of Large Language Models (LLMs) is accelerating at a breakneck pace [@deepseek-r1; @o1; @gpt4]. On the surface, the metrics are dazzling: benchmarks are being saturated in record time, and models now exhibit superhuman performance on tasks like coding [@livecodebench; @swebench] or math competitions [@gsm8k; @aime].</p> <p>However, despite these soaring benchmark scores, models often face an “80% Crisis” in real-world applications. For instance, when tasked with writing or debugging code, they frequently make surprisingly fundamental errors. Fixing one bug often introduces another. This leads to a natural question: Why does this gap exist?</p> <p>In a recent interview [@ilya], Ilya Sutskever offered two key insights: The problem lies in data homogenization (most LLMs are pre-trained on similar web content, code, books) and post-training optimization that overfits to leaderboard benchmarks. Models are fine-tuned with reinforcement learning specifically designed around static test sets [@rl-reasoning], allowing them to excel in exam-like settings while performing like rote-learners in the real world.</p> <p>Therefore, the models’ fragility stems from overfitting to the specific reasoning paradigms present in current static evaluation. Every time a static benchmark is “solved”, the field falls into a cycle: Propose a harder static dataset $\rightarrow$ Scale up the model to overfit the new structure pattern $\rightarrow$ Propose an even harder dataset. This is a race of “memorization capacity vs. reasoning ability.” We mistakenly believe the model is getting smarter, but it is often just expanding its capacity to memorize patterns, thereby missing the opportunity to discover genuine reasoning algorithms and moving further away from the goal of AGI. As a result, in real-world applications, models often fail when encountering novel “reasoning patterns” from users. These patterns are simple for humans but unsolvable via memorization, which accounts for the remaining 20% of tasks where models consistently fall short.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-illusion-of-mastery/figure1-480.webp 480w,/2026/assets/img/2026-04-27-illusion-of-mastery/figure1-800.webp 800w,/2026/assets/img/2026-04-27-illusion-of-mastery/figure1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-illusion-of-mastery/figure1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 1: The vicious cycle: memory capacity vs. reasoning ability.</figcaption> </figure> <p>In this post, we argue that the path to AGI requires a fundamental shift in how we measure intelligence. We will examine how current static evaluation misleads the industry and introduce Generative Evaluation—not merely as a new metric, but as a dynamic engine capable of discovering novel reasoning patterns that are hard for models to generalize.</p> <h2 id="2-the-problem-the-failures-of-static-evaluation">2. The Problem: The Failures of Static Evaluation</h2> <p>The current reliance on fixed, static evaluation benchmarks is actively misleading the industry, creating an “illusion of mastery” where better metrics do not equate to genuinely improved capabilities. This systematic flaw is rooted in four key issues:</p> <h3 id="21-the-contamination-illusion">2.1 The Contamination Illusion</h3> <p>The acceleration of data collection and model training has created a race we are losing: human benchmark design cannot keep pace with data crawler speed. A benchmark considered challenging upon release often sees a rapid, dramatic performance leap within months [@livecodebench; @dynabench]. This improvement frequently signals not an advancement in the model’s reasoning, but data contamination: the test data has leaked into the training set, effectively allowing the model to memorize the answers.</p> <ul> <li> <strong>The Misleading Result:</strong> This data contamination results in deceptively inflated scores. This is evidenced by the significant performance gap observed on benchmarks like LiveCodeBench: models excel on pre-release problems but show a marked drop in performance on post-release problems. This gap strongly suggests that the pre-release data was likely included in the model’s training corpus.</li> <li> <strong>The Memorization Trap:</strong> Data contamination causes models to memorize rather than learn. This isn’t just about remembering answers, for complex tasks, they memorize the expected sequence of a challenge. This is exemplified by OpenAI’s Procgen test: models trained on a fixed order of levels (progressing only upon success) perform perfectly. However, at test time, when the level order is randomized, they fail completely. This proves they did not learn to play the game; they simply memorized the expected sequence of actions.</li> </ul> <div style="width: 100%; overflow: hidden;"> <div style="width: 48%; float: left;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-illusion-of-mastery/figure2-480.webp 480w,/2026/assets/img/2026-04-27-illusion-of-mastery/figure2-800.webp 800w,/2026/assets/img/2026-04-27-illusion-of-mastery/figure2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-illusion-of-mastery/figure2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 2: DeepSeek-Instruct and GPT-4-O perform considerably worse on problems released after their respective release and cutoff dates (September and November 2023), indicating potential contamination in the earlier problems.</figcaption> </figure> </div> <div style="width: 48%; float: right;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-illusion-of-mastery/figure3-480.webp 480w,/2026/assets/img/2026-04-27-illusion-of-mastery/figure3-800.webp 800w,/2026/assets/img/2026-04-27-illusion-of-mastery/figure3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-illusion-of-mastery/figure3.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 3. The agent achieves promising results during training on a fixed sequence but fails to generalize when the level order is shuffled at test time.</figcaption> </figure> </div> </div> <h3 id="22-the-stagnant-80-crisis">2.2 The Stagnant $80\%$ Crisis</h3> <p>While increasing model and dataset sizes have equipped current models with a degree of generalization (e.g., solving unseen math problems), the “memorization trap” persists. It has simply evolved into a higher-level fixed pattern matching. Models memorize the fixed path to solve a specific set of problems but lack the ability to dynamically adjust reasoning path on novel context. A clear symptom of this trend is the widespread 80% crisis:</p> <p>The first 80% of performance comes from high-frequency, common-knowledge patterns that follow the head of a power-law distribution. Early models like BERT made huge leaps, quickly reaching around 80% accuracy on challenging benchmarks like SuperGLUE [@superglue]. However, vastly larger models such as GPT-4 and LLaMA variants now only push performance up by a few marginal percentage points. This slowdown occurs because the final 20% consists of rare and diverse corner cases—the “long tail pattern”. We are essentially spending billions of dollars to buy those final, expensive $1\%$ gains.</p> <p>This leads to a resource paradox: According to scaling laws, improving performance on these sparse long-tail examples requires exponentially more parameters and data. Scaling laws describe how model loss (L) decreases as we scale up model size (N) and dataset size (D). A common form is: [ L \propto \alpha N^{-\beta} D^{-\gamma} ] Here:</p> <ul> <li>(L) is the model’s loss (lower is better)</li> <li>(N) is the number of model parameters</li> <li>(D) is the dataset size</li> <li>(\alpha, \beta, \gamma) are constants (typically less than 1)</li> </ul> <p>As (N) or (D) increases, loss decreases but at a slowing rate. Early gains are rapid; later improvements become far more expensive. We are now spending billions for each marginal gain, chasing perfection via fixed pattern matching instead of developing generalizable algorithms for future challenges. Relying only on scaling is inefficient and unsustainable.</p> <h3 id="23-ignoring-high-impact-corner-cases">2.3 Ignoring High-Impact Corner Cases</h3> <p>Static benchmarks are often gathered from real-life data distributions. While this seems reasonable, it inherently biases the evaluation against the most critical failures. In real-world, high-stakes applications (like autonomous driving), $98\%$ of the data might be common, safe scenarios, while the crucial $2\%$ are extreme, high-impact corner cases (e.g., a pedestrian unexpectedly darting out, or extreme weather conditions). A model can achieve a $98\%$ performance score on a static dataset without solving a single corner case. However, in a real-world deployment, these $2\%$ failures are the ones that lead to catastrophic results. The current evaluation paradigm systematically ignores the importance of these rare and high-impact events because they are too infrequent to significantly affect the average score on a static benchmark.</p> <h3 id="24-high-cost-and-inefficiency">2.4 High Cost and Inefficiency</h3> <p>The creation of robust static benchmarks is an incredibly resource-intensive endeavor, and even the best efforts are short-lived.</p> <ul> <li> <strong>Example Cost:</strong> Projects like BIG-bench [@bigbench] involved the labor of over 440 top researchers for two years to collect $204$ diverse tasks, representing an implicit cost of millions of dollars. Similarly, the SuperGLUE [@superglue] benchmarks required $80+$ expert annotators.</li> <li> <strong>Fast Saturation:</strong> Despite this immense investment, even these meticulously curated datasets face the same risk of rapid saturation and contamination. The moment a score is achieved, the data distribution is “seen,” and the expensive benchmark begins its inevitable slide toward being a memory test. Even an Olympic-level difficult problem set AIME [@aime] can be 98.7% saturated within months of its release.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-illusion-of-mastery/figure4-480.webp 480w,/2026/assets/img/2026-04-27-illusion-of-mastery/figure4-800.webp 800w,/2026/assets/img/2026-04-27-illusion-of-mastery/figure4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-illusion-of-mastery/figure4.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 4. Benchmark saturation over time for popular benchmarks, normalized with initial performance at minus one and human performance at zero [@international-safety].</figcaption> </figure> <h3 id="25-the-mismatch-on-the-path-to-agi">2.5 The Mismatch on the Path to AGI</h3> <p>If our final destination is AGI, we have a fundamental problem: we are currently using finite sets to evaluate an AGI that is defined by its ability to solve unlimited diverse tasks. There is a mismatch between our target (AGI) and our actual evaluation methods, creating a gap between AGI and current SOTA models. We want agents to be open-ended, possessing the capacity to generate endless solutions for scenarios that may not yet exist. As Elon Musk said in an interview: if an spaceship lands on the highway, a truly intelligent autonomous driving system must react correctly. Our objective is a “super-agent” capable of handling infinite novelty, not merely taking a fixed exam.</p> <h2 id="3-the-blueprint-of-generative-evaluation">3. The Blueprint of Generative Evaluation</h2> <p>As we have discussed, static benchmarks are facing an existential crisis due to data contamination and saturation. The industry is shifting towards Generative Evaluation, a paradigm where the benchmark is not a fixed dataset, but an intelligent engine capable of producing an infinite stream of novel tasks.</p> <h3 id="31-why-generative-evaluation-solves-the-problems">3.1 Why Generative Evaluation Solves the Problems?</h3> <p>Generative Evaluation fundamentally addresses the core failures of static benchmarks by shifting the focus from a fixed dataset to a dynamic, infinite task-generation engine. Crucially, it resolves the issues of contamination, saturation, and the high cost of manual curation through three mechanisms:</p> <ul> <li> <strong>Infinite Diversity:</strong> By generating an unbounded stream of diverse and novel tasks, the system ensures test cases are truly unseen during training, making rote memorization mathematically impossible. This forces models to rely on genuine reasoning and generalization.</li> <li> <strong>Targets the New Pattern:</strong> Unlike static benchmarks biased toward high-frequency patterns, generative evaluation can be deliberately engineered to probe high-impact corner cases and “sensible factors” that challenge a model’s true generalization limits. Thus, evaluation shifts from measuring average performance to stress-testing critical weaknesses.</li> <li> <strong>Scalability and Efficiency:</strong> It replaces costly, slow human labor with an automated pipeline that continuously generates and verifies tasks. Since tasks are generated programmatically, the time and financial costs are often orders of magnitude lower than manual curation, making sustainable, long-term progress feasible.</li> </ul> <h3 id="32-generating-diverse-contamination-resistant-tasks">3.2 Generating Diverse, Contamination-Resistant Tasks</h3> <p>Simply instructing an LLM to “generate 100 new questions” often yields repetitive, low-quality output. A robust generative framework must follow a structured pipeline ensuring both diversity (to prevent memorization) and validity (to ensure fairness). Based on recent cutting-edge research in generative evaluation [@mcu; @dynabench; @procegen; @kumo; @unicode], we can distill diversity into two main directions:</p> <h4 id="321-inter-task-diversity-the-breadth-of-knowledge">3.2.1 Inter-task Diversity: The Breadth of Knowledge</h4> <p>This dimension represents coverage across distinct domains. Just as a student must study Math, History, and Science, an AI agent must be tested across different domains. Inter-task diversity has long been valued in static datasets (e.g., the ALE benchmark [@ALE] with 55 different games). Generative evaluation frameworks maintain this breadth: for instance, MCU spans 11 major categories and 41 subcategories (e.g., Combat, Farming) [@mcu], UniCode organizes tasks by 15 algorithmic tags (e.g., Dynamic Programming) [@unicode], and KUMO generates scenarios across 100 distinct domains [@kumo]. This prevents models from becoming narrow specialists.</p> <h4 id="322-intra-task-diversity-the-depth-of-variation">3.2.2 Intra-task Diversity: The Depth of Variation</h4> <p>This often-overlooked dimension refers to generating variations within a single task type—tasks that share a goal but differ in their initial states or parameters. Using ALE as an example: typically, a game level’s layout is fixed, allowing an agent to memorize a specific trajectory. However, this is where generative evaluation truly shines: it enables state space explosion. Consider this comparison: adding 100 different game levels merely requires the model to memorize 100 separate solutions. However, when introducing intra-task diversity, e.g., identifying 10 control variables for a game level (monster count, enemy health, inventory tools, etc.), each with 5 possible values, the state space grows to $10^5$ distinct configurations. This dramatically raises the difficulty of rote memorization and encourages generalized problem-solving.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-illusion-of-mastery/figure5-480.webp 480w,/2026/assets/img/2026-04-27-illusion-of-mastery/figure5-800.webp 800w,/2026/assets/img/2026-04-27-illusion-of-mastery/figure5-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-illusion-of-mastery/figure5.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 5. The Procgen benchmark expands intra-task diversity, increasing the state space to massive magnitudes (x-axis). As observed, only when the state space exceeds a certain threshold can we truly measure generalization performance (where training and testing curves converge) [@procegen].</figcaption> </figure> <h3 id="33-discovering-novel-reasoning-patterns">3.3 Discovering Novel Reasoning Patterns</h3> <p>However, not all task variables are effective. A common pitfall is generating variables that alter superficial aspects without creating new reasoning challenges. Research from UniCode [@unicode] reveals that sampling seed questions and merely changing the “story background” variable without altering the core logic does not result in significant performance differences. For example: when converted a card-game queue/stack simulation into an operating-system scheduling scenario (different narrative, same logic). This indicates that textual diversity alone is a solved problem for advanced LLMs and is not an “effective variable” for rigorous evaluation.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-illusion-of-mastery/figure6-480.webp 480w,/2026/assets/img/2026-04-27-illusion-of-mastery/figure6-800.webp 800w,/2026/assets/img/2026-04-27-illusion-of-mastery/figure6-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-illusion-of-mastery/figure6.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 6. Modifying superficial text variables yields no performance gap between 'seed' and 'shadow' questions. In contrast, variables that alter core algorithm logic or introduce new knowledge combinations (CodeGenQS) create a substantial performance drop, proving they generate novel reasoning patterns [@unicode].</figcaption> </figure> <p>Therefore, the key is to identify “effective variables”—factors where a model’s generalization is prone to break down. Current approaches often use expert intuition: decompose a task into candidate variables, and adjust one at a time while keeping others fixed. If a variable causes significant performance variation, it signals incomplete generalization and qualifies as effective. By identifying the right set of such variables, we unlock an infinite array of unique test cases, each embodying a novel reasoning pattern.</p> <h3 id="34-ensuring-reliability-in-the-generative-pipeline">3.4 Ensuring Reliability in the Generative Pipeline</h3> <p>The biggest risk in generative evaluation is producing “garbage”—unsolvable problems or incorrect metrics. Since we cannot rely on human annotators for infinite tasks, we must automate the verification process.</p> <h4 id="341-ensuring-solvability">3.4.1 Ensuring Solvability</h4> <p>We must guarantee that the generated preconditions allow for a solution. Domain-specific tools are often used for verification. Here are two examples:</p> <ul> <li> <strong>Symbolic Guarantees:</strong> KUMO [@kumo] employs a SAT Solver (Boolean Satisfiability) during the generation phase. This mathematically enforces that every generated game board has a valid logical path to the truth, preventing impossible scenarios.</li> <li> <strong>Simulator Verification:</strong> MCU [@mcu] utilizes the MineStudio simulator, a popular test bed for the Minecraft platform, as a ground-truth verifier. The LLM-generated initialization commands are executed in the game engine; if the engine throws an error (e.g., Spawning a mob type that doesn’t exist.), the system detects it and triggers a self-reflection loop to correct the initial configuration.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-illusion-of-mastery/figure7-480.webp 480w,/2026/assets/img/2026-04-27-illusion-of-mastery/figure7-800.webp 800w,/2026/assets/img/2026-04-27-illusion-of-mastery/figure7-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-illusion-of-mastery/figure7.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 7. MineStudio acts as a natural verification environment, returning error codes to help correct mistakes in generative tasks [@mcu].</figcaption> </figure> <h4 id="342-ensuring-label-correctness">3.4.2 Ensuring Label Correctness</h4> <p>Without human labels, grading must also be automated. Solutions vary by task type:</p> <ul> <li> <strong>Programmatic Signals:</strong> On established testing platforms like MineStudio, well-defined tasks (e.g., “mine a diamond”) provide inherent success signals from the environment.</li> <li> <strong>Model-as-Judge:</strong> For open-ended tasks (e.g., “build a scary house”), LLMs or VLMs act as judges. For example, MCU uses a Vision-Language Model (GPT-4V). By feeding the VLM specific, generated criteria (e.g., “Does the structure have a roof?”), it achieves 91.5% alignment with human raters [@mcu].</li> <li> <strong>Algorithmic Oracles:</strong> For logic-based tasks, KUMO [@kumo] computes an optimal search policy as the oracle, while Unicode uses brute-force algorithms to maximize accuracy [@unicode].</li> </ul> <p>Both stages can be validated by periodically sampling tasks for human review.</p> <h2 id="4-discussion">4. Discussion</h2> <h3 id="41-managing-error-in-generative-frameworks">4.1 Managing Error in Generative Frameworks</h3> <p>One might worry: “Is an automated evaluation pipeline as accurate as human evaluation?” In practice, as data scales up, 100% accuracy becomes an impractical goal. When the test set is uncontaminated, the total error primarily comes from two sources:</p> <ul> <li>Sampling error, influenced by the number of tasks;</li> <li>System error, introduced by the generative evaluation process.</li> </ul> <p>Human-curated evaluation carries zero system error, but due to the limited number of tasks, sampling error can be high. For example, if Model A and Model B perform equally well overall but excel in different areas, a small task set biased toward Model A’s strengths may misleadingly show it as superior. In contrast, a generative evaluation system, while having some inherent system error, allows that error to be estimated and corrected. For instance, by testing the model on a small set of known wrong examples, we can measure its false pass rate $q_e$. Then, using the formula: [ p = \frac{\text{Observed Pass Rate} - \text{Generation Error Rate} \times q_e}{1 - \text{Generation Error Rate}} ] We can recover a calibrated estimate of the model’s true performance. Moreover, with a sufficiently large and diverse set of tasks, the sampling error of the generative system becomes negligible. Therefore, by combining scalable task generation with systematic error correction, we can achieve a more reliable evaluation framework, even if it requires embracing a small amount of controlled noise.</p> <h3 id="42-potential-influence-on-society">4.2 Potential Influence on Society</h3> <p>Static datasets inevitably suffer from inherent human bias, conflicts of interest, and financial incentives [@peeking]. For instance, when an evaluation firm also provides training data, it faces an ethical conflict, incentivized to design benchmarks that favor its clients’ models. Furthermore, expert annotators introduce subjective preference bias; if they previously contributed to a model’s training data, their unconscious criteria may align with that model’s style. This systematic human bias prevents scores from reflecting real-world performance for a diverse user base. Generative Evaluation offers a critical path to mitigate these external biases by automating and standardizing the task creation process, potentially utilizing multiple LLM generators to further diversify and neutralize output biases.</p> <h3 id="43-limitations">4.3 Limitations</h3> <p>Generative evaluation still heavily rely on human priors to pick variables. Previously, datasets like Dynabench used human annotators to manually flag adversarial examples where models failed [@dynabench]. Now, we have elevated the abstraction level from the “sample” to the “variable,” which significantly saves time and allows for automated generation. However, the selection of these variable factors still relies strongly on expert knowledge. If the variables are poorly defined, the quality of the entire dataset suffers.</p> </body></html>