<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> On the Measure of a Model - From Intelligence to Generality | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Benchmarks like ARC, Raven-style puzzles, and the Blackbird Task are often treated as measures of LLM intelligence. But intelligence is a moving target—hard to define and even harder to link to what we actually need models to do, like answer questions, summarize text, or write code. Optimizing for these abstract tests can pull evaluation away from real-world usefulness. We argue for a shift from chasing intelligence to measuring generality. This reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/measuregen/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "On the Measure of a Model - From Intelligence to Generality",
            "description": "Benchmarks like ARC, Raven-style puzzles, and the Blackbird Task are often treated as measures of LLM intelligence. But intelligence is a moving target—hard to define and even harder to link to what we actually need models to do, like answer questions, summarize text, or write code. Optimizing for these abstract tests can pull evaluation away from real-world usefulness. We argue for a shift from chasing intelligence to measuring generality. This reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>On the Measure of a Model - From Intelligence to Generality</h1> <p>Benchmarks like ARC, Raven-style puzzles, and the Blackbird Task are often treated as measures of LLM intelligence. But intelligence is a moving target—hard to define and even harder to link to what we actually need models to do, like answer questions, summarize text, or write code. Optimizing for these abstract tests can pull evaluation away from real-world usefulness. We argue for a shift from chasing intelligence to measuring generality. This reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#why-intelligence-is-problematic">Why Intelligence is Problematic</a> </div> <ul> <li> <a href="#conceptual-instability">Conceptual Instability</a> </li> <li> <a href="#illusion-of-competence">Illusion of Competence</a> </li> <li> <a href="#rethinking-our-ground">Rethinking our Ground</a> </li> </ul> <div> <a href="#unpacking-intelligence">Unpacking Intelligence</a> </div> <ul> <li> <a href="#generality-is-independent">Generality is Independent</a> </li> <li> <a href="#only-generality-is-necessary">(Only) Generality is Necessary</a> </li> </ul> <div> <a href="#what-generality-offers">What Generality Offers</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Many people see large language models (LLMs) <d-cite key="radford2019language, brown2020language, burton2024large, steyvers2025large"></d-cite> as steps toward <em>artificial intelligence</em>—or even early hints of <em>artificial general intelligence</em> (AGI) <d-cite key="bubeck2023sparks, zhong2024evaluation, ilic2024evidence"></d-cite>. But if we want to know whether LLMs are actually becoming more capable, we first have to ask a question: <strong>what does it mean for an AI system to be “intelligent”?</strong></p> <p>Despite its appeal, we discuss how <em>intelligence</em> is a vague and unstable concept, used differently across fields <d-cite key="howard1993intelligence, legg2007collection, wang2019defining, mitchell2024debates"></d-cite> and difficult to pin down. In empirical evaluation practice, the quest for intelligence is outsourced to benchmarks of intelligence-indicating tasks <d-cite key="horn1968organization, simon2024identifying"></d-cite> such as pattern abstraction <d-cite key="chollet2019measure"></d-cite>, reasoning <d-cite key="srivastava2023beyond, kazemi2025big"></d-cite>, or even general knowledge <d-cite key="phan2025humanity, hendrycks2020measuring, rein2024gpqa"></d-cite>. Yet these benchmarks frequently fall short of predicting the outcomes we actually care about, such as human preferences or real-world task performance (as we show in the next section). This raises a deeper question: are we evaluating the right thing at all?</p> <p>We ask what commitments are implicitly bundled into the idea of evaluating models by their “intelligence.” We unpack these into three assumptions: <strong>generality</strong>, <strong>stability</strong>, and <strong>realism</strong>. We then ask which of these are actually needed for evaluation—and find that only <strong>generality</strong> is necessary. The other two, while often taken for granted, have little conceptual or empirical grounding.</p> <p>Next, we ask whether generality itself provides a solid foundation. We show that it does: generality is conceptually stable (avoiding the ambiguity of intelligence) and it is formally grounded in multitask learning theory, which offers a principled way to measure reliable performance across tasks.</p> <p>Ultimately, we ask the field to shift the core evaluative question. Instead of <em>“Is this model intelligent?”</em>, we propose a more concrete and actionable one: <em>“How general and reliable is this system across the tasks we care about?”</em> This reframing leads to a more practical foundation for assessing progress in modern AI.</p> <h2 id="why-intelligence-is-problematic">Why Intelligence is Problematic</h2> <p>The idea that some models are simply <em>more intelligent</em> than others has become central to how progress in language modeling is communicated <d-cite key="bubeck2023sparks, morris2023levels"></d-cite>. Benchmarks such as ARC <d-cite key="chollet2019measure"></d-cite>, Raven tests <d-cite key="abdelkarim2025evaluating"></d-cite> or the Blackbird Task <d-cite key="merlo-2023-blackbird"></d-cite> are often used to make such claims, implicitly treating benchmark performance as a proxy for general intelligence or capability. Yet, what these benchmarks actually measure is rarely interrogated, and our analysis suggests that the picture they paint might be incomplete and potentially misleading.</p> <h3 id="conceptual-instability">Conceptual Instability</h3> <p>The discussion about general intelligence originates with the controversy between Spearman and Thomson <d-cite key="thomson1916"></d-cite>, but has since resurfaced within neuroscience <d-cite key="sims2013theory"></d-cite>, cognitive science <d-cite key="sternberg1990metaphors, pfeifer2001understanding, sternberg2005cognition"></d-cite>, and education <d-cite key="demetriou20051, ritchie2018much"></d-cite> — yet there is little consensus on what constitutes intelligence. Efforts to anchor intelligence in neuroscience have fallen short <d-cite key="mackintosh1986biology, sternberg2003biological, pietschnig2015meta, gignac2017brain"></d-cite>. Similarly, intelligence has long resisted a stable, unified definition in cognitive science. Intelligence may not be a unitary, well-specified capacity, but a family of loosely related, context-sensitive abilities, shaped by cultural, developmental, neural, and environmental factors <d-cite key="nisbett2001culture, sternberg2004culture, kan2013nature"></d-cite>.</p> <p>In AI, we have largely inherited this ambiguity. The focus has been on defining intelligence in relation to the abilities of systems to perform certain tasks in environments in ways similar to humans <d-cite key="legg2007collection, poole2010artificial, russell2016artificial, kaplan2019siri, Bartneck2021"></d-cite>. Just as IQ Tests are incapable of indicating performance in significant cognitive capacities <d-cite key="schonemann1983iq, detterman1989correlations, stanovich2009intelligence, raven1989standard, gould1996mismeasure, henrich2010weirdest"></d-cite>, intelligence benchmarks have been argued to be problematic <d-cite key="bender2021dangers"></d-cite>.</p> <h3 id="illusion-of-competence">Illusion of Competence</h3> <p>Despite their widespread use, intelligence benchmarks often fail to reflect a model’s effectiveness in real-world applications. We provide empirical evidence in Figure 1 and Figure 2 that strong performance on intelligence benchmarks exhibits limited correlation with human preference or task performance. As shown in Figure 1, the performance trends of different models for both ARC-AGI <d-cite key="chollet2019measure"></d-cite> (considered frontier intelligence benchmarks) and LMArena <d-cite key="chiang2024chatbot"></d-cite> differ significantly, i.e., a model that scores higher in ARC is not necessarily also better in LMArena (considered a human preference benchmark).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-measuregen/figure1-480.webp 480w,/2026/assets/img/2026-04-27-measuregen/figure1-800.webp 800w,/2026/assets/img/2026-04-27-measuregen/figure1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-measuregen/figure1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 1: Performance comparison across ARC-AGI and LMArena benchmarks. </div> <p>Also, as seen in Figure 2, these performance trends do not translate into universal capability and into reliable performance across the kinds of real-world tasks language models are often used for. This undermines the assumption that intelligence benchmark scores are good predictors of general-purpose competence.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-measuregen/figure2-480.webp 480w,/2026/assets/img/2026-04-27-measuregen/figure2-800.webp 800w,/2026/assets/img/2026-04-27-measuregen/figure2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-measuregen/figure2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 2: Performance comparison across ARC-AGI and real-world task like OpenBookQA, Entity Extraction, and StackUnseen. </div> <h3 id="rethinking-our-ground">Rethinking our Ground</h3> <p>The issues point to a deeper problem with how intelligence has been framed in AI evaluation. The continued reliance on intelligence-style benchmarks reflects an implicit belief that we are measuring something stable and meaningful, even as evidence suggests otherwise. Benchmarks are treated as if they reflect an underlying cognitive trait, and success on some fixed tasks is seen as progress toward intelligence (but never quite reaching intelligence). These patterns persist because the ambiguity surrounding intelligence allows these assumptions to remain unchallenged. What, then, are we actually assuming when we say a model is becoming more intelligent? The next section will detail assumptions through evaluation studies.</p> <h2 id="unpacking-intelligence">Unpacking Intelligence</h2> <p>One of the earliest definitions of intelligence came from Marvin Minsky, a founding father of AI, and was as follows: <em>Artificial Intelligence is the science of making machines do things that would require intelligence if done by people</em> <d-cite key="bolter1984turing, fjelland2020general"></d-cite>. However, this leaves open the question of what intelligence is, and how we best make machines do tasks that require it. In the literature, we have two divergent commitments.</p> <ul> <li> <strong>i)</strong> The first view emphasizes <strong>Generality</strong>. A system is intelligent because it is able to achieve a wide range of goals. Intelligence is demonstrated directly by the scope of performance.<d-footnote>Here, generality refers to the empirical manifestation of task performance across environments, not an abstract property of "generalization" in the learning-theoretic sense.</d-footnote> </li> <li> <strong>ii)</strong> The second view emphasizes <strong>Realism</strong>. A system is intelligent because it possesses some latent property that allows it to achieve a wide range of goals. Intelligence is not only observed but posited as an explanatory trait.</li> </ul> <p>Once we take the realist route, however, further commitments follow. Realism, i.e., the idea that intelligence refers to a fixed, real property, implies that the capacities unlocked by intelligence are fixed. This means that if we can enumerate these capacities, we can design representative task suites and treat performance there as evidence, i.e, a system is intelligent if it does well on a fixed representative set of tasks that embody intelligence <d-cite key="chollet2019measureintelligence"></d-cite>. If not, benchmarks only approximate intelligence, i.e, a system is intelligent by possessing intelligence <d-cite key="ilievski2025aligning"></d-cite>, and the benchmark performance of a system is only a direct measure of its intelligence.</p> <p>Synthesizing these positions, we can see that the primary approaches to evaluating intelligence collapse onto three fundamental commitments:</p> <table> <thead> <tr> <th>Assumption</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><strong>Generality</strong></td> <td>We should develop an <em>all-purpose</em> or <em>multi-task</em> system.</td> </tr> <tr> <td><strong>Stability</strong></td> <td>A system should solve a <em>fixed</em> set of high-value intelligence indicating tasks.</td> </tr> <tr> <td><strong>Realism</strong></td> <td>We should focus on modelling <em>intelligence</em> itself.</td> </tr> </tbody> </table> <p><strong>Generality:</strong> This assumption is also common in both the AGI literature <d-cite key="morris2023levels"></d-cite> and in practical NLP benchmarking initiatives, like Big Bench <d-cite key="srivastava2023beyond, kazemi2025big"></d-cite> or HELM <d-cite key="bommasani2023holistic"></d-cite>, that prioritize cross-domain competence on multiple language tasks. Even the instruction-tuning of models like T0 <d-cite key="sanh2022multitask"></d-cite>, FLAN-T5 <d-cite key="longpre2023flan"></d-cite>, or OPT-IML <d-cite key="iyer2022opt"></d-cite> all focus training on diverse prompts and task formulations with the explicit aim of cross-task generalization. These efforts reflect a growing recognition that narrow task performance is insufficient, especially as LLMs are increasingly expected to act as generalist agents. <d-cite key="hernandez2021general, moor2023foundation, zhang2024generalist"></d-cite></p> <p><strong>Stability:</strong> Stability assumes that there exists a fixed set of tasks on which evaluation can reliably represent intelligence or capability. Recent research has often tried to identify the fixed set of such tasks <d-cite key="hendrycks2025definition"></d-cite> with explicit focus on few such tasks as reasoning <d-cite key="ilic2024evidence, 10.5555/3600270.3602070, kojima2022large, morishita2024enhancing"></d-cite> or planning <d-cite key="valmeekam2022large, valmeekam2023planning, valmeekam2023planbench"></d-cite> as important indicators of LLM performance.</p> <p><strong>Realism:</strong> Perhaps the most pervasive assumption is that benchmark success reflects a singular underlying cognitive trait called “intelligence”. This can be seen in general tests of intelligence <d-cite key="chollet2019measure, phan2025humanity, cai2025mm"></d-cite> or IQ-style comparisons. <d-cite key="pellert2024ai, huang2024measuring, abdelkarim2025evaluating"></d-cite></p> <p>Next, we show why generality is independent and also necessary and sufficient for evaluating models.</p> <h3 id="generality-is-independent">Generality is Independent</h3> <p>We want to argue that accepting <strong>Generality</strong> should not automatically lead us also to accept <strong>Stability</strong> and <strong>Realism</strong>.</p> <p><strong>Formal setup.</strong> Let $ T $ be a (possibly infinite) set of tasks endowed with a probability measure $ Q $ (the <em>task environment</em>). Each model $ M $ induces a measurable performance function</p> \[f_M : \mathcal{T} \to [0,1], \qquad t \mapsto f_M(t),\] <p>where $ f_M(t) $ denotes the normalized performance of model $ M $ on task $ t $. We want to define what “evaluating M” means under the three alternative assumptions.</p> <p><strong>Definition (Generality).</strong> The <em>generality</em> of a model is its expected performance across the task environment:</p> \[E_G(M) = \mathbb{E}_{t \sim Q}\big[ f_M(t) \big].\] <p>It assumes no fixed task set or latent variable, only performance averaged over the environment $ Q $.</p> <p><strong>Definition (Stability).</strong> The <em>stability</em> of a model is its aggregated performance on a fixed benchmark subset $ S \subset \mathcal{T} $:</p> \[E_S(M) = F\big( ( f_M(t) )_{t \in S} \big),\] <p>where $ F $ is a predetermined aggregation functional. It assumes that the same benchmark tasks remain representative.</p> <p><strong>Definition (Realism).</strong> The <em>realism</em> assumption posits a latent cognitive representation $ I(M) \in \mathbb{R}^k $ and task-specific decoding functions $ g_t : \mathbb{R}^k \to [0,1] $, such that performance derives from this shared latent space:</p> \[E_R(M) = \mathbb{E}_{t \sim Q}\big[ g_t(I(M)) \big].\] <p>It assumes that observable task success reflects an underlying property, interpreted as “intelligence”. Now consider the following thought experiment!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-measuregen/genthought-480.webp 480w,/2026/assets/img/2026-04-27-measuregen/genthought-800.webp 800w,/2026/assets/img/2026-04-27-measuregen/genthought-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-measuregen/genthought.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Thought experiment </div> <p>This thought experiment illustrates a key insight: <strong>generality can be pursued without assuming either stability or realism</strong>. One can seek broad capability without committing to a fixed task set or a unified latent construct.</p> <hr> <h3 id="only-generality-is-necessary">(Only) Generality is Necessary</h3> <p>The necessity of generality becomes clear when we ask what evaluation aims to uncover. Following Hernández-Orallo et al. (2021) <d-cite key="hernandez2021general"></d-cite>, evaluation can be expressed as a mapping from task to expected success, yielding an <em>agent-characteristic curve</em> (ACC). Each model $ M $ induces a performance function $ f_M : \mathcal{T} \to [0,1] $. Let $ h(t) $ denote the <em>difficulty</em> of task $ t $. The ACC captures how success declines as difficulty increases:</p> \[\psi_M(h) = \mathbb{E}_{t\sim Q \,|\, h(t)=h}\big[f_M(t)\big]\] <p><strong>Why evaluation requires generality.</strong> If two systems $ M_1 $ and $ M_2 $ yield the same mean performance but exhibit different $ \psi(h) $ curves, they cannot be regarded as equivalent: one may fail on trivial tasks while the other fails only on hard ones. A meaningful evaluation must depend on the <em>shape</em> of $ \psi_M(h) $, not merely its average.</p> <p>We define the <em>spread</em> of the ACC as:</p> \[S_M^2 = \int_0^\infty (h - \bar{h}_M)^2 \psi_M(h)\,dh\] <p>A small $ S_M $ means performance is concentrated before a clean decline—predictable and <em>general</em> behavior. A large $ S_M $ indicates scattered or specialized success. The reciprocal quantity</p> \[\Gamma_M = \frac{1}{S_M}\] <p>is thus a direct measure of generality.<d-footnote>This formulation follows the generality-spread relation introduced by Hernández-Orallo et al. (2021).</d-footnote></p> <p><strong>Why only generality.</strong> Without accounting for $ \Gamma_M $, evaluation collapses into an unanchored average that changes arbitrarily with task sampling. Once tasks are organized along a difficulty axis, the measure of how tightly performance is distributed becomes the only element ensuring comparability across environments. Therefore, <em>generality is the necessary condition for evaluation to be coherent and transferable</em>. Neither <em>Stability</em> nor <em>Realism</em> is required: fixing a task subset removes the difficulty structure, and positing a latent “intelligence” adds nothing to the observable shape of $ \psi_M(h) $.</p> <hr> <h2 id="what-generality-offers">What Generality Offers</h2> <p>We identify two factors here: generality is conceptually stable and it is theoretically grounded in multitask learning.</p> <p><strong>Conceptual Stability:</strong> Generality offers a conceptually stable and empirically grounded alternative i.e one that aligns more directly with how models are used and deployed. Prior work in cognitive science and AI emphasizes generalization as the hallmark of intelligent behavior <d-cite key="lake2017building, tenenbaum2001generalization, yu2020meta, tomov2021multi, ilievski2025aligning"></d-cite>. Moreover, generality is flexible to task drift and evolving use cases. Unlike static benchmarks, which quickly lose relevance as models saturate their task sets or learn test-specific heuristics, generality-based evaluation can accommodate new tasks as they emerge. It requires only that we specify a diverse and representative sample of tasks at evaluation time - not that we define a canonical set of “core” challenges in advance. In short, generality is not only the most practically relevant evaluation principle, but it is also the most conceptually resilient.</p> <p><strong>Theoretical Grounding.</strong> Generality is supported by learning theory and empirical evidence—especially multitask learning (MTL) <d-cite key="caruana1993multitask,caruana1997multitask"></d-cite> and “learning to learn” in humans <d-cite key="thrun1998learning,ilievski2025aligning"></d-cite>.</p> <p><strong>Theorem:</strong> Consider an environment $\mathcal{E}$ consisting of a distribution $Q$ over tasks, where each task $P \sim Q$ is a distribution over data in a learning problem. Let $\mathcal{H}$ be a hypothesis class, $L_P(h)$ be the loss on task $P$, and $L_Q(h)$ be the model’s environment average error. Then, for any $\delta&gt;0$ (where $\delta$ is the confidence parameter), with probability at least $1-\delta$, the generalization bound is reduced by approximately a factor of $\sqrt{n}$ in the multi-task case.</p> <p><strong>Proof:</strong> We proceed in three steps:</p> <p><strong>Step 1: Generalization Bound for Single-Task Environment (STE)</strong> Let $\mathbb{E}$ be an environment consisting of a distribution $Q$ over tasks. For each task $P \sim Q$, let true loss of $h \in \mathcal{H}$ be given by $L_P(h) = \mathbb{E}_{(x,y)\sim P}[\ell(h(x),y)]$ where $\ell$ is a loss function. The empirical loss computed from $m$ i.i.d. samples drawn from $P$:</p> \[\hat{L}_P(h) = \frac{1}{m} \sum_{i=1}^{m} \ell\bigl(h(x_i), y_i\bigr)\] <p>and the environment-average loss is: $L_Q(h) = \mathbb{E}_{P\sim Q}[L_P(h)].$ By standard PAC-learning results (see <d-cite key="baxter2000model"></d-cite>), with probability at least $1-\delta$:</p> \[\sup_{h\in \mathcal{H}} |L_P(h) - \hat{L}_P(h)| = O\Bigg(\sqrt{\frac{C + \ln(1/\delta)}{m}}\Bigg).\] <p><strong>Step 2: Generalization Bound for Multi-Task Environment (MTE)</strong> Here we evaluate $h$ on $n$ tasks $P_1,\dots,P_n \sim Q$, each with $m$ samples, yielding the average empirical error as an estimate of $L_Q(h)$: $\frac{1}{n}\sum_{i=1}^n \hat{L}_{P_i}(h)$.</p> <p>There are two sources of generalization error:</p> <ol> <li> <p><em>Within-task generalization:</em> By the same PAC bound as in STE, for each $P_i$, \(\sup_{h\in \mathcal{H}} |L_{P_i}(h) - \hat{L}_{P_i}(h)| = O\Bigg(\sqrt{\frac{C + \ln(n/\delta)}{m}}\Bigg).\)</p> </li> <li> <p><em>Across-task generalization:</em> Since tasks are drawn i.i.d. from $Q$, by Hoeffding’s inequality: \(\sup_{h\in\mathcal{H}} |L_Q(h) - M(h)| = O\Bigg(\sqrt{\frac{C + \ln(1/\delta)}{n}}\Bigg),\) where $M(h) = \frac{1}{n} \sum_{i=1}^{n} L_{P_i}(h)$ is the average true error across task.</p> </li> </ol> <p>Combining these bounds via the triangle inequality,</p> \[\sup_{h\in\mathcal{H}} \Big|L_Q(h) - \frac{1}{n} \sum_{i=1}^n \hat{L}_{P_i}(h)\Big| \leq O\Bigg(\sqrt{\frac{C + \ln(n/\delta)}{m}} + \sqrt{\frac{C + \ln(1/\delta)}{n}}\Bigg).\] <p><strong>Step 3: Comparison of Bounds.</strong></p> <p>We derive the STE bound:</p> \[L_Q(h) \leq \hat{L}_{P}(h) + O\Bigg(\sqrt{\frac{C + \ln(1/\delta)}{m}}\Bigg),\] <p>We derive the MTE bound as:</p> \[L_Q(h) \leq \frac{1}{n}\sum_{i=1}^n \hat{L}_{P_i}(h) + O\Bigg(\sqrt{\frac{C + \ln(1/\delta)}{n m}}\Bigg).\] <p>Thus, we see that in a learning environment where tasks are drawn i.i.d. from a distribution Q, the single–task generalization bound decays at a rate inversely proportional to $m$ (the number of samples in the task) while for multi-task environments, the error decays much faster at a rate of $1/\sqrt{mn}$ where $n$ is the number of tasks evaluated in our environment. This $\sqrt{n}$ reduction results from combining within-task PAC bounds with an across-task concentration (via Hoeffding’s inequality), thereby demonstrating that multi–task evaluation (or learning) effectively reduces the estimation variance.</p> <h2 id="conclusion">Conclusion</h2> <p>In this work, we have put forth the perspective that model evaluation should be grounded in <em>generality</em>—the breadth and consistency of performance across tasks—rather than in abstract notions of <em>intelligence</em>. Unlike intelligence, which rests on unstable conceptual and empirical foundations, generality offers a measurable, theoretically grounded, and operationally meaningful principle. It captures what truly matters for deployment: how reliably a system performs when tasks vary or evolve.</p> <p>By showing that generality is both independent from, and sufficient for, coherent evaluation, we provide a framework that unifies conceptual clarity with formal rigor. This reframing is increasingly necessary as language models are applied in open and shifting environments, where success cannot be defined by static benchmarks <d-cite key="kiela2021dynabench,hofmann2025fluid,kim2025benchmark"></d-cite> or latent cognitive claims <d-cite key="gignac2015raven,blili2025stop"></d-cite>. Future progress in AI should therefore be assessed not by how “intelligent’’ a model appears, but by how <em>generally and dependably</em> it performs across the diverse tasks we ask of it.</p> <hr> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-measuregen.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/general-agent-evaluation/">Ready For General Agents? Let's Test It.</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-vlms-waste-their-vision/">Why vlms waste their vision</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>