<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Speeding up Training of Model-Free Reinforcement Learning :A Comparative Evaluation for Fast and Accurate Learning | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Reinforcement Learning (RL) represents a powerful framework for solving sequential decision-making problems in dynamic environments across diverse domains, such as control of robots or optimization of profit. However, its practical implementation requires navigating a variety of software packages, encompassing deep learning libraries (e.g., TensorFlow, PyTorch, JAX/Flax), environment frameworks (e.g., Gymnasium, Numpy), and hyperparameter optimization techniques and libraries. This post critically evaluates the common PyTorch, Gymnasium, and NumPy RL stack by comparing it to a faster alternative:JAX/Flax for both of model training and environment simulation. A Gridworld example evaluating both training speed and accuracy is utilized to test each of these packages. Additionally, we complement our example by a comprehensive tracking and monitoring of the training process using MLflow along with a thorough hyperparameters optimization via Optuna. The post concludes with a discussion of the results and final recommendations for optimal use cases of each of these packages."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/speeding-up-rl/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Speeding up Training of Model-Free Reinforcement Learning :A Comparative Evaluation for Fast and Accurate Learning",
            "description": "Reinforcement Learning (RL) represents a powerful framework for solving sequential decision-making problems in dynamic environments across diverse domains, such as control of robots or optimization of profit. However, its practical implementation requires navigating a variety of software packages, encompassing deep learning libraries (e.g., TensorFlow, PyTorch, JAX/Flax), environment frameworks (e.g., Gymnasium, Numpy), and hyperparameter optimization techniques and libraries. This post critically evaluates the common PyTorch, Gymnasium, and NumPy RL stack by comparing it to a faster alternative:JAX/Flax for both of model training and environment simulation. A Gridworld example evaluating both training speed and accuracy is utilized to test each of these packages. Additionally, we complement our example by a comprehensive tracking and monitoring of the training process using MLflow along with a thorough hyperparameters optimization via Optuna. The post concludes with a discussion of the results and final recommendations for optimal use cases of each of these packages.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "https://en.wikipedia.org",
                "affiliations": [
                  {
                    "name": "Anonymous",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Speeding up Training of Model-Free Reinforcement Learning :A Comparative Evaluation for Fast and Accurate Learning</h1> <p>Reinforcement Learning (RL) represents a powerful framework for solving sequential decision-making problems in dynamic environments across diverse domains, such as control of robots or optimization of profit. However, its practical implementation requires navigating a variety of software packages, encompassing deep learning libraries (e.g., TensorFlow, PyTorch, JAX/Flax), environment frameworks (e.g., Gymnasium, Numpy), and hyperparameter optimization techniques and libraries. This post critically evaluates the common PyTorch, Gymnasium, and NumPy RL stack by comparing it to a faster alternative:JAX/Flax for both of model training and environment simulation. A Gridworld example evaluating both training speed and accuracy is utilized to test each of these packages. Additionally, we complement our example by a comprehensive tracking and monitoring of the training process using MLflow along with a thorough hyperparameters optimization via Optuna. The post concludes with a discussion of the results and final recommendations for optimal use cases of each of these packages.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction-and-installation">Introduction and Installation</a> </div> <div> <a href="#standardize-your-environment-with-gymnasium">Standardize Your Environment with Gymnasium</a> </div> <div> <a href="#an-example-for-creating-a-custom-gym-environment-and-training-with-dqn">An Example for Creating a Custom Gym Environment and Training with DQN</a> </div> <div> <a href="#tracking-rl-experiments-with-mlflow">Tracking RL Experiments with MLflow</a> </div> <div> <a href="#optimizing-rl-hyperparameters-with-optuna">Optimizing RL Hyperparameters with Optuna</a> </div> <ul> <li> <a href="#types-of-hyperparameter-optimization-methods">Types of Hyperparameter Optimization Methods</a> </li> <li> <a href="#steps-for-hyperparameter-optimization-in-optuna">Steps for Hyperparameter Optimization in Optuna</a> </li> <li> <a href="#training-code-structure-in-optuna">Training Code Structure in Optuna</a> </li> </ul> <div> <a href="#accelerating-environment-rollout-and-model-training-with-jax-and-flax">Accelerating Environment Rollout and Model Training with JAX and Flax</a> </div> <div> <a href="#flax">FLAX</a> </div> <div> <a href="#results-and-final-take-away">Results and Final Take-away</a> </div> <div> <a href="#additional-jax-libraries">Additional JAX Libraries</a> </div> <div> <a href="#references">References</a> </div> </nav> </d-contents> <h1 id="introduction-and-installation">Introduction and Installation</h1> <p>The typical workflow for applying Reinforcement Learning to optimize an objective involves defining Markov Decision Process (MDP) variables, such as state and action spaces $\mathcal{S}, \mathcal{A}$, actor model (agent) $\pi$ , and a reward function $r(a,s,s’)$ <d-cite key="24"> </d-cite> among others. Furthermore, an environment model is required for simulating the forward application of our RL agent in model-free algorithms. The training process alternates between collecting experience data (rollouts) and training the agent on that data. Consequently, the runtime of our program is influenced by two key components: neural network parameter updates and environment simulation. OpenAI-Gym, which is proposed initially in <d-cite key="01"> </d-cite>, and its successor Gymnasium <d-cite key="02"> </d-cite> are well-established Python libraries providing a structured approach to building RL simulation environments. Popular libraries like TensorFlow and PyTorch are commonly used for training the agent model itself. This paper explores JAX <d-cite key="21"> </d-cite> and its neural network extension, Flax <d-cite key="20"> </d-cite>, as a promising alternative for both simulation and training, aiming to accelerate training and improve optimization.</p> <p>Our tests on the GridWorld environment indicate that using JAX for environment batching yields significant speedups on GPU hardware, while maintaining same performance levels as training programs with other packages. We also focused on the hyperparameter search problem, which is particularly critical in Reinforcement Learning due to its interactive nature. We employed the Optuna <d-cite key="06"> </d-cite> implementation of different hyperparameter search methods, demonstrating its impact on the results. All trials and experiments were also tracked using MLflow <d-cite key="05"> </d-cite>, providing a detailed overview of key metrics during training.</p> <p>Beyond that, each implementation section begins with a concise overview of the package’s capabilities and main functions, providing the readers with minimally needed understanding for effective utilization in their projects. Finally, the main experimental results are presented and discussed, followed by concluding takeaways.</p> <p>The installation of the packages needed in this post with <code class="language-plaintext highlighter-rouge">pip</code> (python) can be done simply as follows:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>gymnasium
pip <span class="nb">install </span>mlflow
pip <span class="nb">install </span>optuna

<span class="c">#replace with your cuda version </span>
pip <span class="nb">install</span> <span class="s2">"jax[cuda12]"</span>
pip <span class="nb">install </span>flax
</code></pre></div></div> <h1 id="standardize-your-environment-with-gymnasium">Standardize Your Environment with Gymnasium</h1> <p><a href="https://gymnasium.farama.org/v0.29.0/" rel="external nofollow noopener" target="_blank">Gymnasium</a> is an updated version of the popular Gym package, originally developed by OpenAI <d-cite key="01"> </d-cite>. It provides a collection of standardized simulated environments with unified interfaces, which are regularly updated. This standardization is beneficial for benchmarking different RL algorithms, as well as for improving readability and collaboration. Several other advantages motivate further the use of Gym and Gymnasium:</p> <ul> <li> <strong>Vectorized Environments</strong> (<code class="language-plaintext highlighter-rouge">VecEnv</code>): This feature allows running multiple instances of the same environment concurrently, enabling batching of states and actions. This significantly speeds up trajectory rollout and, consequently, the training of the RL agent. There are two methods for deploying vectorized environments in Gymnasium: <em>Synchronous</em> and <em>Asynchronous</em> environments. A comparison of these two is presented in Table 1 below.</li> </ul> <center> <div class="caption"> Table 1: Comparison between Gymnasium vectorization methods `SyncVectorEnv` and `AsyncVectorEnv` </div> <table style="border: 1px solid black"> <tr> <th style="border: 1px solid black"> `gymnasium.Vector.SyncVectorEnv` </th> <th style="border: 1px solid black"> `gymnasium.Vector.AsyncVectorEnv` </th> </tr> <tr> <td style="border: 1px solid black"> creates all environments in the main thread serially and batch the output (state,reward,done flags) </td> <td style="border: 1px solid black"> each environment is created with its own subprocess (computational thread) </td> </tr> <tr> <td style="border: 1px solid black"> best used when environment process is simple, and faster than running independent subprocesses for each instance. </td> <td style="border: 1px solid black"> best used when the environment processes are computationally expensive and there's enough memory for subprocesses. </td> </tr> <tr> <td colspan="2" style="text-align: center;border: 1px solid black"> Input to both functions should be a list of creation functions of environments (e.g., using a lambda function). </td> </tr> <tr> <td colspan="2" style="text-align: center;border: 1px solid black"> If you set the optional key input (`shared_memory`) to True, then the output observation data will be referenced directly without copying, which can speed up the stepping when its size is large. </td> </tr> </table> </center> <ul> <li> <strong>Spaces Objects</strong>: These are used to define the state and action values and distributions. These spaces represent specific constraints. Examples of possible space sets are shown in Figure 1, imported from <code class="language-plaintext highlighter-rouge">gymnasium.spaces</code>.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-speeding-up-rl/gymspaces-480.webp 480w,/2026/assets/img/2026-04-27-speeding-up-rl/gymspaces-800.webp 800w,/2026/assets/img/2026-04-27-speeding-up-rl/gymspaces-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-speeding-up-rl/gymspaces.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 1: Gymnasium basic and compound spaces </div> <ul> <li> <p><strong>Registry</strong>: Custom environments can be registered within the installation so that they can be instanced directly like a standard Gym package (with <code class="language-plaintext highlighter-rouge">gym.make</code>).</p> </li> <li> <p><strong>Wrappers</strong>: The <code class="language-plaintext highlighter-rouge">gymnasium.wrappers</code> module contains useful classes to <em>modify</em> specific environment behavior. Examples include:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">ObservationWrapper</code>: Modifies the observation space.</li> <li> <code class="language-plaintext highlighter-rouge">ActionWrapper</code>: Modifies the action space.</li> <li> <code class="language-plaintext highlighter-rouge">RewardWrapper</code>: Modifies the reward function.</li> <li> <code class="language-plaintext highlighter-rouge">TimeLimit</code>: Used to truncate an episode after a specific number of steps.</li> <li> <code class="language-plaintext highlighter-rouge">AutomaticReset</code>: When the environment reaches a terminal state or is truncated, this wrapper resets it on the next call to <code class="language-plaintext highlighter-rouge">.step()</code>, returning the last observed state.</li> <li> <code class="language-plaintext highlighter-rouge">RecordEpisodeStatistics</code>: Important for collecting episodic rewards, which indicate the success or failure of a policy during training.</li> </ul> </li> <li> <p>If your environment is a subclass of <code class="language-plaintext highlighter-rouge">gymnasium.Env</code>, you benefit from automatic testing using the <code class="language-plaintext highlighter-rouge">gymnasium.utils.env_checker.check_env</code> function, which performs common tests on the Gym environment methods and their spaces.</p> </li> </ul> <p>Additionally, Gymnasium introduces the following changes over Gym:</p> <ul> <li> <strong>Termination and Truncation</strong>: Instead of the <code class="language-plaintext highlighter-rouge">done</code> flag, Gymnasium uses <code class="language-plaintext highlighter-rouge">termination</code> and <code class="language-plaintext highlighter-rouge">truncation</code> flags. <em>Termination</em> occurs naturally when the episode’s goal is achieved (e.g., the goal is reached), while <em>truncation</em> happens only after a specific number of steps to prevent episodes from running indefinitely. Figure 2 depicts these differences.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-speeding-up-rl/termination-480.webp 480w,/2026/assets/img/2026-04-27-speeding-up-rl/termination-800.webp 800w,/2026/assets/img/2026-04-27-speeding-up-rl/termination-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-speeding-up-rl/termination.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 2: Difference between terminating (goal achieved) and truncating (time limit reached) a simulated episode. </div> <ul> <li> <strong>Functional Environment Creation</strong>: A new and experimental function for environment creation, <code class="language-plaintext highlighter-rouge">gymnasium.experimental.functional.FuncEnv()</code>, is introduced. This function utilizes a purely functional structure (as the environment class is stateless) to reflect more closely the formulation of POMDP (Partial Observable Markov Decision Process). Additionally, this structure facilitates direct compatibility with JAX.</li> </ul> <h1 id="an-example-for-creating-a-custom-gym-environment-and-training-with-dqn">An Example for Creating a Custom Gym Environment and Training with DQN</h1> <p>This section demonstrates the application of Gym and other associated libraries to implement a custom Gridworld environment called <em>Doors</em>. In this environment, an agent occupies a cell within a grid and is tasked with navigating towards a goal cell by passing through one of three gaps (doors) that divide the grid in two, as illustrated in Figure 3. The figure also depicts state-action configurations.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-speeding-up-rl/doors-480.webp 480w,/2026/assets/img/2026-04-27-speeding-up-rl/doors-800.webp 800w,/2026/assets/img/2026-04-27-speeding-up-rl/doors-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-speeding-up-rl/doors.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 3: The *Doors* environment, with its state, action, and reward configurations. </div> <p><strong>Note:</strong> The complete code repository is available <a href="https://github.com/engyasin/ilsurvey" rel="external nofollow noopener" target="_blank">here</a>, with the final script integrating all libraries located <a href="https://github.com/engyasin/ilsurvey/blob/main/dqn_hopt_flax.py" rel="external nofollow noopener" target="_blank">here</a>.</p> <p>Below, we present parts of the code of the environment creation class in Gymnasium:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">gymnasium.wrappers</span> <span class="kn">import</span> <span class="n">Autoreset</span><span class="p">,</span> <span class="n">RecordEpisodeStatistics</span>

<span class="c1">#creating the environment
</span><span class="k">class</span> <span class="nc">DoorsGym</span><span class="p">(</span><span class="n">gym</span><span class="p">.</span><span class="n">Env</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">gridSize</span><span class="o">=</span><span class="p">[</span><span class="mi">15</span><span class="p">,</span><span class="mi">15</span><span class="p">],</span><span class="n">nDoors</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">render_frames</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">gridSize</span> <span class="o">=</span> <span class="n">gridSize</span>
        <span class="n">self</span><span class="p">.</span><span class="n">nDoors</span> <span class="o">=</span> <span class="n">nDoors</span>
        <span class="n">self</span><span class="p">.</span><span class="n">render_frames</span> <span class="o">=</span> <span class="n">render_frames</span>

        <span class="n">self</span><span class="p">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">spaces</span><span class="p">.</span><span class="nc">Discrete</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

        <span class="c1"># representing the four states of cells for the entire size of the grid (flattened)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">spaces</span><span class="p">.</span><span class="nc">MultiDiscrete</span><span class="p">([</span><span class="mi">4</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">prod</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">gridSize</span><span class="p">))])</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">options</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>

        <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">reset</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">action</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>

        <span class="k">pass</span>

</code></pre></div></div> <p>This environment can then be utilized in a separate script as shown in the following figure:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-speeding-up-rl/gymcode-480.webp 480w,/2026/assets/img/2026-04-27-speeding-up-rl/gymcode-800.webp 800w,/2026/assets/img/2026-04-27-speeding-up-rl/gymcode-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-speeding-up-rl/gymcode.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 4: Environment registration and creation. </div> <p>Given that the action space for this environment is discrete, we selected the Deep Q-Network (DQN) training algorithm <d-cite key="03"> </d-cite>, based on the CleanRL <d-cite key="04"> </d-cite> implementation, to learn the policy. The subsequent sections detail how to track training metrics and plot them against training time using MLflow.</p> <h1 id="tracking-rl-experiments-with-mlflow">Tracking RL Experiments with MLflow</h1> <p><a href="https://mlflow.org/" rel="external nofollow noopener" target="_blank"><em>Mlflow</em></a> is a popular Python library for tracking, versioning, collaborating on, and deploying machine learning models. Its primary functionality involves displaying training metrics either on a local server (by running <code class="language-plaintext highlighter-rouge">mlflow ui</code> in a new terminal, with the default port set to 5000) or on an online cloud server such as <em>Databricks</em>.</p> <p>Mlflow organizes training by creating an <em>experiment</em> for each machine learning task (for example, cat/dog image classification). Within each experiment, multiple <em>runs</em> can be defined, representing individual training trials for that task (e.g., different ML approaches for the same task). Furthermore, smaller runs can be <em>nested</em> within larger runs (which we will utilize for our hyperparameter trails, described below).</p> <p>This structure enables comprehensive saving of all testing parameters and metrics, and provides a unified interface for tracking them. Additionally, Mlflow offers seamless integration with PyTorch, TensorFlow, and Keras, along with numerous other functionalities and features that are beyond the scope of this post but can be explored on its website <a href="https://mlflow.org/" rel="external nofollow noopener" target="_blank">https://mlflow.org/</a>.</p> <p>To initiate a new experiment in <strong>Mlflow</strong>, execute the following command: <code class="language-plaintext highlighter-rouge">mlflow.create_experiment('experiment_name')</code>. This defines a new task for training a machine learning model or allows continuation of an existing experiment. The code for working with an existing experiment is as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">mlflow</span>
<span class="n">mlflow</span><span class="p">.</span><span class="nf">set_tracking_uri</span><span class="p">(</span><span class="sh">"</span><span class="s">http://localhost:5000</span><span class="sh">"</span><span class="p">)</span>
<span class="n">mlflow</span><span class="p">.</span><span class="nf">set_experiment</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">runs/</span><span class="si">{</span><span class="n">experiment_name</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>Note that you must specify the URI where the server will publish the results (in this case, <code class="language-plaintext highlighter-rouge">http://localhost:5000</code>) while the local server is running in a separate terminal using the command <code class="language-plaintext highlighter-rouge">mlflow ui</code>.</p> <p>Subsequently, you can start a specific <code class="language-plaintext highlighter-rouge">run</code> within the experiment or create <em>multiple child runs</em> nested within the parent run by utilizing the <code class="language-plaintext highlighter-rouge">nested</code> keyword argument. The latter is particularly useful for hyperparameter optimization, where each trial can be tracked independently in its own child run. The following code illustrates this functionality.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">with</span> <span class="n">mlflow</span><span class="p">.</span><span class="nf">start_run</span><span class="p">(</span><span class="n">run_name</span><span class="o">=</span><span class="sh">'</span><span class="s">main_run</span><span class="sh">'</span><span class="p">,</span><span class="n">nested</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>

    <span class="c1"># log main parameters here
</span>    <span class="n">mlflow</span><span class="p">.</span><span class="nf">log_params</span><span class="p">(</span><span class="n">MainConfigs</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">mlflow</span><span class="p">.</span><span class="nf">start_run</span><span class="p">(</span><span class="n">nested</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="c1"># train here
</span>
        <span class="n">mlflow</span><span class="p">.</span><span class="nf">log_params</span><span class="p">(</span><span class="n">argsDict</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="p">.</span><span class="nf">log_metric</span><span class="p">(</span><span class="sh">'</span><span class="s">metric_name</span><span class="sh">'</span><span class="p">,</span><span class="n">metric_value</span><span class="p">,</span><span class="n">step</span><span class="o">=</span><span class="n">global_step</span><span class="p">)</span>
        <span class="n">mlflow</span><span class="p">.</span><span class="nf">set_tag</span><span class="p">(</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">)</span>

        <span class="n">mlflow</span><span class="p">.</span><span class="nf">log_figure</span><span class="p">()</span> <span class="c1"># matplotlib figure object
</span>        <span class="n">mlflow</span><span class="p">.</span><span class="nf">log_image</span><span class="p">()</span> <span class="c1"># numpy array and PIL image
</span>
        <span class="n">mlflow</span><span class="p">.</span><span class="n">pytorch</span><span class="p">.</span><span class="nf">save_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="c1"># saving pytorch model on the server
</span>
        <span class="n">mlflow</span><span class="p">.</span><span class="nf">log_artifacts</span><span class="p">()</span> <span class="c1"># saving other data types
</span>
    <span class="c1"># save final model
</span>    <span class="n">model_uri</span> <span class="o">=</span> <span class="sh">'</span><span class="s">copy from dashboard usually starting with models:/</span><span class="sh">'</span>
    <span class="n">model_info</span> <span class="o">=</span> <span class="n">mlflow</span><span class="p">.</span><span class="n">pytorch</span><span class="p">.</span><span class="nf">log_model</span><span class="p">(</span><span class="n">pytroch_model</span><span class="p">,</span><span class="n">model_uri</span><span class="p">)</span>

    <span class="c1"># load the model
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">mlflow</span><span class="p">.</span><span class="n">pytorch</span><span class="p">.</span><span class="nf">load_model</span><span class="p">(</span><span class="n">model_uri</span><span class="p">)</span>

</code></pre></div></div> <p>Then, in a new browser tab, navigate to the URL <code class="language-plaintext highlighter-rouge">http://localhost:5000</code> to view all experiments. If you select an active experiment, you can track individual runs either as a list or, as illustrated in Figure 5, as a chart displaying all tracked metrics.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-speeding-up-rl/mlflowcharts-480.webp 480w,/2026/assets/img/2026-04-27-speeding-up-rl/mlflowcharts-800.webp 800w,/2026/assets/img/2026-04-27-speeding-up-rl/mlflowcharts-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-speeding-up-rl/mlflowcharts.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 5: The MLflow interface (chart-view) displaying tracked parameters for the active run. </div> <h1 id="optimizing-rl-hyperparameters-with-optuna">Optimizing RL Hyperparameters with Optuna</h1> <p>Reinforcement Learning training typically requires the tuning of numerous hyperparameters, exceeding the number usually required for supervised learning counterparts. Therefore, applying efficient hyperparameter optimization methods such as Bayesian optimization <d-cite key="07"> </d-cite> or HyperBand <d-cite key="16"> </d-cite> is highly beneficial for RL. In the following sections, we will begin by reviewing prominent hyperparameter optimization methods, focusing on their implementation using the <code class="language-plaintext highlighter-rouge">Optuna</code> package, and later demonstrate some of them on our Doors demo example.</p> <p>These hyperparameters include, in the context of RL, parameters such as the learning rate, episode length, discount factor (in the Bellman equation), as well as the agent network architecture, like its layer sizes and depth.</p> <h2 id="types-of-hyperparameter-optimization-methods">Types of Hyperparameter Optimization Methods</h2> <p>Generally, there are four main branches of hyperparameter optimization methodologies, differing in their complexity and algorithem, as depicted in the following figure.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-speeding-up-rl/hyperopt-480.webp 480w,/2026/assets/img/2026-04-27-speeding-up-rl/hyperopt-800.webp 800w,/2026/assets/img/2026-04-27-speeding-up-rl/hyperopt-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-speeding-up-rl/hyperopt.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 6: Main search methodologies for optimizing hyperparameters of machine learning models. </div> <h3 id="uninformed-methods">Uninformed Methods</h3> <p>These methods represent the simplest approach, involving the manual testing of different samples directly from the search space. Depending on their sampling strategy, they can be categorized as:</p> <ul> <li> <p><strong>Manual:</strong> Samples are chosen manually by the developer.</p> </li> <li> <p><strong>Uniform:</strong> Samples are chosen uniformly with fixed step between each value and the next, which is also known as Grid Search.</p> </li> <li> <p><strong>Random:</strong> Samples are chosen randomly within the specified range.</p> </li> </ul> <h3 id="bayesian-optimization-methods">Bayesian Optimization Methods</h3> <p>This category of methods utilizes a surrogate model to approximate the <strong>objective function</strong> – the function that estimates the learning objective (e.g., accuracy or negative loss) given the training hyperparameters. The training data for this model consists of the values from previous training attempts. The new set of hyperparameters to be tested is then proposed by another model called an <strong>acquisition function</strong>.</p> <p>Based on the nature of the surrogate model, Bayesian Optimization (BO) methods <d-cite key="07"> </d-cite> can be categorized as follows:</p> <ul> <li> <p><strong>Sequential Model-based Algorithmic Configuration (SMAC)</strong> <d-cite key="08"> </d-cite>: Employs a random forest to approximate the objective function, making it suitable for searching categorical and discrete parameters.</p> </li> <li> <p><strong>Sequential Model-based Bayesian Optimization (SMBO)</strong> <d-cite key="09"> </d-cite>: Utilizes a Gaussian Process model, suitable for continuous hyperparameters.</p> </li> <li> <p><strong>Tree-structured Parzen Estimators (TPE):</strong> Employs a random forest, suitable for large search spaces encompassing both continuous and discrete parameters, with fast run-time. In <code class="language-plaintext highlighter-rouge">Optuna</code>, its implementation enables the learning of interactive relationships between different hyperparameters. Its Optuna function is <code class="language-plaintext highlighter-rouge">optuna.samplers.TPESampler</code>.</p> </li> <li> <p><strong>MATIS</strong> <d-cite key="10"> </d-cite>: Gaussian Process-based, utilizing a Gaussian Mixture Model also as its acquisition function.</p> </li> </ul> <h3 id="heuristic-search">Heuristic Search</h3> <p>This branch of methods samples the hyperparameters for the next training iteration within the neighborhood of the best set of hyperparameters found so far. The definition of this neighborhood significantly impacts search performance, leading to various variants:</p> <ul> <li> <p><strong>Simulated Annealing (SA)</strong> <d-cite key="11"> </d-cite>: Searches for the next sample around the best or next-to-best set of values found so far, aiming to avoid local minima.</p> </li> <li> <p><strong>Genetic Algorithm</strong> <d-cite key="12"> </d-cite>: Applies evaluation-inspired methods to select the next set of values. This typically involves pairing the best samples found for different parameters or mutating them.</p> </li> <li> <p><strong>Particle Swarm Optimization</strong> <d-cite key="13"> </d-cite>: This method specifically focuses on continuous hyperparameters.</p> </li> <li> <p><strong>Population-based Training</strong> <d-cite key="14"> </d-cite>: This method specializes in neural network optimization, searching for both hyperparameters and standard training parameters. It gradually adds new layers to the model during training, while retaining the previously trained layers. However, it cannot recover the exact best hyperparameters for the best model, as it only finds the parameters of the final trained model.</p> </li> </ul> <h3 id="multi-fidelity-optimization-mfo">Multi-Fidelity Optimization (MFO)</h3> <p>This approach enhances hyperparameter optimization by enabling faster training through early stopping on less promising samples, achieved by training on subsets of the data or for a reduced number of epochs (as in Optuna). This is more efficient than full training for all samples, as it avoids unnecessary computational resources spent on evaluating many samples with a low probability of being optimal, while focusing on areas with promising performance. MFO methods frame this concept as resource management algorithms. Notably, MFO methods can be directly combined with the aforementioned sampling methods, addressing different aspects of the optimization problem. In Optuna, MFO methods are termed <strong>Pruners</strong>, while the sampling methods are called <strong>Samplers</strong>.</p> <p>Popular MFO methods include:</p> <ul> <li> <p><strong>Coarse-to-Fine Pruner:</strong> As the name suggests, this method initiates training with a small number of samples and gradually focuses on a more promising subset.</p> </li> <li> <p><strong>Successive Halving (SH)</strong> <d-cite key="15"> </d-cite>: This method allocates computational resources strategically across different training trails.</p> </li> <li> <p><strong>HyperBand (HB)</strong> <d-cite key="16"> </d-cite>: This method defines pairs of candidate numbers and their allocated resources, called <em>brackets</em>, and initiates full training on a subset of these brackets. This prevents prematurely discarding promising candidates, a potential issue with SH due to shallow training. Its Optuna function is: <code class="language-plaintext highlighter-rouge">optuna.pruners.HyperbandPruner</code>.</p> </li> <li> <p><strong>Bayesian Optimization HyperBand (BOHB):</strong> Combining a Bayesian Optimization sampler with a HyperBand pruner often yields improved results, as detailed in <d-cite key="17"> </d-cite>. In Optuna, this can be achieved by setting the sampler to TPE and the pruner to HB.</p> </li> </ul> <h2 id="steps-for-hyperparameter-optimization-in-optuna">Steps for Hyperparameter Optimization in Optuna</h2> <p>Hyperparameters in RL training programs are numerous and have varying effects on the training process. Therefore, manually tuning them requires significant experience and experimentation to identify optimal values. Optuna [6] provides a direct and efficient solution for saving effort in practical applications by utilizing automated search with well-established implementations. Optuna simplifies this process with clear implementation steps, supporting most of the aforementioned methods and offering direct integration with libraries like MLflow, PyTorch, and JAX. These steps can be summarized as follows:</p> <ol> <li>Define the objective function, which, in the case of RL, returns the average episodic return.</li> <li>Within this objective function, define the ranges and types of hyperparameters to be optimized using the <code class="language-plaintext highlighter-rouge">optuna.trail.suggest_</code> group of functions.</li> <li>Initialize the optimization object (called the <em>study</em>) using <code class="language-plaintext highlighter-rouge">create_study()</code> and define the desired <code class="language-plaintext highlighter-rouge">sampler</code> and <code class="language-plaintext highlighter-rouge">pruner</code> methods, along with the <em>direction</em> (defaulting to minimization).</li> <li>Optionally, save the current training session by passing the <code class="language-plaintext highlighter-rouge">storage</code> argument (a database URL) to <code class="language-plaintext highlighter-rouge">create_study()</code>. This allows for resuming training from a saved session of trails by passing <code class="language-plaintext highlighter-rouge">load_if_exists=True</code> to the same function.</li> <li>Initiate training using the <code class="language-plaintext highlighter-rouge">.optimize()</code> method of the previous study object, passing the objective function as a callable and the number of trials.</li> <li>Upon completion of optimization, the best set of parameters can be accessed via <code class="language-plaintext highlighter-rouge">study.best_params</code>, and the trained model can be saved.</li> </ol> <p>It is also worth noting that Optuna includes a visualization module (<code class="language-plaintext highlighter-rouge">optuna.visualization</code>) whose functions take the optimized study object as input and generate various useful plots, such as those illustrating the most influential hyperparameters on the results. This module requires the installation of the <code class="language-plaintext highlighter-rouge">plotly</code> package.</p> <p>In the following we show some illustrative code snippet to implement the above steps.</p> <h2 id="training-code-structure-in-optuna">Training Code Structure in Optuna</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">optuna</span>
<span class="kn">from</span> <span class="n">optuna.samplers</span> <span class="kn">import</span> <span class="n">TPESampler</span>
<span class="kn">from</span> <span class="n">optuna.pruners</span> <span class="kn">import</span> <span class="n">HyperbandPruner</span>

<span class="kn">from</span> <span class="n">functional</span> <span class="kn">import</span> <span class="n">partial</span>


<span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">trail</span><span class="p">,</span><span class="n">argsParams</span><span class="o">=</span><span class="p">{}):</span>

    <span class="n">argsParams</span><span class="p">.</span><span class="nf">update</span><span class="p">({</span><span class="sh">"</span><span class="s">num_steps</span><span class="sh">"</span><span class="p">:</span><span class="n">trial</span><span class="p">.</span><span class="nf">suggest_int</span><span class="p">(</span><span class="sh">"</span><span class="s">num_steps</span><span class="sh">"</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)})</span>
    <span class="n">argsParams</span><span class="p">.</span><span class="nf">update</span><span class="p">({</span><span class="sh">"</span><span class="s">learning_rate</span><span class="sh">"</span><span class="p">:</span><span class="n">trial</span><span class="p">.</span><span class="nf">suggest_float</span><span class="p">(</span><span class="sh">"</span><span class="s">learning_rate</span><span class="sh">"</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="bp">True</span><span class="p">)})</span> 
    <span class="c1"># log argument makes it more probable to sample lower values.
</span>    <span class="n">argsParams</span><span class="p">.</span><span class="nf">update</span><span class="p">({</span><span class="sh">"</span><span class="s">buffer_size</span><span class="sh">"</span><span class="p">:</span><span class="n">trial</span><span class="p">.</span><span class="nf">suggest_int</span><span class="p">(</span><span class="sh">"</span><span class="s">buffer_size</span><span class="sh">"</span><span class="p">,</span><span class="mi">16</span> <span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="bp">True</span><span class="p">)})</span>
    <span class="n">argsParams</span><span class="p">.</span><span class="nf">update</span><span class="p">({</span><span class="sh">"</span><span class="s">batch_size</span><span class="sh">"</span><span class="p">:</span><span class="n">trial</span><span class="p">.</span><span class="nf">suggest_int</span><span class="p">(</span><span class="sh">"</span><span class="s">batch_size</span><span class="sh">"</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">16</span><span class="p">)})</span>
    <span class="n">argsParams</span><span class="p">.</span><span class="nf">update</span><span class="p">({</span><span class="sh">"</span><span class="s">train_frequency</span><span class="sh">"</span><span class="p">:</span><span class="n">trial</span><span class="p">.</span><span class="nf">suggest_int</span><span class="p">(</span><span class="sh">"</span><span class="s">train_frequency</span><span class="sh">"</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="bp">True</span><span class="p">)})</span>
    <span class="n">argsParams</span><span class="p">.</span><span class="nf">update</span><span class="p">({</span><span class="sh">"</span><span class="s">optimizer_name</span><span class="sh">"</span><span class="p">:</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_categorical</span><span class="p">(</span><span class="sh">"</span><span class="s">optimizer_name</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">Adam</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">SGD</span><span class="sh">"</span><span class="p">])})</span>

    <span class="c1"># define network
</span>
    <span class="c1"># define optimizers
</span>
    <span class="c1"># training loop
</span>
    <span class="k">with</span> <span class="n">mlflow</span><span class="p">.</span><span class="nf">start_run</span><span class="p">(</span><span class="n">nested</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>

        <span class="n">mlflow</span><span class="p">.</span><span class="nf">log_params</span><span class="p">(</span><span class="n">argsParams</span><span class="p">)</span>

        <span class="c1"># training logic
</span>        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">NumberOfEpochs</span><span class="p">):</span>
            <span class="c1"># training logic
</span>

            <span class="c1"># send the final metrics
</span>            <span class="n">mlflow</span><span class="p">.</span><span class="nf">log_metrics</span><span class="p">({</span><span class="sh">"</span><span class="s">charts/episodic_return</span><span class="sh">"</span><span class="p">:</span> <span class="n">infos</span><span class="p">[</span><span class="sh">"</span><span class="s">episode</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">][</span><span class="n">finished</span><span class="p">].</span><span class="nf">mean</span><span class="p">(),</span>
                                <span class="sh">"</span><span class="s">charts/episodic_length</span><span class="sh">"</span><span class="p">:</span> <span class="n">infos</span><span class="p">[</span><span class="sh">"</span><span class="s">episode</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">l</span><span class="sh">"</span><span class="p">][</span><span class="n">finished</span><span class="p">].</span><span class="nf">mean</span><span class="p">(),</span>
                                <span class="sh">"</span><span class="s">charts/epsilon</span><span class="sh">"</span><span class="p">:</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">epsilon</span><span class="si">:</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">},</span>
                                <span class="n">step</span><span class="o">=</span><span class="n">global_step</span><span class="p">)</span>

            <span class="c1"># break training whenever sample seems not optimal (early stopping)
</span>            <span class="k">if</span> <span class="n">trial</span><span class="p">.</span><span class="nf">should_prune</span><span class="p">():</span>
                <span class="k">raise</span> <span class="n">optuna</span><span class="p">.</span><span class="nc">TrialPruned</span><span class="p">()</span>


    <span class="c1"># return average episodic reward (objective)
</span>
<span class="k">with</span> <span class="n">mlflow</span><span class="p">.</span><span class="nf">start_run</span><span class="p">(</span><span class="n">run_name</span><span class="o">=</span><span class="n">run_name</span><span class="p">)</span> <span class="k">as</span> <span class="n">run</span><span class="p">:</span>

    <span class="n">study</span> <span class="o">=</span> <span class="n">optuna</span><span class="p">.</span><span class="nf">create_study</span><span class="p">(</span><span class="n">sampler</span><span class="o">=</span><span class="nc">TPESampler</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">multivariate</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span> 
                                <span class="c1"># if multivariate is true the sampler can learn the mutual interactions of variables
</span>                                <span class="n">pruner</span><span class="o">=</span><span class="nc">HyperbandPruner</span><span class="p">(</span><span class="n">min_resource</span><span class="o">=</span><span class="mi">240</span><span class="p">,</span> <span class="n">max_resource</span><span class="o">=</span><span class="n">max_epochs</span><span class="p">,</span> <span class="n">reduction_factor</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span> <span class="c1">#resource represents epochs
</span>                                <span class="n">direction</span><span class="o">=</span><span class="sh">"</span><span class="s">maximize</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># objective function should be passed as callable without arguments to optimize method
</span>    <span class="n">objective_func</span> <span class="o">=</span> <span class="nf">partial</span><span class="p">(</span>
        <span class="n">objective</span><span class="p">,</span> <span class="n">argsParams</span><span class="o">=</span><span class="nf">vars</span><span class="p">(</span><span class="n">argsParams</span><span class="p">).</span><span class="nf">copy</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
    <span class="p">)</span>

    <span class="n">study</span><span class="p">.</span><span class="nf">optimize</span><span class="p">(</span><span class="n">objective_func</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span> <span class="c1"># hom many trails to test
</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">study</span><span class="p">.</span><span class="n">best_parameters</span><span class="p">)</span> <span class="c1"># the results
</span>
    <span class="n">mlflow</span><span class="p">.</span><span class="nf">log_params</span><span class="p">(</span><span class="n">study</span><span class="p">.</span><span class="n">best_params</span><span class="p">)</span> <span class="c1"># log it with mlflow
</span>
    <span class="c1"># visualizations require plotly installed
</span>
    <span class="n">plotly_fig</span> <span class="o">=</span> <span class="n">optuna</span><span class="p">.</span><span class="n">visualization</span><span class="p">.</span><span class="nf">plot_param_importances</span><span class="p">(</span><span class="n">study</span><span class="p">,</span><span class="n">evaluator</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span> 
    <span class="n">plotly_fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    <span class="c1"># evaluator is optuna.importance.FanovaImportanceEvaluator by default or optuna.importance.MeanDecreaseImpurityImportanceEvaluator
</span>
    <span class="n">plotly_fig</span> <span class="o">=</span> <span class="n">optuna</span><span class="p">.</span><span class="n">visualization</span><span class="p">.</span><span class="nf">plot_contour</span><span class="p">(</span><span class="n">study</span><span class="p">)</span>
    <span class="n">plotly_fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>


    <span class="n">plotly_fig</span> <span class="o">=</span> <span class="n">optuna</span><span class="p">.</span><span class="n">visualization</span><span class="p">.</span><span class="nf">plot_optimization_history</span><span class="p">(</span><span class="n">study</span><span class="p">)</span>
    <span class="n">plotly_fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
    <span class="c1"># these images can be viewed in new widows or sent to MLflow server to view them alongside other parameters
</span>
    <span class="n">mlflow</span><span class="p">.</span><span class="nf">log_figure</span><span class="p">(</span><span class="n">plotly_fig</span><span class="p">,</span><span class="n">artifact_file</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">opt_history.html</span><span class="sh">"</span><span class="p">)</span> 

</code></pre></div></div> <p>In our accompanying <a href="https://github.com/engyasin/ilsurvey" rel="external nofollow noopener" target="_blank">code repository</a>, we conducted 40 trials to search for optimal hyperparameters, and visualized the results using the following methods:</p> <ul> <li> <strong>Parameter Importance:</strong> We used <code class="language-plaintext highlighter-rouge">optuna.visualization.plot_param_importances</code> to assess the relative importance of each hyperparameter on model training performance. The two most influential parameters were found to be episode length and learning rate.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-speeding-up-rl/params_importance-480.webp 480w,/2026/assets/img/2026-04-27-speeding-up-rl/params_importance-800.webp 800w,/2026/assets/img/2026-04-27-speeding-up-rl/params_importance-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-speeding-up-rl/params_importance.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 7: Hyperparameters' estimated relative importance on model training performance. The episode length and learning rate is found to be the most influential parameters. </div> <ul> <li> <strong>Interactive Pairwise Importance:</strong> We generated 2D heatmaps using <code class="language-plaintext highlighter-rouge">optuna.visualization.plot_contour</code> to visualize the interactive importance of parameter pairs. Darker regions indicate optimal combinations of these parameters.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-speeding-up-rl/contours-480.webp 480w,/2026/assets/img/2026-04-27-speeding-up-rl/contours-800.webp 800w,/2026/assets/img/2026-04-27-speeding-up-rl/contours-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-speeding-up-rl/contours.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 8: 2D heatmaps illustrating the interactive importance of parameter pairs on model performance. The darker regions highlight the most effective parameters values combinations. </div> <ul> <li> <strong>Optimization History:</strong> We plotted the performance of trails over time using <code class="language-plaintext highlighter-rouge">optuna.visualization.plot_optimization_history</code> to track the progress of the optimization process.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-speeding-up-rl/optimization_history-480.webp 480w,/2026/assets/img/2026-04-27-speeding-up-rl/optimization_history-800.webp 800w,/2026/assets/img/2026-04-27-speeding-up-rl/optimization_history-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-speeding-up-rl/optimization_history.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 9: Improvement in trail performance over the course of training. The results demonstrate that the hyperparameter optimization process progressively identified better parameter sets, leading to improved performance. Further search is expected to continue this upward trend. </div> <p>Finally, these visualizations provide valuable insights into the effective ranges and combinations of hyperparameters that yield optimal performance. This information can potentially guide manual enhancements to other configuration components and deepen the understanding of their effects in the optimization process.</p> <h1 id="accelerating-environment-rollout-and-model-training-with-jax-and-flax">Accelerating Environment Rollout and Model Training with JAX and Flax</h1> <p>A common approach to training reinforcement learning agents using simulated environments involves utilizing PyTorch or TensorFlow for agent training and NumPy/Gym for environment simulation. However, Google’s <strong>JAX</strong> <a href="https://docs.jax.dev/en/latest/index.html" rel="external nofollow noopener" target="_blank"><em>(Just After Execution)</em></a> presents an increasingly popular alternative. JAX offers a faster method for performing matrix computations efficiently (replacing NumPy) and training neural network models (replacing PyTorch) by leveraging hardware acceleration on devices like GPUs and TPUs. While JAX can be directly used to update neural network parameters, a targeted JAX-based package, Flax, simplifies model structuring and algorithm development.</p> <p>In the following sections, we will highlight key features of JAX, focusing on its NumPy-compatible functionalities. We will demonstrate these features later by rewriting the Doors Gym environment in JAX and comparing its runtime performance with the original implementation.</p> <ul> <li> <p>JAX employs the <strong>XLA</strong> <em>(Accelerated Linear Algebra)</em> compiler to translate code into a statically typed expression language called <strong>Jexpr</strong>. This compiled code executes faster on CPUs, GPUs, and TPUs. Specifically, JAX functions can be compiled by passing them to the <code class="language-plaintext highlighter-rouge">jax.jit()</code> function or by using the <code class="language-plaintext highlighter-rouge">@jax.jit</code> decorator directly above their definitions.</p> </li> <li> <p>JAX largely replaces NumPy functions with similarly named counterparts, minimizing code modification efforts.<br> Typically, replacing <code class="language-plaintext highlighter-rouge">import jax.numpy as np</code> with <code class="language-plaintext highlighter-rouge">import numpy as np</code> is sufficient. However, certain considerations are important, as detailed below:</p> </li> </ul> <blockquote> <p><em>Note</em>: Unlike NumPy, JAX arrays are immutable. Consequently, in-place modification is not possible. Instead, array elements must be updated using operations like:</p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="n">arr</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">numpy</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">arr</span> <span class="o">=</span> <span class="n">arr</span><span class="p">.</span><span class="n">at</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">add</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># equivalent to arr[1] += 2 in NumPy
</span></code></pre></div></div> <blockquote> <p><em>Note</em>: JAX arrays do not raise an <code class="language-plaintext highlighter-rouge">OutofIndex</code> error when accessing elements outside their bounds; instead, they default to returning the last element in the array.</p> </blockquote> <blockquote> <p><em>Note</em>: JAX defaults to <code class="language-plaintext highlighter-rouge">float32</code> precision, unlike NumPy’s <code class="language-plaintext highlighter-rouge">float64</code>.</p> </blockquote> <blockquote> <p><em>Note</em>: JAX provides alternative implementations of SciPy functions through the <code class="language-plaintext highlighter-rouge">jax.scipy</code> module.</p> </blockquote> <p>The following code shows an example of a JAX compatible function compiled with jit, measuring its runtime</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="n">arr</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">numpy</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">35</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># 7x5 array
</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">JAX running on : </span><span class="si">{</span><span class="n">arr</span><span class="p">.</span><span class="n">device</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="nd">@jax.jit</span>
<span class="k">def</span> <span class="nf">ATA</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>

<span class="c1"># run in IPython :
</span><span class="o">%</span><span class="n">timeit</span> <span class="o">-</span><span class="n">n</span> <span class="mi">100</span> <span class="nc">ATA</span><span class="p">(</span><span class="n">arr</span><span class="p">).</span><span class="nf">block_until_ready</span><span class="p">()</span>

</code></pre></div></div> <ul> <li> <p>JAX’s <code class="language-plaintext highlighter-rouge">jax.vmap()</code> function (or the <code class="language-plaintext highlighter-rouge">@jax.vmap</code> decorator) enables automatic vectorization of functions, facilitating parallel processing of multiple inputs. Instead of iterating through each input individually, you can pass them as a <em>batch</em> to achieve significant speed improvements over standard Python and NumPy code. The input and output are effectively stacked and concatenated, adding a new dimension to their matrices (the placement of this dimension is configurable). We demonstrate that this approach is also faster than the Gymnasium environment vectorization methods.</p> </li> <li> <p>JAX also supports vectorization across computational resources, enabling parallel processing. This functionality is implemented similarly to <code class="language-plaintext highlighter-rouge">vmap</code>, using either <code class="language-plaintext highlighter-rouge">jax.pmap()</code> or the <code class="language-plaintext highlighter-rouge">@jax.pmap</code> decorator.</p> </li> </ul> <blockquote> <p><em>Note</em>: JAX execution is, by default, asynchronous. This means that code returns immediately before calculating the output of a function. To ensure the function completes before returning, use <code class="language-plaintext highlighter-rouge">.block_until_ready()</code> to append the function call.</p> </blockquote> <ul> <li> <p>Beyond compilation with XLA, JAX effectively calculates gradients through <strong>automatic differentiation</strong> (<em>autodiff</em>) of all variable calculations. This is particularly beneficial for accelerating the training of neural networks.</p> </li> <li> <p>Control statements (<em>for, while, if, switch</em>) are known performance bottlenecks in Python. In JAX, these can be replaced with functional equivalents as follows:</p> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">jax</span> <span class="kn">import</span> <span class="n">lax</span>

<span class="n">lax</span><span class="p">.</span><span class="n">cond</span> <span class="c1"># if
</span><span class="n">lax</span><span class="p">.</span><span class="n">switch</span> <span class="c1"># switch, case
</span><span class="n">lax</span><span class="p">.</span><span class="n">while_loop</span> <span class="c1"># while
</span><span class="n">lax</span><span class="p">.</span><span class="n">fori_loop</span> <span class="c1"># for
</span>
<span class="c1"># example for fori_loop
</span><span class="nd">@jax.jit</span>
<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>

    <span class="k">def</span> <span class="nf">for_loop_body</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">accumulator</span><span class="p">):</span>

        <span class="n">accumulator</span> <span class="o">+=</span> <span class="n">accumulator</span>

        <span class="k">return</span> <span class="n">accumulator</span>

    <span class="n">accumulator</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">init_val</span> <span class="o">=</span> <span class="n">accumulator</span>
    <span class="n">start_i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">end_i</span> <span class="o">=</span> <span class="mi">100</span>

    <span class="n">final_value</span> <span class="o">=</span> <span class="n">lax</span><span class="p">.</span><span class="nf">fori_loop</span><span class="p">(</span><span class="n">start_i</span><span class="p">,</span> <span class="n">end_i</span><span class="p">,</span> <span class="n">for_loop_body</span><span class="p">,</span> <span class="n">init_val</span><span class="p">)</span>

</code></pre></div></div> <blockquote> <p><em>Note</em>: For code to be compiled or vectorized correctly in JAX, it must be exclusively <em>functional</em>. Object-oriented code (such as classes with stateful attributes) cannot be compiled. However, stateless class objects can be used, provided they do not retain internal variables (or use them solely as static variables). If these variables are modified, they are inherently part of the state.</p> </blockquote> <blockquote> <p><em>Note</em>: This functional code restriction should not be viewed as a limitation. In fact, functional code is commonly considered more readable and better structured.</p> </blockquote> <ul> <li>The following code snippet presents an example of our Doors environment converted to a <em>stateless</em> class, while remaining compatible with Gymnasium. Specific new functions are explained in the comments.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">gymnasium</span> <span class="k">as</span> <span class="n">gym</span>
<span class="kn">import</span> <span class="n">cv2</span>

<span class="kn">from</span> <span class="n">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="kn">import</span> <span class="n">jax</span>
<span class="kn">from</span> <span class="n">jax</span> <span class="kn">import</span> <span class="n">jit</span><span class="p">,</span><span class="n">random</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">jax</span> <span class="kn">import</span> <span class="n">lax</span><span class="p">,</span><span class="n">vmap</span><span class="p">,</span> <span class="n">pmap</span>


<span class="k">class</span> <span class="nc">DoorsEnvJax</span><span class="p">(</span><span class="n">gym</span><span class="p">.</span><span class="n">Env</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">gridSize</span><span class="o">=</span><span class="p">[</span><span class="mi">15</span><span class="p">,</span><span class="mi">15</span><span class="p">],</span><span class="n">nDoors</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1"># Static variables - not to be changed: otherwise an error is thrown.
</span>        <span class="n">EnvConfig</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gridSize</span> <span class="o">=</span> <span class="n">gridSize</span>
        <span class="n">self</span><span class="p">.</span><span class="n">nDoors</span> <span class="o">=</span> <span class="n">nDoors</span>

        <span class="n">self</span><span class="p">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">spaces</span><span class="p">.</span><span class="nc">Discrete</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">observation_space</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">spaces</span><span class="p">.</span><span class="nc">MultiDiscrete</span><span class="p">([</span><span class="mi">4</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">gridSize</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">gridSize</span><span class="p">[</span><span class="mi">1</span><span class="p">])])</span>

        <span class="n">self</span><span class="p">.</span><span class="n">actions_vocal</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]]).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>


    <span class="nd">@partial</span><span class="p">(</span><span class="n">jit</span><span class="p">,</span><span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span> <span class="c1"># ignore the first (self) input
</span>    <span class="nd">@partial</span><span class="p">(</span><span class="n">vmap</span><span class="p">,</span><span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span> <span class="c1"># vectorize along the first dimension (order 0) of all inputs except the first (None)
</span>    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">env_state</span><span class="p">,</span> <span class="n">info</span><span class="p">):</span>

        <span class="n">key</span> <span class="o">=</span> <span class="n">env_state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env_state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">agent_location</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="sh">'</span><span class="s">agent_location</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">goal_location</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="sh">'</span><span class="s">goal_location</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">episodic_reward</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="sh">'</span><span class="s">episode</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">timestep</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="sh">'</span><span class="s">episode</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">l</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">max_steps</span> <span class="o">=</span> <span class="n">info</span><span class="p">[</span><span class="sh">"</span><span class="s">num_steps</span><span class="sh">"</span><span class="p">]</span>


        <span class="n">movement</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">actions_vocal</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>
        <span class="n">new_location</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">agent_location</span><span class="o">+</span><span class="n">movement</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">gridSize</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">terminated</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">truncated</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">max_steps</span><span class="o">&lt;=</span><span class="n">timestep</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">bool_</span><span class="p">)</span> 
        <span class="n">past_position</span> <span class="o">=</span> <span class="n">agent_location</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

        <span class="c1"># check if wall (2)
</span>
        <span class="n">cell_state</span> <span class="o">=</span> <span class="n">state</span><span class="p">.</span><span class="n">at</span><span class="p">[</span><span class="o">*</span><span class="nf">tuple</span><span class="p">(</span><span class="n">new_location</span><span class="p">)].</span><span class="nf">get</span><span class="p">()</span> <span class="c1"># array elements are returned by .get()
</span>
        <span class="n">possible_moves</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">logical_or</span><span class="p">(</span><span class="n">cell_state</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="n">cell_state</span> <span class="o">==</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># conditions should be performed by jax functions
</span>
        <span class="c1"># boolean indexing can be done utilizing jax.np.where
</span>        <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">possible_moves</span><span class="p">,</span> <span class="c1"># boolean mask array
</span>                <span class="n">state</span><span class="p">.</span><span class="n">at</span><span class="p">[</span><span class="nf">tuple</span><span class="p">(</span><span class="n">agent_location</span><span class="p">)].</span><span class="nf">set</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">at</span><span class="p">[</span><span class="nf">tuple</span><span class="p">(</span><span class="n">new_location</span><span class="p">)].</span><span class="nf">set</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="c1"># value if True
</span>                <span class="n">state</span> <span class="c1"># value if False
</span>                 <span class="p">)</span>

        <span class="n">agent_location</span> <span class="o">=</span> <span class="n">new_location</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

        <span class="n">terminated</span> <span class="o">=</span> <span class="p">(</span><span class="n">cell_state</span> <span class="o">==</span> <span class="mi">3</span><span class="p">)</span> 

        <span class="n">reward</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_get_reward</span><span class="p">(</span><span class="n">past_position</span><span class="p">,</span><span class="n">agent_location</span><span class="p">,</span><span class="n">goal_location</span><span class="p">)</span>
        <span class="n">info</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">_get_info</span><span class="p">(</span><span class="n">agent_location</span><span class="p">,</span><span class="n">goal_location</span><span class="p">))</span>

        <span class="c1"># automatic reset
</span>        <span class="n">new_state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">logical_or</span><span class="p">(</span><span class="n">terminated</span><span class="p">,</span><span class="n">truncated</span><span class="p">),</span>
                 <span class="n">self</span><span class="p">.</span><span class="nf">reset</span><span class="p">(</span><span class="n">key</span><span class="p">[</span><span class="bp">None</span><span class="p">,:])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">,...],</span> <span class="c1"># to remove vector dimension
</span>                <span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">copy</span><span class="p">())</span>

        <span class="n">info</span><span class="p">.</span><span class="nf">update</span><span class="p">({</span><span class="sh">"</span><span class="s">new_state</span><span class="sh">"</span><span class="p">:</span><span class="n">new_state</span><span class="p">,</span>
                     <span class="sh">"</span><span class="s">episode</span><span class="sh">"</span><span class="p">:{</span><span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">:</span><span class="n">episodic_reward</span><span class="o">+</span><span class="n">reward</span><span class="p">,</span><span class="sh">'</span><span class="s">l</span><span class="sh">'</span><span class="p">:</span><span class="n">timestep</span><span class="o">+</span><span class="mi">1</span><span class="p">},</span>
                     <span class="sh">"</span><span class="s">agent_location</span><span class="sh">"</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">new_state</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">)),</span>
                     <span class="sh">"</span><span class="s">goal_location</span><span class="sh">"</span><span class="p">:</span><span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">new_state</span><span class="o">==</span><span class="mi">3</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">))})</span>

        <span class="c1"># Random keys should be used only once. Therefore we generate a new one each step.
</span>        <span class="n">new_key</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">)[</span><span class="mi">0</span><span class="p">,:]</span>

        <span class="nf">return </span><span class="p">(</span><span class="n">new_state</span><span class="p">,</span><span class="n">new_key</span><span class="p">),</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">info</span>

</code></pre></div></div> <p>As illustrated in the preceding example, the environment class is inherently vectorized, enabling the parallel execution of multiple environments by passing matrices of actions stacked along the first dimension. This is initialized within the <code class="language-plaintext highlighter-rouge">.reset()</code> function by generating a corresponding set of random keys. Specifically:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="n">key</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
   <span class="n">NUM_ENVS</span> <span class="o">=</span> <span class="mi">24</span> <span class="c1"># vmap
</span>   <span class="n">keys</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span><span class="n">NUM_ENVS</span><span class="p">)</span> <span class="c1"># generate new keys from existing ones.
</span></code></pre></div></div> <p>This vectorization has proven to be significantly advantageous in our experiments. To confirm this, we evaluated the runtime performance for a range of DOORS environment counts, employing JAX, Gym Synchronous, Gym Asynchronous, and JAX with accelerated looping between steps (a common performance bottleneck in Python). The runtime results for these methods are presented in Figure 11 below.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-speeding-up-rl/runtimeenvs-480.webp 480w,/2026/assets/img/2026-04-27-speeding-up-rl/runtimeenvs-800.webp 800w,/2026/assets/img/2026-04-27-speeding-up-rl/runtimeenvs-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-speeding-up-rl/runtimeenvs.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 10: Comparing runtime of different vectorization methods. JAX demonstrates resilience to increasing environment counts, maintaining performance up to 500 environments. Accelerating the for loop resulted in exceptionally fast performance, achieving a runtime of only 0.07 seconds. </div> <p><strong>JAX-based environments exhibit no runtime degradation with increasing environment instances.</strong> This observation is particularly noteworthy, as it allows for scaling up environment counts and accelerating the rollout phase in various reinforcement learning algorithms. The complete results and plotting script are available in the <code class="language-plaintext highlighter-rouge">display.py</code> script within the accompanying repository, facilitating reproducibility and allowing researchers to test the implementation on their own hardware. Furthermore, the synchronous execution method consistently outperformed the Asynchronous version, likely due to the relatively simple environment stepping operations in DOORS, which minimize the overhead associated with spawning numerous subprocesses.</p> <h1 id="flax">FLAX</h1> <p>FLAX <d-cite key="20"> </d-cite> is a specialized library built upon JAX for constructing and training neural networks. It is often favored over PyTorch or TensorFlow for deep learning due to JAX’s inherent speed and improved readability.</p> <p>In addition to FLAX, we leverage another JAX-based library, <code class="language-plaintext highlighter-rouge">optax</code> <d-cite key="21"> </d-cite>, to facilitate composable gradient transformations within JAX while defining the model and training state in FLAX.</p> <p>FLAX’s neural network classes inherit from <code class="language-plaintext highlighter-rouge">flax.linen.Module</code>. The forward pass of a network is defined within its <code class="language-plaintext highlighter-rouge">__call__()</code> function, annotated with <code class="language-plaintext highlighter-rouge">@flax.linen.compact</code>. This design results in an object-oriented network creation interface that remains stateless and compatible with Just-In-Time (JIT) compilation.</p> <p>The following code illustrates the definition of a neural network in Flax and passing a random input to it, as a necessary step for initializing its parameters. It’s important to note that these parameters are required inputs for model inference via the <code class="language-plaintext highlighter-rouge">.apply()</code> method, as the class is stateless.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">jax</span> <span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="n">flax</span> <span class="kn">import</span> <span class="n">linen</span> <span class="k">as</span> <span class="n">nn</span>


<span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="nd">@nn.compact</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">512</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">activation</span><span class="p">.</span><span class="nf">swich</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="mi">10</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    
<span class="n">model</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">()</span>
<span class="n">main_key</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">key1</span><span class="p">,</span> <span class="n">key2</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">main_key</span><span class="p">)</span>

<span class="n">random_data</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">key1</span><span class="p">,(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">key2</span><span class="p">,</span> <span class="n">random_data</span><span class="p">)</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">random_data</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">tabulate</span><span class="p">(</span><span class="n">key2</span><span class="p">,</span><span class="n">random_data</span><span class="p">))</span> <span class="c1"># shows the model structure
</span>
</code></pre></div></div> <p>A key advantage of FLAX is its automatic vectorization of network functions, eliminating the need for explicit <code class="language-plaintext highlighter-rouge">jax.vmap</code> calls. The batch dimension defaults to the first dimension, simplifying parallelization.</p> <p>Following the definition of the network, we define the optimizer using <code class="language-plaintext highlighter-rouge">optax</code> and the training state class to manage the training process, as shown below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">from</span> <span class="n">flax</span> <span class="kn">import</span> <span class="n">train_state</span>
<span class="kn">import</span> <span class="n">optax</span>

<span class="n">state</span> <span class="o">=</span> <span class="n">train_state</span><span class="p">.</span><span class="n">TrainState</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
    <span class="n">apply_fn</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="nb">apply</span><span class="p">,</span>
    <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
    <span class="n">tx</span><span class="o">=</span><span class="n">optax</span><span class="p">.</span><span class="nf">sgd</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span><span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="p">)</span>

<span class="nd">@jax.jit</span>
<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">train_state</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">inputData</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="n">train_state</span><span class="p">.</span><span class="nf">apply_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">inputData</span><span class="p">)</span>
        <span class="n">log_preds</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">-</span> <span class="n">jax</span><span class="p">.</span><span class="n">np</span><span class="p">.</span><span class="nf">logsumexp</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

        <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">target</span><span class="o">*</span><span class="n">log_preds</span><span class="p">)</span>

    <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">value_and_grad</span><span class="p">(</span><span class="n">loss</span><span class="p">)(</span><span class="n">train_state</span><span class="p">.</span><span class="n">params</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

    <span class="n">train_state</span> <span class="o">=</span> <span class="n">train_state</span><span class="p">.</span><span class="nf">apply_gradients</span><span class="p">(</span><span class="n">grads</span><span class="o">=</span><span class="n">grads</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">train_state</span><span class="p">,</span> <span class="n">loss_value</span>

</code></pre></div></div> <p>Using the preceding code, we can update the model’s parameters based on the calculated loss. To save the trained Flax model, we utilize the following code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="sh">"</span><span class="s">wb</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
   <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">flax</span><span class="p">.</span><span class="n">serialization</span><span class="p">.</span><span class="nf">to_bytes</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">params</span><span class="p">))</span>
<span class="c1"># This code saves the model parameters in a data object. To load the parameters again, use:
</span>
<span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
   <span class="n">q_state</span><span class="p">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">flax</span><span class="p">.</span><span class="n">serialization</span><span class="p">.</span><span class="nf">from_bytes</span><span class="p">(</span><span class="n">q_state</span><span class="p">.</span><span class="n">params</span><span class="p">,</span> <span class="n">f</span><span class="p">.</span><span class="nf">read</span><span class="p">())</span>
</code></pre></div></div> <blockquote> <p><em>Note</em>: The <code class="language-plaintext highlighter-rouge">orbax</code> library provides a higher-level abstraction for automatically saving Flax models.</p> </blockquote> <h1 id="results-and-final-take-away">Results and Final Take-away</h1> <p>Table 2 presents the performance (measured as the final step-wise mean reward over the last 2000 episodes – returned during the total of 5e5 training episodes) and training runtime for three variations of the training program:</p> <ul> <li>PyTorch with Gym (synchronous environments) <a href="https://github.com/engyasin/ilsurvey/blob/main/dqn_hopt.py" rel="external nofollow noopener" target="_blank">available here</a> </li> <li>Flax with Gym (synchronous environments) <a href="https://github.com/engyasin/ilsurvey/blob/main/dqn_hopt_flax.py" rel="external nofollow noopener" target="_blank">available here</a> </li> <li>Flax with Gym and JAX automatic vectorization on GPU <a href="https://github.com/engyasin/ilsurvey/blob/main/dqn_hopt_flax_jax.py" rel="external nofollow noopener" target="_blank">available here</a> </li> </ul> <p>These results were obtained on an <strong>NVIDIA GeForce RTX 5060 Ti</strong> GPU and an <strong>AMD Ryzen 5 7600X 6-Core Processor</strong> for the CPU. Each of the first and last tests was run with 40 trials, while the hyperparameters for the second test were copied from the best-performing configuration of the final trial.</p> <center> <div class="caption"> Table 2: Performance and Runtime of Training a DQN Agent to solve the DOORS Environment utilizing three different combinations of packages (JAX, Flax, and PyTorch) </div> <table style="border: 1px solid black" class="l-page"> <tr> <th style="border: 1px solid black"> </th> <th style="border: 1px solid black"> Pytorch for DQN </th> <th style="border: 1px solid black"> FLAX for DQN </th> <th style="border: 1px solid black"> FLAX-DQN and JAX for Env </th> </tr> <tr> <td style="border: 1px solid black"> Rolling Reward</td> <td style="border: 1px solid black"> <strong>0.73 </strong> </td> <td style="border: 1px solid black"> 0.64</td> <td style="border: 1px solid black"> 0.71 </td> </tr> <tr> <td style="border: 1px solid black"> Training Time</td> <td style="border: 1px solid black"> 22.5 min </td> <td style="border: 1px solid black"> 22.8 min </td> <td style="border: 1px solid black"> <strong>2.3 min</strong> </td> </tr> <tr> <td style="border: 1px solid black"> Training Cruves </td> <td style="border: 1px solid black"> Figure 11 </td> <td style="border: 1px solid black"> Figure 12 </td> <td style="border: 1px solid black"> <strong> Figure 13 </strong> </td> </tr> </table> <br> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-speeding-up-rl/pytorchnumpy-480.webp 480w,/2026/assets/img/2026-04-27-speeding-up-rl/pytorchnumpy-800.webp 800w,/2026/assets/img/2026-04-27-speeding-up-rl/pytorchnumpy-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-speeding-up-rl/pytorchnumpy.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 11: Pytorch for DQN with Numpy for Environment </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-speeding-up-rl/flaxnumpy-480.webp 480w,/2026/assets/img/2026-04-27-speeding-up-rl/flaxnumpy-800.webp 800w,/2026/assets/img/2026-04-27-speeding-up-rl/flaxnumpy-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-speeding-up-rl/flaxnumpy.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 12: FLAX for DQN with Numpy for Environment </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-speeding-up-rl/jax_flax-480.webp 480w,/2026/assets/img/2026-04-27-speeding-up-rl/jax_flax-800.webp 800w,/2026/assets/img/2026-04-27-speeding-up-rl/jax_flax-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-speeding-up-rl/jax_flax.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 13: FLAX for DQN with JAX for Environment </div> </center> <p>The results in Table 2 indicate that hyperparameter optimization was crucial for achieving strong performance with PyTorch, yielding a final reward of 0.73 after 40 trials. The other implementations utilizing JAX and Flax achieved comparable but slightly lower results, potentially due to random initialization conditions. Increasing the number of search trials may yield further improvements across all methods. It is also important to note that in off-policy methods like DQN, a larger buffer size is beneficial for maximizing the speedup gained from saved experiences; otherwise, performance cannot benefit from the fast environment rollout.</p> <p>The most significant performance gain of training time, can be attributed to the replacement of standard NumPy operations within the DOORS environment with JAX-accelerated, vectorized functional code. This is made possible by increasing the number of environments knowing that the speed of JAX’s functional stateless classes is not affected by that increase. Consequently, we leveraged this characteristic by increasing the number of environments by a factor of 16 in the JAX-based implementation, resulting in a substantial speedup on our hardware of approximately 10 times. We anticipate further speedup potential with even bigger number of environments. The remaining settings and hyperparameter ranges were fixed across all three tested setups.</p> <p>With these findings, we conclude by offering recommendations on when and why to utilize each of the discussed packages:</p> <ol> <li> <p><strong>Gymnasium:</strong> If the goal is to create novel environments and facilitate sharing and collaboration with the broader research community, Gymnasium is a suitable choice.</p> </li> <li> <p><strong>MLflow:</strong> For comprehensive tracking of training metrics and parameters, complete visualization of hyperparameters, and streamlined deployment, MLflow provides a direct and effective solution.</p> </li> <li> <p><strong>Optuna:</strong> When dealing with complex models possessing numerous hyperparameters that are difficult to tune manually (a common scenario in Reinforcement Learning), Optuna offers implementations of advanced hyperparameter search algorithms with seamless integration with MLflow.</p> </li> <li> <p><strong>JAX:</strong> If environment simulation is computationally expensive and representing a bottleneck of training runtime, then vectorizing the environment using JAX on GPU or TPU devices can yield significant speedups, enabling faster sampling of larger batches.</p> </li> <li> <p><strong>Flax:</strong> As a JAX-based library, Flax benefits from accelerated gradient calculations, potentially leading to performance gains on specialized hardware. However, this benefit may be diminished for smaller models and datasets, as observed in our results where PyTorch performance was close. Flax is particularly advantageous when dealing with large observation spaces, such as those containing images or videos requiring numerous trainable parameters.</p> </li> </ol> <p>Therefore, a thorough examination of the training pipeline is recommended to identify the computational bottleneck, especially in model-free Reinforcement Learning, which involves rollout generation and model parameter updates phases. For the former, we suggest leveraging accelerated JAX matrix operations, and for the latter, we recommend Flax’s autodiff and optimizer capabilities.</p> <h1 id="additional-jax-libraries">Additional JAX Libraries</h1> <p>To avoid reinventing the wheel when writing JAX programs, it is useful to explore open-source clean JAX projects for Reinforcement Learning or Environment Simulation, that can be imported and edited as needed.</p> <h2 id="brax">Brax</h2> <p><a href="https://github.com/google/brax" rel="external nofollow noopener" target="_blank">Brax</a> <d-cite key="22"> </d-cite>, a JAX-based reimplementation of MuJoCo developed by Google, demonstrates significant speedups over standard MuJoCo, the framework of physics simulation, and includes implementations of SAC and PPO RL algorithms.</p> <h2 id="dopamine">Dopamine</h2> <p><a href="https://github.com/google/dopamine" rel="external nofollow noopener" target="_blank">Dopamine</a> <d-cite key="23"> </d-cite>, another Google-developed package, provides a JAX implementation of a variety of RL algorithms for researchers, facilitating rapid training and testing across diverse environments.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-speeding-up-rl.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-long-context/">Text-as-Image, A Visual Encoding Approach for Long-Context Understanding</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/using-large-language-models-to-simulate-and-predict-human-decision-making/">Using Large Language Models to Simulate and Predict Human Decision-Making</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/useful-calibrated-uncertainties/">What (and What Not) are Calibrated Uncertainties Actually Useful for?</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>