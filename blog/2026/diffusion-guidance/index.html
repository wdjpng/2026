<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Diffusion Guidance - Opportunities for Physical Sciences | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Guidance has been a central driver of the success of diffusion models, enabling precise control over the sampling process toward desired target conditions. The most widely used techniques include Classifier Guidance and Classifier-Free Guidance. Recently, however, there has been growing interest in alternative guidance strategies. In this blog post, we review recent progress in training-free diffusion guidance methods and highlight their applications in scientific domains."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/diffusion-guidance/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">:root{--color-unconditional:#f00;--color-posterior:#004e64;--color-prior:#25a18e;--color-likelihood:#00a5cf}.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}details{overflow:hidden;background-color:#f8f9fa!important;border-radius:4px;margin:0}details summary{background-color:#f8f9fa!important;padding:8px 12px;cursor:pointer}details[open]{background-color:#f8f9fa!important}details[open] summary{background-color:#f8f9fa!important}details[open] summary ~ *{animation:slideDown .3s ease-out}@keyframes slideDown{from{opacity:0;transform:translateY(-10px)}to{opacity:1;transform:translateY(0)}}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Diffusion Guidance - Opportunities for Physical Sciences",
            "description": "Guidance has been a central driver of the success of diffusion models, enabling precise control over the sampling process toward desired target conditions. The most widely used techniques include Classifier Guidance and Classifier-Free Guidance. Recently, however, there has been growing interest in alternative guidance strategies. In this blog post, we review recent progress in training-free diffusion guidance methods and highlight their applications in scientific domains.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Diffusion Guidance - Opportunities for Physical Sciences</h1> <p>Guidance has been a central driver of the success of diffusion models, enabling precise control over the sampling process toward desired target conditions. The most widely used techniques include Classifier Guidance and Classifier-Free Guidance. Recently, however, there has been growing interest in alternative guidance strategies. In this blog post, we review recent progress in training-free diffusion guidance methods and highlight their applications in scientific domains.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#background">Background</a> </div> <ul> <li> <a href="#diffusion-models">Diffusion Models</a> </li> <li> <a href="#conditional-diffusion-models">Conditional Diffusion Models</a> </li> </ul> <div> <a href="#classifier-guidance">Classifier Guidance</a> </div> <div> <a href="#analytical-likelihoods">Analytical Likelihoods</a> </div> <div> <a href="#applications-for-physical-sciences">Applications for Physical Sciences</a> </div> <div> <a href="#closing-takeaways">Closing takeaways</a> </div> </nav> </d-contents> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-guidance/diff_prior-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-guidance/diff_prior-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-guidance/diff_prior-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-diffusion-guidance/diff_prior.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> \[% Define macros for common notation \newcommand{\bx}{\mathbf{x}} \newcommand{\bc}{\mathbf{c}} \newcommand{\bu}{\mathbf{u}} \newcommand{\by}{\mathbf{y}} \newcommand{\bz}{\mathbf{z}} \newcommand{\bI}{\mathbf{I}} \newcommand{\w}{\mathbf{w}} \newcommand{\bzt}{\mathbf{z}_t} \newcommand{\xs}{\mathbf{x}_s} \newcommand{\xzero}{\mathbf{x}_0} \newcommand{\bf}{\mathbf{f}} \newcommand{\gt}{\mathrm{g}_t} \newcommand{\fg}{\mathrm{g}} \newcommand{\ff}{\mathrm{f}} \newcommand{\at}{\alpha_t} \newcommand{\st}{\sigma_t} \newcommand{\eps}{\boldsymbol{\epsilon}} \newcommand{\thetav}{\boldsymbol{\theta}} \newcommand{\muv}{\boldsymbol{\mu}} \newcommand{\sigmav}{\boldsymbol{\sigma}} \newcommand{\logp}{\log p} \newcommand{\score}{\nabla_{\mathbf{x}_t} \log p(\mathbf{x}_t)} \newcommand{\scoreprior}{\nabla_{\mathbf{x}_t} \log p(\mathbf{x}_t)} \newcommand{\scorepost}{\nabla_{\mathbf{x}_t} \log p(\mathbf{x}_t \mid \mathbf{y})} \newcommand{\scorelike}{\nabla_{\mathbf{x}_t} \log p(\mathbf{y} \mid \mathbf{x}_t)} \newcommand{\posterior}{p(\mathbf{x} \mid \mathbf{y})} \newcommand{\prior}{p(\mathbf{x})} \newcommand{\likelihood}{p(\mathbf{y} \mid \mathbf{x})} \newcommand{\evidence}{p(\mathbf{y})} \newcommand{\E}{\mathbb{E}} \newcommand{\Var}{\mathbb{V}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\dt}{\mathrm{d}t} \newcommand{\dw}{d\mathbf{w}} \newcommand{\SDE}{d\bzt} \newcommand{\normal}{\mathcal{N}} \newcommand{\uniform}{\mathcal{U}}\] <h2 id="introduction">Introduction</h2> <p>Diffusion models have emerged as a state-of-the-art approach for sampling from complex probability distributions. Prominent examples are image-generation models like Stable Diffusion, where the model generates a high-quality image within seconds based on a given text prompt like <em>“A corgi with sunglasses on the beach”</em>. What makes these models stand out is their ability to faithfully adhere to a given prompt.</p> <p>These capabilities arise from techniques collectively known as guidance, which direct the output of diffusion models toward specified conditions. To guide the models, we use a score function divided into a <strong style="color: #25a18e;">prior</strong> and <strong style="color: #00a5cf;">likelihood term</strong>:</p> \[\begin{equation*} \textcolor{#A125A1}{\nabla_{\bx} \log \, p(\bx \mid \by)} = \textcolor{#25a18e}{\nabla_{\bx} \log p(\bx)} + \textcolor{#00a5cf}{\nabla_{\bx} \log p(\mathbf{y} \mid \bx)} \end{equation*}\] <div class="text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-guidance/score_decomposition.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-diffusion-guidance/score_decomposition.svg" class="img-fluid" width="100%" height="auto" style=" max-width: 25%; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>This blog post aims to share insights into methods that go beyond traditional guidance techniques. We begin by explaining the fundamentals of the classifier guidance approach in the first section and then explore recent developments in guiding diffusion models. Our goal is not to favor any specific method but to present alternatives, especially useful when data is limited or training resources are constrained.</p> <h2 id="background">Background</h2> <p>In this section, we will briefly summarize the key features of diffusion models. Readers familiar with the score-based formulation might want to skip ahead.</p> <h3 id="diffusion-models">Diffusion Models</h3> <p>A <em>forward</em> diffusion process, which gradually destroys data over time, can be defined via a stochastic differential equation (SDE)<d-cite key="song2021generative"></d-cite>:</p> \[\begin{equation} d\bzt = \bf(\bzt, t) \; dt + \fg(t) \; \dw \textrm{,} \end{equation}\] <p>where $\bf$ and $\fg$ are determined by the noise schedule, and $\dw$ is a Brownian motion. For an affine drift $\bf$, the forward process can be rewritten in closed form:</p> \[\begin{equation} \label{eq:diff_co} \bz_t = \alpha_t \bx + \sigma_t \boldsymbol{\epsilon} , \end{equation}\] <p>with $\bx \sim p_{\text{data}}(\bx)$ sampled from the data distribution and $\epsilon \sim \normal(\mathbf{0}, \mathbf{I})$ drawn from a standard gaussian distribution. The forward diffusion processes starts with a clean data sample $\bz_0=\bx$ (where $\alpha_0=1$ and $\sigma_0=0$), and gradually adds noise. In the case of a variance-exploding noise schedule, $\alpha_t$ remains $1$ and $\sigma_t$ grows with $t$.</p> <p>The <em>reverse</em> process<d-cite key="song2021generative,anderson1982reverse"></d-cite> is also an SDE running backward in time:</p> \[\begin{equation} \label{eq:reverse_process} \mathrm{d}\bzt = [\bf(\bzt, t) - \fg(t)^2 \; \textcolor{#25a18e}{\nabla_{\bzt} \log p(\bzt)}]\; dt + \fg(t) \; \dw , \end{equation}\] <p>where $\textcolor{#25a18e}{\nabla_{\bzt} \log p(\bzt)}$ is called the score function (aka Stein score), and is approximated by a neuronal network $s_{\theta}(\bzt, t) \approx \nabla_{\bzt} \log p(\bzt)$ using denoising score matching<d-cite key="hyvarinen2005estimation"></d-cite>.</p> <p>The score function defines a <strong>time-dependent vector field</strong> that guides points toward the data distribution.</p> <h3 id="conditional-diffusion-models">Conditional Diffusion Models</h3> <p>Previously, we demonstrated how to create a process for sampling from an unconditional distribution. Extending this to the conditional case, we aim to sample from a posterior $p(\bx \mid \by) \text{,}$ which we can decompose using Bayes’ rule:</p> \[\begin{equation} p(\bx \mid \by) = \frac{p(\by \mid \bx) p(\bx)}{p(\by)} . \end{equation}\] <p>Applying the logarithm and differentiating with respect to $\bx$, allows us to define a conditional form of the score function (relying on the fact that the denominator does not depend on $\bx$):</p> \[\begin{equation} \textcolor{#A125A1}{\nabla_{\bx} \log \, p(\bx \mid \by)} = \textcolor{#25a18e}{\nabla_{\bx} \log p(\bx)} + \textcolor{#00a5cf}{\nabla_{\bx} \log p(\mathbf{y} \mid \bx)} . \end{equation}\] <p>Where the conditional <em>reverse process</em> from Eq. \eqref{eq:reverse_process} is given by:</p> \[\begin{equation} \mathrm{d}\bzt = [\bf(\bzt, t) - \fg(t)^2 \; (\textcolor{#25a18e}{\nabla_{\bzt} \log p(\bz_t)} + \textcolor{#00a5cf}{\nabla_{\bz_t} \log p(\mathbf{y} \mid \bz_t)})] \; dt + \fg(t) \; d\mathbf{w} . \end{equation}\] <p>This formulation allows us to reuse our unconditional model ($\textcolor{#25a18e}{\nabla_{\bzt} \log p(\bz_t)}$) and simply add a guidance term $\textcolor{#00a5cf}{\nabla_{\bz_t} \log p(\mathbf{y} \mid \bz_t)}$. The guidance term acts like a force pushing our samples to be consistent with the condition $\by$. The remaining practical challenge is deriving</p> \[\textcolor{#00a5cf}{\nabla_{\bz_t} \log p(\mathbf{y} \mid \bz_t)} .\] <p>There are two distinct strategies for doing so:</p> <div style="padding: 10px 10px 10px 10px; border-left: 6px solid #FFD700; margin-bottom: 20px;"> <p>1. <strong>Classifier Guidance (Learning the Likelihood)</strong>: <br>Learn a time-dependent classifier that approximates the likelihood score through supervised training on labeled data.</p> <p style="margin: 0;">2. <strong>Analytical Likelihoods (Defining the Likelihood)</strong>: <br>Leverage analytically tractable likelihood functions, which are particularly valuable for inverse problems, where paired training data $(\bx, \by)$ is scarce or unavailable.</p> </div> <p><strong>In both approaches, the diffusion model itself remains frozen</strong>, serving only as a fixed prior distribution $p(\bx)$. We are left to define or learn how to assess likelihoods, particularly their gradients, which makes this framework highly data-efficient for adapting to new tasks.</p> <h2 id="classifier-guidance-cg">Classifier Guidance (CG)</h2> <p>Classifier Guidance<d-cite key="dhariwal2021diffusion"></d-cite> trains a time-dependent neuronal network to approximate the likelihood:</p> \[\begin{equation} p_{\phi}(\by \mid \bzt, t) \approx p(\by \mid \bzt, t) . \end{equation}\] <p>We therefore train a classifier whose inputs are noisy samples that resemble the intermediate steps of the reverse diffusion process. In particular, given a dataset of paired samples $(\bx,\by)$, we can train a time-conditional classifier $p_{\phi}(\by \mid \bzt, t)$ by minimizing</p> \[\begin{equation*} \E_{t \sim \uniform(0, T), (\bx, \by) \sim p_{\text{data}}, \boldsymbol{\epsilon} \sim \normal(\mathbf{0}, \mathbf{I})}[-\log p_{\phi}(\by \mid \bzt, t)] , \end{equation*}\] <p>with the noisy sample $\bz_t = \alpha_t \bx + \sigma_t \boldsymbol{\epsilon}$. Handling inputs at varying noise levels allows the classifier to operate across the entire diffusion trajectory.</p> <p>The trained classifier can be used as an approximation of the <strong><span style="color: var(--color-likelihood);">log-likelihood gradient</span></strong>:</p> \[\begin{equation} \textcolor{#00a5cf}{\nabla_{\bzt} \log p(\mathbf{y} \mid \bzt)} \approx \textcolor{#00a5cf}{\nabla_{\bzt} \log p_{\phi}(\by \mid \bzt, t)} , \end{equation}\] <p>where the gradient with respect to the input is easy to compute using automatic differentiation frameworks such as PyTorch or JAX.</p> <p>Finally we combine the prior score with the log likelihood gradient. In practice, classifier guidance works with a scaled version of the guidance term controlled by the parameter $\gamma$,</p> \[\begin{equation} \textcolor{#A125A1}{\nabla_{\bz_t} \log \, p_{\gamma}(\bz_t \mid \by)} = \textcolor{#25a18e}{\nabla_{\bz_t} \log p(\bz_t)} + \gamma \textcolor{#00a5cf}{\nabla_{\bz_t} \log p(\mathbf{y} \mid \bz_t)} . \end{equation}\] <p>This scaling lets us adjust the guidance strength: if $\gamma$ is too high, we obtain unrealistic samples that move away from the data manifold, whereas if $\gamma$ is too low, the guidance has little effect.</p> <p>Before continuing, we want to mention that classifier-free guidance is currently more commonly used, as it often outperforms classifier guidance and does not require training a separate classifier. However, since this blog post focuses on techniques that modify sampling behavior without retraining the original diffusion model, classifier guidance serves as a better illustrative example. Additionally, the name classifier-free guidance is somewhat misleading, because a classifier is still involved, it is simply embedded implicitly within the diffusion model itself. For completeness, we have included classifier-free guidance in the expandable section below.</p> <details> <summary> Classifier-Free Guidance (CFG) </summary> <div style="color: black;"> Classifier-free guidance can be derived in a similar manner, the main difference is that the diffusion model itself acts as classifier. Applying Bayes' rule to $$ \nabla_{\bz_t}\,p(\by \mid \bz_t) $$ results in $$ \nabla_{\bz_t}\,p(\by \mid \bz_t)=\nabla_{\bz_t}\,p(\bz_t \mid \by) - \nabla_{\bz_t}\,p(\bz_t). $$ To avoid training of an unconditional and a conditional model, we train one single model with an additional condition: $$ \nabla_{\bz_t}\,p(\by \mid \bz_t)=\nabla_{\mathbf{\bz_t}}\,p(\bz_t \mid \by) - \nabla_{\bz_t}\,p(\bz_t \mid \emptyset), $$ where we condition the model on the null token $\emptyset$, to represent the unconditional model. We can now replace the likelihood term in classifier guidance: $$ \nabla_{\bz_t} \log \, p_{\gamma}(\bz_t \mid \by) = (1 - \gamma) \nabla_{\bz_t} \log p(\bz_t, \mid \emptyset) + \gamma \nabla_{\mathbf{\bz_t}}\,p(\bz_t \mid \by) $$ </div> </details> <h2 id="analytical-likelihoods">Analytical Likelihoods</h2> <p>In the last section, we discussed how to train an additional model serving as a likelihood. For a class of problems where the relationship between the data and the observations follows a known probabilistic model, the likelihood function can be derived analytically. A canonical example are inverse problems, where we have observations $\by$ and we want to recreate $\bx$. We consider observations of the form:</p> \[\begin{equation} \mathbf{y} = \mathcal{A}(\bx) + \mathbf{n} \quad \textrm{where} \quad \mathbf{y}, \mathbf{n} \in \mathbb{R}^m, \mathbf{x} \in \mathbb{R}^n. \end{equation}\] <p>Here \(\mathbf{y}\) is our observation, \(\mathbf{x}\) is the underlying clean data, \(\mathcal{A}:\mathbb{R}^n \mapsto \mathbb{R}^m\) is the <strong>forward operator</strong> (which may be linear or nonlinear), and $\mathbf{n}$ is observation noise.</p> <p>For Gaussian noise $\mathbf{n} \sim \normal(\mathbf{0}, \sigma^2 \mathbf{I})$, the likelihood is defined by:</p> \[\begin{equation} \label{eq:y-given-x-gaussian-noise} p(\by \mid \bx) = \normal(\by; \mathcal{A}(\bx), \sigma^2 \mathbf{I}) \propto \exp \left(-\frac{1}{2 \sigma^2} ||\by - \mathcal{A}(\bx)||^2 \right) . \end{equation}\] <p>Typical inverse problems are:</p> <ul> <li> <p><strong>Inpainting</strong>, where the operator $\mathcal{A}$ acts as a mask that zeros out out certain pixels, requiring the model to fill in the missing areas.</p> </li> <li> <p><strong>Super-resolution</strong>, where the operator $\mathcal{A}$ performs a downsampling operation and the model’s task is to generate the image at high resolution.</p> </li> <li> <p><strong>Deblurring</strong>, where the operator $\mathcal{A}$ acts as a gaussian filter operation blurring out pixels.</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-diffusion-guidance/cover-480.webp 480w,/2026/assets/img/2026-04-27-diffusion-guidance/cover-800.webp 800w,/2026/assets/img/2026-04-27-diffusion-guidance/cover-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-diffusion-guidance/cover.png" class="img-fluid" width="100%" height="auto" title="Posterior Sampling with Diffusion Models" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 1: Source: https://dps2022.github.io/diffusion-posterior-sampling-page</figcaption> </figure> <h3 id="diffusion-posterior-sampling-dps">Diffusion Posterior Sampling (DPS)</h3> <p>Direct computation of the time-dependent likelihood gradient $\textcolor{#00a5cf}{\nabla_{\bz_t} \log p(\mathbf{y} \mid \bz_t)}$ poses significant computational challenges. Let’s calculate the likelihood by marginalizing over $\bx$:</p> \[\begin{equation} p(\by \mid \bz_t) = \int p(\by \mid \bx) \, p(\bx \mid \bz_t) d \bx = \E_{\bx \sim p(\bx \mid \bz_t)}[p(\by \mid \bx)] . \end{equation}\] <p>The intractability arises from two sources: (1) the marginalization over the high-dimensional space of clean samples $\bx$, and (2) the dependence of $p(\by \mid \bz_t)$ on the reverse diffusion process, which requires integrating over all trajectories from time $t$ to $0$. DPS<d-cite key="chung2022diffusion"></d-cite> addresses this through an approximation that avoids explicit marginalization:</p> \[\begin{equation} p(\by \mid \bz_t) \approx p(\by \mid \hat{\bx}(\bz_t)), \end{equation}\] <p>where \(\hat{\bx}(\bz_t) = \E[\bx \mid \bz_t]\). By Tweedie’s formula<d-cite key="efron2011tweedie,tweedie1957"></d-cite>, the posterior mean is given by</p> \[\begin{equation} \E[\bx \mid \bz_t] = \frac{1}{\alpha_t} (\bzt + \sigma_t^2 \nabla_{\bzt} \log p(\bz_t)) . \end{equation}\] <p>Substituting $\nabla_{\bzt} \log p(\bz_t)$ with the learned score function $s_\theta(\bzt;t)$ gives the estimator:</p> \[\begin{equation} \bx_\theta(\bzt;t) = \frac{1}{\alpha_t} (\bzt + \sigma_t^2 s_\theta(\bzt;t)) . \end{equation}\] <p>Instead of learning the score function, we can train a network to predict the clean data directly, i.e., act as a denoiser<d-cite key="karras2022elucidating"></d-cite>. There are three main parameterizations: score prediction, data prediction, and noise prediction. These parameterizations are mathematically equivalent and allow seamless integration of all current diffusion model formulations into this framework. <d-cite key="kingma2023understanding"></d-cite> provides an extensive analysis.</p> <p>We can now combine the Gaussian likelihood from Eq. \eqref{eq:y-given-x-gaussian-noise} with the network’s data prediction, \(\bx_\theta(\bzt;t)\), to analytically obtain the log-likelihood gradient:</p> \[\begin{equation} \nabla_{\bzt} \log p(\by \mid \bz_t) \approx -\frac{1}{\sigma^2} \nabla_{\bzt} ||\by - \mathcal{A}(\bx_\theta(\bzt;t))||^2_2 . \end{equation}\] <p>Again, we combine the prior and the likelihood term to obtain the score function:</p> \[\begin{equation} \textcolor{#A125A1}{\nabla_{\bzt} \log p(\bx \mid \bz_t)} \approx \textcolor{#25a18e}{s_\theta(\bzt;t)} - \textcolor{#00a5cf}{\zeta \nabla_{\bzt} ||\by - \mathcal{A}(\bx_\theta(\bzt;t))||^2_2} . \end{equation}\] <h3 id="example">Example</h3> <p>Let’s consider a simple example using the Swiss Roll data to demonstrate the effectiveness of diffusion priors in solving inverse problems. We define a linear forward operator $\mathcal{A}$ that performs a projection of the 2D data onto the \(x_1\) axis:</p> \[\begin{equation} \by = \mathcal{A}(\bx) = \mathbf{A}\bx = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 0 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \end{equation}\] <p>We sample observations from the ground-truth manifold but restrict them to the left side of the Swiss Roll. Next, for each observation \(\by\), we sample from the posterior distribution \(p(\bx \mid \by)\), which places the samples back on the spiral manifold. The results are displayed in the figure below, where we have:</p> <ol> <li> <p><strong>Observations (Left)</strong>: This plot shows the observations $p(\by \mid \bx)$. Since the operator projects everything onto the x-axis, the unique spiral structure of the Swiss Roll is entirely gone. This creates an ill-posed inverse problem: for any observed point on this line, there are multiple possible “correct” locations on the original spiral that could have produced the outcome.</p> </li> <li> <p><strong>Standard Sampling (Right)</strong>: This represents the unconditional generation from our trained diffusion model. While these points perfectly inhabit the Swiss Roll manifold, they are random samples from the prior $p(\bx)$. They show us what the model “knows” about the data distribution, but they have no connection to the specific measurements we observed.</p> </li> <li> <p><strong>DPS Sampling (Center)</strong>: Here we show the reconstruction using Diffusion Posterior Sampling with $\zeta=0.25$. By using the measurement gradient to guide the generation process, DPS pushes \(\bx\) to match the observation \(\by\). It solves the ambiguity by finding points that are both consistent with the measurement $\by$ and highly probable under the learned prior $p(\bx)$, recovering the spiral shape despite information loss through the operator.</p> </li> </ol> <iframe src="/2026/assets/html/2026-04-27-diffusion-guidance/measurements_dps_standard.html" frameborder="0" scrolling="no" height="400px" width="100%"></iframe> <p><br></p> <p>We note that DPS is only one possible way to estimate the likelihood term \(\textcolor{#00a5cf}{\nabla_{\bz_t} \log p(\mathbf{y} \mid \bz_t)}\). For interested readers, we recommend the excellent survey by Daras et al. <d-cite key="daras2024survey"></d-cite>, which compares a range of different approaches.</p> <h2 id="applications-for-physical-sciences">Applications for Physical Sciences</h2> <p>We now turn to scientific applications and examine how flexible the discussed approaches are in practice, particularly in settings where physical constraints must be respected. The following is just an high level overview.</p> <h3 id="diffusionpde">DiffusionPDE</h3> <p>DiffusionPDE<d-cite key="huang2024diffusionpde"></d-cite> use DPS to sample Partial Differential Equations (PDEs) solutions from sparse observation, by exploiting the structure of the underlying PDE, a guidance term can be derived to align the sampling to the underlying structure of the PDE.</p> <p>We take the <strong>Darcy flow</strong> PDE equation as an example:</p> \[\begin{aligned} -\nabla \cdot \big(a(\bc)\nabla u(\bc)\big) &amp;= q(\bc), \quad \bc \in \Omega, \\ u(\bc) &amp;= 0, \quad \bc \in \partial\Omega. \end{aligned}\] <p>Defining the residual as an operator</p> \[\begin{aligned} f(\bc) = \nabla \cdot \big(a(\bc)\nabla u(\bc)\big) + q(\bc), \end{aligned}\] <p>so that valid solutions satisfy $f(\bc) = 0 $. This residual is used to construct an additional physics-based guidance loss:</p> \[\begin{aligned} \mathcal{L}_{\textrm{pde}} = ||\mathbf{0} - f(\hat{\bx}(\bz_t)) ||^2_2 . \end{aligned}\] <p>DiffusionPDE then augments the conditional score function as</p> \[\begin{equation} \textcolor{#A125A1}{\nabla_{\bz_t} \log p(\bz_t \mid \by, f)} \approx \textcolor{#25a18e}{s_\theta(\bzt;t)} \textcolor{#00a5cf}{+ \zeta \nabla_{\bz_t} \log p(\mathbf{y} \mid \bz_t) - \zeta_{\text{pde}} \nabla_{\bz_t} \mathcal{L}_{\text{pde}}}. \end{equation}\] <p>The sampling process is very similar to the one we saw before, combining the part we saw in DPS sampling and the additional term for the PDE solution.</p> <h3 id="inequality-constraints-for-rare-event-sampling">Inequality constraints for rare event sampling</h3> <p>Rare events play a central role in many scientific settings, yet they are often difficult to sample. This is evident in weather prediction, where extreme events such as floods are of particular concern.</p> <p>The work by Finzi et al. <d-cite key="finzi2023user"></d-cite> showed that operators can also be defined via inequality constraints. For a one-dimensional inequality constraint \(\mathcal{A}(\bx) &gt; y\), we want to sample from \(p(\mathcal{A}(\bx) &gt; y \mid \bz_t)\).</p> <p>This inequality constraint is defined by a Gaussian CDF function \(\Phi\):</p> \[\begin{equation} p(\mathcal{A}(\bx) &gt; y \mid \bz_t) \approx \Phi \left( \frac{\mathcal{A}(\bx_\theta(\bzt;t)) &gt; y}{\sqrt{\nabla \mathcal{A}(\bx_\theta(\bzt;t))^T \hat{\Sigma}(\bz_t) \nabla \mathcal{A}(\bx_\theta(\bzt;t)) }} \right) , \end{equation}\] <p>with the covariance of \(\bx\) given \(\bz_t\)</p> \[\hat{\Sigma}(\bz_t)=\frac{\sigma_t^2}{\alpha_t^2} (\bI + \sigma_t^2 \nabla_{\bzt}^2 \log p(\bzt) )\] <p>and</p> \[\nabla \mathcal{A}(\bx_\theta(\bzt;t))= \left. \nabla \mathcal{A}(\bx) \right|_{\bx = \bx_\theta(\bzt;t)} .\] <p>Using the Gaussian CDF function assigns high probability to events with \(\mathcal{A}(\bx) &gt; y\).</p> <p>We can then sample from these event by using</p> \[\begin{equation} \textcolor{#A125A1}{\nabla_{\bz_t} \log \, p(\bz_t \mid \mathcal{A}(\bx) &gt; y)} = \textcolor{#25a18e}{\nabla_{\bz_t} \log p(\bz_t)} + \textcolor{#00a5cf}{\zeta \nabla_{\bz_t} \log p(\mathcal{A}(\bx) &gt; y \mid \bz_t)} . \end{equation}\] <p>The work shows how to effectively sample extreme events in a Fitzhugh-Nagumo system, modeling neuron spiking events occurring only in 1/30 of the trajectories.</p> <h2 id="closing-takeaways">Closing takeaways</h2> <p>To sum it up, guidance enables us to adapt the sampling process of diffusion models by modifying the direction of the underlying vector field. While Classifier Guidance and Classifier-Free Guidance are well known tools, guidance based on analytical likelihoods is still less widely known, especially in scientific applications.</p> <ul> <li> <strong>Analytical Likelihoods:</strong> Enable pre-trained diffusion models to be reused as flexible priors across many downstream tasks, without retraining.</li> <li> <strong>Applications in Physical Sciences:</strong> By defining forward operators for PDE residuals, or inequality constraints, guidance can steer the sampler toward solutions that are not only data-consistent but also physically more plausible.</li> </ul> <p>The approaches we discussed, represent just a small part of the full landscape and we see that there is currently growing interest in new guidance strategies. A particularly exciting path is the idea of learning strong universal priors in the physics domain and using them across various downstream tasks.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-diffusion-guidance.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/general-agent-evaluation/">Ready For General Agents? Let's Test It.</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-vlms-waste-their-vision/">Why vlms waste their vision</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>