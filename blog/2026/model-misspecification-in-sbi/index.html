<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Model Misspecification in Simulation-Based Inference - Recent Advances and Open Challenges | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Model misspecification is a critical challenge in simulation-based inference (SBI), particularly in neural SBI methods that use simulated data to train flexible neural density estimators. These methods typically assume that simulators faithfully represent the true data-generating process, an assumption that is often violated in practice. Resulting discrepancies can make observed data effectively out-of-distribution relative to the simulations, leading to biased posterior distributions and misleading uncertainty quantification. This post reviews recent work on model misspecification in neural SBI, covering formal definitions, methods for detection and mitigation, and their underlying assumptions. It also discusses practical implications for SBI workflows and outlines open challenges for developing robust SBI methods that remain reliable in realistic, imperfectly specified applications."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/model-misspecification-in-sbi/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Model Misspecification in Simulation-Based Inference - Recent Advances and Open Challenges",
            "description": "Model misspecification is a critical challenge in simulation-based inference (SBI), particularly in neural SBI methods that use simulated data to train flexible neural density estimators. These methods typically assume that simulators faithfully represent the true data-generating process, an assumption that is often violated in practice. Resulting discrepancies can make observed data effectively out-of-distribution relative to the simulations, leading to biased posterior distributions and misleading uncertainty quantification. This post reviews recent work on model misspecification in neural SBI, covering formal definitions, methods for detection and mitigation, and their underlying assumptions. It also discusses practical implications for SBI workflows and outlines open challenges for developing robust SBI methods that remain reliable in realistic, imperfectly specified applications.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Model Misspecification in Simulation-Based Inference - Recent Advances and Open Challenges</h1> <p>Model misspecification is a critical challenge in simulation-based inference (SBI), particularly in neural SBI methods that use simulated data to train flexible neural density estimators. These methods typically assume that simulators faithfully represent the true data-generating process, an assumption that is often violated in practice. Resulting discrepancies can make observed data effectively out-of-distribution relative to the simulations, leading to biased posterior distributions and misleading uncertainty quantification. This post reviews recent work on model misspecification in neural SBI, covering formal definitions, methods for detection and mitigation, and their underlying assumptions. It also discusses practical implications for SBI workflows and outlines open challenges for developing robust SBI methods that remain reliable in realistic, imperfectly specified applications.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#a-concrete-example-sir-model-with-weekend-reporting-delay">A Concrete Example - SIR Model with Weekend Reporting Delay</a> </div> <div> <a href="#defining-model-misspecification">Defining Model Misspecification</a> </div> <ul> <li> <a href="#model-misspecification-in-simulation-based-inference">Model Misspecification in Simulation-Based Inference</a> </li> </ul> <div> <a href="#mitigating-model-misspecification-in-sbi">Mitigating Model Misspecification in SBI</a> </div> <ul> <li> <a href="#learning-explicit-misspecification-models">Learning Explicit Misspecification Models</a> </li> <li> <a href="#detecting-misspecification-with-learned-summary-statistics">Detecting Misspecification with Learned Summary Statistics</a> </li> <li> <a href="#learning-misspecification-robust-summary-statistics">Learning Misspecification-Robust Summary Statistics</a> </li> <li> <a href="#addressing-misspecification-with-optimal-transport">Addressing Misspecification with Optimal Transport</a> </li> </ul> <div> <a href="#practical-implications-for-sbi-workflows">Practical Implications for SBI Workflows</a> </div> <div> <a href="#open-challenges">Open Challenges</a> </div> </nav> </d-contents> <p>Simulation-based inference (SBI) provides a powerful framework for applying Bayesian inference to complex scientific simulators where direct likelihood computation is infeasible <d-cite key="cranmer_frontier_2020"></d-cite>. By requiring only simulated data to approximate the posterior distribution over simulator parameters, $p(\mathbf{\theta}\mid \mathbf{x_o})$, SBI has found applications across neuroscience, physics, climate science, and epidemiology <d-cite key="goncalves_training_2020,brehmer_simulationbased_2020,mckinley2014simulation"></d-cite>. All of these applications rely on a critical assumption: that the simulator faithfully represents the true data-generating process. When this assumption is violated, the resulting model misspecification can undermine the reliability of inference.</p> <p>This issue is particularly acute in <em>neural</em> SBI, where neural networks are trained on simulated data to approximate posterior distributions, likelihoods, or likelihood ratios. Neural networks are known to produce arbitrarily incorrect predictions when queried with out-of-distribution (OOD) inputs <d-cite key="szegedy_intriguing_2014"></d-cite>. Under model misspecification, the observed data $\mathbf{x}_o$ can be effectively OOD relative to the training simulations, leading to biased posterior estimates and misleading uncertainty quantification. Accordingly, empirical studies confirm that even seemingly minor mismatches between simulator and reality can induce substantial parameter bias and severely miscalibrated credible intervals <d-cite key="cannon_investigating_2022"></d-cite>.</p> <p>These observations motivate methods that detect and mitigate model misspecification in SBI, rather than assuming perfectly specified simulators. This blog post reviews recent advances in this area for neural SBI. It introduces a concrete running example that illustrates key concepts, formalizes the notion of model misspecification in SBI, surveys four categories of methods for addressing misspecification, discusses their assumptions and practical implications, and concludes with open challenges for the field.</p> <h2 id="a-concrete-example---sir-model-with-weekend-reporting-delay">A Concrete Example - SIR Model with Weekend Reporting Delay</h2> <p>Before formalizing these concepts, we introduce a concrete running example: the Susceptible-Infected-Recovered (SIR) epidemic model with weekend reporting delays</p> <d-cite key="ward_robust_2022"></d-cite> <p>. The SIR model describes disease spread through infection and recovery rates $(\beta, \gamma)$, with reproduction number $R_0 = \beta/\gamma$. In the clean simulator, infection reports occur uniformly across all days. In contrast, real-world data often exhibit systematic patterns—here, a fraction $\alpha$ of weekend infections goes unreported until Monday, creating characteristic weekly oscillations (Figure 1).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-model-misspecification-in-sbi/sir_figure_row-480.webp 480w,/2026/assets/img/2026-04-27-model-misspecification-in-sbi/sir_figure_row-800.webp 800w,/2026/assets/img/2026-04-27-model-misspecification-in-sbi/sir_figure_row-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-model-misspecification-in-sbi/sir_figure_row.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <strong>Figure 1:</strong> SIR model with weekend reporting delay. <strong>A</strong>: Schematic SIR model and misspecification: a fraction α of weekend infections is reported on Monday. <strong>B</strong>: Example epidemic trajectory showing true S, I, R curves and observed infections with a weekend delay. <strong>C</strong>: SBI posterior samples for $(\beta, \gamma)$ for clean data (α = 0%, blue) and misspecified data (α=20%, orange), with true parameters marked. <strong>D</strong>: Posterior predictive checks for both cases, illustrating how misspecification shifts and distorts the inferred dynamics. Results generated using the [sbi](https://sbi.readthedocs.io/)<d-cite key="boelts_sbi_2025"></d-cite> package and SIR specifications from Cannon et al.. </div> <p>To infer infection parameters from observed data, a common SBI approach would be using neural posterior estimation (NPE, <d-cite key="papamakarios_fast_2016"></d-cite>), training a conditional density estimator $q_{\phi}(\theta \mid x)$ on clean simulated parameter–data pairs. When NPE is trained on clean simulations but encounters observations with weekend delays, the network faces out-of-distribution data. Figure 1 demonstrates the consequences: even with mild misspecification (α=20%), the posterior shifts away from true parameters, uncertainty increases, and posterior predictive samples fail to capture the systematic Monday spikes. Ward et al. (2022) <d-cite key="ward_robust_2022"></d-cite> quantify this effect, showing that such discrepancies can bias parameter estimates by over 40% and cause credible interval coverage to drop from the nominal 95% to below 60%.</p> <p><strong>Note on the example:</strong> This scenario is designed for pedagogical illustration—in practice, if the weekend reporting delay were known, practitioners would explicitly model it in the simulator. The methods discussed below target situations where misspecification is unknown or difficult to parametrize, or where the goal is to develop inference procedures that remain robust to unanticipated discrepancies.</p> <p>Having seen the practical impact of misspecification in this example, we now turn to formal definitions and a systematic review of approaches for addressing this challenge.</p> <h2 id="defining-model-misspecification">Defining Model Misspecification</h2> <p>Model misspecification occurs when the assumptions of the model do not align with the true data-generating process. In Bayesian inference, this problem arises when the true data-generating process cannot be captured within the family of distributions defined by the model. Walker (2013) provides a foundational definition <d-cite key="walker_bayesian_2013"></d-cite>:</p> <blockquote> A statistical model $p(\mathbf{x}_s | \theta)$ that relates a parameter of interest $\theta \in \Theta$ to a conditional distribution over simulated observations $\mathbf{x}_s$ is said to be misspecified if the true data-generating process $p(\mathbf{x}_o)$ of the real observations $\mathbf{x}_o \sim p(\mathbf{x}_o)$ does not belong to the family of distributions $\{p(\mathbf{x}_s | \theta); \theta \in \Theta\}$. </blockquote> <p>This structural definition provides a theoretical basis for understanding model misspecification but does not yet specify how misspecification manifests in SBI workflows.</p> <h3 id="model-misspecification-in-simulation-based-inference">Model Misspecification in Simulation-Based Inference</h3> <p>SBI is particularly sensitive to model misspecification because the model is defined through a simulator, and inference relies entirely on simulator-generated data. Unlike classical Bayesian inference, where the likelihood function is explicit, simulators in SBI may introduce subtle discrepancies that propagate through the inference pipeline, resulting in biased posterior estimates.</p> <p>The consequences of model misspecification in SBI were first analyzed systematically by Frazier et al. (2019) in the context of rejection sampling-based Approximate Bayesian Computation (ABC) <d-cite key="frazier_model_2019"></d-cite>. They showed that, under misspecification, ABC procedures concentrate on pseudo-true parameters that minimize discrepancies between simulated and observed data, and that resulting credible sets can exhibit distorted frequentist coverage. This work established that misspecification is not merely a philosophical concern but has concrete implications for SBI workflows.</p> <p>In <em>neural</em> SBI methods, where neural networks approximate posterior distributions (or likelihoods or likelihood ratios) based on simulations, the problem becomes particularly acute. A popular approach is neural posterior estimation (NPE), where a neural network learns a parametric approximation of the posterior distribution (e.g., a mixture of Gaussians, a normalizing flow, or a diffusion model) using simulated data. Cannon et al. (2022) <d-cite key="cannon_investigating_2022"></d-cite> conducted a first comprehensive study of neural SBI algorithms under different forms of model misspecification and found that performance can degrade severely: neural networks trained on simulations can fail catastrophically when applied to observed data that lie outside the training distribution—producing arbitrarily incorrect predictions with overconfident uncertainty estimates—and existing mitigation strategies do not prevent failure in all cases.</p> <p>Before reviewing methods to mitigate misspecification in neural SBI, it is important to distinguish between different sources of misspecification in the workflow:</p> <ol> <li> <strong>Misspecification of the Simulator:</strong> The true data-generating process does not belong to the family of distributions induced by the simulator. This corresponds to the classical Bayesian notion of misspecification described by Walker (2013). For example, if a simulator lacks the capacity to model key features of the observed data, the resulting posterior may fail to capture the true parameter values accurately.</li> <li> <strong>Misspecification of the Prior:</strong> Misspecification can also occur when the prior used in the inference process does not incorporate the “true parameter” underlying the data-generating process. Prior mismatch can distort posterior estimates, leading to inferences that reflect artifacts of the assumed prior rather than the true underlying process.</li> </ol> <p>Prior misspecification is a general challenge in Bayesian inference and can be addressed with standard Bayesian workflow tools like prior predictive checks <d-cite key="gelman_bayesian_2020"></d-cite>. While it has received less attention in the SBI-specific literature in general, there has been growing interest in this topic recently <d-cite key="wehenkel_addressing_2024,bockting_simulationbased_2024,bishop_learning_2025"></d-cite>.</p> <p>The methods reviewed below, and much of the recent literature on model misspecification in neural SBI, primarily address the first case: detecting and mitigating simulator-related misspecification. The remainder of this post provides an overview of these approaches.</p> <h2 id="mitigating-model-misspecification-in-sbi">Mitigating Model Misspecification in SBI</h2> <p>Recent works have introduced a range of methods to address model misspecification in simulation-based inference (SBI). These approaches can be broadly categorized into four strategies: learning explicit mismatch models, detecting misspecification through learned summary statistics, learning misspecification-robust statistics, and aligning simulated and observed data using optimal transport. Each method has unique strengths and limitations, which we summarize below.</p> <h3 id="learning-explicit-misspecification-models">Learning Explicit Misspecification Models</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-model-misspecification-in-sbi/ward_et_al-480.webp 480w,/2026/assets/img/2026-04-27-model-misspecification-in-sbi/ward_et_al-800.webp 800w,/2026/assets/img/2026-04-27-model-misspecification-in-sbi/ward_et_al-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-model-misspecification-in-sbi/ward_et_al.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2 (adapted from <d-cite key="ward_robust_2022"></d-cite>): Visualization of the robust neural posterior estimation (RNPE) framework. </div> <p>Ward et al. (2022) <d-cite key="ward_robust_2022"></d-cite> propose <strong>Robust Neural Posterior Estimation (RNPE)</strong>, an extension of neural posterior estimation (NPE) that addresses misspecification by explicitly modeling discrepancies between simulated and observed data. RNPE introduces an error model $p(\mathbf{y} \mid \mathbf{x})$, where $\mathbf{y}$ denotes observed data and $\mathbf{x}$ simulated data, and combines it with a marginal density model $q(\mathbf{x})$ trained on simulations. A standard NPE is trained on $(\theta, \mathbf{x})$ pairs, while at inference time Monte Carlo sampling from $p(\mathbf{x} \mid \mathbf{y}) \propto q(\mathbf{x}) p(\mathbf{y} \mid \mathbf{x})$ produces denoised latent variables $\mathbf{x}_m$ that are passed through NPE to approximate $p(\theta \mid \mathbf{x}_m)$.</p> <p>The results in <d-cite key="ward_robust_2022"></d-cite> demonstrate that RNPE can substantially improve robustness to misspecification on several benchmark tasks and an epidemic application. By explicitly modeling the error for each data dimension, the approach also facilitates model criticism, highlighting features of the data that are likely to be misspecified. However, performance hinges on choosing a suitable error model (such as a spike-and-slab distribution), which may not generalize across scenarios, and the additional inference steps make the method computationally demanding, particularly for high-dimensional observations.</p> <p>In the SIR weekend-delay example, RNPE uses a spike-and-slab error model to capture the Monday aggregation effect. Ward et al. show that this effectively denoises the Monday spikes back to plausible weekend values, recovering parameter estimates within about 5% of ground truth, compared to roughly 40% bias with standard NPE.</p> <h3 id="detecting-misspecification-with-learned-summary-statistics">Detecting Misspecification with Learned Summary Statistics</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-model-misspecification-in-sbi/schmitt_et_al-480.webp 480w,/2026/assets/img/2026-04-27-model-misspecification-in-sbi/schmitt_et_al-800.webp 800w,/2026/assets/img/2026-04-27-model-misspecification-in-sbi/schmitt_et_al-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-model-misspecification-in-sbi/schmitt_et_al.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 3 (adapted from <d-cite key="schmitt_detecting_2024"></d-cite>): Simulated data is used to train a neural network to map into a latent space designed to detect misspecification. At inference time, the observed data is embedded mapped into the latent space to detect misspecification. </div> <p>Schmitt et al. (2024) <d-cite key="schmitt_detecting_2024"></d-cite> focus on <em>detecting</em> misspecification using learned summary statistics. The method employs a summary network, $h_\psi(\mathbf{x})$, to encode both observed and simulated data into a structured summary space, typically following a multivariate Gaussian distribution. Discrepancies between distributions in this space are quantified using metrics like Maximum Mean Discrepancy (MMD), with significant divergences indicating misspecification.</p> <p>The training procedure for this approach remains the same as in standard neural SBI methods except for an additional MMD term in the NPE loss function:</p> \[\mathcal{L}_{\phi, \psi} = \mathcal{L}_{\text{inference}}(\phi) + \lambda \cdot \text{MMD}^2[p(h_{\psi}(\mathbf{x})), \mathcal{N}(\mathbf{0}, \mathbb{I})].\] <p>Intuitively, the additional MMD loss term encourages the embedding network to obtain a Gaussian structure in the latent summary space, while not directly affecting the quality of the posterior estimation ensured by the standard NPE loss <d-cite key="schmitt_detecting_2024"></d-cite>. At inference time, the learned embedding network can then be used to detect misspecification for unseen, e.g., observed, data points.</p> <p>This approach is adaptable to diverse data types and does not require explicit knowledge of the true data-generating process. Additionally, it is amortized, i.e., it can be applied to new observed data without re-training because the training does not depend on $\mathbf{x}_o$. However, its performance depends on the design of the summary network and the choice of divergence metric. While effective for detecting misspecification, it does not directly correct for it, instead providing insights for iterative simulator refinement.</p> <p>For the SIR weekend delay example, practitioners could apply embedding-based detection to summary statistics (mean, median, maximum, day of maximum, day when half of cumulative infections reached, and autocorrelation) or train an unconditional normalizing flow over these statistics. Both approaches would quantitatively flag the observed data as out-of-distribution, signaling that the simulator fails to capture systematic patterns present in real epidemic data.</p> <p>Beyond the embedding-based approach described above, another practical option for detection is to learn the marginal distribution $p(\mathbf{x})$ directly using density estimation—for instance, via normalizing flows. Trained on simulated data, the learned density can then evaluate whether observed data $\mathbf{x}_o$ has anomalously low log-probability, flagging it as out-of-distribution. This density-based approach is conceptually straightforward but limited to relatively low-dimensional data, whereas the embedding approach scales better to higher dimensions including time series.</p> <h3 id="learning-misspecification-robust-summary-statistics">Learning Misspecification-Robust Summary Statistics</h3> <p>Huang &amp; Bharti et al. (2023) <d-cite key="huang_learning_2023"></d-cite> propose a method for learning summary statistics that are both informative about parameters and robust to misspecification. Their approach modifies the standard NPE loss function by introducing a regularization term that balances robustness to misspecification with informativeness:</p> \[\mathcal{L} = \mathcal{L}_{\text{inference}} + \lambda \cdot \text{MMD}^2[h_\psi(\mathbf{x}_{s}), h_\psi(\mathbf{x}_{o})].\] <p>Here, $h_\psi$ represents the summary network, $\mathbf{x}_{s}$ and $\mathbf{x}_{o}$ are simulated and observed data, respectively, and $\lambda$ controls the trade-off between inference accuracy and robustness. Unlike diagnostic methods, this approach directly adjusts the summary network during training to mitigate the impact of misspecification on posterior estimation.</p> <p>Benchmarking results presented in Huang &amp; Bharti et al. (2023) demonstrate improved performance compared to the RNPE approach, with the additional advantage of applicability to high-dimensional data. However, the method has several limitations. The modified loss function introduces additional complexity, and its success depends on selecting appropriate divergence metrics and regularization parameters, which often require domain-specific tuning. Furthermore, because robustness is implicitly learned during training and operates in the latent space, there is limited direct control over how and where misspecification is mitigated.</p> <h3 id="addressing-misspecification-with-optimal-transport">Addressing Misspecification with Optimal Transport</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-model-misspecification-in-sbi/wehenkel_gamella_et_al-480.webp 480w,/2026/assets/img/2026-04-27-model-misspecification-in-sbi/wehenkel_gamella_et_al-800.webp 800w,/2026/assets/img/2026-04-27-model-misspecification-in-sbi/wehenkel_gamella_et_al-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-model-misspecification-in-sbi/wehenkel_gamella_et_al.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <strong>Figure 4:</strong> Schematic of RoPE <d-cite key="wehenkel_addressing_2024"></d-cite>. Standard NPE with embedding network is trained on potentially misspecified simulations; a calibration set is then used to fine-tune the embedding to the observed data and learn an optimal transport mapping, which is used to construct a misspecification-robust posterior as a weighted mixture of NPE posteriors. </div> <p>Wehenkel &amp; Gamella et al. (2024) <d-cite key="wehenkel_addressing_2024"></d-cite> propose Robust Posterior Estimation (RoPE), which combines neural posterior estimation (NPE) with optimal transport (OT) to address model misspecification. The approach requires a calibration set of real-world observations with known ground-truth parameters, for example from expensive experiments where parameters can be measured directly while a cheaper simulator models only part of the process.</p> <p>The core idea is to use OT to relate simulated and observed data in an embedding space learned by NPE. A standard NPE is first trained on simulated $(\theta, \mathbf{x}_s)$ pairs to obtain an embedding network and posterior estimator. The embedding is then fine-tuned on the calibration set to better align simulated and observed data. At inference time, OT computes a matching between embedded simulated data $x_s^j$ and observed data $\mathbf{x}_o$, yielding weights $\alpha_{ij}$ that define a mixture of NPE posteriors,</p> \[\tilde{p}(\theta \mid \mathbf{x}_o) = \sum_{j=1}^{N_s} \alpha_{ij} \, q(\theta \mid \mathbf{x}_s^j),\] <p>where $q(\theta \mid \mathbf{x}_s^j)$ denotes the NPE posterior for simulated input $\mathbf{x}_s^j$. Increasing the number of simulated samples $N_s$ makes the mixture more conservative and, in the limit, approach the prior. This underconfidence property provides a mechanism to avoid overconfident posteriors under severe misspecification, at the cost of broader, less informative posteriors.</p> <p>In the SIR weekend-delay example, RoPE would use a calibration set of outbreaks with known ground-truth transmission parameters and learn to align Monday spikes in real data with weekend patterns in simulations, yielding corrected posteriors for new outbreaks despite the simulator’s limitations.</p> <p>While conceptually elegant, RoPE relies on calibration data, which may not be available in all fields. Moreover, it is transductive: the OT problem is solved on a batch of test observations, so inference for one observation depends on which others appear in the batch and cannot be performed fully independently.</p> <p>Addressing this limitation, Senouf et al. (2025) <d-cite key="senouf_inductive_2025a"></d-cite> introduced an extension called FRISBI, which shifts the OT step from test time to training time and makes the approach inductive and amortized. During training, FRISBI applies mini-batch OT on the calibration set to learn aligned embeddings and then trains a conditional normalizing flow to approximate the resulting mixture posterior. At inference, a single forward pass yields a misspecification-robust posterior, making FRISBI more scalable while preserving the robustness properties of RoPE.</p> <h3 id="summary-of-approaches">Summary of Approaches</h3> <p>The methods discussed above tackle different facets of model misspecification in SBI, ranging from explicit error modeling to the development of robust summary statistics and the alignment of simulated and observed data distributions. While each approach demonstrates unique strengths, their applicability depends on the specific misspecification scenario, computational budget, and the availability of calibration data.</p> <p>It is useful to contrast what “robustness” means in each family of methods. RNPE aims for robustness under an explicitly <em>parameterized error model</em>, providing reliable inference when this model captures the dominant discrepancies between simulator and reality. Detection-oriented methods based on embeddings or density estimation instead provide robustness in the sense of <em>awareness</em>: they quantify when simulated and observed data are incompatible but do not directly correct the posterior. Approaches that learn misspecification-robust summary statistics trade some information for stability, implicitly downweighting aspects of the data that are particularly sensitive to simulator errors. OT-based methods such as RoPE and FRISBI promote robustness through conservative posteriors aligned with calibration data, at the cost of broader posteriors when misspecification is severe.</p> <p>These approaches also rely on different assumptions, which shape their practical applicability. RNPE requires an explicit error model and is most natural in low- to medium-dimensional observation spaces where plausible mismatch mechanisms can be formulated. Detection methods require only simulated and observed data but depend on choices of summary network, density estimator, and divergence thresholds, and they are most useful when the primary goal is diagnosis rather than automatic correction. Methods based on robust summary statistics assume access to observed data during training and hinge on regularization choices that encode the desired robustness–accuracy balance. OT-based methods, in turn, rely critically on calibration data with known parameters and on the assumption that optimal transport in an embedding space yields meaningful correspondences between simulated and real observations; FRISBI additionally assumes that the induced mixture posteriors can be well-approximated by an amortized conditional density estimator.</p> <p>Taken together, these differences illustrate that there is no single “best” approach to model misspecification in SBI, but rather a spectrum of tools that make different assumptions and offer different robustness properties. The next section discusses the resulting practical implications for SBI workflows and outlines typical use cases and trade-offs for the available methods.</p> <h2 id="practical-implications-for-sbi-workflows">Practical Implications for SBI Workflows</h2> <p>Building on this comparison, the remarks in this section summarize typical use cases and trade-offs rather than prescribing a fixed workflow. The choice of method depends on the application, data characteristics, and available resources.</p> <p>For <em>detection</em> of misspecification, a natural starting point is prior predictive checks, which are often the most interpretable diagnostic. Quantitative approaches include density estimation-based detection (learning $p(\mathbf{x})$ via normalizing flows, typically limited to relatively low-dimensional data) and embedding-based detection (using learned summary spaces with divergence metrics, which scale to higher-dimensional settings). Both are implemented in popular toolboxes such as the <code class="language-plaintext highlighter-rouge">sbi</code> Python package (<a href="https://sbi.readthedocs.io/en/latest/how_to_guide/18_model_misspecification.html" rel="external nofollow noopener" target="_blank">documentation</a>) and the <code class="language-plaintext highlighter-rouge">BayesFlow</code> package (<a href="https://bayesflow.org/stable-legacy/_examples/Model_Misspecification.html" rel="external nofollow noopener" target="_blank">documentation</a>).</p> <p>For <em>mitigation</em> of misspecification, the methods reviewed above target different scenarios. RNPE is most effective when the structure of the misspecification can be characterized and data dimensionality is moderate, offering interpretability through the learned error model. Methods that learn robust summary statistics scale better to high-dimensional data when the misspecification structure is unclear, though they sacrifice amortization and introduce additional hyperparameters governing the robustness–accuracy trade-off. RoPE and FRISBI leverage optimal transport for domain alignment when calibration data (real observations with ground-truth parameters) is available: RoPE is transductive and operates on batches, while FRISBI is fully inductive and amortized, making it more scalable for per-sample inference once training is complete.</p> <p>The table below summarizes key characteristics of the different approaches to help organize their typical use cases:</p> <table> <thead> <tr> <th>Method</th> <th>Primary Goal</th> <th>Calibration Data?</th> <th>Amortized?</th> <th>Data Dimensionality</th> <th>Best For</th> </tr> </thead> <tbody> <tr> <td><strong>Detection (Flow)</strong></td> <td>Identify misspecification</td> <td>No</td> <td>Yes</td> <td>Low ($&lt;$20D)</td> <td>Diagnostics, low-dimensional data</td> </tr> <tr> <td><strong>Detection (Embedding)</strong></td> <td>Identify misspecification</td> <td>No</td> <td>Yes</td> <td>Medium-High</td> <td>Diagnostics, scalable detection</td> </tr> <tr> <td><strong>RNPE</strong></td> <td>Correct via error model</td> <td>No</td> <td>Yes</td> <td>Low ($&lt;$20D)</td> <td>Known error structure, interpretability</td> </tr> <tr> <td><strong>Robust Summary Stats</strong></td> <td>Learn implicit robustness</td> <td>Partial (observed $\mathbf{x}_o$)</td> <td>No</td> <td>High ($&gt;$50D)</td> <td>High-dimensional data, unclear misspecification</td> </tr> <tr> <td><strong>RoPE</strong></td> <td>Align distributions (transductive)</td> <td>Yes (small set)</td> <td>No</td> <td>Any</td> <td>Calibration data, batch inference</td> </tr> <tr> <td><strong>FRISBI</strong></td> <td>Align distributions (inductive)</td> <td>Yes (small set)</td> <td>Yes</td> <td>Any</td> <td>Calibration data, per-sample inference</td> </tr> </tbody> </table> <p>While this high-level overview helps to organize current approaches, it also highlights several gaps in our understanding and tooling, which motivate the open challenges discussed next.</p> <h2 id="open-challenges">Open Challenges</h2> <p>The recent works outlined above have made significant progress in addressing model misspecification in simulation-based inference (SBI), introducing methods for detecting and mitigating its effects. However, the problem is far from resolved, and several key challenges remain:</p> <ol> <li> <p><strong>Better Methods for Detecting and Addressing Model Misspecification:</strong> Recent methods have improved our ability to diagnose and mitigate model misspecification, but important limitations remain. Many techniques target specific aspects of misspecification (e.g., discrepancies in summary statistics or shifts in data distributions via optimal transport) and often require additional assumptions, computational overhead, or prior knowledge about the misspecification. A central challenge is to develop more flexible and scalable methods that can</p> <ul> <li>detect misspecification in a principled, data-driven manner, without relying heavily on hand-crafted summaries or manual tuning,</li> <li>provide interpretable diagnostics that clarify the sources and consequences of misspecification, and</li> <li>offer robust mitigation strategies that work across different misspecification regimes without requiring large additional datasets or costly corrections.</li> </ul> </li> <li> <p><strong>A Common and Precise Definition of Model Misspecification in SBI:</strong> As highlighted in this post, model misspecification in SBI can arise from different sources, including mismatches between the simulator and the true data-generating process and prior misspecification. A common and formally precise definition of these cases is essential for unifying the field. Such a framework would provide clarity for researchers and practitioners, enabling more systematic comparisons of methods and their applicability to specific types of misspecification.</p> </li> <li> <p><strong>Common Benchmarking Tasks for Evaluating Methods:</strong> Another obstacle to progress is the lack of an established set of benchmarking tasks tailored to the various forms of model misspecification. Current evaluations often focus on specific scenarios or datasets, limiting generalizability. There are promising developments—for instance, Wehenkel &amp; Gamella et al. <d-cite key="wehenkel_addressing_2024"></d-cite> re-used tasks proposed by Ward et al. <d-cite key="ward_robust_2022"></d-cite> and introduced new tasks to probe different aspects of misspecification—but these efforts need to be integrated into a common benchmarking framework and made accessible through open-source software. Such a framework would enable rigorous, comparable evaluations under realistic misspecification conditions and encourage the development of methods that are robust across diverse settings.</p> </li> <li> <p><strong>Comprehensive Practical Guidelines for Model Misspecification:</strong> While initial guidance exists (such as the practical implications outlined above and in <d-cite key="deistler_simulationbased_2025a"></d-cite>), the field would benefit from a comprehensive workflow study on model misspecification in SBI, similar in scope to the Bayesian workflow of Gelman et al. <d-cite key="gelman_bayesian_2020"></d-cite>. The preliminary guidance already shows that assembling current methods into a coherent workflow involves many subjective choices, underscoring the need for more systematic studies. A dedicated workflow, systematically investigating different types of misspecification, their detection, and suitable mitigation strategies, would provide practitioners with actionable decision frameworks, including recommendations on how to diagnose misspecification, select methods based on problem characteristics, and interpret posteriors under potential misspecification.</p> </li> </ol> <p>Addressing these challenges will pave the way for more robust and practical SBI methods capable of handling model misspecification effectively. A unified framework, rigorous benchmarks, and practical guidelines will not only advance research on model misspecification but also simplify its handling in applied settings. Together, these efforts will strengthen SBI as a reliable tool for scientific inference in complex and realistic scenarios.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-model-misspecification-in-sbi.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/symbolic-connect/">Symbolism Outside, Connectionism Inside: The Trend of Fusing LLMs and Automatic Programs with Symbolic Intermediate Representations</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/sac-massive-sim/">Getting SAC to Work on a Massive Parallel Simulator: An RL Journey With Off-Policy Algorithms</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/nlp-for-human-sciences/">Language as a Window Into the Mind: How NLP and LLMs Advance Human Sciences</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/mislead-lm/">Do Language Models Really Learn to Mislead Humans via RLHF?</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>