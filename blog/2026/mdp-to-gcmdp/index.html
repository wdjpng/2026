<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Learning to Maximize Rewards via Reaching Goals | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Goal-conditioned reinforcement learning learns to reach goals instead of optimizing hand-crafted rewards. Despite its popularity, the community often categorizes goal-conditioned reinforcement learning as a special case of reinforcement learning. In this post, we aim to build a direct conversion from any reward-maximization reinforcement learning problem to a goal-conditioned reinforcement learning problem, and to draw connections with the stochastic shortest path framework. Our conversion provides a new perspective on the reinforcement learning problem: &lt;i&gt;maximizing rewards is equivalent to reaching some goals&lt;/i&gt;."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/mdp-to-gcmdp/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}.detail{color:#1f77b4;cursor:pointer;display:inline-block;margin-bottom:1rem}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Learning to Maximize Rewards via Reaching Goals",
            "description": "Goal-conditioned reinforcement learning learns to reach goals instead of optimizing hand-crafted rewards. Despite its popularity, the community often categorizes goal-conditioned reinforcement learning as a special case of reinforcement learning. In this post, we aim to build a direct conversion from any reward-maximization reinforcement learning problem to a goal-conditioned reinforcement learning problem, and to draw connections with the stochastic shortest path framework. Our conversion provides a new perspective on the reinforcement learning problem: <i>maximizing rewards is equivalent to reaching some goals</i>.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Learning to Maximize Rewards via Reaching Goals</h1> <p>Goal-conditioned reinforcement learning learns to reach goals instead of optimizing hand-crafted rewards. Despite its popularity, the community often categorizes goal-conditioned reinforcement learning as a special case of reinforcement learning. In this post, we aim to build a direct conversion from any reward-maximization reinforcement learning problem to a goal-conditioned reinforcement learning problem, and to draw connections with the stochastic shortest path framework. Our conversion provides a new perspective on the reinforcement learning problem: <i>maximizing rewards is equivalent to reaching some goals</i>.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#three-building-blocks">Three building blocks</a> </div> <div> <a href="#bridging-rl-and-gcrl">Bridging RL and GCRL</a> </div> <div> <a href="#does-the-conversion-work-in-practice">Does the conversion work in practice?</a> </div> <div> <a href="#closing-remarks">Closing remarks</a> </div> </nav> </d-contents> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-mdp-to-gcmdp/rl-ssp-gcrl-480.webp 480w,/2026/assets/img/2026-04-27-mdp-to-gcmdp/rl-ssp-gcrl-800.webp 800w,/2026/assets/img/2026-04-27-mdp-to-gcmdp/rl-ssp-gcrl-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-mdp-to-gcmdp/rl-ssp-gcrl.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Reinforcement learning (RL) has achieved great success in solving many decision-making problems, from outperforming human-level control on challenging games<d-cite key="silver2018general"></d-cite> and fine-tuning large language models<d-cite key="shao2024deepseekmath"></d-cite> to learning scalable robotic manipulation policies<d-cite key="amin2025pi"></d-cite>. In the standard view, RL is a reward-maximization problem: given a hand-crafted reward function, we search for a policy that maximizes expected return. This perspective is powerful, but it leans heavily on reward design and often obscures alternative ways of thinking about the same underlying control problem.</p> <p>If you talk to different RL researchers, the same problem might be described in three languages. One focuses on standard RL and maximizing discounted return. Another uses the stochastic shortest path (SSP) view, emphasizing the cost of reaching a terminal state<d-cite key="bertsekas1991analysis,bertsekas1996neuro,kolobov2012planning"></d-cite>. A third prefers goal-conditioned RL (GCRL), where the agent is explicitly tasked to reach specified goals<d-cite key="kaelbling1993learning,schaul2015universal"></d-cite>. These three framings sound different, but they are closely related: RL emphasizes long-horizon reward accumulation, SSP emphasizes termination with minimal cost, and GCRL emphasizes reaching desired goals.</p> <p>Two sides of this triangle are already well understood. Prior work shows that any RL problem can be converted into an equivalent SSP problem<d-cite key="bertsekas1991analysis,bertsekas1996neuro,kolobov2012planning"></d-cite>. At the same time, recent GCRL methods demonstrate that simply “learning to reach goals” can drive strong exploration and skill acquisition, even with very sparse rewards<d-cite key="liu2025a,bastankhah2025"></d-cite>. This naturally leads to a question:</p> <p align="center"><i>Can we systematically turn any reward-maximization problem into a GCRL problem and, in doing so, expose it to the tools and intuitions developed for GCRL and SSP?</i></p> <p>In this post, we answer this question by constructing, from any reward-maximization Markov decision process (MDP), an equivalent goal-conditioned MDP (GCMDP). Our construction augments the original MDP with two absorbing states and pushes the reward function into the transition dynamics. We show that, under this construction, maximizing discounted return in the original MDP is exactly equivalent to maximizing the probability of reaching one of the absorbing states in the augmented GCMDP. Combined with the known RL–SSP equivalence, this yields a clean relationship.</p> \[\text{RL} \iff \text{SSP} \iff \text{GCRL}\] <h2 id="three-building-blocks">Three building blocks</h2> <p>We consider a Markov decision process <d-cite key="sutton1998reinforcement"></d-cite> \(\mathcal{M} = (\mathcal{S}, \mathcal{A}, p, r, p_0, \gamma)\) defined by a state space \(\mathcal{S}\), an action space \(\mathcal{A}\), a transition probability measure \(p: \mathcal{S} \times \mathcal{A} \to \Delta(\mathcal{S})\), a reward function \(r: \mathcal{S} \times \mathcal{A} \to \mathbb{R}\), an initial state probability measure \(p_0 \in \Delta(\mathcal{S})\), and a discount factor \(\gamma \in [0, 1)\), where \(\Delta(\cdot)\) denotes the set of all possible probability measures over a space. We use \(t\) to denote the time step in MDP. With slight abuse of notation, we use the probability measure to denote the probability mass in discrete MDPs and denote the probability density in continuous MDPs.</p> <h3 id="rl-objectives-and-successor-measures">RL objectives and successor measures</h3> <p>The goal of RL is to learn a policy \(\pi: \mathcal{S} \to \Delta(\mathcal{A})\) that maximizes the expected discounted return</p> \[\begin{align} \max_{\pi} J_{\gamma}(\pi), \quad J_{\gamma}(\pi) = (1 - \gamma) \mathbb{E}_{\tau \sim \pi(\tau)} \left[ \sum_{t = 0}^{\infty} \gamma^t r(s_t, a_t) \right], \label{eq:rl-obj} \end{align}\] <p>where \(\tau\) is a trajectory sampled by the policy \(\pi\). Alternatively, we can swap the discounted sum over rewards into a discounted sum over states and use it to describe the expected discounted return. Namely, the discounted sum over states is called <em>the discounted state occupancy measure</em><d-cite key="touati2021learning,janner2020gamma,eysenbach2022contrastive,zheng2023contrastive"></d-cite>, i.e., the <em>successor measures</em><d-cite key="dayan1993improving,barreto2017successor"></d-cite>,</p> \[\begin{align} p_{\gamma}^{\pi}(s) = (1 - \gamma) \sum_{t = 0}^{\infty} \gamma^t p^{\pi}_t(s), \label{eq:succ-measure} \end{align}\] <p>where \(p^{\pi}_t(s)\) is the probability measure of reaching state \(s\) at exact time step \(t\) under policy \(\pi\):</p> \[\begin{align} p^{\pi}_t(s) = p_0(s_0) \prod_{k = 0}^{t - 1} \pi(a_k \mid s_k) p(s_{k + 1} \mid s_k, a_k), \quad s_t = s. \label{eq:time-dependent-succ-measure} \end{align}\] <p>We note that the probability measure of visiting state \(s\) at the beginning is \(p^{\pi}_0(s) \triangleq p_0(s)\).<d-footnote>The summation in Eq.$~\ref{eq:succ-measure}$ starts from the current time step<d-cite key="eysenbach2022contrastive,touati2021learning"></d-cite> instead of the next time step as in some prior approaches<d-cite key="janner2020gamma,zheng2023contrastive"></d-cite>.</d-footnote> Using the successor measure, we can rewrite the expected discounted return as</p> \[\begin{align} J_{\gamma}(\pi) = \mathbb{E}_{s \sim p_{\gamma}^{\pi}(s), a \sim \pi(a \mid s)}\left[ r(s, a) \right]. \label{eq:succ-measure-rl-obj} \end{align}\] <p>This alternative definition implies that maximizing the expected discounted return is equivalent to maximizing the expected reward under the successor measure.</p> <h3 id="goal-conditioned-rl">Goal-conditioned RL</h3> <p>Goal-conditioned RL<d-cite key="kaelbling1993learning,schaul2015universal"></d-cite> is a special problem in RL, where a standard MDP is augmented with a goal space \(\mathcal{G}\) and the reward function \(r_{\text{GC}}: \mathcal{S} \times \mathcal{G} \to \mathbb{R}\) is defined using goals \(g \in \mathcal{G}\) sampled from the goal measure \(p_{\mathcal{G}} \in \Delta(\mathcal{G})\), i.e., a goal-conditioned MDP \(\mathcal{M}_{\text{GCRL}} = (\mathcal{S}, \mathcal{A}, \mathcal{G}, p, r _{\text{GC}}, p_0, \gamma, p _{\mathcal{G}} )\). For the goal space, prior work defined it in various forms. Specifically, the goal space can be the entire state space (e.g., x-y position of the agent or a RGB image)<d-cite key="wang2023optimal,park2024value,nachum2018data,walke2023bridgedata"></d-cite> or a subspace of the state space (e.g., joint positions of a robot arm or x-y positions of a block)<d-cite key="eysenbach2022contrastive,park2025horizon,andrychowicz2017hindsight"></d-cite>. Among the two choices, setting the goal space to be the entire state space, i.e., \(\mathcal{G} = \mathcal{S}\), is both widely used and generic. In the following sections, we will construct a special goal space to convert a standard reward-maximizing MDP into a GCMDP, enabling solving general RL problems using GCRL algorithms.</p> <p>Prior work also defined the goal-conditioned reward function in various forms, including a cost for not reaching the goal \(r_{\text{GC}}(s, g) = - \mathbb{1}(s \neq g)\)<d-cite key="wang2023optimal,park2024value"></d-cite> or a delta measure centered at the goal \(r_{\text{GC}}(s, g) = \delta(s \mid g)\)<d-footnote>The delta measure is an indicator function $\delta(s \mid g) = \mathbb{1}(s = g)$ for discrete GCMDPs and a Dirac delta function $\delta(s \mid g) = \delta_{g}(s)$ for continous GCMDPs.</d-footnote><d-cite key="eysenbach2022contrastive,zheng2023contrastive"></d-cite>. If we choose to define the reward function as the delta measure, using Eq.\(~\ref{eq:succ-measure-rl-obj}\), the GCRL objective for a goal-conditioned policy \(\pi_{\text{GC}}: \mathcal{S} \times \mathcal{G} \to \Delta(\mathcal{A})\) can be written as</p> \[\begin{align} \max_{\pi_{\text{GC}}} J_{\text{GCRL}}(\pi_{\text{GC}}), \quad J_{\text{GCRL}}(\pi_{\text{GC}}) &amp;= \mathbb{E}_{\substack{g \sim p_{\mathcal{G}}(g), \, s \sim p^{\pi_{\text{GC}}}_{\gamma}(s \mid g) }} \left[ \delta(s_t \mid g) \right] = \mathbb{E}_{g \sim p_{\mathcal{G}}(g)} \left[ p^{\pi_{\text{GC}}}_{\gamma}(g \mid g) \right]. \label{eq:gcrl-obj} \end{align}\] <p>Intuitively, this objective indicates that solving GCRL is equivalent to finding the goal-conditioned policy that maximizes the probability measure of reaching desired goals when commanded towards those goals. Note that the GCRL problem is also equivalent to a multi-task RL problem<d-cite key="wilson2007multi,yu2020meta,teh2017distral"></d-cite>, where tasks correspond to reaching different goals.</p> <p>Until now, we have not discussed what happens after the agent reaches the goal. Since GCRL problems typically has infinite horizons and the agent can still move after reaching the goal, the optimal behavior is not only reaching the goal but also staying at the goal as long as possible. We next discuss a special type of GCMDP that simplifies agents’ behaviors after reaching the goal, namely, the stochastic shortest path problem.</p> <h3 id="stochastic-shortest-path">Stochastic shortest path</h3> <p>The stochastic shortest path problem<d-cite key="bertsekas1991analysis,bertsekas1996neuro"></d-cite> \(\mathcal{M}_{\text{SSP}} = (\mathcal{S}, \mathcal{A}, \mathcal{G}, p_\text{SSP}, r_{\text{SSP}}, p_0, p_{\mathcal{G}} )\) is built on top of the GCRL problem, where the environment will terminate the episode once the agent reaches the goal. Many prior GCRL methods actually underpin the underlying algorithm using an SSP problem, and invoke intriguing tools such as a constant cost<d-cite key="park2024ogbench"></d-cite> or the triangle inequality (quasimetric)<d-cite key="wang2023optimal,myers2024learning,park2025transitive"></d-cite> to solve the problem. Since the SSP focuses on agents’ behavior before termination, it requires the policy to hit the goal eventually, i.e., a proper policy.</p> <p align="left"><strong>Definition 1. </strong><i>Given a goal $g \in \mathcal{G}$, a SSP policy $\pi_{\text{SSP}}: \mathcal{S} \times \mathcal{G} \to \Delta(\mathcal{A})$ is proper, if there exists a finite time step $t &lt; \infty$ such that the policy reaches the goal $g$ when commanded towards the same goal $g$, i.e., $p^{\pi_{\text{SSP}}}_t(g \mid g) &gt; 0$.</i></p> <p>Formally, an SSP policy is <em>proper</em> if it will reach the goal with a finite number of time steps:</p> <p>Therefore, a mild assumption is to assume all policies of interest are proper. Using the definition of a proper policy, the SSP problem can be adapted from a GCRL problem with the following modifications:</p> <ol> <li> <p>A negative cost for not reaching the goal, e.g., \(r_{\text{SSP}}(s, g) = - \mathbb{1}(s \neq g)\).</p> </li> <li> <p>A <em>goal-conditioned</em> transition probability measures \(p_{\text{SSP}}: \mathcal{S} \times \mathcal{A} \times \mathcal{G} \to \Delta(\mathcal{S})\): for any transition \((s, a, s')\) and any goal \(g\),</p> \[\begin{align} p_{\text{SSP}}(s' \mid s, a, g) = \begin{cases} p(s' \mid s, a) &amp; \text{if} \, s \neq g \\ \\ \delta(s' \mid g) &amp; \text{otherwise} \end{cases}. \end{align}\] <p>Intuitively, the agent will always stay at the goal after reaching it.</p> </li> <li> <p>Finite horizon \((\gamma = 1)\) with proper policies. For any proper policy, the SSP objective is to minimize the sum of costs (maximize the sum of rewards) before hitting the goal:</p> \[\begin{align} \max_{\pi_\text{SSP}: \: \pi_\text{SSP} \text{ is proper}} J_{\text{SSP}}(\pi_{\text{SSP}}), \quad J_{\text{SSP}}(\pi_{\text{SSP}}) = \mathbb{E}_{\tau \sim \pi_{\text{SSP}}(\tau)} \left[ \sum_{t = 0}^{\infty} r_{\text{SSP}}(s_t, g) \right]. \end{align}\] <p>This objective is finite because of the proper policy assumption.</p> </li> </ol> <p>As indicated by the goal-conditioned transitions, the agent will always stay at the desired goals once it reaches them. Therefore, the optimal behavior is to reach the goal as quickly as possible. We include an example comparing the optimal behaviors of GCRL and SSP in the figure below.</p> <div class="col mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-mdp-to-gcmdp/1d-car-480.webp 480w,/2026/assets/img/2026-04-27-mdp-to-gcmdp/1d-car-800.webp 800w,/2026/assets/img/2026-04-27-mdp-to-gcmdp/1d-car-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-mdp-to-gcmdp/1d-car.png" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Goal-conditioned RL (GCRL) methods aim to not only reach the goal but also stay at the goal as long as possible, while stochastic shortest path (SSP) algorithms are tasked to reach the goal as quickly as possible. </div> <p><span style="color: #e56a1eff;"> Prior work has already shown that (See Sec. 2.3 of Bertsekas and Tsitsiklis (1996)<d-cite key="bertsekas1996neuro"></d-cite> or Theorem 2.21 of Mausam and Kolobov (2012)<d-cite key="kolobov2012planning"></d-cite>) any infinite horizon RL problem can be converted into an SSP problem. </span>This appealing connection between RL and SSP raises the question:</p> <p align="center"><i>Can we convert any RL problem into a GCRL problem?</i></p> <p>We will construct a conversion next.</p> <h2 id="bridging-rl-and-gcrl">Bridging RL and GCRL</h2> <p>The goal of our construction is to preserve policy optimality: the optimal policy in a standard MDP \(\mathcal{M} = (\mathcal{S}, \mathcal{A}, p, r, p_0, \gamma)\) is equivalent to the optimal goal-conditioned policy for reaching some goal in the resulting GCMDP. We start the construction by augmenting the state space and the transition probability measure.</p> <ol> <li> <p>The augmented state space additionally includes two terminal states, a success state \(g_+\) and a failure state \(g_-\): \(\mathcal{S}_{\text{aug}} = \mathcal{S} \cup \{ g_+, g_- \}\).</p> </li> <li> <p>The augmented probability measures of transiting into states in the original state space are proportional to the discout factor; The augmented probability measures of transiting into the two terminal states are proportional to the reward in the original MDP: for any state \(s \in \mathcal{S}_{\text{aug}}\), action \(a \in \mathcal{A}\), and next state \(s' \in \mathcal{S}_{\text{aug}}\),</p> \[\begin{align} p_{\text{aug}}(s' \mid s, a) = \begin{cases} \gamma p(s' \mid s, a) &amp; \text{if } s \in \mathcal{S} \text{ and } s' \in \mathcal{S} \\ (1 - \gamma) r(s, a) &amp; \text{if } s \in \mathcal{S} \text{ and } s' = g_+ \\ (1 - \gamma) (1 - r(s, a)) &amp; \text{if } s \in \mathcal{S} \text{ and } s' = g_- \\ \delta(s' \mid s) &amp; \text{if } s \in \{ g_+, g_- \} \end{cases}. \label{eq:aug-transition-prob-measure} \end{align}\] <details style="background-color: #f4f4f4ff; padding: 15px; border-left: 4px solid #1E88E5; margin: 20px 0;"> <summary>Remarks</summary> Since the range of $p_{\text{aug}}(s' \mid s, a)$ is $[0, \infty)$, for a reward function $\tilde{r}(s, a)$ taking values in $[r_{\min}, r_{\max}]$, we can apply the min-max normalization to transform the range into $[0, 1]$: \begin{align*} r(s, a) = \frac{\tilde{r}(s, a) - r_{\min}}{r_{\max} - r_{\min}}. \end{align*} We can also easily verify that $p_{\text{aug}}(s' \mid s, a)$ is still normalized: for any state $s \in \mathcal{S}_{\text{aug}}$ and action $a \in \mathcal{A}$, \begin{align*} \int_{\mathcal{S}_{\text{aug}}} p(s' \mid s, a) ds' &amp;= 1. \end{align*} </details> </li> </ol> <p>These augmentations allow us to define the goal state as a singleton \(\mathcal{G}_{\text{aug}} = \{ g_+ \}\) with goal prior \(p_{\mathcal{G}_{\text{aug}}}(g) = \delta(g \mid g_+)\), choose the reward function as the delta measure \(r_\text{aug}(s, g) = \delta(s \mid g)\), and introduce a <em>separate</em> discount factor \(\gamma_{\text{aug}} \in [0, 1]\). Taken together, the resulting augmented GCMDP shares the same action space and initial state measure as the original MDP: \(\mathcal{M}_{\text{aug}} = (\mathcal{S}_{\text{aug}}, \mathcal{A}, \mathcal{G}_{\text{aug}}, p_{\text{aug}}, p_0, \gamma_{\text{aug}}, p_{\mathcal{G}_{\text{aug}}})\). However, there is one question remaining:</p> <p align="center"><i>Does the augmented GCMDP preserve policy optimality?</i></p> <p>Fortunately, the answer is <strong>YES</strong> and we provide a formal proof below.</p> <h3 id="harnessing-the-policy-optimality">Harnessing the policy optimality</h3> <p>We start by examining the GCRL objective of the augmented GCMDP. Given that the augmented reward function is a delta measure \(r_\text{aug}(s, g) = \delta(s \mid g)\) and the goal space only contains \(g_+\), the GCRL objective of the augmented GCMDP (Eq.\(~\ref{eq:gcrl-obj}\)) for the goal-conditioned policy \(\pi_{\text{aug}}(a \mid s, g)\) reduces to</p> \[\begin{align} J_{\text{GCRL}}(\pi_{\text{aug}}) = \mathbb{E}_{g \sim \delta(g \mid g_+)}[p^{\pi_{\text{aug}}}_{\gamma_{\text{aug}}}( g \mid g)] = p^{\pi_{\text{aug}}}_{\gamma_{\text{aug}}}( g_+ \mid g_+), \end{align}\] <p>which is the success measure of reaching the single success state<d-cite key="liu2025a,bastankhah2025"></d-cite> under the goal-conditioned policy. To simplify notations, we denote the goal-conditioned policy and the GCRL objective as $\pi_{\text{aug}}(a \mid s) \triangleq \pi_{\text{aug}}(a \mid s, g_+)$ and \(p^{\pi_{\text{aug}}}_{\gamma_{\text{aug}}}(g_+) \triangleq p^{\pi_{\text{aug}}}_{\gamma_{\text{aug}}}( g_+ \mid g_+)\).</p> <p>We are now ready to prove that the optimal goal-conditioned policy \(\pi_{\text{aug}}^{\star}\) in the augmented GCMDP corresponds to the optimal policy \(\pi^{\star}\) in the original MDP. The main idea is to relate the GCRL objective to the RL objective, showing that they are equivalent to each other. We will first introduce the hitting time and then use it to compute the successor measure.</p> <p align="left" id="def:hitting-time"><strong>Definition 2. </strong> Given a goal-conditioned policy $\pi_{\text{aug}}: \mathcal{S} \to \Delta(\mathcal{A})$ and any state $s \in \mathcal{S}_{\text{aug}}$, the hitting time $H^{\pi_{\text{aug}}}(s)$ indicates the first time step that the agent reaches the state $s$, \begin{align} H^{\pi_{\text{aug}}}(s) = \inf \left\{ h \in \mathbb{N}: s_h = s \text{ following } \pi_{\text{aug}} \right\}. \end{align} </p> <p>We can relate the probability mass of the hitting time of the success state $g_+$ to the success measure in the original MDP.</p> <p align="left" id="lemma:hitting-time-prob-mass"><strong>Lemma 1. </strong> <i> Given the goal-conditioned policy $\pi_{\text{aug}}$, the probability mass of the hitting time of the success state $g_+$ satisfies, for any $h \in \mathbb{N}$, \begin{align} p \left( H^{\pi_{\text{aug}}}(g_+) = h \right) = \begin{cases} 0 &amp; h = 0 \\ (1 - \gamma) \gamma^{h - 1} \mathbb{E}_{s \sim p^{\pi_{\text{aug}}}_{h - 1}(s), a \sim \pi_{\text{aug}}(a \mid s) } \left[ r(s, a) \right] &amp; h = 1, 2, \cdots. \end{cases} \end{align} </i> </p> <details style="background-color: #f4f4f4ff; padding: 15px; border-left: 4px solid #1E88E5; margin: 20px 0;"> <summary>Proof of Lemma 1</summary> When $h = 0$, since the augmented GCMDP and the original MDP shares the initial state measure $p_0$, which has non-zero probability measure only within $\mathcal{S}$, it is not possible to reach $g_{+}$ at time step $h = 0$, i.e, $p \left( H^{\pi_{\text{aug}}}(g_+) = 0 \right) = 0$. When $h = 1, 2, \cdots$, we can examine them separately and find a general expression: \begin{align*} p \left( H^{\pi_{\text{aug}}}(g_+) = 1 \right) &amp;= \int_{\mathcal{S} \times \mathcal{A}} p_0(s_0) \pi_{\text{aug}}(a_0 \mid s_0) p_{\text{aug}}( g_+ \mid s_0, a_0) ds_0 da_0 \\ &amp;\overset{(a)}{=} (1 - \gamma) \int_{\mathcal{S} \times \mathcal{A}} p_0(s_0) \pi_{\text{aug}}(a_0 \mid s_0) r(s_0, a_0) ds_0 da_0 \\ &amp;\overset{(b)}{=} (1 - \gamma) \mathbb{E}_{s \sim p^{\pi_{\text{aug}}}_{0}(s), a \sim \pi_{\text{aug}}(a \mid s) } \left[ r(s, a) \right], \\ p \left( H^{\pi_{\text{aug}}}(g_+) = 2 \right) &amp;= \int_{\mathcal{S} \times \mathcal{A} \times \mathcal{S} \times \mathcal{A}} p_0(s_0) \pi_{\text{aug}}(a_0 \mid s_0) p_{\text{aug}}( s_1 \mid s_0, a_0) \pi_{\text{aug}}(a_1 \mid s_1) p_{\text{aug}}( g_+ \mid s_1, a_1) ds_0 da_0 ds_1 d a_1\\ &amp;\overset{(c)}{=} (1 - \gamma) \gamma \int_{\mathcal{S} \times \mathcal{A} \times \mathcal{S} \times \mathcal{A}} p_0(s_0) \pi_{\text{aug}}(a_0 \mid s_0) p(s_1 \mid s_0, a_0) \pi_{\text{aug}}(a_1 \mid s_1) r(s_1, a_1) ds_0 da_0 ds_1 da_1 \\ &amp;\overset{(d)}{=} (1 - \gamma) \gamma \mathbb{E}_{s \sim p^{\pi_{\text{aug}}}_{1}(s), a \sim \pi_{\text{aug}}(a \mid s) } \left[ r(s, a) \right], \\ \cdots \end{align*} where in <i>(a)</i> and <i>(c)</i> we apply the definition of the augmented transition probability measure in Eq.$~\ref{eq:aug-transition-prob-measure}$, and in <i>(b)</i> and <i>(d)</i> we rewrite the intergral using definition of $p^{\pi_{\text{aug}}}_{t}(s)$ in Eq.$~\ref{eq:time-dependent-succ-measure}$. Therefore, we can obtain a general expression for any $h = 1, 2, \cdots$ as \begin{align*} p \left( H^{\pi_{\text{aug}}}(g_+) = h \right) = (1 - \gamma) \gamma^{h - 1} \mathbb{E}_{s \sim p^{\pi_{\text{aug}}}_{h - 1}(s), a \sim \pi_{\text{aug}}(a \mid s) } \left[ r(s, a) \right]. \end{align*} </details> <p>Eventually, using this lemma, we can compute the successor measure of the goal-conditioned policy \(\pi_{\text{aug}}: \mathcal{S} \to \Delta(\mathcal{A})\) in the augmented GCMDP.</p> <p align="left" id="thm:"><strong>Theorem 1. </strong> <i> The successor measure of the goal-conditioned policy $\pi_{\text{aug}}$ at the success state $g_+$ in the augmented GCMDP is equivalent to the RL objective in the original MDP, i.e., \begin{align} p^{\pi_{\text{aug}}}_{\gamma_{\text{aug}}}(g_+) = J_{\text{GCRL}}(\pi_{\text{aug}}) = \frac{(1 - \gamma) \gamma_{\text{aug}} }{1 - \gamma \gamma_{\text{aug}}} J_{\gamma \gamma_{\text{aug}}}(\pi_{\text{aug}}). \end{align} Therefore, when $\gamma_{\text{aug}} = 1$, the augmented GCMDP preserves policy optimality: \begin{align} \pi_{\text{aug}}^{\star} = \text{argmax} \, J_{\text{GCRL}}(\pi_{\text{aug}}) = \text{argmax} \, J_{\gamma}(\pi) = \pi^{\star}. \end{align} </i> </p> <details style="background-color: #f4f4f4ff; padding: 15px; border-left: 4px solid #1E88E5; margin: 20px 0;"> <summary>Proof of Theorem 1</summary> <p> Given the goal-conditioned policy $\pi_{\text{aug}}$, by definition of the successor measure (Eq.$~\ref{eq:succ-measure}$), we have \begin{align*} p^{\pi_{\text{aug}}}_{\gamma_{\text{aug}}}(g_+) &amp;= (1 - \gamma_{\text{aug}}) \sum_{t = 0}^{\infty} \gamma_{\text{aug}}^t p^{\pi_{\text{aug}}}_{t}(g_+) \\ &amp;\overset{(a)}{=} (1 - \gamma_{\text{aug}}) \sum_{t = 0}^{\infty} \gamma_{\text{aug}}^t p \left( H^{\pi_{\text{aug}}}(g_+) \leq t \right) \\ &amp;\overset{(b)}{=} (1 - \gamma_{\text{aug}}) \sum_{t = 0}^{\infty} \gamma_{\text{aug}}^t \cdot \sum_{h = 0}^{t} p \left( H^{\pi_{\text{aug}}}(g_+) = h \right) \\ &amp;\overset{(c)}{=} (1 - \gamma_{\text{aug}}) \sum_{h = 0}^{\infty} p \left( H^{\pi_{\text{aug}}}(g_+) = h \right) \cdot \sum_{t = h}^{\infty} \gamma^t_{\text{aug}} \\ &amp;= \sum_{h = 0}^{\infty} \gamma_{\text{aug}}^h p \left( H^{\pi_{\text{aug}}}(g_+) = h \right) \\ &amp;\overset{(d)}{=} \sum_{h = 1}^{\infty} \gamma_{\text{aug}}^h \left( (1 - \gamma) \gamma^{h - 1} \mathbb{E}_{s \sim p^{\pi_{\text{aug}}}_{h - 1}(s), a \sim \pi_{\text{aug}}(a \mid s) } \left[ r(s, a) \right] \right) \\ &amp;= (1 - \gamma) \gamma_{\text{aug}} \sum_{h = 1}^{\infty} (\gamma \gamma_{\text{aug}})^{h - 1} \mathbb{E}_{s \sim p^{\pi_{\text{aug}}}_{h - 1}(s), a \sim \pi_{\text{aug}}(a \mid s) } \left[ r(s, a) \right] \\ &amp;\overset{(e)}{=} \frac{(1 - \gamma) \gamma_{\text{aug}}}{1 - \gamma \gamma_{\text{aug}}} \cdot (1 - \gamma \gamma_{\text{aug}}) \sum_{h = 0}^{\infty} (\gamma \gamma_{\text{aug}})^{h - 1} \mathbb{E}_{s \sim p^{\pi_{\text{aug}}}_{h}(s), a \sim \pi_{\text{aug}}(a \mid s) } \left[ r(s, a) \right] \\ &amp;\overset{(f)}{=} \frac{(1 - \gamma) \gamma_{\text{aug}} }{1 - \gamma \gamma_{\text{aug}}} \mathbb{E}_{s \sim p_{\gamma \gamma_{\text{aug}}}^{\pi_{\text{aug}}}(s), a \sim \pi_{\text{aug}}(a \mid s)}\left[ r(s, a) \right] \\ &amp;= \frac{(1 - \gamma) \gamma_{\text{aug}} }{1 - \gamma \gamma_{\text{aug}}} J_{\gamma \gamma_{\text{aug}}}(\pi_{\text{aug}}) \end{align*} where in <i>(a)</i> we apply the observation that reaching the success state $g_+$ at exact time step $t$ in the augmented GCMDP suggests that the hitting time must be less than $t$, in <i>(b)</i> we use the additive axiom of probability for disjoint events, in <i>(c)</i> we swap the sum over time step $t$ and the sum over hitting time $h$ by grouping the terms using differet hitting time, in <i>(d)</i> we plug in the definition of the hitting time (<a href="#lemma:hitting-time-prob-mass">Lemma 1</a>), in <i>(e)</i> we apply change of variable, and in <i>(f)</i> we plug in the definition of the successor measure (Eq.$~\ref{eq:succ-measure}$). </p> <p> Therefore, when setting $\gamma_{\text{aug}} = 1$, we have $p^{\pi_{\text{aug}}}_{\gamma_{\text{aug}} = 1}(g_+) = J_{\gamma}(\pi_{\text{aug}})$. Thus, maximizing the successor measure in the augmented GCMDP is equivalent to maximizing the RL objective in the original MDP, i.e., \begin{align*} \pi_{\text{aug}}^{\star} = \text{argmax} \, J_{\text{GCRL}}(\pi_{\text{aug}}) = \text{argmax} \, J_{\gamma}(\pi) = \pi^{\star}. \end{align*} </p> </details> <p><span style="color: #e56a1eff;"> <strong style="color: #e56a1eff;">Remark.</strong> When setting \(\gamma_{\text{aug}} = 1\), one intriguing observation is that the augmented GCMDP is also equivalent to a SSP problem. </span></p> <p>We next mention some practical caveats of our conversion from an RL problem to a GCRL problem.</p> <h3 id="caveats">Caveats</h3> <p>Hindsight relabeling<d-cite key="andrychowicz2017hindsight"></d-cite> is a widely used technique for solving GCRL and SSP problems. This technique builds upon the intuition that learning to reach some close (easy) goals helps the agent reach far away (difficult) goals later, resulting in automatic curriculum learning. However, using GCRL algorithms with hindsight relabeling to solve our augmented GCMDP might not speed up learning because the augmented states $g_+$ and $g_-$ are not in the original state space.</p> <p>In practice, solving the augmented GCMDP might not be easier than solving the original MDP directly (e.g., using TD learning methods). The reasons are twofold. First, the main component of our construction is to augment the transition probability measure using the reward function in the original MDP. After augmentation, the rewards go into the stochasticity of the transition, resulting in much higher variance when interacting with the environment. Second, for dense rewards that provide supervision for RL algorithms at each time step, those supervisions have been deferred into the two additional states $g_+$ and $g_-$ (a sparse reward problem), inducing much more challenging exploration.</p> <h2 id="does-the-conversion-work-in-practice">Does the conversion work in practice?</h2> <div class="col mt-3"> <video autoplay="" loop="" muted="" playsinline="" class="img-fluid rounded" preload="metadata"> <source src="/2026/assets/img/2026-04-27-mdp-to-gcmdp/cliff_walking.mp4" type="video/mp4"></source> Your browser doesn’t support the video tag. </video> </div> <div class="caption"> Cliff Walking involves crossing a gridworld from a random start (stool) to the goal (cookie) while avoiding falling off a cliff. </div> <p>To study whether we can use GCRL algorithms to solve the augmented GCMDP, we conduct experiments in a canonical discrete MDP called Cliff Walking. This MDP is adapted from Example 6.6 from Sutton and Barto (1998)<d-cite key="sutton1998reinforcement"></d-cite> and requires the agent to navigate from a random start to the goal while avoiding falling off a cliff. The state space covers $48$ discrete states in the gridworld and the action space contains $4$ actions: left, right, up, and down. The agents receive a reward of $0$ when staying at the goal, a reward of $-1$ when not reaching the goal, and a reward of $-100$ when stepping into the cliff. We construct the augmented GCMDP by <em>(1)</em> normalizing the rewards into \([0, 1]\), <em>(2)</em> augmenting the state space with the success state $g_+$ and the failure state $g_-$, and <em>(3)</em> modifying the reward function as in Eq.$~\ref{eq:aug-transition-prob-measure}$.</p> <p>We select four different state-of-the-art GCRL and SSP algorithms to solve the augmented GCMDP, comparing against the standard Q-learning algorithm in the original MDP.</p> <ol> <li> <p>GCRL is the goal-conditioned variant of Q-learning with the indicator reward function \(r_{\text{aug}}(s, g) = \delta(s \mid g) = \mathbb{1}(s = g)\).</p> </li> <li> <p>GCQL w/ HER applies the hindsight relabeling<d-cite key="andrychowicz2017hindsight"></d-cite> on top of GCQL.</p> </li> <li> <p>CRL<d-cite key="eysenbach2022contrastive"></d-cite> is a representative GCRL algorithm that reframes the modeling of the success measure as a contrastive learning (classification) problem.</p> </li> <li> <p>QRL<d-cite key="wang2023optimal"></d-cite> is a representative SSP algorithm that finds goal-conditioned shortest distance (a quasimetric) using the triangle inequality. In particular, this method was developed for deterministic MDPs but also works well for stochastic MDPs in practice.</p> </li> </ol> <p>To prevent confounding errors from data collection and speed up learning, we collect a dataset with \(100\text{K}\) transitions in the original MDP for training Q-learning, and also collect another dataset with \(100\text{K}\) transitions in the GCMDP for training the aforementioned GCRL and SSP algorithms. For evaluation, we compare the average success rates for reaching the goal in the original MDP over \(100\) trajectories, reporting means and standard deviations over $4$ random seeds.</p> <div class="col mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-mdp-to-gcmdp/cliff_walking_aug_gcmdp.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-mdp-to-gcmdp/cliff_walking_aug_gcmdp.svg" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Q-learning quickly solves Cliff Walking by learning the original MDP, while GCRL and SSP methods struggle to match its performance by learning the augmented GCMDP. We evaluate the success rate of all methods in the original MDP for fair comparisons. </div> <p>The results in the figure above suggest that Q-learning quickly converges to $100\%$ success rate, while GCRL and SSP methods struggle to match its performance. Although CRL and QRL typically estimate a dense success measure or a dense distance function, they still suffer from the high variance in environmental transitions. This observation is consistent with the caveat that solving the augmented GCMDP is not necessarily easier than solving the original MDP, due to high transition variance and challenging exploration. We also observe that GCQL achieves a success rate similar to its variant with HER, indicating that hindsight relabeling might not speed up learning since $g_+$ and $g_-$ are not in the original state space.</p> <details style="background-color: #f4f4f4ff; padding: 15px; border-left: 4px solid #1E88E5; margin: 20px 0;"> <summary>Additional online experiments</summary> <div class="col mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-mdp-to-gcmdp/riverswim.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-mdp-to-gcmdp/riverswim.svg" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> River Swim involves swimming across a river by choosing to move right at each state. The exploration is challenging due to stochstic transitions. </div> Additionally, we conduct experiments in the River Swim MDP<d-cite key="strehl2008analysis"></d-cite>, which requires exploration to reach the end of the river (linear chain of states) by choosing to move right at each state. The agent receives a reward of $1.0$ in the rightmost state and receives a small distractor reward of $0.005$ if it moves left at any other state. To avoid policy initialization bias, we randomize which action moves left versus right at each state. We compare the standard Q-learning and PPO<d-cite key="schulman2017proximal"></d-cite> algorithms in the original MDP against the GCQL in the augmented GCMDP, measuring the success rates in the augmented GCMDP. We also plot the success rate of an oracle agent that always goes right. Results are shown in the figure below. <div class="col mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-mdp-to-gcmdp/riverswim_results.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-mdp-to-gcmdp/riverswim_results.svg" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Success rates of GCQL, Q-learning, PPO, and the oracle in the augmented River Swim environment. We show the mean and standard deviations over 5 random seeds. </div> We observe that for shorter river lengths and shorter horizons, Q-learning converges faster and achieves higher returns than GCQL. However, as river length and horizon increase, GCQL achieves similar or higher returns than Q-learning. These results suggest that, while GCQL is not sample efficient for short-horizon exploration tasks, the GCMDP formulation enables improved exploration in longer-horizon exploration tasks. </details> <h2 id="closing-remarks">Closing remarks</h2> <p>In this blog post, we share new ideas about converting a standard RL problem into a GCRL problem. These ideas are motivated by prior connections between the RL and the SSP and, in turn, complementarily bridge all three building blocks. Although our preliminary experiments do not show a positive sign for using GCRL algorithms to solve <em>any</em> reward-maximization RL problems in practice, we believe our constructions are still of interest to the broader community and raise many intriguing research directions.</p> <h3 id="open-questions">Open questions</h3> <ul> <li> <p>Prior work has used unique properties for the (deterministic) shortest path problem, such as the triangle inequality<d-cite key="wang2023optimal"></d-cite> and divide-and-conquer strategy<d-cite key="park2025transitive"></d-cite>, to tackle GCRL tasks, resulting in significant improvements. Extending these tools to the stochastic shortest path problem and, eventually, to any RL problem is an important step towards <a href="https://bair.berkeley.edu/blog/2025/11/01/rl-without-td-learning/" rel="external nofollow noopener" target="_blank">RL without TD learning</a>.</p> </li> <li> <p>Another key question is how to make our conversions practical? Perhaps there are alternative constructions that convert an MDP into a GCMDP, enabling efficient application of modern GCRL algorithms. Perhaps we need to develop powerful GCRL algorithms to harness these conversions.</p> </li> <li> <p>Importantly, our conversions also bring up an open-ended question: After all, RL, SSP, and GCRL are different frameworks describing some mechanisms (blinds). So what are the underlying physical rules that actually govern the world (the elephant)?</p> </li> </ul> <p>Answering these questions not only introduces new interpretations to the RL problems but also motivates the development of efficient and scalable RL algorithms. Taken together, we are excited to see more progress towards general-purpose reinforcement learning agents that can actually understand the world.</p> <hr> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-mdp-to-gcmdp.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-vlms-waste-their-vision/">Why vlms waste their vision</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>