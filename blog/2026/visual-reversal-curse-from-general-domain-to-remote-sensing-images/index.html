<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Visual Reversal Curse: From General Domain to Remote Sensing Images | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="The 'Reversal Curse' highlights a fundamental limitation in AI: models often fail to infer inverse relationships. This post investigates whether this curse extends to Vision Foundation Models and proposes remote sensing image translation as the optimal testbed for evaluating bidirectional visual generalization."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Visual Reversal Curse: From General Domain to Remote Sensing Images",
            "description": "The 'Reversal Curse' highlights a fundamental limitation in AI: models often fail to infer inverse relationships. This post investigates whether this curse extends to Vision Foundation Models and proposes remote sensing image translation as the optimal testbed for evaluating bidirectional visual generalization.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Visual Reversal Curse: From General Domain to Remote Sensing Images</h1> <p>The 'Reversal Curse' highlights a fundamental limitation in AI: models often fail to infer inverse relationships. This post investigates whether this curse extends to Vision Foundation Models and proposes remote sensing image translation as the optimal testbed for evaluating bidirectional visual generalization.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#the-reversal-curse-a-logic-failure">The Reversal Curse: A Logic Failure</a> </div> <div> <a href="#from-language-to-vision">From Language to Vision</a> </div> <div> <a href="#the-problem-with-synthetic-benchmarks">The Problem with Synthetic Benchmarks</a> </div> <div> <a href="#remote-sensing-a-physically-grounded-testbed">Remote Sensing: A Physically Grounded Testbed</a> </div> <div> <a href="#testing-in-context-visual-inversion">Testing In-Context Visual Inversion</a> </div> </nav> </d-contents> <h2 id="the-reversal-curse-a-logic-failure">The Reversal Curse: A Logic Failure</h2> <div style="border: 2px solid #d32f2f; background-color: #ffebee; padding: 15px; border-radius: 5px; margin: 15px 0;"> <strong>Takeaway:</strong> The "Reversal Curse" exposes a critical flaw in modern AI: learning "A is B" does not guarantee that a model understands "B is A". This logical gap challenges the assumption that current architectures learn robust, invertible representations of the world. </div> <p>Recent findings in Large Language Models (LLMs) have unveiled a puzzling phenomenon: the <strong>Reversal Curse</strong>. Berglund et al. <d-cite key="berglundReversalCurseLLMs2024"></d-cite> demonstrated that models trained on factual statements like “Olaf Scholz was the ninth Chancellor of Germany” frequently fail to answer the inverse question, “Who was the ninth Chancellor of Germany?”. Further studies have delved deeper into the limits of this generalization <d-cite key="linDelvingReversalCurse2024"></d-cite>, investigating whether models merely mimic understanding (“Potemkin Understanding”) rather than building a coherent world model <d-cite key="mancoridisPotemkinUnderstandingLarge2025,vafaEvaluatingWorldModel2024,vafaWhatHasFoundation2025"></d-cite>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-visual-reversal-curse-from-general-domain-to-remote-sensing-images/reversal_curse_gpt4_example-480.webp 480w,/2026/assets/img/2026-04-27-visual-reversal-curse-from-general-domain-to-remote-sensing-images/reversal_curse_gpt4_example-800.webp 800w,/2026/assets/img/2026-04-27-visual-reversal-curse-from-general-domain-to-remote-sensing-images/reversal_curse_gpt4_example-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-visual-reversal-curse-from-general-domain-to-remote-sensing-images/reversal_curse_gpt4_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Inconsistent knowledge in GPT-4. GPT-4 correctly gives the name of Tom Cruise’s mother (left). Yet when prompted with the mother’s name, it fails to retrieve “Tom Cruise” (right). We hypothesize this ordering effect is due to the Reversal Curse. Models trained on “A is B” (e.g. “Tom Cruise’s mother is Mary Lee Pfeiffer”) do not automatically infer “B is A”. <d-cite key="berglundReversalCurseLLMs2024"></d-cite> </div> <p>This is not merely a failure of memory, but a breakdown in <strong>logical generalization</strong>. If a model truly understood the relationship, the direction of inquiry shouldn’t matter. Research suggests this stems from fundamental architectural and training constraints:</p> <ul> <li> <strong>Factorization Bias</strong> <d-cite key="kitouniFactorizationCurseWhich2024"></d-cite>: The left-to-right prediction objective biases the model against reverse inference.</li> <li> <strong>Binding Failures</strong> <d-cite key="wangReversalCurseBinding2025"></d-cite>: Transformers struggle to robustly associate entities bidirectionally.</li> <li> <strong>Training Dynamics</strong> <d-cite key="zhuTheoreticalUnderstandingReversal2024"></d-cite>: Standard gradient descent does not naturally encourage invertible mappings without explicit bidirectional data.</li> </ul> <p>While this phenomenon is well-documented in text, a critical question remains: <strong>Does the Reversal Curse extend to vision?</strong> If a vision model learns to map a Sketch ($A$) to a Photo ($B$), can it instinctively infer how to map the Photo back to a Sketch?</p> <h2 id="from-language-to-vision">From Language to Vision</h2> <div style="border: 2px solid #ff9800; background-color: #fff3e0; padding: 15px; border-radius: 5px; margin: 15px 0;"> <strong>Takeaway:</strong> Image-to-image translation is the visual equivalent of the "A is B" relationship. However, visual inversion is complex, requiring the model to understand structural, textural, and illumination transformations rather than just semantic links. </div> <p>In computer vision, <strong>image-to-image translation</strong> <d-cite key="isolaImageToImageTranslationConditional2017,zhuUnpairedImageToImageTranslation2017"></d-cite> serves as the closest parallel to semantic inversion. Classic tasks include:</p> <ul> <li> <strong>Segmentation $\leftrightarrow$ Image</strong>: Converting labels to realistic scenes <d-cite key="leeMaskGANDiverseInteractive2020"></d-cite>.</li> <li> <strong>Sketch $\leftrightarrow$ Photo</strong>: hallucinating texture from outlines.</li> <li> <strong>Map $\leftrightarrow$ Aerial</strong>: translating abstract cartography to satellite views.</li> </ul> <div style="border: 2px dashed #999; padding: 20px; text-align: center; color: #555; margin: 20px 0; background-color: #f9f9f9;"> <strong>[Figure Placeholder]</strong><br> Suggested: <em>Conceptual Diagram of Visual Reversal</em><br> Show a comparison between Text Reversal (Name $\leftrightarrow$ Description) and Visual Reversal (Sketch $\leftrightarrow$ Photo). This visually explains the analogy being made. </div> <p>While specialized models like CycleGAN or BBDM <d-cite key="Li_2023_CVPR,xueBiBBDMBidirectionalImage2025"> handle these tasks effectively, and diffusion-based bridge models have further refined this <d-cite key="chungDirectDiffusionBridge2023,liuI$^2$SBImagetoImageSchrodinger2023,zhouDenoisingDiffusionBridge2024"></d-cite>, they typically rely on **explicit bidirectional training** (seeing both $A \to B$ and $B \to A$) or cycle-consistency losses. The deeper question concerns **generalization**: Can a Vision Foundation Model that has primarily seen forward generations (e.g., conditioning on a sketch to generate an image) zero-shot the inverse task?</d-cite></p> <h2 id="the-problem-with-synthetic-benchmarks">The Problem with Synthetic Benchmarks</h2> <div style="border: 2px solid #ff9800; background-color: #fff3e0; padding: 15px; border-radius: 5px; margin: 15px 0;"> <strong>Takeaway:</strong> Standard benchmarks often rely on man-made or algorithmically generated features (sketches, edges) which create information asymmetry. This makes them poor candidates for testing true reasoning, as failure may stem from information loss rather than a lack of understanding. </div> <p>Current evaluation datasets often utilize <strong>man-made intermediate representations</strong> which are inherently lossy:</p> <ul> <li> <strong>Sketches/Edges</strong>: Often abstract away texture, lighting, and depth.</li> <li> <strong>Segmentation Maps</strong>: Reduce complex objects to uniform class labels.</li> </ul> <div style="border: 2px dashed #999; padding: 20px; text-align: center; color: #555; margin: 20px 0; background-color: #f9f9f9;"> <strong>[Figure Placeholder]</strong><br> Suggested: <em>Asymmetry of Synthetic Data</em><br> Illustrate how Photo $\to$ Edge is deterministic/lossy, while Edge $\to$ Photo requires generative hallucination. This highlights why synthetic benchmarks are unfair for testing logic. </div> <p>These mappings are <strong>asymmetric</strong>. Generating a photo from a sketch requires “hallucinating” missing information, while generating a sketch from a photo involves “discarding” information (edge detection). If a model fails to invert $Sketch \to Photo$, it might simply be because the inverse task ($Photo \to Sketch$) is a fundamentally different algorithmic process (feature extraction) rather than a logical inversion. To test the Reversal Curse fairly, we need a domain where both directions are non-trivial and information-rich.</p> <h2 id="remote-sensing-a-physically-grounded-testbed">Remote Sensing: A Physically Grounded Testbed</h2> <div style="border: 2px solid #ff9800; background-color: #fff3e0; padding: 15px; border-radius: 5px; margin: 15px 0;"> <strong>Takeaway:</strong> Remote sensing offers native, high-fidelity multimodal pairs (Optical $\leftrightarrow$ SAR). These modalities represent the same physical reality through different sensors, providing a rigorous testbed for bidirectional translation without the artifacts of synthetic data. </div> <p>Remote sensing provides the ideal experimental ground with <strong>native multimodal data</strong>. Unlike general computer vision datasets where one modality is often man-made (e.g., edge maps, segmentation labels), earth observation satellites capture aligned views of the same location using distinct physical mechanisms. Common sources include <strong>Sentinel-1 (SAR)</strong>, <strong>Sentinel-2 (Optical)</strong>, and high-resolution commercial imagery.</p> <p>This data is natively multi-spectral, often containing bands beyond the traditional RGB channels (e.g., Near-Infrared, Short-Wave Infrared). For a specific geolocation and timestamp, we can obtain multiple image instances—Optical, SAR, multi-spectral, or even hyperspectral—each capturing different physical attributes of the same underlying object.</p> <div style="border: 2px dashed #999; padding: 20px; text-align: center; color: #555; margin: 20px 0; background-color: #f9f9f9;"> <strong>[Figure Placeholder]</strong><br> Suggested: <em>Optical vs. SAR Examples</em><br> Show a side-by-side comparison of the same location in Optical (RGB) and SAR. Highlight features visible in one but different in the other (e.g., metal structures, water bodies). </div> <p>This creates an optimal setup for image-to-image translation that reflects real-world physical complexity. The modalities are complementary: different bands and spectrums represent distinct information channels about the same scene. Intuitively, yielding optimal translation results from one modality to another is a non-trivial task that requires understanding the underlying physical properties, not just surface-level statistics.</p> <p>Datasets such as <strong>OpenEarthMap-SAR</strong> <d-cite key="xiaOpenEarthMapSARBenchmarkSynthetic2025"></d-cite>, <strong>SARLANG-1M</strong> <d-cite key="weiSARLANG1MBenchmarkVisionLanguage2025"></d-cite>, <strong>EarthView</strong> <d-cite key="Velazquez_2025_WACV"></d-cite>, <strong>TerraFM</strong> <d-cite key="danishTerraFMScalableFoundation2025"></d-cite>, <strong>SAR-TEXT</strong> <d-cite key="heSARTEXTLargeScaleSAR2025"></d-cite>, and <strong>MMEarth</strong> <d-cite key="nedungadiMMEarthExploringMultimodal2024"></d-cite> provide these naturally aligned pairs. Moreover, recent foundation models like <strong>Prithvi-EO-2.0</strong> <d-cite key="szwarcmanPrithviEO20VersatileMultiTemporal2025"></d-cite>, <strong>TerraMesh</strong> <d-cite key="blumenstielTerraMeshPlanetaryMosaic2025"></d-cite>, <strong>Galileo</strong> <d-cite key="tsengGalileoLearningGlobal2025"></d-cite>, and others <d-cite key="wangUnifiedCopernicusFoundation2025"></d-cite> offer a rigorous testbed for investigating visual reasoning.</p> <h2 id="testing-in-context-visual-inversion">Testing In-Context Visual Inversion</h2> <div style="border: 2px solid #ff9800; background-color: #fff3e0; padding: 15px; border-radius: 5px; margin: 15px 0;"> <strong>Takeaway:</strong> We propose an in-context learning evaluation where models are shown forward examples (Optical $\to$ SAR) and tested on the inverse (SAR $\to$ Optical). This setup isolates the model's ability to generalize a learned transformation rule in reverse. </div> <p>We propose a rigorous evaluation protocol leveraging the in-context learning capabilities of modern <strong>Vision Foundation Models</strong>. We examine unified understanding and generation models like <strong>OmniGen</strong> <d-cite key="xiaoOmniGenUnifiedImage2025,wuOmniGen2ExplorationAdvanced2025"></d-cite>, <strong>UniWorld</strong> <d-cite key="linUniWorldV1HighResolutionSemantic2025"></d-cite>, and <strong>UniReal</strong> <d-cite key="chenUniRealUniversalImage2025"></d-cite>, as well as emerging unified architectures <d-cite key="dengEmergingPropertiesUnified2025"></d-cite>. We also evaluate commercial and open-source editing models such as <strong>GPT-Image-1</strong> <d-cite key="openaiGPTImage1"></d-cite>, <strong>Qwen-Image</strong> <d-cite key="wuQwenImageTechnicalReport2025"></d-cite>, <strong>Nano Banana Pro</strong> <d-cite key="googleNanoBananaPro"></d-cite>, and <strong>Flux</strong> (e.g., Flux.1 Kontext <d-cite key="labsFLUX1KontextFlow2025"></d-cite>). Furthermore, frameworks like <strong>Step1X-Edit</strong> <d-cite key="liuStep1XEditPracticalFramework2025"></d-cite> and <strong>In-Context Edit</strong> <d-cite key="zhangInContextEditEnabling2025"></d-cite> enable us to define these tasks via prompting.</p> <p>Our protocol formulates the inversion problem as a <strong>zero-shot or one-shot in-context learning task</strong>:</p> <ol> <li> <strong>Context</strong>: We present the model with a source image (e.g., Optical Image A) and its corresponding target image (SAR Image A).</li> <li> <strong>Prompting</strong>: We instruct the model implicitly: “Here is an input image [Optical A] and its result [SAR A]. Now, given this new result image [SAR B], generate the original input image.”</li> <li> <strong>Constraint</strong>: Crucially, we do <strong>not</strong> explicitly explain the physical transformation (i.e., “convert optical to SAR”). We rely on the visual cues alone. The model must infer the transformation rule from the context pair and then apply its inverse to the test image.</li> </ol> <div style="border: 2px dashed #999; padding: 20px; text-align: center; color: #555; margin: 20px 0; background-color: #f9f9f9;"> <strong>[Figure Placeholder]</strong><br> Suggested: <em>Experiment Flowchart</em><br> Visualizing the prompt structure: [Context: Optical_1-&gt;SAR_1, ..., Optical_k-&gt;SAR_k] + [Query: SAR_test] -&gt; [Output: ?]. Contrast this with the training direction. </div> <p>We hypothesize that even powerful generative models may struggle with this inversion if they have not explicitly learned the bidirectional physics of these sensors. This mirrors the LLM Reversal Curse and would suggest that current “world models” are more akin to <strong>associative engines</strong> than true simulators of physical reality.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-visual-reversal-curse-from-general-domain-to-remote-sensing-images.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-long-context/">Text-as-Image, A Visual Encoding Approach for Long-Context Understanding</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/using-large-language-models-to-simulate-and-predict-human-decision-making/">Using Large Language Models to Simulate and Predict Human Decision-Making</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/useful-calibrated-uncertainties/">What (and What Not) are Calibrated Uncertainties Actually Useful for?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/unlearning-or-untraining/">Is your algorithm Unlearning or Untraining?</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>