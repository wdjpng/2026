<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Visual Reversal Curse: From General Domain to Remote Sensing Images | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="The 'Reversal Curse' highlights a fundamental limitation in AI: models often fail to infer inverse relationships. This post investigates whether this curse extends to Vision Foundation Models and proposes remote sensing image translation as the optimal testbed for evaluating bidirectional visual generalization."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Visual Reversal Curse: From General Domain to Remote Sensing Images",
            "description": "The 'Reversal Curse' highlights a fundamental limitation in AI: models often fail to infer inverse relationships. This post investigates whether this curse extends to Vision Foundation Models and proposes remote sensing image translation as the optimal testbed for evaluating bidirectional visual generalization.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Visual Reversal Curse: From General Domain to Remote Sensing Images</h1> <p>The 'Reversal Curse' highlights a fundamental limitation in AI: models often fail to infer inverse relationships. This post investigates whether this curse extends to Vision Foundation Models and proposes remote sensing image translation as the optimal testbed for evaluating bidirectional visual generalization.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#the-reversal-curse-a-logic-failure">The Reversal Curse: A Logic Failure</a> </div> <div> <a href="#from-language-to-vision">From Language to Vision</a> </div> <div> <a href="#the-problem-with-synthetic-benchmarks">The Problem with Synthetic Benchmarks</a> </div> <div> <a href="#remote-sensing-a-physically-grounded-testbed">Remote Sensing: A Physically Grounded Testbed</a> </div> <div> <a href="#testing-in-context-visual-inversion">Testing In-Context Visual Inversion</a> </div> </nav> </d-contents> <h2 id="the-reversal-curse-a-logic-failure">The Reversal Curse: A Logic Failure</h2> <div style="border: 2px solid #d32f2f; background-color: #ffebee; padding: 15px; border-radius: 5px; margin: 15px 0;"> <strong>Takeaway:</strong> The "Reversal Curse" exposes a critical flaw in modern AI: learning "A is B" does not guarantee that a model understands "B is A". This logical gap challenges the assumption that current architectures learn robust, invertible representations of the world. </div> <p>Recent findings in Large Language Models (LLMs) have unveiled a puzzling phenomenon: the <strong>Reversal Curse</strong>. Berglund et al. <d-cite key="berglundReversalCurseLLMs2024"></d-cite> demonstrated that models trained on factual statements like “Olaf Scholz was the ninth Chancellor of Germany” frequently fail to answer the inverse question, “Who was the ninth Chancellor of Germany?”.</p> <p>This is not merely a failure of memory, but a breakdown in <strong>logical generalization</strong>. If a model truly understood the relationship, the direction of inquiry shouldn’t matter. Research suggests this stems from fundamental architectural and training constraints:</p> <ul> <li> <strong>Factorization Bias</strong> <d-cite key="kitouniFactorizationCurseWhich2024"></d-cite>: The left-to-right prediction objective biases the model against reverse inference.</li> <li> <strong>Binding Failures</strong> <d-cite key="wangReversalCurseBinding2025"></d-cite>: Transformers struggle to robustly associate entities bidirectionally.</li> <li> <strong>Training Dynamics</strong> <d-cite key="zhuTheoreticalUnderstandingReversal2024"></d-cite>: Standard gradient descent does not naturally encourage invertible mappings without explicit bidirectional data.</li> </ul> <p>While this phenomenon is well-documented in text, a critical question remains: <strong>Does the Reversal Curse extend to vision?</strong> If a vision model learns to map a Sketch ($A$) to a Photo ($B$), can it instinctively infer how to map the Photo back to a Sketch?</p> <h2 id="from-language-to-vision">From Language to Vision</h2> <div style="border: 2px solid #ff9800; background-color: #fff3e0; padding: 15px; border-radius: 5px; margin: 15px 0;"> <strong>Takeaway:</strong> Image-to-image translation is the visual equivalent of the "A is B" relationship. However, visual inversion is complex, requiring the model to understand structural, textural, and illumination transformations rather than just semantic links. </div> <p>In computer vision, <strong>image-to-image translation</strong> <d-cite key="isolaImageToImageTranslation2017,zhuUnpairedImageToImage2017"></d-cite> serves as the closest parallel to semantic inversion. Classic tasks include:</p> <ul> <li> <strong>Segmentation $\leftrightarrow$ Image</strong>: Converting labels to realistic scenes.</li> <li> <strong>Sketch $\leftrightarrow$ Photo</strong>: hallucinating texture from outlines.</li> <li> <strong>Map $\leftrightarrow$ Aerial</strong>: translating abstract cartography to satellite views.</li> </ul> <p>While specialized models like CycleGAN or BBDM <d-cite key="liBBDMImagetoimageTranslation2023,xueBiBBDMBidirectionalImage2025"> handle these tasks effectively, they typically rely on **explicit bidirectional training** (seeing both $A \to B$ and $B \to A$) or cycle-consistency losses. The deeper question concerns **generalization**: Can a Vision Foundation Model that has primarily seen forward generations (e.g., conditioning on a sketch to generate an image) zero-shot the inverse task?</d-cite></p> <h2 id="the-problem-with-synthetic-benchmarks">The Problem with Synthetic Benchmarks</h2> <div style="border: 2px solid #ff9800; background-color: #fff3e0; padding: 15px; border-radius: 5px; margin: 15px 0;"> <strong>Takeaway:</strong> Standard benchmarks often rely on man-made or algorithmically generated features (sketches, edges) which create information asymmetry. This makes them poor candidates for testing true reasoning, as failure may stem from information loss rather than a lack of understanding. </div> <p>Current evaluation datasets often utilize <strong>man-made intermediate representations</strong> which are inherently lossy:</p> <ul> <li> <strong>Sketches/Edges</strong>: Often abstract away texture, lighting, and depth.</li> <li> <strong>Segmentation Maps</strong>: Reduce complex objects to uniform class labels.</li> </ul> <p>These mappings are <strong>asymmetric</strong>. Generating a photo from a sketch requires “hallucinating” missing information, while generating a sketch from a photo involves “discarding” information (edge detection). If a model fails to invert $Sketch \to Photo$, it might simply be because the inverse task ($Photo \to Sketch$) is a fundamentally different algorithmic process (feature extraction) rather than a logical inversion. To test the Reversal Curse fairly, we need a domain where both directions are non-trivial and information-rich.</p> <h2 id="remote-sensing-a-physically-grounded-testbed">Remote Sensing: A Physically Grounded Testbed</h2> <div style="border: 2px solid #ff9800; background-color: #fff3e0; padding: 15px; border-radius: 5px; margin: 15px 0;"> <strong>Takeaway:</strong> Remote sensing offers native, high-fidelity multimodal pairs (Optical $\leftrightarrow$ SAR). These modalities represent the same physical reality through different sensors, providing a rigorous testbed for bidirectional translation without the artifacts of synthetic data. </div> <p>Remote sensing provides the ideal experimental ground with <strong>native multimodal data</strong>. Satellites capture aligned views of the same location using distinct physical mechanisms:</p> <ol> <li> <strong>Optical Sensors (RGB)</strong>: Passive sensing, similar to human vision, dependent on sunlight and cloud-free conditions.</li> <li> <strong>Synthetic Aperture Radar (SAR)</strong>: Active sensing that penetrates clouds and captures surface roughness and dielectric properties.</li> </ol> <p>Datasets such as <strong>OpenEarthMap-SAR</strong> <d-cite key="xiaOpenEarthMapSARBenchmark2025"></d-cite>, <strong>SARLANG-1M</strong> <d-cite key="weiSARLANG1MBenchmark2025"></d-cite>, and <strong>EarthView</strong> <d-cite key="velazquezEarthViewLargeScale2025"></d-cite> provide these naturally aligned pairs. The relationship here is <strong>physically coupled</strong>. A building reflects light (Optical) and backscatters radar waves (SAR) based on its geometry and material. If a model truly understands the concept of “building” in a physical world sense, it should theoretically map between these representations. Failure to do so implies a limitation in <strong>visual reasoning</strong>.</p> <h2 id="testing-in-context-visual-inversion">Testing In-Context Visual Inversion</h2> <div style="border: 2px solid #ff9800; background-color: #fff3e0; padding: 15px; border-radius: 5px; margin: 15px 0;"> <strong>Takeaway:</strong> We propose an in-context learning evaluation where models are shown forward examples (Optical $\to$ SAR) and tested on the inverse (SAR $\to$ Optical). This setup isolates the model's ability to generalize a learned transformation rule in reverse. </div> <p>We propose a rigorous evaluation protocol using <strong>Vision Foundation Models</strong> (e.g., OmniGen <d-cite key="xiaoOmniGenUnifiedImage2025"></d-cite>, UniReal <d-cite key="chenUniRealUniversalImage2025"></d-cite>) in a <strong>one-shot or few-shot in-context setting</strong>:</p> <ol> <li> <strong>Context</strong>: Provide pairs of ${Optical_i, SAR_i}$ demonstrating the $Optical \to SAR$ transformation.</li> <li> <strong>Inference</strong>: Present a new $SAR_{test}$ image and prompt the model to generate the corresponding $Optical_{test}$.</li> <li> <strong>Constraint</strong>: No fine-tuning. The model must rely on its pre-trained representations and the in-context examples to deduce the inverse mapping.</li> </ol> <p>We hypothesize that even powerful generative models may struggle with this inversion if they have not explicitly learned the bidirectional physics of these sensors. This mirrors the LLM Reversal Curse and would suggest that current “world models” are more akin to <strong>associative engines</strong> than true simulators of physical reality.</p> <p>Recent progress in video <d-cite key="liuWorldWeaverGeneratingLongHorizon2025,gillmanForcePromptingVideo2025"></d-cite> and 3D modeling <d-cite key="guiImageWorldGenerating2025,zhouLearning3DPersistent2025"></d-cite> is promising, but the <strong>Visual Reversal Curse</strong> remains a critical litmus test. Can our models look at the world in one way and instinctively understand it from another? Remote sensing holds the answer.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-visual-reversal-curse-from-general-domain-to-remote-sensing-images.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-long-context/">Text-to-Image compression for long context understanding</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/using-large-language-models-to-simulate-and-predict-human-decision-making/">Using Large Language Models to Simulate and Predict Human Decision-Making</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/useful-calibrated-uncertainties/">What (and What Not) are Calibrated Uncertainties Actually Useful for?</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/unlearning-or-untraining/">Is your algorithm Unlearning or Untraining?</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>