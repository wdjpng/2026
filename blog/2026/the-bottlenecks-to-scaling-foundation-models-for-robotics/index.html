<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Bottlenecks to Scaling Foundation Models for Robotics | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Current approaches to building Vision-Language-Action (VLA) models largely rely on combining pre-trained Vision-Language Models (VLMs) with imitation learning. While effective in narrow benchmarks, this paradigm faces fundamental limitations for developing general-purpose robots that operate in complex, dynamic environments. In this article, I first review the standard training recipe and identify key bottlenecks, drawing on both my observations and existing empirical evidence. I then outline a path forward: integrating online reinforcement learning with pre-trained VLMs to enable lightweight, computationally efficient methods that scale with available resources."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/the-bottlenecks-to-scaling-foundation-models-for-robotics/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "The Bottlenecks to Scaling Foundation Models for Robotics",
            "description": "Current approaches to building Vision-Language-Action (VLA) models largely rely on combining pre-trained Vision-Language Models (VLMs) with imitation learning. While effective in narrow benchmarks, this paradigm faces fundamental limitations for developing general-purpose robots that operate in complex, dynamic environments. In this article, I first review the standard training recipe and identify key bottlenecks, drawing on both my observations and existing empirical evidence. I then outline a path forward: integrating online reinforcement learning with pre-trained VLMs to enable lightweight, computationally efficient methods that scale with available resources.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Anonymous",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>The Bottlenecks to Scaling Foundation Models for Robotics</h1> <p>Current approaches to building Vision-Language-Action (VLA) models largely rely on combining pre-trained Vision-Language Models (VLMs) with imitation learning. While effective in narrow benchmarks, this paradigm faces fundamental limitations for developing general-purpose robots that operate in complex, dynamic environments. In this article, I first review the standard training recipe and identify key bottlenecks, drawing on both my observations and existing empirical evidence. I then outline a path forward: integrating online reinforcement learning with pre-trained VLMs to enable lightweight, computationally efficient methods that scale with available resources.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#will-scaling-solve-robotics">Will Scaling Solve Robotics?</a> </div> <div> <a href="#the-common-recipe-behind-robot-foundation-models">The Common Recipe Behind Robot Foundation Models</a> </div> <div> <a href="#challenges">Challenges</a> </div> </nav> </d-contents> <p><strong>Will Scaling Solve Robotics?</strong> This question has been circulating widely in the robotics community in the last couple of years <d-cite key="Kumar_2024,icra2025debate"></d-cite>. When I first encountered it, I found it oddly vague. Scaling what - data, compute, memory, neural network capacity? And for which algorithms and settings? As I probed deeper, the roots of the question became clearer: it is shaped by two ideas that loom large in modern AI discussions - (i) the empirical success of increasingly large transformer models <d-cite key="vaswani2017attention"></d-cite> trained on internet-scale data at solving language understanding and generation tasks (e.g., the so-called scaling laws in large language models) <d-cite key="kaplan2020scaling"></d-cite> and (ii) the philosophy espoused in Richard Sutton’s influential blog post, The Bitter Lesson <d-cite key="sutton2019bitter"></d-cite> <d-footnote>Rich is actually vociferouly against LLMs, stating that as powerful as they might turn out to be, they will not lead to AGI. </d-footnote>. Simply put, the oversimplified narrative is that large data solved language, large data solved vision, and therefore large data will also solve robotics.</p> <p>There are genuine concerns about large language models: the illusion of “thinking” <d-cite key="shojaee2025illusion"></d-cite>, their substantial carbon footprint <d-cite key="faiz2023llmcarbon"></d-cite>, and the looming limitation that we are quickly exhausting the scrapeable text on the public internet <d-cite key="villalobos2022will"></d-cite>. At the same time, there is countervailing optimism. Many expect training and inference costs to fall through improved model distillation techniques, more efficient architectures, and advances in hardware. Others believe that access to higher-quality data, synthetic data generation, or improved scaling practices will mitigate many of the current shortcomings. More details around this can also be found in the excellent article by Nishanth.</p> <p>All this speculative optimism has set the field of robotics in a frenzy, with the community in the middle of a genuine hype cycle. Demo videos of humanoids performing carefully orchestrated household tasks flood social media. Announcements from well-funded startups arrive weekly, each promising that general-purpose robots are imminent. Venture capital has followed this optimism aggressively, backing companies with valuations that far outpace their actual deployments or revenue. A major source of this confidence is the belief that the LLM playbook can be straightforwardly transplanted into robotics. In language modeling, the recipe is well established: gather internet-scale data, pretrain a large transformer, curate higher-quality datasets, and fine-tune on downstream tasks. At its core, this is next-token prediction using supervised learning. In robotics, the analogy maps neatly onto expert demonstrations and imitation learning—collect sequences of state-action pairs and train a model to imitate.</p> <p>What this argument conveniently misses is a simple fact: <em>imitation is only one facet of intelligence, but it is far from the whole story.</em> This is the point where we should all question our motivations. What is it that we are trying to build? Is it a general-purpose robot that can just live and function in the real world like we do? Or is it a more narrow, specialist robot that can handle a couple of tasks.</p> <h1 id="the-common-recipe-behind-robot-foundation-models">The Common Recipe Behind Robot Foundation Models</h1> <p><strong>Step 1: Use a vision-language model (VLM) pre-trained on large scale data</strong></p> <p>The current recipe for building robot foundation models typically begins with the use of a vision-language model (VLM) that has been pre-trained on large-scale data. Researchers select a backbone for processing visual and language inputs, such as CLIP <d-cite key="radford2021learning"></d-cite>, Gemma <d-cite key="team2024gemma"></d-cite>, or SigLIP <d-cite key="zhai2023sigmoid"></d-cite>. These models provide general scene understanding and instruction grounding, allowing downstream robot policies to start from a rich multimodal representation rather than learning perception from scratch.</p> <p><strong>Step 2: Collect expert demonstrations</strong></p> <p>The next step is to collect expert demonstrations. These are gathered through human teleoperation, kinesthetic teaching, or VR-based imitation. The datasets usually span a range of objects, environments, and manipulation skills to ensure broad coverage. Synthetic data produced through procedural generation or large-scale simulation is often added to augment real demonstrations, increasing diversity and coverage at lower cost.</p> <p><strong>Step 3: Train vision-language-action (VLA) models</strong></p> <p>With these ingredients in place, researchers train vision-language-action (VLA) models. The objective is to learn mappings from visual observations, language or task goals, and proprioceptive information to continuous robot actions. The VLM is kept frozen, and new layers are trained to translate VLM embeddings and proprioceptive states into actions using behavior cloning or diffusion policy objectives, following a standard imitation learning setup.</p> <p><strong>Step 4: Fine-tune and deploy to the real world</strong></p> <p>Finally, the VLA is fine-tuned and deployed in the real world. Fine-tuning is performed on a small, task-specific dataset while keeping the VLM backbone frozen. When needed, policy distillation is applied to reduce model size and achieve faster inference for deployment.</p> <p><strong>N.B:</strong> While specific design choices vary across projects, this general recipe is shared by most state-of-the-art systems. $\pi_{0.5}$ <d-cite key="intelligence2025pi_"></d-cite>, RT-2 <d-cite key="zitkovich2023rt"></d-cite>, OpenVLA <d-cite key="kim2025openvla"></d-cite>, and Gr00t N1 <d-cite key="bjorck2025gr00t"></d-cite> all follow the same broad pattern: start from a powerful VLM backbone, collect a diverse demonstration dataset, train a VLA through imitation learning or diffusion policies, and perform light task-specific fine-tuning before deployment. Despite architectural differences, these works reflect the same underlying paradigm that now dominates robot foundation model development.</p> <h1 id="can-we-just-scale-our-way-to-embodied-intelligence">Can We Just Scale Our Way to Embodied Intelligence?</h1> <blockquote> “… general methods that leverage computation are ultimately the most effective, and by a large margin” <br> - The Bitter Lesson (Sutton 2019) <d-cite key="sutton2019bitter"></d-cite> </blockquote> <p>Sutton’s Bitter Lesson highlights a pattern that has repeatedly played out in AI: methods that can effectively leverage additional compute to improve their performance tend to win out over more specialized, handcrafted approaches. This view now shapes much of the thinking in robotics, where researchers are attempting to scale the standard VLA recipe described earlier—large scale demonstration collection, more computational resources—in hopes of achieving more general and reliable manipulation capabilities.</p> <p>In this pipeline, imitation learning plays a central role. Because most current VLAs rely on behavior cloning <d-cite key="pomerleau1988alvinn"></d-cite> or diffusion-based action sampling <d-cite key="chi2025diffusion"></d-cite>, their performance is tightly coupled to the coverage and quality of expert demonstrations. These models excel when the test-time distribution closely matches the demonstrations, but often struggle with long-horizon tasks, unseen objects, or novel environment configurations. And since the VLM backbone is kept frozen, it cannot adapt its perceptual representations to the specific affordances or dynamics that matter for manipulation. As a result, VLA training tends to resemble supervised regression on expert trajectories rather than the acquisition of new skills, limiting generalization and robustness once deployed in the wild.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-the-bottlenecks-to-scaling-foundation-models-for-robotics/bender_shovel_data-480.webp 480w,/2026/assets/img/2026-04-27-the-bottlenecks-to-scaling-foundation-models-for-robotics/bender_shovel_data-800.webp 800w,/2026/assets/img/2026-04-27-the-bottlenecks-to-scaling-foundation-models-for-robotics/bender_shovel_data-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-the-bottlenecks-to-scaling-foundation-models-for-robotics/bender_shovel_data.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 50%; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Early scaling results show some promise. Larger VLAs trained on more diverse demonstrations exhibit better generalization, stronger zero-shot transfer within related task families, and improved robustness to modest distribution shifts. These gains have fueled real optimism that simply scaling the existing imitation-driven recipe might meaningfully increase the breadth of robotic competence.</p> <p>But this optimism runs headlong into the realities of embodied learning. Training and inference remain expensive, requiring substantial compute and infrastructure. Robot data does not scale like internet data, and collecting demonstrations is slow, costly, and closely tied to specific hardware and environments. While the ceiling for robot foundation models is still unknown, it is already clear that only a handful of well-resourced organizations can realistically pursue this scaling recipe. And even if one could afford it, imitation learning on its own cannot drive genuine discovery or the acquisition of novel skills—it reproduces what experts have demonstrated, but does not inherently push beyond the boundaries of the data it is given.</p> <h2 id="limitations-of-current-vla-approaches">Limitations of Current VLA Approaches</h2> <p><strong>1. Computational Efficiency</strong></p> <p>A major challenge in current robot foundation models is computational efficiency. Training and inference remain expensive and demand substantial compute and infrastructure. Large models also run slowly at inference time, which directly translates into sluggish robot motion during real-world execution. These limitations highlight the need for more efficient algorithms that can achieve strong performance with far less compute.</p> <p><strong>2. Passive Data Collection</strong></p> <p>Data collection presents an equally significant bottleneck. Robot data does not scale in the same way as internet-scale corpora; it is slow, costly, and often tied to specific environments. At present, most approaches rely on passive data collection, which depends heavily on humans in the loop—whether through scraping online resources, generating synthetic data, or providing expert demonstrations. This raises the question of whether data collection can be automated through active exploration.</p> <p><strong>3. Loss of Plasticity and Adaptation</strong></p> <p>Offline training seeks to achieve zero-shot generalization across tasks and environments, but performance often degrades once models are deployed. When this happens, practitioners typically collect additional data, either through synthetic augmentation with domain randomization or through new rounds of expert demonstrations. One option is to retrain from scratch, which requires expensive retraining cycles and substantial human supervision.</p> <p>Another option is to fine-tune a larger base model using methods such as Low-Rank Adaptation (LoRA) <d-cite key="hu2021lora"></d-cite>. However, this approach often forgets aspects of the pre-training distribution and tends to be less robust in continual learning settings compared to full fine-tuning, as shown in Shuttleworth et al. (2024). As a result, adaptation is generally treated as an afterthought rather than a core design choice.</p> <p>The real challenges—handling uncertainty and adapting to novel situations—live outside the imitation paradigm.</p> <h1 id="learning-like-a-squirrel-online-adaptation-in-action">Learning Like a Squirrel: Online Adaptation in Action</h1> <center> <iframe width="560" height="315" src="https://www.youtube.com/embed/DkmeZwsi3HA?si=J29yCdy35SU-f4d5" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </center> <p>The above video shows a squirrel navigating an obstacle course to reach its target: hazelnuts. There are two types of platforms: blue ones, which are rigid and stable, and red ones, which are attached to a slinky and thus unstable.</p> <p>Initially, the squirrel engages in trial-and-error interactions with the world, testing different sequences of steps. Over time, we observe a brilliant learned strategy: stepping once on the red platform and twice on the blue platforms to stabilize its trajectory. Finally, the squirrel exhibits fast, reactive movements, executing the sequence efficiently and reaching the goal with minimal hesitation.</p> <p>This example illustrates key hallmarks of natural intelligence: the ability to react in milliseconds, learn from ongoing experience, dynamically update an internal model of the world, and plan actions based on predicted consequences.</p> <p>An intelligent robot aiming to emulate this level of adaptability should similarly be able to:</p> <ul> <li>React in real time to changes in the environment</li> <li>Learn on the fly, continuously updating its policy from new experiences</li> <li>Maintain and update an internal model of the world to guide decisions</li> <li>Plan sequences of actions toward a goal, using its world model to anticipate outcomes</li> </ul> <p>Recasting these ideas in reinforcement learning (RL) terms:</p> <ul> <li> <strong>Online learning:</strong> Acting and learning are intertwined—the agent continuously updates its policy as it interacts with the environment.</li> <li> <strong>Planning with learned world models:</strong> The agent maintains an internal predictive model to reason about consequences, simulate potential action sequences, and select actions that maximize expected reward.</li> </ul> <p>The squirrel’s performance highlights the difference between reactive trial-and-error learning and slow, memorization-drive behavior in large VLAs. For robots, achieving this level of natural intelligence requires integrating fast perception, online learning, and model-based planning in a single, adaptive system.</p> <h1 id="from-passive-observation-to-active-experience-the-role-of-online-learning-in-vlas">From Passive Observation to Active Experience: The Role of Online Learning in VLAs</h1> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-the-bottlenecks-to-scaling-foundation-models-for-robotics/data_sources-480.webp 480w,/2026/assets/img/2026-04-27-the-bottlenecks-to-scaling-foundation-models-for-robotics/data_sources-800.webp 800w,/2026/assets/img/2026-04-27-the-bottlenecks-to-scaling-foundation-models-for-robotics/data_sources-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-the-bottlenecks-to-scaling-foundation-models-for-robotics/data_sources.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" max-width: 100%; " loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The Data We Can Use in Robot Learning. </div> <p>When considering how to integrate online reinforcement learning with VLAs, it helps to compare different sources of training data. Internet-scale datasets, scraped from images, videos, and text, provide broad coverage for vision and language but are passively collected and exhibit low environment fidelity. Simulation allows researchers to choose sensors and modalities freely, also in a passive manner, though the usefulness of the resulting data depends heavily on the sim-to-real gap. Expert demonstrations, collected by human pilots, offer high-fidelity experiences with the chosen sensors, but are still passively gathered. Finally, real robot experience represents the highest-fidelity data, collected actively through reinforcement learning with human-in-the-loop safeguards, enabling the agent to learn from direct interaction with the environment. Each source represents a trade-off between data scale, fidelity, and the ability to adapt online.</p> <h3 id="how-do-we-leverage-expert-demonstrations-in-online-rl">How Do We Leverage Expert Demonstrations in Online RL?</h3> <p>When VLAs imitate, they directly imitate actions — predicting the next token in a sequence When animals imitate, they imitate outcomes and must infer the underlying actions In online RL, demonstrations (expert trajectories) can be leveraged to infer desirable actions via off-policy RL methods such as SAC or TD3: Demonstrations can be added to replay alongside online experience Learning updates sample both demonstration data and newly collected data, enabling online adaptation without forgetting expert demonstrations **This is technically possible in the streaming setting as well, but would require the development novel, off-policy streaming algorithms with robust performance Existing batch methods are already capable (e.g., Ball et al. 2023)</p> <h2 id="teacher-student-distillation-multiple-expert-policies-and-large-transformers">Teacher Student Distillation, Multiple Expert Policies and Large Transformers</h2> <h2 id="re-defining-what-we-mean-by-a-world-model">Re-defining what we mean by a World Model</h2> <p>In control theory and RL, a world model represents the system dynamics: ￼ predicts next state given current state and action Examples: (i) Dyna, Dreamer and TD-MPC algorithms in RL; (ii) Physics-based models in model-predictive control (MPC) algorithms such as MPPI These models support learning and planning in imagination — by simulating outcomes within the model If the models are lightweight enough for real-time operation, they enable decision-time planning for: Safer exploration in real-world environments Robust performance under uncertainty</p> <h3 id="what-world-models-mean-in-llms-and-vlms-today">What “World Models” Mean in LLMs and VLMs Today</h3> <p>LLM- or VLM-style world models capture associations between vision and language via next token prediction They model ￼ rather than ￼ Unlike world models in RL, they are not explicitly conditioned on actions Would we benefit from action-conditioned world models in dexterous manipulation? This requires the design and large-scale training of model-based RL methods like Dyna for controlling robots</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-the-bottlenecks-to-scaling-foundation-models-for-robotics.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-vlms-waste-their-vision/">Why vlms waste their vision</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>