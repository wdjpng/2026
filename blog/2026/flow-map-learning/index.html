<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From Trajectories to Operators — A Unified Flow Map Perspective on Generative Modeling | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="In this post, we reframe continuous-time generative modeling from integrating trajectories to learning two-time operators (flow maps). This operator view unifies diffusion, flow matching, and consistency models, and suggests a practical diagnostic — semigroup-consistent jumps yield both step-robust generation and low compositional drift. We derive Eulerian/Lagrangian distillation objectives and use inpainting experiments to show why semigroup-consistent jumps can be both step-robust and composition-stable."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/flow-map-learning/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}.box-note{font-size:18px;padding:15px 15px 10px 10px;margin:2px 2px 2px 5px;border-left:7px solid #1976d2;border-radius:1px}d-article .box-note{background-color:#f5f9ff;border-left-color:#1976d2}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "From Trajectories to Operators — A Unified Flow Map Perspective on Generative Modeling",
            "description": "In this post, we reframe continuous-time generative modeling from integrating trajectories to learning two-time operators (flow maps). This operator view unifies diffusion, flow matching, and consistency models, and suggests a practical diagnostic — semigroup-consistent jumps yield both step-robust generation and low compositional drift. We derive Eulerian/Lagrangian distillation objectives and use inpainting experiments to show why semigroup-consistent jumps can be both step-robust and composition-stable.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>From Trajectories to Operators — A Unified Flow Map Perspective on Generative Modeling</h1> <p>In this post, we reframe continuous-time generative modeling from integrating trajectories to learning two-time operators (flow maps). This operator view unifies diffusion, flow matching, and consistency models, and suggests a practical diagnostic — semigroup-consistent jumps yield both step-robust generation and low compositional drift. We derive Eulerian/Lagrangian distillation objectives and use inpainting experiments to show why semigroup-consistent jumps can be both step-robust and composition-stable.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#1-background-and-motivation">1. Background and Motivation</a> </div> <div> <a href="#2-the-flow-map-paradigm">2. The Flow Map Paradigm</a> </div> <div> <a href="#3-training-two-time-operators-via-eulerian-and-lagrangian-distillation">3. Training Two-Time Operators via Eulerian and Lagrangian Distillation</a> </div> <div> <a href="#4-experiments">4. Experiments</a> </div> <div> <a href="#5-conclusion">5. Conclusion</a> </div> </nav> </d-contents> <h2 id="1-background-and-motivation">1. Background and Motivation</h2> <p>The landscape of continuous-time generative modeling has largely crystallized into three dominant lineages, each defining a specific relationship between a complex data distribution and a simple prior.</p> <ul> <li> <p><strong>Score-based Diffusion (SDEs) <d-cite key="song2019generative,song2020score"></d-cite>:</strong> Models learn the score function $\nabla_x \log p_t(x)$ to reverse a stochastic diffusion process. Sampling requires simulating a reverse-time SDE or its deterministic probability-flow ODE:</p> \[\frac{dx}{dt} = f(x, t) - \frac{g^2(t)}{2}\,s_{\theta}(x_t, t).\] </li> <li> <p><strong>Flow Matching (CNFs) / Rectified flow <d-cite key="lipman2022flow,liu2022flow"></d-cite>:</strong> Instead of deriving dynamics from a forward diffusion, Flow Matching directly regresses a velocity field $v_\theta(x,t)$ that pushes a Gaussian source probability path toward the data target:</p> \[\frac{dx}{dt} = v_\theta(x,t).\] </li> <li> <p><strong>Consistency Models (CMs) <d-cite key="song2023consistency"></d-cite>:</strong> Aiming for speed, CMs attempt to bypass integration entirely by learning a map $f_\theta$ that projects any point on a trajectory directly to its origin ($t=0$):</p> \[f_\theta(x_t,t) \approx x_0 \implies f_\theta(x_t,t) = f_\theta(x_{t'}, t').\] </li> </ul> <p>While these methods have achieved remarkable success, they face a dichotomy between <strong>flexibility</strong> and <strong>efficiency</strong>.</p> <p>Integration-based methods (Diffusion, Flow Matching) are flexible but computationally expensive, requiring dozens or hundreds of sequential function evaluations (NFEs) to traverse the trajectory. Conversely, distillation methods like Consistency Models are fast (1-2 steps) but structurally <strong>rigid</strong>: they typically memorize the mapping to a fixed endpoint ($s=0$). This “schedule lock-in” makes them brittle under guidance, editing, or varying step counts, as the model lacks knowledge of the intermediate geometry of the flow.</p> <p>To achieve <strong>fast</strong>, <strong>few-step</strong>, and <strong>highly controlled</strong> generation, we need a primitive that is richer than a single-point mapping but faster than numerical integration.</p> <h2 id="2-the-flow-map-paradigm">2. The Flow Map Paradigm</h2> <p>Sampling from modern generative models is traditionally viewed as “following a trajectory”: starting from noise and iteratively integrating a velocity field. We propose a shift in perspective: what if the fundamental object of learning is not the infinitesimal velocity, but the <strong>finite-time transformation</strong> itself?</p> <p>We define the <strong>Flow Map</strong> as the family of two-time solution operators $\Phi_{t\to s}$ that answers the question:</p> <div class="box-note"> <p><strong>The Operator Question:</strong> If the system is currently at state $x_t$ at time $t$, exactly where will it be at time $s$?</p> </div> <h3 id="21-from-trajectories-to-operators">2.1. From trajectories to operators</h3> <p>Consider a time-dependent vector field $v_{\phi}(x, t)$ defining the ODE: \(dx/dt = v_{\phi}(x, t)\). While the vector field describes <strong>local</strong> motion, the flow map \(\Phi_{t\to s}: \mathbb{R}^d \to \mathbb{R}^d\) describes the <strong>global</strong> transport. It is characterized by the integral equation:</p> \[x_s = \Phi_{t\to s}(x_t) \triangleq x_t + \int_{t}^{s} v_{\phi}(x_{\tau},\tau) d\tau, \qquad 0 \le s \le t \le 1.\tag{2.1}\label{eq:21}\] <p>The <strong>Flow Map Paradigm</strong> reframes generative modeling from an <strong>integration problem</strong> to a <strong>function approximation problem</strong>. Instead of solving the integral at inference time, we train a neural network $f_\theta(x_t, t, s)$ to approximate the operator directly:</p> \[f_\theta(x_t, t, s) \approx \Phi_{t\to s}(x_t).\] <p>A mathematically valid flow map must satisfy two intrinsic algebraic properties, which serve as the “physics” of the operator:</p> <ol> <li> <strong>Identity:</strong> $\Phi_{t\to t}(x) = x$.</li> <li> <p><strong>Semigroup (Composition) Property:</strong> For any intermediate time $u$,</p> \[\Phi_{u\to s} \circ \Phi_{t\to u} = \Phi_{t\to s}.\] </li> </ol> <h3 id="22-unifying-existing-methods">2.2. Unifying Existing Methods</h3> <p>The Flow Map formulation provides a generalized coordinate system for understanding recent acceleration techniques. As illustrated in Figure 1, we can interpret existing models as restricting the flow map $f_\theta(x, t, s)$ to specific subsets of the time domain $(t, s)$.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-map-learning/flow_map_vs_four_paradigms_clean-480.webp 480w,/2026/assets/img/2026-04-27-flow-map-learning/flow_map_vs_four_paradigms_clean-800.webp 800w,/2026/assets/img/2026-04-27-flow-map-learning/flow_map_vs_four_paradigms_clean-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-flow-map-learning/flow_map_vs_four_paradigms_clean.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 1. Flow maps as a unifying operator view.</figcaption> </figure> <ul> <li> <p><strong>Consistency Models (CM) are Boundary Operators:</strong> CMs restrict the target time to the data manifold ($s=0$). They learn the slice:</p> \[f_\theta^{\text{CM}}(x_t, t) \equiv f_\theta(x_t, t, 0).\] <p><strong>Limitation:</strong> Because they ignore $s &gt; 0$, they cannot support iterative refinement or arbitrary start-stop editing.</p> </li> <li> <p><strong>Flow Matching (FM) is the Infinitesimal Limit:</strong> FM learns the velocity field $v_\phi$, which is the derivative of the flow map as $s \to t$:</p> \[v_\phi(x,t) = \lim_{s\to t} \frac{f_\theta(x, t, s) - x}{s-t}.\] <p><strong>Limitation:</strong> Recovering finite jumps requires expensive numerical integration.</p> </li> <li> <p><strong>MeanFlow is a Linearized Operator:</strong> MeanFlow <d-cite key="geng2025mean"></d-cite> parameterizes the map using an average velocity $u_\theta$:</p> \[f_\theta(x, t, s) = x + (s-t)u_\theta(x, t, s).\] <p>where</p> \[u_{\theta}(x_t,t,s) \approx \frac{1}{s-t}\,\int_{t}^{s} v_\theta(x_\tau,\tau)\,d\tau.\] <p>This enforces a linear inductive bias on the trajectory between $t$ and $s$.</p> </li> <li> <p><strong>Consistency Trajectory Models (CTM) are Discrete Compositions:</strong> CTM <d-cite key="kim2023consistency"></d-cite> enforces the composition property on a discrete grid of time steps, acting as a discretized surrogate of the full flow map algebra:</p> \[f_\theta(f_\theta(x, t_2, t_1), t_1, t_0) \approx f_\theta(x, t_2, t_0).\] </li> </ul> <h3 id="23-why-operators-speed-control-and-composability">2.3. Why Operators? Speed, Control, and Composability</h3> <p>By lifting the abstraction from <strong>trajectories</strong> to <strong>operators</strong>, we gain distinct advantages:</p> <ol> <li> <p><strong>Amortization of Integration (Speed):</strong> We trade the cost of repeated integration during training for $O(1)$ inference. A jump from $t=1$ to $t=0$ becomes a single function call, yet unlike Consistency Models, we retain the ability to stop at any $s$.</p> </li> <li> <p><strong>Decoupling Physics from Discretization (Control):</strong> Trajectory-based samplers entangle the ODE dynamics with the solver’s step size. With a flow map, the “desired jump” $(t, s)$ is an explicit input. This makes the model robust to changing step counts and enables non-uniform schedules tailored to specific editing tasks.</p> </li> <li> <p><strong>Algebraic Consistency:</strong> Complex pipelines (e.g., SDEdit, in-painting) often require jumping back and forth in time. A flow map trained with composition constraints ensures that $x_t \to x_s \to x_u$ yields a valid state, preventing the accumulation of drift errors common in heuristic distillations.</p> </li> </ol> <h2 id="3-training-two-time-operators-via-eulerian-and-lagrangian-distillation">3. Training Two-Time Operators via Eulerian and Lagrangian Distillation</h2> <p>We have defined the object of interest: a neural operator $f_\theta(x_t, t, s)$ approximating the true solution $\Phi_{t\to s}$ of a teacher velocity field $v_\phi$. The challenge is training this operator efficiently without running expensive ODE solvers in the loop.</p> <p>We can derive training objectives by analyzing the partial derivatives of the flow map $\Phi_{t\to s}$ with respect to its two time arguments. These lead to two complementary “distillation” paradigms.</p> <h3 id="31-the-lagrangian-view-moving-the-endpoint">3.1. The Lagrangian View: Moving the Endpoint</h3> <p>Fix the start state $x_t$ at time $t$. As we vary the target time $s$, the output $x_s$ traces a trajectory of the teacher ODE. This is the <strong>Lagrangian</strong> perspective (following the particle).</p> <p><strong>The Identity <d-cite key="boffi2025flow"></d-cite>:</strong> Differentiation w.r.t. $s$ yields:</p> \[\boxed{\ \frac{\partial}{\partial s} \Phi_{t\to s}(x_t) = v_\phi(\Phi_{t\to s}(x_t), s) \ }\] <p><strong>Intuition:</strong> As the target time $s$ advances, the value of the map changes according to the teacher’s velocity at the current location.</p> <p><strong>Lagrangian Distillation (LMD) <d-cite key="sabour2025align"></d-cite>:</strong> Directly matching this derivative gives us a training objective. In practice, we use a robust “predict-then-correct” discretization. We predict a jump to an intermediate time $s’$, and then ensure that taking one Euler step from $s’$ leads to the same result as predicting $s$ directly.</p> <p>Let $s’ = s + \varepsilon (t-s)$. A discrete loss is:</p> \[\mathcal{L}_{\mathrm{LMD}} = \mathbb{E}_{t,s} \left[ \left\| f_\theta(x_t, t, s) - \underbrace{\left( f_{\theta^-}(x_t, t, s') + (s-s') v_\phi(f_{\theta^-}(x_t, t, s'), s') \right)}_{\text{1-step Teacher correction}} \right\|^2 \right].\] <p>Here, $f_{\theta^-}$ is a target network (EMA). This effectively trains the student to match the teacher’s <em>future</em> velocity.</p> <h3 id="32-the-eulerian-view-moving-the-start">3.2. The Eulerian View: Moving the Start</h3> <p>Alternatively, fix the target time $s$ and the spatial coordinate $x$, but vary the start time $t$. This describes how the solution operator itself evolves and leads to the <strong>Eulerian</strong> perspective (observing the flow at a fixed point).</p> <p><strong>The Identity <d-cite key="boffi2025flow"></d-cite>:</strong> The composition law implies a transport PDE (the Kolmogorov Backward Equation):</p> \[\boxed{\ \frac{\partial}{\partial t} \Phi_{t\to s}(x) + J_x \Phi_{t\to s}(x) \, v_\phi(x, t) = 0 \ }\] <p><strong>Intuition:</strong> If we perturb the start time $t$ (move it slightly toward $s$), we must compensate by moving the input $x$ along the flow $v_\phi$ by that same amount, such that the final destination $s$ remains unchanged.</p> <p><strong>Eulerian Distillation (EMD) <d-cite key="sabour2025align"></d-cite>:</strong> This identity enforces <strong>consistency under transport</strong>. A discrete approximation involves taking a small teacher step from $t$ to $t’$ and demanding that the flow map from $t’$ yields the same result.</p> <p>Let $t’ = t + \varepsilon (s-t)$. We first compute $x_{t’} = x_t + (t’-t)v_\phi(x_t, t)$. The loss is:</p> \[\mathcal{L}_{\mathrm{EMD}} = \mathbb{E}_{t,s} \left[ \left\| f_\theta(x_t, t, s) - f_{\theta^-}(x_{t'}, t', s) \right\|^2 \right].\] <p>This is the core mechanism behind Consistency Models, but generalized here for arbitrary target times $s$.</p> <h3 id="33-synthesis-a-unified-objective">3.3. Synthesis: A Unified Objective</h3> <p>The Lagrangian and Eulerian views provide orthogonal supervision signals:</p> <ul> <li> <strong>Lagrangian (LMD)</strong> stabilizes the <strong>destination</strong>: it ensures the map outputs valid states along the ODE trajectory.</li> <li> <strong>Eulerian (EMD)</strong> stabilizes the <strong>path</strong>: it ensures the map is invariant to the choice of starting time along the flow.</li> </ul> <p>Combining these allows for training flow maps that are both locally accurate (matching the vector field) and globally consistent (respecting composition). By learning the full operator $\Phi_{t\to s}$, we obtain a generative model that offers the speed of distilled models without sacrificing the flexibility of differential equations.</p> <h2 id="4-experiments">4. Experiments</h2> <p>In the previous sections, we proposed a shift from <strong>trajectory integration</strong> (solving an SDE/ODE step-by-step) to <strong>operator learning</strong> (predicting the jump $\Phi_{t\to s}$ directly). This theoretical reframing leads to two verifiable hypotheses regarding the behavior of generative models:</p> <ol> <li> <strong>Robustness to Discretization:</strong> If a model learns the finite-time operator directly, its generation quality should be decoupled from the inference step count (NFEs), unlike solvers that rely on infinitesimal approximations.</li> <li> <strong>Algebraic Consistency (Semigroup Property):</strong> a genuine flow map should approximately satisfy the semigroup law \(\Phi_{u\to s} \circ \Phi_{t\to u} \approx \Phi_{t\to s}\), so chaining multiple jumps should induce <strong>small drift</strong> compared to a direct jump.</li> </ol> <p>To validate these claims without repeating ourselves, we use <strong>two experiments under a shared inpainting setup</strong>:</p> <ul> <li> <p><strong>Exp-I (Step sensitivity, Sec 4.3) :</strong> Compare integration-based samplers (SDE/ODE) vs jump-distilled samplers (CM-like / Flow-like) under different NFEs. This answers <strong>why jumps help at low steps.</strong></p> </li> <li> <p><strong>Exp-II (Composition drift, Sec 4.4) :</strong> Compare <strong>direct</strong> \(\Phi_{t\to s}\) vs <strong>composed</strong> \(\Phi_{u_{k-1}\to s}\circ\cdots\circ\Phi_{t\to u_1}\) for different hop counts $k$. This answers <strong>why two-time operators are more stable than endpoint-anchored jumps in multi-hop pipelines.</strong></p> </li> </ul> <h3 id="41-shared-experimental-setup-common-to-both-experiments">4.1. Shared experimental setup (common to both experiments)</h3> <p><strong>Task: image inpainting.</strong> Given an image \(x\in\mathbb{R}^{H\times W}\) and a binary mask \(m\in\{0,1\}^{H\times W}\), the goal is to fill the masked region $\Omega$ (where \(\Omega=\{(i,j) \mid m_{ij}=1\}\)) in a way consistent with the context and text prompt. This is a good stress test for step robustness because failures often show up as geometry glitches or texture incoherence when NFEs are too small.</p> <p><strong>Backbone &amp; pipeline.</strong> We use the same Stable Diffusion v1.5 inpainting backbone instantiated via <code class="language-plaintext highlighter-rouge">AutoPipelineForInpainting</code>, and vary only the scheduler / distilled head (LoRA) depending on the method.</p> <p><strong>Methods.</strong> We compare four inference paradigms built on the same backbone (same prompt encoder / VAE / UNet weights unless stated otherwise):</p> <ul> <li> <strong>SDE-like sampler:</strong> Euler ancestral discretization (<code class="language-plaintext highlighter-rouge">EulerAncestralDiscreteScheduler</code>).</li> <li> <strong>ODE-like sampler (probability-flow style):</strong> deterministic DDIM (<code class="language-plaintext highlighter-rouge">DDIMScheduler</code>, \(\eta=0\)).</li> <li> <strong>Endpoint-jump distillation (CM-like, proxy):</strong> <code class="language-plaintext highlighter-rouge">LCMScheduler</code> <d-cite key="luo2023latent"></d-cite> with an LCM LoRA fused into the pipeline.</li> <li> <strong>Two-time jump distillation (Flow-like, proxy):</strong> <code class="language-plaintext highlighter-rouge">TCDScheduler</code> <d-cite key="zheng2024trajectory"></d-cite> with a TCD LoRA fused into the pipeline (used as a practical approximation to “two-time operator distillation”).</li> </ul> <p><strong>Fairness controls (Exp-I &amp; Exp-II).</strong> To isolate step/schedule effects rather than prompt/guidance variance, we enforce:</p> <ul> <li>Same inpainting input (image + mask) and same prompt/negative prompt.</li> <li>Same step budgets (Exp-I uses ({2,4,8,16,30}), and Exp-II uses ({2,4,6,8})).</li> <li>Same random seed and the same guidance scale across all methods.</li> </ul> <p><strong>Extra controls for composition drift (Exp-II).</strong> Because composition drift is subtle, we add two controls specifically for Exp-II:</p> <ul> <li> <strong>Fixed noise for the endpoint re-noise step</strong> in the CM-proxy, so drift reflects structural inconsistency rather than stochastic variance.</li> <li> <strong>Unmasked-region locking:</strong> we explicitly lock the unmasked context region to a shared reference latent trajectory at every intermediate time, ensuring the metric measures drift only inside the edited region.</li> </ul> <h3 id="42-a-note-on-proxies-what-we-did-and-did-not-implement">4.2. A note on “proxies”: what we <em>did</em> and <em>did not</em> implement</h3> <p>A key practical detail is that we do <strong>not</strong> train a full flow-map network \(f_\theta(x_t,t,s)\) from scratch in this blog post. Instead, we use <strong>public distilled LoRAs <d-cite key="hu2022lora"></d-cite></strong> as <strong>stand-ins</strong> for two different structural paradigms:</p> <ul> <li> <p><strong>LCM-LoRA as an endpoint-jump proxy (CM-like):</strong> it behaves like a boundary map \(B_t(x_t)\approx x_0\), and any \(t\to s\) jump is synthesized via “denoise to $x_0$, then re-noise to $s$”.</p> </li> <li> <p><strong>TCD-LoRA as a two-time-jump proxy (Flow-like):</strong> we treat the model as supporting a DDIM-coupled jump \(a\to b\) that depends on the target time through \((\alpha_b,\sigma_b)\), rather than always anchoring at \(t=0\).</p> </li> </ul> <p>Concretely, in Exp-II we instantiate two “operators”:</p> <ul> <li> <p><strong>Flow-proxy jump (DDIM-coupled):</strong></p> \[\Phi^{\text{flow}}_{a\to b}(x_a) = \frac{\alpha_b}{\alpha_a} x_a + \Bigl(\sigma_b-\frac{\alpha_b}{\alpha_a}\sigma_a\Bigr)\,\varepsilon_\theta(x_a,a),\] <p>where \(\varepsilon_\theta\) comes from the TCD-distilled predictor and \((\alpha_t,\sigma_t)\) are from a reference DDIM schedule.</p> </li> <li> <p><strong>CM-proxy jump (endpoint-anchored):</strong></p> \[\Phi^{\text{CM}}_{a\to b}(x_a) = \alpha_b\,B_a(x_a) + \sigma_b\,\epsilon_0,\] <p>where \(B_a(x_a)\) is a learned endpoint map: \(x_a \to x_0\), comes with a fixed \(\epsilon_0\) shared across all compositions.</p> </li> </ul> <p><strong>Why this is still meaningful.</strong> Our goal is <strong>not</strong> to claim “we trained the definitive flow map.” The goal is to empirically test a <em>structural hypothesis</em> from Chapters 2–3: <strong>multi-hop pipelines should be more stable when the underlying primitive is a genuine two-time operator (semigroup-consistent), and less stable when every hop must “return to \(x_0\)” as an anchor.</strong></p> <p>Even with proxies, Exp-II directly measures the <strong>semigroup violation diagnostic</strong> (direct vs composed). As long as the two proxies reflect the two paradigms (endpoint-anchored vs time-to-time jump), the diagnostic remains aligned with the theory.</p> <p><strong>Limitation (explicit):</strong> the absolute numerical values of drift depend on the particular checkpoints; a true \(f_\theta(x_t,t,s)\) trained with explicit composition constraints could reduce drift further. We view the proxy study as an <strong>evidence-of-principle</strong> validation.</p> <h3 id="43-exp-i-inpainting-under-different-nfes-robustness-to-discretization">4.3. Exp-I: Inpainting under different NFEs (Robustness to Discretization)</h3> <p>We first test whether jump-based inference indeed reduces step sensitivity. Using the common setup in Sec. 4.1, we run each method with step counts ({2,4,8,16,30}) and visualize the inpainted outputs, as shown in Figure 2.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-map-learning/compare_inpaint_dog-480.webp 480w,/2026/assets/img/2026-04-27-flow-map-learning/compare_inpaint_dog-800.webp 800w,/2026/assets/img/2026-04-27-flow-map-learning/compare_inpaint_dog-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-flow-map-learning/compare_inpaint_dog.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 2. Inpainting results vs NFE. Integration-based samplers (SDE/ODE) are sensitive to step count; jump-distilled samplers (LCM/TCD) are stable at very small NFEs.</figcaption> </figure> <p><strong>Observation.</strong> The qualitative trend is consistent:</p> <ul> <li> <p><strong>SDE/ODE (integration)</strong> are notably worse at very low NFEs and improve as NFEs increase, as expected from discretization/integration error accumulation.</p> </li> <li> <p><strong>LCM/TCD (jump-distilled)</strong> already produce plausible inpaintings at 2 steps and change less across the step range.</p> </li> </ul> <p><strong>Interpretation.</strong> This experiment supports the “trajectory vs operator” narrative: once inference is amortized into learned jumps, the output becomes less tied to the solver resolution (step count). Importantly, Exp-I is <strong>not</strong> meant to separate CM vs FlowMap; at this level both are “jump” methods, so the main takeaway is <strong>jump distillation vs integration</strong>.</p> <h3 id="44-exp-ii-direct-vs-composed-jumps-composition-drift-diagnostic">4.4. Exp-II: Direct vs composed jumps (composition drift diagnostic)</h3> <p>Exp-I shows that jump distillation helps at low NFEs. Exp-II asks a stricter question tied to flow-map algebra: If we want to jump from $t$ to $s$, does it matter whether we do it directly, or via multiple intermediate hops?</p> <p><strong>Protocol Setup: direct vs $k$-hop composition</strong></p> <p>For each trial we sample a start time $t$, an end time \(s\approx 0\), and \(k-1\) intermediate times</p> \[t &gt; u_1 &gt; \dots &gt; u_{k-1} &gt; s.\] <p>We compare:</p> <ul> <li> <p><strong>Direct:</strong></p> \[x_s^{\text{dir}} = \Phi_{t\to s}(x_t)\] </li> <li> <p><strong>Composed:</strong></p> \[x_s^{\text{comp}} = \Phi_{u_{k-1}\to s}\circ\cdots\circ\Phi_{t\to u_1}(x_t)\] </li> </ul> <p>Ideally, for a valid flow map, <strong>Direct $\approx$ Composed</strong>. We define the <strong>Compositional Drift</strong> as the Mean Squared Error (MSE) between these two outputs in the latent space.</p> \[\Delta_k=\mathrm{MSE}_{\text{mask}}\!\left(x_s^{\text{dir}},x_s^{\text{comp}}\right), \qquad k\in \{2,4,6,8\}.\] <p><strong>Quantitative Result: Error Accumulation</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-map-learning/composition_boxplot_multi-480.webp 480w,/2026/assets/img/2026-04-27-flow-map-learning/composition_boxplot_multi-800.webp 800w,/2026/assets/img/2026-04-27-flow-map-learning/composition_boxplot_multi-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-flow-map-learning/composition_boxplot_multi.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 3. Composition drift $\Delta_k$ (masked latent MSE, log-scale) vs hop count. Flow-proxy (TCD) shows much smaller drift and slower growth than CM-proxy (LCM).</figcaption> </figure> <p>Figure 3 reveals a stark difference in algebraic properties:</p> <ul> <li> <strong>Flow Map (Left):</strong> The drift is minimal ($10^{-2}$ range) and grows slowly with hops. This indicates the model has learned a “straight” transport path where $\Phi_{t \to s}$ acts as a true semi-group.</li> <li> <strong>CM (Right):</strong> The drift is an order of magnitude larger ($10^{-1} \to 10^0$) and grows faster than Flow Map. Every time the CM “touches the boundary” ($x_0$) and comes back, it introduces prediction error that compounds rapidly.</li> </ul> <p><strong>Qualitative Result: Semantic Drift</strong></p> <p>To make the effect tangible, we also decode “direct vs composed” and visualize \(\|\Delta\|\) in image space (masked region only). Under ideal semigroup consistency (direct \(\approx\) composed), the residual map \(\|\Delta\|\) should be negligible (nearly black).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-flow-map-learning/composition_qual_examples-480.webp 480w,/2026/assets/img/2026-04-27-flow-map-learning/composition_qual_examples-800.webp 800w,/2026/assets/img/2026-04-27-flow-map-learning/composition_qual_examples-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-flow-map-learning/composition_qual_examples.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Figure 4. Qualitative examples of direct vs 8-hop composed jumps. Flow-proxy remains nearly invariant; CM-proxy shows visible structural drift, highlighted by brighter $\mid \Delta \mid$ maps.</figcaption> </figure> <ul> <li> <p><strong>CM Failure:</strong> The “CM Composed” image shows the object on the stool mutating into a black-and-white artifact. The $|\Delta|$ map is bright, indicating severe pixel-level deviation.</p> </li> <li> <p><strong>Flow Success:</strong> The “Flow Composed” image is visually closer to the “Direct” one. The $|\Delta|$ map is nearly black.</p> </li> </ul> <p><strong>Interpretation.</strong> This experiment operationalizes semigroup consistency: the flow-proxy behaves closer to a path-independent two-time operator, while the endpoint-anchored CM-proxy accumulates drift because each hop reprojects to an estimated \(x_0\) and then re-noises—small endpoint errors become amplified across hops.</p> <h2 id="5-conclusion">5. Conclusion</h2> <p>This post advocated a simple shift in abstraction: <strong>from integrating trajectories to learning operators</strong>. Instead of treating sampling as the repeated numerical solution of an ODE/SDE, we view generation as composing <strong>finite-time maps</strong>. The flow map family \(\{\Phi_{t\to s}\}\) is the minimal object that retains <em>control</em> (arbitrary start/stop times) while enabling <em>speed</em> (amortized jumps), and its algebraic structure—<strong>identity</strong> and the <strong>semigroup law</strong>—acts as the “physics” that distinguishes a genuine two-time operator from a boundary-only shortcut.</p> <p>We then showed how this operator view organizes a large body of recent work:</p> <ul> <li>Consistency Models emerge as <strong>boundary operators</strong> \(f_\theta(x_t,t,0)\), fast but structurally rigid.</li> <li>Flow Matching is the <strong>infinitesimal limit</strong> of a flow map, flexible but expensive at inference.</li> <li>MeanFlow and CTM can be interpreted as intermediate points that impose linearization or discrete composition constraints.</li> </ul> <p>On the training side, we derived two complementary distillation perspectives:</p> <ul> <li>The <strong>Lagrangian view (LMD)</strong> supervises <strong>how the endpoint moves</strong> with the target time $s$, anchoring the operator to the teacher’s dynamics.</li> <li>The <strong>Eulerian view (EMD)</strong> supervises <strong>how the operator evolves</strong> with the start time $t$, enforcing invariance under transport and connecting naturally to consistency-style objectives.</li> </ul> <p>Together, they suggest that “learning \(\Phi_{t\to s}\)” is not a single recipe but a <strong>design space</strong>: which derivatives you match (and how strongly you enforce composition) determines how well the learned operator behaves under chaining and editing.</p> <p>Finally, we proposed two concrete, falsifiable hypotheses for operator learning and tested them under a shared inpainting setup:</p> <ol> <li> <p><strong>Robustness to discretization:</strong> jump-based inference should be less sensitive to the step budget than integration-based sampling.</p> </li> <li> <p><strong>Algebraic consistency:</strong> if an operator is semigroup-consistent, <strong>direct</strong> and <strong>composed</strong> jumps should agree, and the discrepancy should grow slowly with hop count.</p> </li> </ol> <p>Even with practical proxies (LCM as endpoint-anchored, TCD as DDIM-coupled two-time jump), the experiments support the central narrative: <strong>operator-like jumps are step-robust, and two-time jumps exhibit substantially lower compositional drift than endpoint-anchored hops</strong>. This points to a useful takeaway beyond any specific method: <strong>“semigroup violation” can serve as a unit test for generative operators</strong>, especially in pipelines that repeatedly jump across time (editing, multi-stage refinement, or back-and-forth schedules).</p> <h3 id="limitations-and-outlook">Limitations and Outlook</h3> <p>Our study deliberately prioritized <em>structure</em> over <em>full-scale training</em>: we did not train a complete \(f_\theta(x_t,t,s)\) with explicit semigroup regularization. The next step is therefore clear: train true flow maps with (i) broad coverage over $(t,s)$, (ii) explicit composition constraints, and (iii) evaluation on tasks where chaining is unavoidable (e.g., iterative editing loops, guided refinement schedules, and long-horizon compositions). If successful, operator learning could offer a principled route to <strong>fast</strong> generation that remains <strong>composable</strong>, <strong>controllable</strong>, and <strong>stable under changing schedules</strong>—precisely where many endpoint-anchored distillations tend to break.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-flow-map-learning.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/general-agent-evaluation/">Ready For General Agents? Let's Test It.</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-vlms-waste-their-vision/">Why vlms waste their vision</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>