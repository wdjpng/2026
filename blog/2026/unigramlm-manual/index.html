<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> UnigramLM - An Attempt at Writing the Missing Manual | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="This post is my attempt to write down the UnigramLM tokenization algorithm cleanly and explicitly because, well, I still haven't found such a derivation and I think understanding the theory behind the method could help us make it better. I'll formalize the generative model around which the algorithm is based, derive the EM updates, explain why pruning is needed (and how it's done), and point out the spots where the practical implementation defined by the SentencePiece library diverges from the pretty mathematical models."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/unigramlm-manual/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "UnigramLM - An Attempt at Writing the Missing Manual",
            "description": "This post is my attempt to write down the UnigramLM tokenization algorithm cleanly and explicitly because, well, I still haven't found such a derivation and I think understanding the theory behind the method could help us make it better. I'll formalize the generative model around which the algorithm is based, derive the EM updates, explain why pruning is needed (and how it's done), and point out the spots where the practical implementation defined by the SentencePiece library diverges from the pretty mathematical models.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>UnigramLM - An Attempt at Writing the Missing Manual</h1> <p>This post is my attempt to write down the UnigramLM tokenization algorithm cleanly and explicitly because, well, I still haven't found such a derivation and I think understanding the theory behind the method could help us make it better. I'll formalize the generative model around which the algorithm is based, derive the EM updates, explain why pruning is needed (and how it's done), and point out the spots where the practical implementation defined by the SentencePiece library diverges from the pretty mathematical models.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#intro-and-origins-of-this-blog-post">Intro and origins of this blog post</a> </div> <div> <a href="#tokenization-background-and-notation">Tokenization Background and Notation</a> </div> <div> <a href="#what-you-came-here-for-unigramlm">What you came here for - UnigramLM</a> </div> <ul> <li> <a href="#generative-model">Generative model</a> </li> <li> <a href="#inference">Inference</a> </li> <li> <a href="#learning-model-parameters">Learning Model Parameters</a> </li> <li> <a href="#the-unigramlm-algorithm">The UnigramLM Algorithm</a> </li> </ul> <div> <a href="#implementation-in-the-sentencepiece-library">Implementation in the SentencePiece library</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <p><strong>TL;DR</strong>: This post is my attempt to write down the UnigramLM tokenization algorithm cleanly and explicitly because, well, I still haven’t found such a derivation and I think understanding the theory behind the method could help us make it better. I’ll formalize the generative model around which the algorithm is based, derive the EM updates, explain why pruning is needed (and how it’s done), and point out the spots where the practical implementation defined by the SentencePiece library diverges from the pretty mathematical models. I hope this post provides a new lens through which to look at the UnigramLM tokenization algorithim while pointing out some interesting potential extensions/revisions to the current implementation.</p> <h3 id="intro-and-origins-of-this-blog-post">Intro and origins of this blog post</h3> <p><em>(feel free to <a href="#sec:background">skip</a> this section)</em></p> <p>These days, tokenization is basically synonymous with Byte-pair Encoding (BPE). If you ask someone “do you know how tokenization works?”, there’s a decent chance you’ll get an answer like: “Yeah yeah, I know BPE.” But tokenization != BPE. There are numerous (arguably better motivated) algorithms one could use for segmenting text into tokens. This post focuses on UnigramLM (the SentencePiece “unigram” model), which is a pretty far departure from the BPE approach…</p> <h4 id="why-look-at-unigramlm-now-and-not-just-make-bpe-better">Why look at UnigramLM now (and not just “make BPE better”)?</h4> <p>Recent work keeps showing that tokenizers themselves can induce <a href="https://arxiv.org/abs/2305.15425" rel="external nofollow noopener" target="_blank">unfairness</a> and <a href="https://arxiv.org/abs/2305.17179" rel="external nofollow noopener" target="_blank">uneven performance</a> across languages, dialects, and writing systems. A lot of the community response has (reasonably!) focused on patching BPE: adding constraints, regularizers, or parity-aware merges. Those are valuable, but there’s a risk in treating “tokenization = BPE + tweaks” as the whole design space. UnigramLM is a widely deployed alternative (T5, XLNet), and it comes from a fundamentally different modeling viewpoint. Instead of greedily merging pairs, it says: “let’s uncover latent tokens and treat tokenization like inference.” At least to me, that framing feels a lot more linguistically sane (or, at minimum, less like we’re playing subword Tetris). Taking that viewpoint seriously could open different and maybe cleaner directions for addressing tokenizer-induced unfairness—not by iterating on one algorithm forever, but by re-examining the assumptions we bake into tokenization in the first place.</p> <h4 id="why-this-blog-post">Why this blog post</h4> <p>With the above motivation in mind, I figured I should actually understand the algorithm. So I did what everyone does: I went to the <a href="https://aclanthology.org/P18-1007/" rel="external nofollow noopener" target="_blank">original 2018 paper</a>. That… didn’t get me very far. So then I went to the SentencePiece repo, hoping I could reconstruct the missing pieces from the code. After a brief flashback to the terror of my undergraduate CS classes while staring at the C++ implementation, I bailed on that approach too. Then I thought maybe the missing explanation was hiding in the HuggingFace documentation. But let’s just say that rabbit hole ended like this:</p> <blockquote> <p><em>The HuggingFace documentation</em> [on UnigramLM] <em>describes a tokeniser that doesn’t exist. It should not be relied on as an explanation for UnigramLM, because it doesn’t even come close.</em><br> –Claude</p> </blockquote> <p>The original UnigramLM paper gives a nice high-level story, and the code clearly works in practice, but I couldn’t find a single place that actually spells out the full generative model, why the algorithm is mathematically sound, or how all the little “engineering details” (like pruning and vocabulary initialization) fit into that picture. This post is my attempt to provide an approachable but rigorous walkthrough of UnigramLM as a probabilistic model, showing why EM is a reasonable tool here, what the posterior over segmentations actually looks like, and how the SentencePiece-style implementation approximates/diverges from all of this in practice. If you’ve ever felt that UnigramLM is “clear enough to use, but not clear enough to explain on a whiteboard,” my hope is that this takes you the rest of the way to really understanding it, and maybe even extending it. Because at least I think its a pretty cool algorithm that deserves some of BPE’s limelight.</p> <h2 id="sec:background">Tokenization Background and Notation</h2> <p>So that we’re on the same page, let’s start with a formal definition of tokenization.</p> <p>Let \(\mathbf{s}=\langle s_1, s_2,\dots\rangle\) be a string—a sequence of characters (or bytes) such that \(s_t\in\Sigma\) for a base alphabet \(\Sigma\). Let \(\mathcal{V}\) be a finite set, where each \(v\in\mathcal{V}\) consists of a sequence of symbols from \(\Sigma\cup\Gamma\), where \(\Gamma\) denotes a finite set of reserved symbols (e.g., whitespace markers, start/end tokens, etc.); we refer to \(\mathcal{V}\) as our <strong>vocabulary</strong> and to \(v\) as <strong>pieces</strong>.<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> During tokenization, we wish to convert the sequence of characters/bytes \(\mathbf{s}\) into a sequence of tokens \(\mathbf{v}=\langle v_1,\dots,v_m\rangle\), each of which is a piece in the set \(\mathcal{V}\). We refer to this token sequence as a <strong>segmentation</strong> of \(\mathbf{s}\), and it can informally be seen as just a different way of representing the original string.</p> <p>A tokenization algorithm defines a mapping \(h: \Sigma^* \rightarrow \mathcal{V}^*\) and the method for learning the parameters of this mapping. The application of \(h\) (which we’ll call our <strong>tokenization function</strong> here) to a string is sometimes referred to as inference, although perhaps more commonly people just call this process “tokenizing a string.” For example, the byte-pair encoding (BPE) algorithm defines a \(h\) that is parameterized by a list of <em>merge</em> pairs \(\boldsymbol{\mu}=\langle(v_1, v_1'),(v_2, v_2'), \dots \rangle\) and the algorithm for learning \(\boldsymbol{\mu}\). At inference, starting from the representation of \(\mathbf{s}\) as just a sequence of symbols from the base vocabulary \(\Sigma\), \(h_{\boldsymbol{\mu}}\) goes through the text \(i=1, \dots \lvert\boldsymbol{\mu}\rvert\) times. At step \(i\), it replaces all co-occurrences of the pair \((v_i, v_i')\) with a new merged token (typically, of the form \(v_i\circ v_i'\)).<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">2</a></sup></p> <p>Importantly, we assume that \(\mathbf{s}\) can be reconstructed from \(\mathbf{v}\) via a <strong>detokenization function</strong> \(g: \mathcal{V}^* \rightarrow \Sigma^*\); often \(g\) is a simple mapping like string concatenation with some special symbol handling, e.g., \(g(\mathbf{v}) = v_1\circ\dots \circ v_m\). In what follows, we consider \(g\) fixed and treat it as part of the model specification. All probabilities over strings and segmentations are defined with respect to this fixed choice of \(g\). Notably, given just the vocabulary \(\mathcal{V}\), there are often multiple valid \(\mathbf{v}\) for which the application of our simple detokenization function \(g\) would lead to the same \(\mathbf{s}\). In other words, \(g\) is generally non-injective. We use \(\mathcal{T}_{\mathcal{V}}(\mathbf{s}) \mathrel{\stackrel{\textnormal{def}}{=}}g^{-1}(\mathbf{s}) = \{\mathbf{v}\in\mathcal{V}^* : g(\mathbf{v}) = \mathbf{s}\}\) to refer to the set of all valid token sequences that produce \(\mathbf{s}\), i.e., the set-valued inverse of \(g\).</p> <p><strong>Example 1</strong> (A concrete example of the non-injectivity of \(g\).). <em>Consider a toy string \(\mathbf{s}= \text{hat}\) and a small vocabulary \(\mathcal{V}= \{\text{h},\text{a},\text{t},\text{ha},\text{at}\}\). Under our fixed detokenization function \(g\) (simple concatenation of token symbol sequences), the set of all valid segmentations of \(\mathbf{s}\) is</em></p> \[\begin{aligned} \mathcal{T}_{\mathcal{V}}(\mathbf{s}) =\{ \langle \text{h}, \text{a}, \text{t} \rangle, \langle \text{ha}, \text{t} \rangle, \langle \text{h}, \text{at} \rangle\}. \end{aligned}\] <p><em>where all three segmentations detokenize to the same string \(\mathbf{s}= \text{hat}\) under \(g\).</em></p> <p>While it might not seem notable, the non-injectivity of $g$ is actually an interesting property of most tokenization schemes. For one, it’s motivated several variants of different tokenization algorithms in which the inference rule — the mapping $h:\Sigma^*\rightarrow\mathcal{V}^*$ that selects a particular element of $\mathcal{T}_{\mathcal{V}}(\mathbf{s})$ — is replaced or redefined, for example by sampling from a posterior over segmentations<d-cite key="kudo-2018-subword"></d-cite> or by changing the inference objective to something like minimizing token sequence length<d-cite key="hofmann-etal-2022-embarrassingly"></d-cite><d-cite key="schmidt-etal-2024-tokenization"></d-cite>. It also means that we should distinguish between the <strong>canonical tokenization</strong> of $\mathbf{s}$, which is $h(\mathbf{s})$, and any other valid segmentation $\mathbf{v} \in \mathcal{T}_{\mathcal{V}}(\mathbf{s})$ with $\mathbf{v} \neq h(\mathbf{s})$, which are typically called <strong>non-canonical tokenizations</strong>. The existence of non-canonical tokenizations has implications for how one should actually compute the probability of a string under a language model using a given vocabulary. See Cao and Rimell (2021)<d-cite key="cao-rimell-2021-evaluate"></d-cite> for a more detailed discussion of non-canonical tokenizations and why they matter in practice.</p> <h2 id="what-you-came-here-for---unigramlm">What you came here for - UnigramLM</h2> <p>The UnigramLM tokenization algorithm<d-cite key="kudo-2018-subword"></d-cite> takes a probabilistic-modeling approach to string tokenization. It defines an \(h\), together with an algorithm for learning its parameters, by treating tokenization as inference in a latent-variable generative model over strings—in particular, a unigram generative model.</p> <p><strong>Few Sentence Description of UnigramLM</strong>: UnigramLM is basically what it sounds like: a unigram language model. The only parameters of the tokenization scheme are a unigram probability distribution. When learning the tokenizer, we learn both the vocabulary and piece probabilities of this unigram model that (approximately) maximize corpus log-likelihood. At inference time, given a string, UnigramLM chooses the segmentation (sequence of pieces) that has the highest probability under this learned unigram model. In contrast to BPE’s greedy merge story, UnigramLM’s behavior is really “whichever segmentation makes the whole corpus most probable under this unigram model wins.”</p> <h3 id="sec:gen_model">Generative model</h3> <p>The UnigramLM tokenization algorithm assumes that each observed string \(\mathbf{s}\) arises from a latent sequence of tokens \(\mathbf{v}\), where tokens are drawn independently from a fixed probability distribution, i.e., token sequences are produced by a unigram language model. The data-generating distribution can thus be defined in terms of the unigram probabilities \(\boldsymbol{\phi}\in \Delta^{\lvert\mathcal{V}\rvert - 1}\). Before we get to the definition of the data-generating distribution though, we have to establish some other definitions.</p> <p><strong>Warning about notation:</strong> To reduce the number of nested subscripts (and other similarly offensive notational choices), I’m going to primarily use random variables to describe this problem. Don’t worry, you’ll still get a nice sprinkling of nested subscripts even with the random variables! Just fewer than without. Sorry… As is standard, uppercase letters will denote random variables (e.g., \(X\), \(Z\)), and bold uppercase letters will denote sequences of them (e.g., \(\mathbf X\), \(\mathbf Z\)).</p> <p>Formally, let \(V\) be our token-valued random variable: a categorical random variable on \(\mathcal{V}\) with \(\sum_{v\in\mathcal{V}}P(V=v;\boldsymbol{\phi})=1\). Occasionally for shorthand, we’ll use \(\phi_v= P(V=v;\boldsymbol{\phi})\) to refer to the unigram probability of the piece \(v\). Let \(\mathbf{V}\) be a random variable taking values in the space of token <em>sequences</em> \(\mathbf{v}\in \mathcal{V}^*\). For the distribution of \(\mathbf{V}\) to be a valid probability distribution on \(\mathcal{V}^*\), we must specify a length prior, i.e., a random variable \(M\) on \(\mathbb{N}\) with \(\sum_{m=0}^\infty P(M=m)=1\).<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> The UnigramLM algorithm then assumes token sequence \(\mathbf{v}=\langle v_1,\dots,v_m\rangle\) are generated as</p> \[m\sim M,\quad v_t\stackrel{\text{i.i.d.}}{\sim} {\small\mathrm{Categorical}}(\boldsymbol{\phi}) \quad (t=1,\dots,m) \tag{1}\] <p>We can thus define the distribution of \(\mathbf{V}\) as</p> \[P(\mathbf{V}=\mathbf{v};\boldsymbol{\phi}) \mathrel{\stackrel{\textnormal{def}}{=}} P(M=\lvert\mathbf{v}\rvert)\prod_{t=1}^{\lvert\mathbf{v}\rvert}P(V=v_t;\boldsymbol{\phi}) \tag{2}\] <p>The likelihood of a sequence conditional on a given length \(m\) is then simply the product of its piece probabilities, i.e., Eq. (2) where the length prior term cancels out:</p> \[P(\mathbf{V}=\mathbf{v}\mid M=m;\boldsymbol{\phi}) = \prod_{t=1}^m P(V=v_t;\boldsymbol{\phi}), \tag{3}\] <h4 id="a-sidebar-on-the-length-prior">A sidebar on the length prior.</h4> <p>\(M\) is pretty much always ignored. I.e., people effectively assume the probability of a sequence being any particular length is constant across all valid lengths and compute token sequence probabilities using Eq. (3). This include the original version of the UnigramLM algorithm, so one could argue that the description given by the original paper and naming this thing “Unigram LM” is slightly misleading: it doesn’t define a valid language model over strings without the length prior. I’ll keep using the length prior in my definitions throughout this exposition so that you can get a sense for how it would have affected the algorithm. Small note: while the parameters of \(P(V;\boldsymbol{\phi})\) are completely specified by \(\boldsymbol{\phi}\), this isn’t the case with \(P(\mathbf{V};\boldsymbol{\phi})\), for which the parameters of \(M\) must also be known to fully specify an actual Unigram LM distribution. I won’t add any additional notation to \(P(\mathbf{V};\boldsymbol{\phi})\) to specify the parameters of \(M\), though, to avoid clutter and because it’s often ignored anyway. <strong>Potential research direction: Exploring the effect of re-including the length prior in the UnigramLM model.</strong> Given that compression is such a desirable property of tokenization, it seems to me that a parameter that influences the length of the chosen segmentation (we could use the length prior to bias segmentations towards shorter lengths) would be something worthwhile playing around with.</p> <p>Given the deterministic mapping \(g\) from tokens to strings, we can derive the distribution over strings—our data-generating distribution—as a pushforward of the distribution over tokens. Let \(\mathbf{S}\) be a random variable on \(\Sigma^*\). The following relationship holds:</p> \[\begin{aligned} P(\mathbf{S}=\mathbf{s};\boldsymbol{\phi}) \mathrel{\stackrel{\textnormal{def}}{=}}\sum_{\mathbf{v}\in \mathcal{T}_{\mathcal{V}}(\mathbf{s})} P(\mathbf{V}=\mathbf{v};\boldsymbol{\phi}) \end{aligned} \tag{4}\] <h4 id="some-useful-relationships-between-mathbfv-and-mathbfs">Some useful relationships between \(\mathbf{V}\) and \(\mathbf{S}\).</h4> <p>We can see from Eq. (4) that distribution of \(\mathbf{S}\) is simply the marginal probability distribution over valid segmentations of \(\mathbf{s}\) under \(\mathcal{V}\). Applying Bayes’ rule then gives us the posterior over segmentations for a fixed \(\mathbf{s}\):</p> \[P(\mathbf{V}=\mathbf{v}\mid \mathbf{S}=\mathbf{s}; \boldsymbol{\phi}) = \begin{cases} &amp;\frac{P(\mathbf{V}=\mathbf{v};\boldsymbol{\phi})}{P(\mathbf{S}=\mathbf{s};\boldsymbol{\phi})} \quad \text{if } \mathbf{v}\in\mathcal{T}_{\mathcal{V}}(\mathbf{s})\\ &amp;0 \quad \quad \text{ otherwise.} \end{cases} \tag{5}\] <p>By just moving some terms in Eq. (5) around, we also get the definition of the joint distribution over strings and token sequences:</p> \[P(\mathbf{S}=\mathbf{s}, \mathbf{V}=\mathbf{v};\boldsymbol{\phi}) = P(\mathbf{V}=\mathbf{v};\boldsymbol{\phi})\mathbb{1}\{\mathbf{v}\in\mathcal{T}_{\mathcal{V}}(\mathbf{s})\},\] <h3 id="inference">Inference</h3> <p>For the moment, let’s assume that we know \(\boldsymbol{\phi}\), or at least have estimates for these parameters. At inference time (i.e., when segmenting text into tokens), the UnigramLM tokenization algorithm aims to find the most likely segmentation of \(\mathbf{s}\) into tokens \(\mathbf{v}= \langle v_1, v_2, \dots\rangle\) under the generative model (defined above) with these parameters. To this end, it uses a Viterbi-style algorithm:</p> \[\begin{aligned} h_{\boldsymbol{\phi}}(\mathbf{s})&amp;= \arg\max_{\mathbf{v}\in \mathcal{T}_{\mathcal{V}}(\mathbf{s})} P(\mathbf{V}=\mathbf{v}\mid \mathbf{S}=\mathbf{s}; \boldsymbol{\phi})\\ &amp;= \arg\max_{\mathbf{v}\in \mathcal{T}_{\mathcal{V}}(\mathbf{s})} P(\mathbf{V}=\mathbf{v};\boldsymbol{\phi})\\ &amp;= \arg\max_{\mathbf{v}\in \mathcal{T}_{\mathcal{V}}(\mathbf{s})} P(M=\lvert\mathbf{v}\rvert)\prod_{t=1}^{\lvert\mathbf{v}\rvert}P(V=v_t;\boldsymbol{\phi})\\ &amp;\overset{?}{=} \arg\max_{\mathbf{v}\in \mathcal{T}_{\mathcal{V}}(\mathbf{s})} \prod_{t=1}^{\lvert\mathbf{v}\rvert}P(V=v_t;\boldsymbol{\phi}) \end{aligned} \tag{6}\] <p>where the second line follows from the relationship in Eq. (5) (\(P(\mathbf{S}=\mathbf{s};\boldsymbol{\phi})\) does not depend on \(\mathbf{v}\) and so it doesn’t affect the argmax).</p> <h4 id="another-sidebar-on-the-length-prior">Another sidebar on the length prior.</h4> <p>As we can see in Eq. 6, the length prior (\(M\)) is part of the posterior distribution and should thus affect the Viterbi segmentation; intuitively speaking, it biases the distribution towards token sequences of certain lengths.</p> <p><strong>Example 2</strong> (Effect of the length prior on Viterbi segmentation). <em>Suppose a string \(\mathbf{s}\) admits two valid segmentations \(\mathbf{v}^{(1)}\) and \(\mathbf{v}^{(2)}\) under \(\mathcal{V}\), with lengths \(\lvert\mathbf{v}^{(1)}\rvert = 1\) and \(\lvert\mathbf{v}^{(2)}\rvert = 3\). Assume that the unigram probabilities are such that</em></p> \[\prod_{t=1}^{\lvert\mathbf{v}^{(1)}\rvert} P(V=v^{(1)}_t;\boldsymbol{\phi}) = \prod_{t=1}^{\lvert\mathbf{v}^{(2)}\rvert} P(V=v^{(2)}_t;\boldsymbol{\phi})\] <p><em>so the two segmentations tie if we ignore the length prior. Now let the length prior favor shorter sequences, e.g.</em></p> \[P(M=1) = 0.9, \qquad P(M=3) = 0.1\] <p><em>Then the full sequence probabilities become</em></p> \[\begin{aligned} P(\mathbf{V}=\mathbf{v}^{(1)};\boldsymbol{\phi}) &amp;= P(M=1) \prod_{t=1}^{\lvert\mathbf{v}^{(1)}\rvert} P(V=v^{(1)}_t;\boldsymbol{\phi}) = 0.9 \cdot C,\\ P(\mathbf{V}=\mathbf{v}^{(2)};\boldsymbol{\phi}) &amp;= P(M=3) \prod_{t=1}^{\lvert\mathbf{v}^{(2)}\rvert} P(V=v^{(2)}_t;\boldsymbol{\phi}) = 0.1 \cdot C, \end{aligned}\] <p><em>for some common factor \(C\). The Viterbi segmentation under the full model (including the length prior) is therefore \(\mathbf{v}^{(1)}\), while under the approximation that drops \(P(M=\cdot)\), the two segmentations are equally probable. This illustrates that the length prior can in principle have a non-trivial affect on the inference result.</em></p> <p>As hinted at earlier, SentencePiece (and all other implementations of UnigramLM that I’ve seen) drop the length prior term. Unless otherwise specified, when talking about inference, we will assume use of \cref{eq:approx-inference} for faithfulness to the original algorithm.</p> <p>The true parameters of the generative process \(\boldsymbol{\phi}\) are unknown, however; this includes both the piece probabilities \(\phi_v\) and the underlying vocabulary \(\mathcal{V}\) over which they are defined. The UnigramLM tokenization algorithm (described next) proposes a method for coming up with an estimate of these parameters from text data.</p> <h3 id="learning-model-parameters">Learning Model Parameters</h3> <p>Maximum likelihood estimation (MLE)—a standard approach to estimating model parameters—aims to find the model parameters that maximize the log-likelihood of our data. Under the UnigramLM assumptions about the generative process of strings, our “complete” dataset actually consists of \((\mathbf{s},\mathbf{v})\) pairs, i.e., strings and the sequence of tokens that produced them. Thus, our complete dataset looks like \(\mathcal{X} = \{(\mathbf{s}_i,\mathbf{v}_i)\}_{i=1}^K\) and the complete-data log likelihood is defined as:</p> \[\begin{aligned} \mathcal{L}(\mathcal{X}; \boldsymbol{\phi}) &amp;\mathrel{\stackrel{\textnormal{def}}{=}}\log\prod_{i=1}^KP(\mathbf{S}=\mathbf{s}_i, \mathbf{V}=\mathbf{v}_i;\boldsymbol{\phi})\\ &amp;= \sum_{i=1}^K\log P(\mathbf{S}=\mathbf{s}_i, \mathbf{V}=\mathbf{v}_i;\boldsymbol{\phi}) \end{aligned} \tag{7}\] <p>Eq. (7) is typically referred to as the <em>complete</em> data log-likelihood. If we actually had this complete data (and we knew \(\mathcal{V}\)), we would simply find the \(\boldsymbol{\phi}\) that maximizes Eq. (7), which would be a fairly clean problem that is easy to solve given our assumptions about the underlying distributions. However, we only see the “post-processed” strings \(\mathbf{s}= g(\mathbf{v})\); the exact underlying pieces that form that string are unknown (can be any in \(\mathcal{T}_{\mathcal{V}}(\mathbf{s})\) and we don’t even know \(\mathcal{V}\)!). So, we can instead try to maximize our <em>observed</em> data log-likelihood, i.e., the likelihood of just our strings under our data-generating distribution defined in Eq. (4). Given our “useful” relationships in from earlier, we can define this likelihood in terms of \(\boldsymbol{\phi}\):</p> \[\begin{aligned} \mathcal{L}(\mathcal{C}; \boldsymbol{\phi}) &amp;\mathrel{\stackrel{\textnormal{def}}{=}}\log\prod_{i=1}^KP(\mathbf{S}=\mathbf{s}_i;\boldsymbol{\phi})\\ &amp;= \log\prod_{i=1}^K\sum_{\mathbf{v}\in \mathcal{T}_{\mathcal{V}}(\mathbf{s}_i)} P(\mathbf{V}=\mathbf{v};\boldsymbol{\phi}) \\ &amp;= \sum_{i=1}^K \log\sum_{\mathbf{v}\in\mathcal{T}_{\mathcal{V}}(\mathbf{s}_i)} P(\mathbf{V}=\mathbf{v};\boldsymbol{\phi}) \end{aligned} \tag{8}\] <p>where \(\mathcal{C}= \{\mathbf{s}\mid \mathbf{s}, \_ \in \mathcal{X} \}\) is simply our observed set of strings, i.e., our corpus. Unfortunately, Eq. (8) is a difficult quantity to maximize directly due to the log–sum structure. Luckily, the expectation-maximization (EM) algorithm provides us a route for working with this situation.</p> <h3 id="sec:unigram_em">The Expectation-Maximization Algorithm in the Context of UnigramLM</h3> <p>EM was designed for exactly the use case where wish to get MLE estimates for a data-generating process in which only part of the data is unobserved.</p> <p><strong>TL;DR of the application of the EM algorithm to UnigramLM</strong>: EM is an iterative algorithm for approximating MLE estimates. The E step computes the expected complete data log-likelihood under current beliefs about model parameters (in our case, \(\boldsymbol{\phi}^{(n)}\)); this quantity is standing in for observed data log-likelihood, which is a much more difficult quantity to compute. The M step then solves for the free parameters (in our case, \(\boldsymbol{\phi}\)) that maximize this quantity, and then updates our current beliefs to the new quantity.</p> <p>In more detail now: the EM algorithm uses Jensen’s inequality to relate the <em>expected value</em> of the complete data log-likelihood to the <em>observed</em> data log-likelihood, i.e., relating the expected value of Eq. (7) to Eq. (8). This is exactly the connection made by Kudo (2018)<d-cite key="kudo-2018-subword"></d-cite> (even if not explicitly) when introducing their algorithm for approximating the parameters \(\boldsymbol{\phi}\).</p> <p><strong>Expected complete-data log-likelihood under observed data and current parameters.</strong></p> <p>Let \(\boldsymbol{\phi}^{(n)}\) denote our current belief about what the unigram parameters might be (more discussion on how we can initialize this distribution coming up!). For now, we will assume that the vocabulary is fixed. These random variables adhere to our original definitions in <a href="#sec:gen_model">4.1</a>{reference-type=”ref+label” reference=”sec:gen_model”}. Note that when we use simply \(\boldsymbol{\phi}\), we are referring to the distributions (and corresponding random variables) induced by a generic \(\boldsymbol{\phi}\); these are the entities for which our parameters are free variables that we are optimizing.</p> <p>The expected complete data log-likelihood under \(\boldsymbol{\phi}^{(n)}\)—which we denote as \(\mathcal{Q}(\boldsymbol{\phi};\boldsymbol{\phi}^{(n)})\)—follows simply from taking the expectated value of Eq. (7), given our observed data \(\mathcal{C}\) and our current model parameters \(\boldsymbol{\phi}^{(n)}\), i.e., the expected value under the posterior \(\mathbf{V}\mid \mathbf{S};\boldsymbol{\phi}^{(n)}\).</p> \[\begin{aligned} \mathcal{Q}(\boldsymbol{\phi};\boldsymbol{\phi}^{(n)}) &amp;\mathrel{\stackrel{\textnormal{def}}{=}} \mathop{\mathrm{\mathbb{E}}} \big[\mathcal{L}(\mathcal{X}; \boldsymbol{\phi}) \mid \mathcal{C}, \boldsymbol{\phi}^{(n)}\big]\\ &amp;= \underset{ \mathbf{V}\mid \mathbf{S};\boldsymbol{\phi}^{(n)}}{\mathop{\mathrm{\mathbb{E}}}}\big[\sum_{i=1}^K \log P(\mathbf{S}, \mathbf{V};\boldsymbol{\phi})\mid \mathcal{C}\big]\\ &amp;= \sum_{i=1}^K\underset{\mathbf{v}\sim\mathbf{V}\mid \mathbf{S}=\mathbf{s}_i;\boldsymbol{\phi}^{(n)}}{\mathop{\mathrm{\mathbb{E}}}}\big[\log P(\mathbf{S}=\mathbf{s}_i, \mathbf{V}=\mathbf{v};\boldsymbol{\phi})\big] \end{aligned}\] <p>In words, we can think of \(\mathcal{Q}(\boldsymbol{\phi};\boldsymbol{\phi}^{(n)})\) as the expected complete data log-likelihood where the (latent) segmentations are induced by the posterior with parameters \(\boldsymbol{\phi}^{(n)}\), while the log-likelihood inside is evaluated using the candidate parameters \(\boldsymbol{\phi}\).</p> <p>Now we will show how this quantity relates to the observed data log-likelihood.</p> <h4 id="observed-data-log-likelihood-and-jensens-inequality">Observed data log-likelihood and Jensen’s inequality.</h4> <p>We start with a reminder of Jensen’s inequality, applied to our definition of \(P(\mathbf{S}=\mathbf{s};\boldsymbol{\phi})\). For any valid distribution probability \(P(\mathbf{V}=\mathbf{v})\), Jensen’s inequality tells us</p> \[\begin{aligned} \log P(\mathbf{S}=\mathbf{s};\boldsymbol{\phi}) &amp;= \log \sum_{\mathbf{v}\in \mathcal{T}_{\mathcal{V}}(\mathbf{s})} P(\mathbf{V}=\mathbf{v})\,\frac{P(\mathbf{V}=\mathbf{v};\boldsymbol{\phi})}{P(\mathbf{V}=\mathbf{v})}\\ &amp;\ge \sum_{\mathbf{v}\in \mathcal{T}_{\mathcal{V}}(\mathbf{s})} P(\mathbf{V}=\mathbf{v})\,\log \frac{P(\mathbf{V}=\mathbf{v};\boldsymbol{\phi})}{P(\mathbf{V}=\mathbf{v})} \end{aligned}\] <p>If we choose \(P(\mathbf{V}=\mathbf{v})\) to be \(P(\mathbf{V}=\mathbf{v}\mid \mathbf{S}= \mathbf{s};\boldsymbol{\phi}^{(n)})\)—the posterior under our current parameter beliefs for a fixed \(\mathbf{s}\)—and apply this to our definition of the observed data log-likelihood from Eq. (8), we get</p> \[\begin{aligned} \mathcal{L}&amp;(\mathcal{C};\boldsymbol{\phi})= \sum_{i=1}^K \log\sum_{\mathbf{v}\in\mathcal{T}_{\mathcal{V}}(\mathbf{s}_i)} P(\mathbf{V}=\mathbf{v};\boldsymbol{\phi})\\ &amp;\ge \sum_{i=1}^K\sum_{\mathbf{v}\in \mathcal{T}_{\mathcal{V}}(\mathbf{s}_i)}P(\mathbf{V}=\mathbf{v}\mid \mathbf{S}=\mathbf{s}_i;\boldsymbol{\phi}^{(n)}) \big[\log P(\mathbf{V}=\mathbf{v};\boldsymbol{\phi})-\log P(\mathbf{V}=\mathbf{v}\mid \mathbf{S}=\mathbf{s}_i;\boldsymbol{\phi}^{(n)})\big]\\ &amp;= \sum_{i=1}^K\sum_{\mathbf{v}\in \mathcal{T}_{\mathcal{V}}(\mathbf{s}_i)}P(\mathbf{V}=\mathbf{v}\mid \mathbf{S}=\mathbf{s}_i;\boldsymbol{\phi}^{(n)}) \log P(\mathbf{S}=\mathbf{s}_i, \mathbf{V}=\mathbf{v};\boldsymbol{\phi})\nonumber\\ &amp; \qquad\qquad\qquad\qquad-\sum_{i=1}^K\sum_{\mathbf{v}\in \mathcal{T}_{\mathcal{V}}(\mathbf{s}_i)}P(\mathbf{V}=\mathbf{v}\mid \mathbf{S}=\mathbf{s}_i;\boldsymbol{\phi}^{(n)})\log P(\mathbf{V}=\mathbf{v}\mid \mathbf{S}=\mathbf{s}_i;\boldsymbol{\phi}^{(n)})\\ &amp;= \underbrace{\sum_{i=1}^K\underset{\mathbf{v}\sim \mathbf{V}\,\mid\, \mathbf{S}=\mathbf{s}_i;\boldsymbol{\phi}^{(n)}}{\mathop{\mathrm{\mathbb{E}}}}\big[\log P(\mathbf{S}=\mathbf{s}_i, \mathbf{V}=\mathbf{v};\boldsymbol{\phi})\big]}_{\mathcal{Q}(\boldsymbol{\phi};\boldsymbol{\phi}^{(n)})} + \sum_{i=1}^K\underbrace{\underset{\mathbf{v}\sim \mathbf{V}\,\mid\, \mathbf{S}=\mathbf{s}_i;\boldsymbol{\phi}^{(n)}}{\mathop{\mathrm{\mathbb{E}}}}\big[\log P(\mathbf{V}=\mathbf{v}\mid \mathbf{S}=\mathbf{s}_i;\boldsymbol{\phi}^{(n)})\big]}_{\mathrm{H}\big(\mathbf{V}\,\mid \,\mathbf{S}=\mathbf{s}_i;\boldsymbol{\phi}^{(n)}\big)}\\ &amp;\geq \mathcal{Q}(\boldsymbol{\phi};\boldsymbol{\phi}^{(n)}) \end{aligned} \tag{9}\] <p>Note that when going from the second to third lines in Eq. (9), we make use of the fact that for any \(\mathbf{v}\in \mathcal{T}_{\mathcal{V}}(\mathbf{s}_i)\) we have \(P(\mathbf{S}=\mathbf{s}_i, \mathbf{V}=\mathbf{v};\boldsymbol{\phi}) = P(\mathbf{V}=\mathbf{v};\boldsymbol{\phi})\) by definition. Then, we’re simply using the equivalence of these values with the definitions of expected values and (Shannon) entropy, respectively.</p> <p>Eq. (9) is typically referred to as the evidence lower bound (ELBO)—a proxy objective that is often used in machine learning. For example, it’s used for training variational autoencoders, where it provides a tractable lower bound on the intractable log-likelihood of the data under a latent-variable model. In the case of EM, we go one step further and use one of the components of the ELBO as our proxy objective for observed data log-likelihood: the expected complete data log-likelihood \(\mathcal{Q}(\boldsymbol{\phi};\boldsymbol{\phi}^{(n)})\). And this is the basis the EM algorithm, which iteratively updates \(\boldsymbol{\phi}\) by choosing the value of it that maximizes \(\mathcal{Q}(\boldsymbol{\phi};\boldsymbol{\phi}^{(n)})\) until convergence.</p> <p>After all those derivations, I do think it’s helpful to look at our ideal and actual objectives side-by-side, just to see what the difference is:</p> \[\underbrace{\sum_{i=1}^K \log\sum_{\mathbf{v}\in\mathcal{T}_{\mathcal{V}}(\mathbf{s}_i)} P(\mathbf{V}=\mathbf{v};\boldsymbol{\phi})}_{\text{objective we'd ideally be maximizing }} \qquad\qquad \underbrace{\sum_{i=1}^K\sum_{\mathbf{v}\in \mathcal{T}_{\mathcal{V}}(\mathbf{s}_i)}P(\mathbf{V}=\mathbf{v}\mid \mathbf{S}=\mathbf{s}_i;\boldsymbol{\phi}^{(n)}) \log P(\mathbf{V}=\mathbf{v};\boldsymbol{\phi})}_{\text{objective we maximize iteratively with EM}}\] <p>And while the solution given by EM should converge to the solution we’d get from maximizing our ideal object, this isn’t the case with the UnigramLM algorithm since it’s not pure EM in isolation. I explain this next.</p> <h3 id="the-unigramlm-algorithm">The UnigramLM Algorithm</h3> <p>The UnigramLM algorithm is typically seen as a “simple” application of EM. This, however, is not exactly the case. Importantly, EM assumes that the support of the distribution whose parameters we’re trying to estimate is known (and fixed), i.e., that we know \(\mathcal{V}\). But, as discussed earlier, we don’t know \(\mathcal{V}\)! The UnigramLM algorithm addresses this by beginning with an intentionally overcomplete initial vocabulary and progressively reducing it through a heuristic pruning step, which is done <em>after</em> an iteration of the standard E-step and M-step, throughout which \(\mathcal{V}\) is held fixed. In short, as the algorithm iteratively re-estimates the model parameters, it gradually shrinks \(\mathcal{V}\) toward the desired final size by removing pieces that are seemingly unimportant for achieving good corpus log-likelihood. You can think of this as putting your vocabulary on a strict likelihood-based diet: pieces that don’t contribute enough to explaining the data get gently but firmly removed.</p> <p>Where its necessary, to make this dependence explicit, we will use \(\mathcal{V}_n\) to denote the current vocabulary. To reduce notational clutter, in defining the below algorithm, we’ll use just \(\mathcal{V}\); at step \(n\) of the algorithm, you can assume \(\mathcal{V}=\mathcal{V}_n\) (and that all random variables are defined over \(\mathcal{V}_n\)) unless otherwise stated.</p> <p>If you’d like to look at a trimmed down version of the pseudocode, you can <a href="#sec:pseudocode">skip to the end</a></p> <ol> <li> <p><strong>Initialization:</strong> Define an initial vocabulary \(\mathcal{V}_0\). This could be something like all possible substrings of \(\mathcal{C}\), subject to a maximum length constraint.<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">4</a></sup> Initialize \(\boldsymbol{\phi}^{(0)}\) by some heuristic: the simplest would be uniform initialization, i.e., all pieces are assigned probability \(1/\lvert\mathcal{V}_0\rvert\).</p> </li> <li> <p><strong>Perform EM for \(n=1, \dots N\) iterations or until piece probability estimates converge:</strong></p> <p>i. <strong>E-step</strong> (Expected data log-likelihood computation): The E-step in EM is for computing the expected complete data log-likelihood under our current parameter beliefs \(\mathcal{Q}(\boldsymbol{\phi};\boldsymbol{\phi}^{(n)})\). It turns out that expected token counts are a sufficient statistic for the M-step objective in this case, and so our problem boils down to computing expected token counts under \(\boldsymbol{\phi}^{(n)}\). To see why this is the case… First, we define the count function on token sequences as</p> \[c_v(\mathbf{v}) \mathrel{\stackrel{\textnormal{def}}{=}}\sum_{t=1}^{\lvert\mathbf{v}\rvert}\mathbb{1}\{v_t= v\} \tag{10}\] <p>Then, note that for any valid \(\mathbf{s},\mathbf{v}\) such that \(\mathbf{v}\in\mathcal{T}_{\mathcal{V}}(\mathbf{s})\), we can write</p> \[\begin{aligned} \log P(\mathbf{S}=\mathbf{s}, \mathbf{V}=\mathbf{v};\boldsymbol{\phi})&amp;=\log P(\mathbf{V}=\mathbf{v};\boldsymbol{\phi})\\ &amp;=\log P(M=\lvert\mathbf{v}\rvert)+\sum_{t=1}^{\lvert\mathbf{v}\rvert}\log P(V=v_t;\boldsymbol{\phi})\\ &amp;=\log P(M=\lvert\mathbf{v}\rvert)+ \sum_{v\in\mathcal{V}} c_v(\mathbf{v})\log P(V=v;\boldsymbol{\phi}) \end{aligned}\] <p>Substituting these relationships into our definition of \(\mathcal{Q}(\boldsymbol{\phi};\boldsymbol{\phi}^{(n)})\) and using the linearity of expectations rule, we get</p> \[\begin{aligned} \mathcal{Q}(\boldsymbol{\phi};\boldsymbol{\phi}^{(n)}) = \underbrace{\sum_{i=1}^K\underset{\mathbf{v}\sim \mathbf{V}\mid \mathbf{S}=\mathbf{s}_i;\boldsymbol{\phi}^{(n)}}{\mathop{\mathrm{\mathbb{E}}}}[\log P(M=\lvert\mathbf{v}\rvert)]}_{\text{constant in }\boldsymbol{\phi}} +\sum_{i=1}^K\sum_{v\in\mathcal{V}} \underbrace{\underset{\mathbf{v}\sim \mathbf{V}\mid \mathbf{S}=\mathbf{s}_i;\boldsymbol{\phi}^{(n)}}{\mathop{\mathrm{\mathbb{E}}}\left[ c_v(\mathbf{v})\right]}}_{\mathrel{\stackrel{\textnormal{def}}{=}}\widetilde{c}_v(\mathbf{s}_i;\boldsymbol{\phi}^{(n)})}\log P(V=v;\boldsymbol{\phi}) \end{aligned} \tag{11}\] <p>where \(\widetilde{c}_v(\mathbf{s};\boldsymbol{\phi}^{(n)})\) are simply expected token counts under unigram model parameters \(\boldsymbol{\phi}^{(n)}\), which can be computed as \(\widetilde{c}_v(\mathbf{s};\boldsymbol{\phi}^{(n)})= \sum_{\mathbf{v}\in\mathcal{T}_{\mathcal{V}}(\mathbf{s})} c_v(\mathbf{v}) P(\mathbf{V}=\mathbf{v}\mid \mathbf{S}=\mathbf{s};\boldsymbol{\phi}^{(n)})\). Lastly, if we define the corpus-level expected counts as</p> \[\widehat{c}_v(\mathcal{C};\boldsymbol{\phi}) \mathrel{\stackrel{\textnormal{def}}{=}} \sum_{\mathbf{s}\in\mathcal{C}} \widetilde{c}_v(\mathbf{s};\boldsymbol{\phi}) \tag{12}\] <p>and substitute them into our expansion in Eq. 11, then the equality reduces to</p> \[\mathcal{Q}(\boldsymbol{\phi};\boldsymbol{\phi}^{(n)}) = \text{const} + \underbrace{\sum_{v\in\mathcal{V}}\widehat{c}_v(\mathcal{C};\boldsymbol{\phi}^{(n)})\log P(V=v;\boldsymbol{\phi})}_{\mathrel{\stackrel{\textnormal{def}}{=}}\bar{\mathcal{Q}}(\boldsymbol{\phi};\boldsymbol{\phi}^{(n)})} \tag{13}\] <p>where we have added the definition of \(\bar{\mathcal{Q}}\) (simply \(\mathcal{Q}\) without the “\(\mathrm{const}\)” term) since it will be useful later. From the above, we can see that the posterior expected counts are sufficient statistics for the M-step objective \(\mathcal{Q}(\boldsymbol{\phi};\boldsymbol{\phi}^{(n)})\).</p> <p>In practice, the per-string expected counts \(\widetilde{c}_v(\mathbf{s};\boldsymbol{\phi})\) can be computed efficiently using a forward–backward dynamic program defined over the segmentation lattice induced by \(\mathcal{T}_{\mathcal{V}}(\mathbf{s})\). In words, this lattice forms a directed acyclic graph: nodes correspond to positions in the string and edges originating from the nodes correspond to tokens \(v\in \mathcal{V}\) that can begin at that position and end at another (i.e., pieces whose symbol sequences match the substring). Each edge is weighted by the token’s probability under the current parameters, \(\phi^{(n)}_v\). Valid paths in this graph correspond to a valid segmentation of \(\mathbf{s}\). The forward–backward algorithm then marginalizes over all valid paths in this graph to compute the posterior probability of each token’s occurrence, from which the expected counts follow.</p> <p>A somewhat interesting observation is that this method of getting counts uses an inference procedure that is different from what is done when actually tokenizing text. In the latter case, only the maximum probability segmentation is ultimately used. Here, though, we consider all segmentations of a \(\mathbf{s}\) that have non-zero probability, weighting the token counts from this segmentation (token sequence) by the probability of the segmentation under our current parameters \(\boldsymbol{\phi}^{(n)}\). Also of note is that this is where a length prior <em>could</em> have an effect on the model parameters we learn. But the term is often never actually used in the model definition. <strong>Another sidebar on your favorite topic: the length prior.</strong> This is where a length prior <em>could</em> have an effect on the model parameters we learn, as it affects the probabilities of the segmentations. <strong>Potential research direction: Exploring effect of difference between the segmentation strategy during training and inference.</strong> This method of getting counts considers all valid segmentations of \(\mathbf{s}\) that have non-zero probability under the current model parameters. In contrast, at inference, we only consider the maximum probability segmentation. The original UnigramLM paper actually proposes an inference strategy that’s much more aligned with training: sampling tokenizations from the posterior. But nowadays, everyone uses the Viterbi version of inference. Looking at the effects of this could be interesting. For example, soft/expected representations computed over the distribution of segmentations could give benefits similar to dropout or data augmentation, especially for low-resource languages or noisy text.</p> <p>ii. <strong>M-step</strong> (maximize \(\boldsymbol{\phi}\) and update \(\boldsymbol{\phi}^{(n)}\)): In the M-step, we want to maximize \(\mathcal{Q}(\boldsymbol{\phi};\boldsymbol{\phi}^{(n)})\) with respect to \(\boldsymbol{\phi}\) subject to these parameters giving us a valid probability distribution, i.e., \(\sum_{v\in\mathcal{V}}\phi_v=1\) and \(\phi_v\ge 0\). Subbing in the relationship established in Eq. 13, this actually boils down to a relatively simple problem: finding the \(\boldsymbol{\phi}\) that maximizes the probability of having observed the expected counts that we got from the segmenting the corpus according to our prior model parameter beliefs:</p> \[\begin{aligned} \max_{\boldsymbol{\phi}}&amp;\sum_{v\in\mathcal{V}}\widehat{c}_v(\mathcal{C};\boldsymbol{\phi}^{(n)})\log P(V=v;\boldsymbol{\phi})\\ &amp;\text{s.t.}\quad \sum_{v\in\mathcal{V}}\phi_v=1,\phi_v\ge 0 \end{aligned}\] <p>The solution (normalized expected counts) is very recognizable, as it is essentially the same as the MLE for a standard multinomial distribution, albeit using expected counts rather than pure counts:</p> \[\phi^{(n+1)}_v = \frac{\widehat{c}_v(\mathcal{C};\boldsymbol{\phi}^{(n)})} {\sum_{v'\in\mathcal{V}}\widehat{c}_{v'}(\mathcal{C};\boldsymbol{\phi}^{(n)})}. \tag{14}\] <p>The length-prior term is constant in \(\boldsymbol{\phi}\) and does not alter the update (for fixed \(\mathcal{V}\)).</p> <p>iii. <strong>Pruning:</strong> After applying the above steps, the vocabulary itself will not have changed (only the per-piece probabilities are updated). Because the initial vocabulary \(\mathcal{V}_0\) is typically over-complete (often \(\lvert\mathcal{V}_0\rvert \gg \lvert\mathcal{V}\rvert\)), we want to trim it down. UnigramLM achieves this by applying a pruning step <em>within</em> the EM iterations. Explicitly, at step \(n\), it removes \(k_n\) of the least “important” pieces, leading to a new \(\mathcal{V}_{n+1}\). Following pruning, the remaining probabilities in \(\boldsymbol{\phi}^{(n+1)}\) are renormalized to form a valid distribution over \(\mathcal{V}_{n+1}\). This pruning is done until the vocabulary reaches the desired size.</p> <p>Formally, let \(\bar{\mathcal{Q}}(\boldsymbol{\phi}^{(n+1)};\boldsymbol{\phi}^{(n)})\) be our expected complete data log-likelihood under updated model parameters (albeit still under the segmentations according to \(\boldsymbol{\phi}^{(n)}\)). The algorithm removes tokens whose absence leads to the smallest decrease in (our proxy for) observed data log-likelihood. Intuitively, we prune tokens that contribute least to explaining the data under the current model. We define the contribution (or “loss”) associated with token \(v\) as the change (typically a decrease) in the corpus log-likelihood when \(v\) is removed from the model:</p> \[L(v) \mathrel{\stackrel{\textnormal{def}}{=}} \bar{\mathcal{Q}}(\boldsymbol{\phi}^{(n+1)};\boldsymbol{\phi}^{(n)}) - \bar{\mathcal{Q}}(\boldsymbol{\phi}^{(n+1)}_{-v};\boldsymbol{\phi}^{(n)}_{-v}), \tag{15}\] <p>The notation \(\boldsymbol{\phi}^{(n)}_{-v}\) in Eq. (15) refers to the unigram distribution obtained from \(\boldsymbol{\phi}^{(n)}\) by removing \(v\) from its support and renormalizing the remaining probabilities. The corresponding string-level distribution is thus identical to the one induced by \(\boldsymbol{\phi}^{(n)}\), except that all segmentations containing \(v\) are assigned zero probability and individual piece probabilities are renormalized over \(\mathcal{V}\setminus \{v\}\) (this logic also applies to \(\boldsymbol{\phi}^{(n+1)}_{-v}\)). After computing \(L(v)\) for all \(v\in \mathcal{V}_n\), we remove the \(k_n\) tokens with the smallest losses, where \(k_n\) is a hyperparameter chosen such that after some number of iterations, we ultimately reach our desired vocabulary size.<sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">5</a></sup> Intuitively, this can be seen as removing the tokens whose removal incurs the <em>least</em> penalty on the corpus log-likelihood. Notably, computing \(\bar{\mathcal{Q}}(\boldsymbol{\phi}^{(n+1)}_{-v};\boldsymbol{\phi}^{(n)}_{-v})\) in Eq. (15) is very computationally expensive since it requires a separate forward–backward pass over the corpus. We discuss some approximations to \(L(v)\) in the following section.</p> </li> </ol> <h4 id="approximations-of-l">Approximations of \(L\).</h4> <p>Computing \(\bar{\mathcal{Q}}(\boldsymbol{\phi}^{(n+1)}_{-v};\boldsymbol{\phi}^{(n)}_{-v})\) in Eq. (15) for a given \(v\) generally requires a separate forward–backward pass over the corpus. This is because disallowing the use of \(v\) in segmentations changes both the set of valid paths and the total probability of those paths.<sup id="fnref:7"><a href="#fn:7" class="footnote" rel="footnote" role="doc-noteref">6</a></sup> The new per-string marginal probabilities (and expected token counts) under \(\boldsymbol{\phi}^{(n+1)}_{-v}\). cannot, in general, be recovered from forward/backward marginals computed under \(\boldsymbol{\phi}^{(n)}\). Hence, we would need a fresh forward–backward evaluation on the pruned lattice to obtain the exact \(\bar{\mathcal{Q}}(\boldsymbol{\phi}^{(n+1)}_{-v};\boldsymbol{\phi}^{(n)}_{-v})\).</p> <p>Performing a separate forward–backward pass for each piece in the vocabulary whenever we want to prune is impractical for vocabularies of any reasonable size. For example, if our initial vocabulary is a mere 100\(k\), then computing per-piece losses would require 100\(k\) forward passes of the corpus on its own. In practice, approximations that reuse the statistics computed from the current EM iteration are done. We discuss those next. To avoid the need to resegment the corpus to compute each \(v\)’s loss, several approximations can be used to compute per-piece losses. A simple approximation would be to use as a token’s loss its contribution to corpus log-likelihood, i.e., \(\widehat{L}(v) \approx \widehat{c}_v(\mathcal{C};\boldsymbol{\phi}^{(n+1)})\log P(V=v;\boldsymbol{\phi}^{(n)})\). An arguably more sound approximation (and the one used by the original implementation of UnigramLM found in the SentencePiece library) is to look at the change in corpus log-likelihood when simply replacing \(v\) by the best alternative segmentation of that piece, i.e., the best alternative segmentation of the string \(g(v)\) when \(v\) is not in the vocabulary.</p> <p>Formally, let \(\mathbf{v}' = h_{\boldsymbol{\phi}^{(n)}_{-v}}(g(v))\) be the best segmentation of the string \(\mathbf{s}= g(v)\) under \(\boldsymbol{\phi}^{(n)}_{-v}\).<sup id="fnref:7:1"><a href="#fn:7" class="footnote" rel="footnote" role="doc-noteref">6</a></sup> The approximate loss is then the change to corpus log-likelihood when replacing every use of \(P(V=v;\boldsymbol{\phi}^{(n)})\) with \(\prod_{t=1}^{\lvert\mathbf{v}'\rvert}P(V=v'_t; \boldsymbol{\phi}^{(n)}_{-v})\) under the new renormalized unigram probabilities \(\boldsymbol{\phi}^{(n)}_{-v}\). This loss can be computed concisely as:</p> \[\widehat{L}(v)\approx \widehat{c}_v(\mathcal{C};\boldsymbol{\phi}^{(n)})\left[\log P(V=v;\boldsymbol{\phi}^{(n)}) - \log \prod_{t=1}^{\lvert\mathbf{v}'\rvert}P(V=v'_t; \boldsymbol{\phi}^{(n)}_{-v})\right]\] <p><strong>Example 3</strong> (Toy pruning example). <em>Suppose our corpus contains the string \(\mathbf{s}= \text{"internationalization"}\)</em></p> <p><em>and our vocabulary includes the tokens</em></p> \[\{ \text{international},\quad \text{inter},\quad \text{national},\quad \text{ization},\quad \text{al},\ldots \}\] <p><em>Assume that under the current parameters \(\boldsymbol{\phi}^{(n)}\), the posterior expected corpus-level counts are</em></p> \[\widehat{c}_{\text{international}}(\mathcal{C};\boldsymbol{\phi}^{(n)}) \ll \widehat{c}_{\text{inter}}(\mathcal{C};\boldsymbol{\phi}^{(n)}), \widehat{c}_{\text{national}}(\mathcal{C};\boldsymbol{\phi}^{(n)}), \widehat{c}_{\text{ization}}(\mathcal{C};\boldsymbol{\phi}^{(n)})\] <p><em>To approximate the contribution of \(v_{\text{international}}\), we consider its best alternative segmentation when it is removed from the vocabulary. Let</em></p> \[\mathbf{v}' = \langle \text{inter}, \text{national} \rangle\] <p><em>be the Viterbi segmentation of the string \(g(\text{international})\) under the renormalized distribution \(\boldsymbol{\phi}^{(n)}_{-v}\). The approximate loss associated with pruning \(\text{international}\) is then</em></p> \[\begin{aligned} \widehat{L}(\text{international};\boldsymbol{\phi}^{(n)}) &amp;\approx \widehat{c}_{\text{international}}(\mathcal{C};\boldsymbol{\phi}^{(n)}) \log \frac{ P(V=\text{international};\boldsymbol{\phi}^{(n)}) }{ P(V=\text{inter}; \boldsymbol{\phi}^{(n)}_{-v}) \cdot P(V=\text{national}; \boldsymbol{\phi}^{(n)}_{-v}) }. \end{aligned}\] <p><em>Intuitively, if \(v_{\text{international}}\) is both rare (small \(\widehat{c}_{\text{international}}(\mathcal{C};\boldsymbol{\phi}^{(n)})\)) and easily replaced by a segmentation whose product of probabilities is similar to \(P(V=\text{international};\boldsymbol{\phi}^{(n)})\), then its (approximate) loss will be small, making it a good candidate for pruning.</em></p> <p>While this approximation does not account for changes in other valid paths’ probabilities that might happen as a result of removing \(v\) from the vocabulary, it seems to work fairly well in practice as a pruning heuristic (although I don’t believe that anyone has actually tried to run the algorithm with the real, brute-force loss computation).</p> <h4 id="sec:pseudocode">Concise Pseudocode</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Algorithm UnigramLM-Train(C, V_target_size, V0, phi0, k_n):
    V   &lt;- V0
    phi &lt;- phi0

    while |V| &gt; V_target_size:

        # ---- E-step ----
        hat_c[v] &lt;- 0 for all v in V
        for each string s in C:
            lattice &lt;- build_lattice(s, V)
            tilde_c &lt;- forward_backward_expected_counts(lattice, phi)
            for each v in V:
                hat_c[v] &lt;- hat_c[v] + tilde_c[v]

        # ---- M-step ----
        Z &lt;- sum_{v in V} hat_c[v]
        for each v in V:
            phi[v] &lt;- hat_c[v] / Z

        # ---- Pruning (approx loss) ----
        for each v in V:
            v_alt &lt;- viterbi_best_segmentation(g(v), V \ {v}, phi)
            alt_prob &lt;- product_{t in v_alt} phi[t]
            Lhat[v] &lt;- hat_c[v] * ( log(phi[v]) - log(alt_prob) )

        V &lt;- V \ bottom_k_tokens_by(Lhat, k_n)
        phi &lt;- renormalize(phi over V)

    return V, phi
</code></pre></div></div> <h2 id="implementation-in-the-sentencepiece-library">Implementation in the SentencePiece library</h2> <p>In practice, the UnigramLM algorithm as we know it is largely defined by the public SentencePiece implementation, since Kudo (2018)<d-cite key="kudo-2018-subword"></d-cite> give only a high-level description and leave many engineering choices under-specified. The library makes a number of concrete design decisions that go beyond the abstract EM + pruning picture above.</p> <h4 id="text-preprocessing">Text Preprocessing.</h4> <p>Arguably some of the more critical design choices to be aware of are those pertaining to normalization and pretokenization, as these change which segmentations are feasible. SentencePiece advertises that it does not apply any pretokenization, but I think that depends on your definition of pretokenization… By default the library, collapses whitespace, inserts a dummy-prefix marker, and treats whitespace (and script/number boundaries) as explicit segmentation cues, i.e., as markers that can be suffixes or prefixes of pieces, but that pieces cannot cross. Most of these behaviors can be disabled via training flags but the fact that they’re used is not well advertised. It also applies NFKC normalization by default.</p> <h4 id="initialization">Initialization.</h4> <p>The seed vocabulary is not “all substrings up to length \(L\)”: SentencePiece uses a version of the Enhanced (Extended) Suffix Array procedure to mine a large lexicon of frequent substrings from the corpus (on the order of \(10^6\) pieces by default), subject to length and frequency thresholds.</p> <h4 id="em-updates">EM Updates.</h4> <p>SentencePiece runs a fixed EM+prune schedule rather than iterating EM to convergence on a fixed vocabulary. Each outer iteration consists of a small fixed number of EM “sub-iterations” (typically two), after which the vocabulary is pruned by a fixed shrinking factor, and training stops once the target vocabulary size is reached. SentencePiece does not use the plain MLE M-step update from Eq. (14). Instead, it adopts a Variational Bayesian approach with a Dirichlet prior, replacing expected counts with their <a href="https://github.com/google/sentencepiece/blob/336900241c4943ae1e5f844b18292f532b3a21c7/src/unigram_model_trainer.cc#L390" rel="external nofollow noopener" target="_blank">digamma-transformed counterparts</a>: \(\phi^{(n+1)}_v\propto\exp(\Psi(\widehat{c}_v(\mathcal{C};\boldsymbol{\phi}^{(n)})+\alpha_v))\). While it might not seem like a large change to the original update rule, this choice is implicitly adding a prior belief about the the number of counts we should observe for each token. Explicitly, we’re now calculating the geometric mean of a posterior Dirichlet distribution, where we’re added in the belief token \(v\) will be observed \(\alpha_v\) times. Notably, SentencePiece uses an improper Haldane prior (\(\alpha_v= 0\) for all \(v\in\mathcal{V}\)). This choice essentially has the opposite effect of performing standard additive smoothing: it’s always the case that \(\exp(\psi(x)) &lt; x\), however, for small \(x\) (rare tokens), the relative “discount” is significantly larger. It thus acts as a regularizer that disproportionately penalizes tokens with low expected counts, sending their assigned probability mass closer to zero. This is done on top of a for tokens whose expected counts are below a certain threshold. <strong>Potential Research Direction: Exploring the effects of a sparse prior.</strong> The Bayesian version of the M step (with the Haldane prior) over-penalizes low-count tokens relative to plain MLE. This is a modeling choice, not just an implementation detail. It raises a concrete question: for which settings (e.g., low-resource languages, morphologically rich languages) does this rare-token downweighting help, and where does it actually harm coverage or fairness?</p> <h4 id="pruning">Pruning.</h4> <p>Pruning is performed as described above in the approximations section, i.e., a piece’s loss is approximated by assuming that the removed piece’s probability mass transfers to its best alternative Viterbi segmentation (\(h_{\boldsymbol{\phi}}(\mathbf{s})_{\mathcal{V}_n}(v)\)). Notably, pieces whose expected counts are below a fixed value (0.5) are <a href="https://github.com/google/sentencepiece/blob/336900241c4943ae1e5f844b18292f532b3a21c7/src/unigram_model_trainer.cc#L381" rel="external nofollow noopener" target="_blank">pre-pruned</a>. Also, not all pruning is done within the EM iterations; there is a final pruning step that removes tokens with the lowest estimated probabilities in order to get to the final desired vocabulary size.</p> <p>Taken together, these implementation details instantiate one particular, very specific version of the abstract UnigramLM model described above, albeit the one that people are typically referring to (rather than an implementation-free mathematical ideal) when talking about “the UnigramLM tokenization algorithm.”</p> <h2 id="conclusions">Conclusions</h2> <p>Tokenization shouldn’t be just a monolithic preprocessing step you fix once and forget; it quietly defines what your model even sees as input, and can have a huge effect on the behavior and fairness of the systems trained on top of it. If we take that seriously, we should treat tokenization as a full-blown modeling choice and explore the whole design space: priors (e.g., over length), supports (which segmentations are even allowed by pretokenization choices), and inference rules (Viterbi vs sampling vs marginalization). UnigramLM occupies just one corner of that space, but understanding it clearly is a step toward thinking about tokenizers as models we can design and question, not just as default settings we inherit.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:2"> <p>\(v\) are also sometimes called subwords; we avoid this naming because \(v\) need not align with orthographic words, in their typical definition. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:3"> <p>\(\circ\) denotes string concatenation and when applied to tokens, indicates the pieces’ symbols are concatenated together (perhaps with some special formatting if symbols from \(\Gamma\) are present in the piece). <a href="#fnref:3" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:4"> <p>The distribution can also be made proper with the use of an EOS symbol, which is the more common way of specifying a language model. The use of \(M\) in this situation (a non-autoregressive model) is a bit more general (if the distribution of \(M\) follows a power law, then our distribution over token sequences could equivalently be represented using an EOS symbol). The use of \(M\) though allows us to handle sequence length without adding a special token to our vocabulary. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:5"> <p>There are several ways that this seed vocabulary can be created. The Enhanced Suffix Array is one of the more common algorithms. Often, pretokenization is performed on the corpus and one of the more common pretokenization rules splits on whitespace, preventing pieces from crossing whitespace boundaries, although that’s kind of an arbitrary rule… <a href="#fnref:5" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:6"> <p>Explicitly, relative to the original model, two coupled changes occur when removing \(v\) from \(\mathcal{V}_n\): (i) the feasible set of paths shrinks from \(\mathcal{T}_{\mathcal{V}_n}(\mathbf{s})\) to \(\mathcal{T}_{\mathcal{V}_{n+1}}(\mathbf{s})\) (all segmentations using \(v\) are removed); (ii) the per-edge weights change after the renormalization of \(\boldsymbol{\phi}^{(n)}\) and the marginal probabilities of remaining paths must be recomputed. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:7"> <p>This segmentation may need to include an UNK token depending on the base vocabulary. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">↩</a> <a href="#fnref:7:1" class="reversefootnote" role="doc-backlink">↩<sup>2</sup></a></p> </li> </ol> </div> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-unigramlm-manual.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/general-agent-evaluation/">Ready For General Agents? Let's Test It.</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-vlms-waste-their-vision/">Why vlms waste their vision</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>