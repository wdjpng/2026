<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Uncertainty Lifecycle in Deep Learning | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Uncertainty modeling in deep learning has different attributes such as uncertainty propagation, uncertainty estimation, uncertainty decomposition, uncertainty attribution and uncertainty sensitivity, that are extensively discussed in literature. However, there is no proper structure explaining how these different components interact with each other at different stages of Deep Learning pipeline. We propose to structure the flow and transformation of uncertainty from input to prediction through the model, by appropriately positioning them. And we call this structure as “Uncertainty Lifecycle”. The “Uncertainty lifecycle” can be represented as a structured process for handling, quantifying, analyzing, and interpreting uncertainties at different stages of Deep Learning pipeline."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/uncertainty-lifecycle-in-deep-learning/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Uncertainty Lifecycle in Deep Learning",
            "description": "Uncertainty modeling in deep learning has different attributes such as uncertainty propagation, uncertainty estimation, uncertainty decomposition, uncertainty attribution and uncertainty sensitivity, that are extensively discussed in literature. However, there is no proper structure explaining how these different components interact with each other at different stages of Deep Learning pipeline. We propose to structure the flow and transformation of uncertainty from input to prediction through the model, by appropriately positioning them. And we call this structure as “Uncertainty Lifecycle”. The “Uncertainty lifecycle” can be represented as a structured process for handling, quantifying, analyzing, and interpreting uncertainties at different stages of Deep Learning pipeline.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Uncertainty Lifecycle in Deep Learning</h1> <p>Uncertainty modeling in deep learning has different attributes such as uncertainty propagation, uncertainty estimation, uncertainty decomposition, uncertainty attribution and uncertainty sensitivity, that are extensively discussed in literature. However, there is no proper structure explaining how these different components interact with each other at different stages of Deep Learning pipeline. We propose to structure the flow and transformation of uncertainty from input to prediction through the model, by appropriately positioning them. And we call this structure as “Uncertainty Lifecycle”. The “Uncertainty lifecycle” can be represented as a structured process for handling, quantifying, analyzing, and interpreting uncertainties at different stages of Deep Learning pipeline.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#uncertainty-lifecycle">Uncertainty Lifecycle</a> </div> <ul> <li> <a href="#1-uncertainty-propagation">1. Uncertainty Propagation</a> </li> <li> <a href="#2-uncertainty-estimation">2. Uncertainty Estimation</a> </li> <li> <a href="#3-uncertainty-decomposition">3. Uncertainty Decomposition</a> </li> <li> <a href="#4-uncertainty-attribution">4. Uncertainty Attribution</a> </li> <li> <a href="#5-uncertainty-sensitivity">5. Uncertainty Sensitivity</a> </li> </ul> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Uncertainty in Machine Learning arises due to <d-cite key="koller2009probabilistic"></d-cite>:</p> <ul> <li>Limitations in our ability to observe the world</li> <li>Limitations in our ability to model it</li> <li>Innate non-determinism itself</li> </ul> <p>These uncertainties exist either in data or in model or both.The uncertainty in <strong>data</strong> arises due to measurement noise, inherent randomness, and label/feature ambiguity. These uncertainties are called as data uncertainty or <strong>Aleatoric Uncertainty</strong> <d-cite key="der2009aleatory"></d-cite> and are often irreducible even with more data. The uncertainty in the <strong>model</strong> arises due to limited training data, model misspecification, optimization imperfections, or out-of-distribution inputs. These uncertainties are called as model uncertainty, knowledge uncertainty or <strong>Epistemic Uncertainty</strong> <d-cite key="der2009aleatory"></d-cite>, and are often reducible with more data or better modelling. Both uncertainties together are called as <strong>Total Uncertainty (or Predictive Uncertainty)</strong>.</p> <p>The different concepts in uncertainty as shown in Figure 1, are discussed and studied in the literature extensively <d-cite key="gawlikowski2023survey, abdar2021review, loquercio2020general, wang2024uncertainty, mucsanyi2024benchmarking"></d-cite> either individually or as a combination of one or two component. But there is no such work that highlight and represent their connection and flow in a single framework.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-uncertainty-lifecycle-in-deep-learning/Uncertainty_Concepts_1-480.webp 480w,/2026/assets/img/2026-04-27-uncertainty-lifecycle-in-deep-learning/Uncertainty_Concepts_1-800.webp 800w,/2026/assets/img/2026-04-27-uncertainty-lifecycle-in-deep-learning/Uncertainty_Concepts_1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-uncertainty-lifecycle-in-deep-learning/Uncertainty_Concepts_1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 1: Different Uncertainty Concepts in Deep Learning </div> <p>In this blogpost, we have focused to provide a holistic framework and call it as <strong>“Uncertainty Lifecycle”</strong>, to show how different concepts of uncertainty are applicable at different stages in the uncertainty modeling.</p> <p><em>“Uncertainty lifecycle” can be represented as a structured process for handling, quantifying, analyzing, and interpreting uncertainties at different stages of Deep Learning pipeline</em></p> <h2 id="uncertainty-lifecycle">Uncertainty Lifecycle</h2> <p>Uncertainty Lifecycle represents the holistic framework to connect and position the major identified concepts of uncertainty modeling which are</p> <ol> <li>Uncertainty Propagation</li> <li>Uncertainty Estimation</li> <li>Uncertainty Decomposition</li> <li>Uncertainty Attribution</li> <li>Uncertainty Sensitivity</li> </ol> <p>In Figure 2, we see the complete lifecycle of uncertainty flow. The inherent uncertainties present in the data(aleatoric) and model(epistemic) propagates through the model to the output. This propagated uncertainty is measured by the uncertainty estimation techniques and further decomposed into aleatoric and epistemic uncertainty. To estimate the propagated uncertainty, we need to model it through uncertainty propagation module either at intermediate layer or at the output layer as per the requirement. The output of this modeling will be the distribution which will be fed to the uncertainty estimation module.</p> <p>Further, the obtained total uncertainty or the decomposed uncertainty can be attributed by the uncertainty attribution concept, to its actual root cause which could be any specific input feature or the specific model layer. The attributed/identified region causing this uncertainty is obtained in form of an explanation/attribution map or importance score. Also, the sensitivity of these estimated uncertainties to the input or the model is analyzed by the uncertainty Sensitivity analysis concept. The sensitivity analysis can be further performed only for specific region in the input/model causing the uncertainty, as identified by the uncertainty attribution.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-uncertainty-lifecycle-in-deep-learning/Uncertainty_Lifecycle_4-480.webp 480w,/2026/assets/img/2026-04-27-uncertainty-lifecycle-in-deep-learning/Uncertainty_Lifecycle_4-800.webp 800w,/2026/assets/img/2026-04-27-uncertainty-lifecycle-in-deep-learning/Uncertainty_Lifecycle_4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-uncertainty-lifecycle-in-deep-learning/Uncertainty_Lifecycle_4.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Figure 2: Uncertainty Lifecycle; This figure shows applicability of different concepts of uncertainty at different stages: Uncertainty propagation, Uncertainty Estimation, Uncertainty attribution and uncertainty Sensitivity. It explains how data and model uncertainty propagate through the model and captured at the output in the form of total uncertainty by estimating the predictive distribution over output (sometimes you may get decomposed uncertainty directly). The total uncertainty can be decomposed into epistemic and aleatoric uncertainty. Uncertainty Attribution and Uncertainty sensitivity method explains the predictive uncertainty or decomposed uncertainty. It also shows how uncertainty attribution and uncertainty sensitivity method interact with each other. </div> <p>Once the cause and sensitivity of uncertainty is identified, that is which region or which feature is causing the uncertainty, appropriate measures can be taken to reduce these uncertainties.</p> <p>This cycle of propagating, measuring, decomposing and attributing the uncertainty can be repeated to gain further insight of the model, and improve the model performance accordingly.</p> <p>One more thing to notice in Figure 2 is that there is no clear demarcation between some of the components. For e.g. uncertainty attribution and sensitivity can be part of same analysis or treated separately as they both observe how uncertainty changes with change in input or model parameters. Further, many methods implemented to do estimation does propagation implicitly. And, many methods directly give decomposed uncertainty, and you may not need separate decomposition. Although they may not be completely separable, this structure gives a framework for positioning our actions, a bit more clearly. Let’s discuss each of the concept in Uncertainty Lifecycle</p> <h3 id="1-uncertainty-propagation">1. Uncertainty Propagation:</h3> <p><strong>• What it is:</strong> The process of tracking how uncertainties in inputs, model parameters, or intermediate layers flow through a neural network to affect its predictions.</p> <p><strong>• What it does:</strong> Simulates the transformation of uncertainties (e.g., noisy data or uncertain weights) through the model’s computations (e.g., convolutions, activations), producing a distribution or samples of outputs. It tries to answer - “How does uncertainty flow through the model?”</p> <p><strong>• Input:</strong> Data with uncertainties (e.g., noisy sensor readings modeled as distributions) and model elements like uncertain weights (e.g., from limited training data).</p> <p><strong>• Output:</strong> A distribution, set of samples, or statistical moments (e.g., mean and variance) representing the uncertainty in predictions.</p> <p><strong>• Role in the Flow:</strong> The foundational step, initiating uncertainty analysis by propagating uncertainties through the model, providing raw data for subsequent steps.</p> <p><strong>• Example Approaches:</strong> Monte Carlo Sampling <d-cite key="blundell2015weight"></d-cite>, BNN <d-cite key="tishby1989consistent, graves2011practical, goan2020bayesian"></d-cite>, Analytical methods (variance propagation rules), probabilistic layers, ADF <d-cite key="gast2018lightweight"></d-cite>.</p> <p>Generally, propagation refers to observing how the uncertainty changes at the output layer. However, many other works also refer propagation as observing the uncertainty changes in intermediate layer. For e.g. ADF method in <d-cite key="gast2018lightweight"></d-cite> propagate uncertainty via moment matching and modeling the activations also as distribution, and not just the output.</p> <p>These recent methods enrich the explanation of uncertainty propagation by emphasizing approximation and efficiency for deep networks, where exact propagation (e.g., full Bayesian integrals) is often infeasible. They all operate within the same framework: starting with input/model uncertainties (e.g., distributions), transforming them through layers (via approximations like moment-matching in ADF <d-cite key="gast2018lightweight"></d-cite>, subspace projection in WGMprop <d-cite key="monchot2023input"></d-cite>, or message-passing in factor graphs <d-cite key="daruna2023uncertainty"></d-cite>, and outputting uncertainty representations (e.g., variances or distributions)</p> <h3 id="2-uncertainty-estimationquantification">2. Uncertainty Estimation/Quantification</h3> <p><strong>• What it is:</strong> The process of quantifying how confident or uncertain a model is about its predictions, often summarizing it into a single metric or distribution. (Estimation and quantification are synonymous, referring to measuring uncertainty magnitude).</p> <p><strong>• What it does:</strong> Distills propagated uncertainties into a concrete measure of confidence (e.g., variance, entropy), enabling assessment of prediction reliability. It tries to answer – “How uncertain is the prediction?”</p> <p><strong>• Input:</strong> The output from uncertainty propagation, such as a distribution, sampled predictions, or intermediate variances.</p> <p><strong>• Output:</strong> A quantified uncertainty metric, e.g., variance, standard deviation, entropy, confidence intervals, or a probability distribution (e.g., “Prediction: 0.85 with variance 0.02”).</p> <p><strong>• Role in the Flow:</strong> The second step, bridging propagation’s raw distributions to interpretable metrics, enabling decomposition and further analysis for the next steps.</p> <p><strong>• Example Approaches:</strong> Measuring the Entropy, Mutual Information of the output distribution, Predictive variance as measure of uncertainty, computing variance over ensemble output. These methods are applicable to output distributions or sampled predictions obtained via Ensemble <d-cite key="lakshminarayanan2017simple, huang2017snapshot, valdenegro2019deep, wenzel2020hyperparameter"></d-cite>, BNN <d-cite key="kendall2017uncertainties, tishby1989consistent, graves2011practical,goan2020bayesian"></d-cite>, Monte Carlo Dropout <d-cite key="gal2016dropout"></d-cite>, Test Time Augmentation <d-cite key="lyzhov2020greedy"></d-cite>.</p> <p><strong>• How it is Measured:</strong> The total uncertainty is quantified using statistical or information-theoretic metrics such as variance/standard deviation, entropy, confidence intervals, mutual information. For e.g., for a classifier, entropy of 0.7 bits indicates high uncertainty, while a regression model’s standard deviation of 2.5 units quantifies spread.</p> <p>Uncertainty propagation and uncertainty estimation are not seen separately in many implementations. The output of propagation is distribution or sampled predictions and output of estimation is an uncertainty score computed from this distribution.</p> <p><strong>• Propagation = mechanism</strong> (push uncertainty through). <br> <strong>• Estimation = measurement</strong> (summarize how much uncertainty there is).</p> <p>Uncertainty Propagates first, and then it is estimated on the prediction. In a few approaches like ADF <d-cite key="gast2018lightweight"></d-cite>, the uncertainty is estimated at the intermediate layer as well. In research community, however the term uncertainty quantification/estimations mostly refer to measuring the uncertainty at the output prediction.</p> <p><strong>3. Uncertainty Decomposition</strong></p> <p><strong>• What it is:</strong> A quantitative technique which split the total uncertainty into distinct components, such as aleatoric (data-related) and epistemic (model-related) uncertainty, or contributions from specific features or layers.</p> <p><strong>• What it does:</strong> Splits the quantified uncertainty to reveal the contribution of each source, enabling targeted diagnostics (e.g., distinguishing data noise from model gaps). It tries to answer - “What type of uncertainty do we have, and how much of each?”</p> <p><strong>• Input:</strong> The quantified total uncertainty (e.g., total variance) and supporting data from propagation (e.g., samples, distributions).</p> <p><strong>• Output:</strong> A breakdown of uncertainty contributions into its components.</p> <p><strong>• Role in the Flow:</strong> The third step, refining total uncertainty into components, providing input simultaneously for uncertainty attribution and uncertainty sensitivity analyses.</p> <p><strong>• Example Approaches:</strong> Variance Decomposition <d-cite key="depeweg2018decomposition, kendall2017uncertainties, kwon2020uncertainty, depeweg2017sensitivity, mucsanyi2024benchmarking"></d-cite>, Entropy-based Decomposition <d-cite key="depeweg2018decomposition, smith2018understanding, hullermeier2021aleatoric"></d-cite>, Mutual Information Decomposition <d-cite key="depeweg2018decomposition, smith2018understanding"></d-cite>, Kendall and Gal’s Heteroscedastic approach<d-cite key="kendall2017uncertainties, kwon2020uncertainty"></d-cite>.</p> <p><strong>• How it is Measured:</strong> Decomposition is measured by the proportion or magnitude of uncertainty attributed to each component. For e.g. a total variance of 0.05, decomposition might yield 0.03 (60%) aleatoric and 0.02 (40%) epistemic.</p> <p>In literature, uncertainty decomposition means splitting the predictive uncertainty into aleatoric and epistemic uncertainty.</p> <h3 id="4-uncertainty-attribution">4. Uncertainty Attribution</h3> <p><strong>• What it is:</strong> The process of explaining and localizing which parts of the input or model contribute most to the uncertainty in a given prediction.</p> <p><strong>• What it does:</strong> Assigns uncertainty to specific causes, providing human-readable explanations. It tries to answer the question - “Where is the uncertainty coming from? Which parts of the input or model contributed to this uncertainty?”</p> <p><strong>• Input:</strong> Predictive uncertainty or decomposed uncertainty components (e.g., aleatoric/epistemic breakdowns)</p> <p><strong>• Output:</strong> Explanations of uncertainty sources, e.g., attribution maps or scores that localize/quantify the root cause of uncertainty.</p> <p><strong>• Role in the Flow:</strong> A parallel final step, focusing on explaining and interpreting uncertainty cause for decision-making and debugging.</p> <p><strong>• Example Approaches:</strong> Techniques adapted from XAI (Explainable AI) field for uncertainty – Perturbation based attribution giving feature importance <d-cite key="wood2024model, watson2023explaining, phillips2018interpretable"></d-cite>, Gradient-based attribution giving saliency map <d-cite key="amanova2024finding, bley2025explaining, wang2023gradient, perez2022attribution"></d-cite>, Counterfactual based attribution giving counterfactuals <d-cite key="antoran2020getting, ley2021delta, ley2022diverse"></d-cite>.</p> <p><strong>• How it is Measured:</strong> Attribution is measured by qualitative or semi-quantitative assignments of uncertainty back to sources. For e.g. the region near nose in the input image of a cat is responsible for the uncertainty in prediction or feature number 3 in the tabular data is responsible for the uncertainty.</p> <p>Uncertainty decomposition tells whether the source is from data or model, whereas uncertainty attribution gives more fine grain and localize root cause analysis of the uncertainty - “Pixel noise in region X contributes 60% to aleatoric uncertainty.”</p> <h3 id="5-uncertainty-sensitivity">5. Uncertainty Sensitivity</h3> <p><strong>• What it is:</strong> An analysis of how sensitive the model’s output uncertainty is to perturbations in inputs, parameters, or other factors.</p> <p><strong>• What it does:</strong> Quantifies the influence of specific factors on uncertainty, revealing which elements (e.g., input noise) most affect uncertainty when changed.</p> <p><strong>• Input:</strong> Total or decomposed uncertainty components (e.g., aleatoric/epistemic variances) and supporting data from attribution.</p> <p><strong>• Output:</strong> Sensitivity metrics, e.g., “Reducing noise in feature Y by 10% decreases uncertainty by 15%,” or rankings of influential factors.</p> <p><strong>• Role in the Flow:</strong> A parallel final step focusing on “what-if” scenarios and robustness, complementing attribution’s explanatory focus.</p> <p><strong>• Example Approaches:</strong> Sobol Sensitivity Analysis <d-cite key="fel2021look"></d-cite>, Gradient-Based Methods<d-cite key="depeweg2017sensitivity"></d-cite>, Perturbation Analysis.</p> <p><strong>• How it is Measured:</strong> Sensitivity is measured by metrics quantifying the change in uncertainty due to perturbations. Relative change in uncertainty, e.g., “10% noise reduction in feature Y lowers variance by 0.01.”</p> <p>In the literature, uncertainty sensitivity and uncertainty attribution are related but they are not the same - attribution tells you “Where uncertainty is coming from”, sensitivity tells you “How fragile uncertainty is to small input changes.”</p> <p>In summary, the uncertainties in data and model flows through the model to the output, where <strong>Propagation</strong> tracks them, <strong>Estimation/Quantification</strong> measures them, <strong>Decomposition</strong> splits them into components, <strong>Attribution</strong> explain and localize uncertainty’s root cause and <strong>Sensitivity</strong> test robustness with respect to the input. Table 1 summarizes the same along with an example</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-uncertainty-lifecycle-in-deep-learning/Uncertainty_Summary_Table-480.webp 480w,/2026/assets/img/2026-04-27-uncertainty-lifecycle-in-deep-learning/Uncertainty_Summary_Table-800.webp 800w,/2026/assets/img/2026-04-27-uncertainty-lifecycle-in-deep-learning/Uncertainty_Summary_Table-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-uncertainty-lifecycle-in-deep-learning/Uncertainty_Summary_Table.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Table 1: Summary of Uncertainty Lifecycle along with usecase and example </div> <h2 id="conclusion">Conclusion</h2> <p>We discussed the following concept in detail and how they flow through the pipeline from input to output</p> <ul> <li>Propagation: how uncertainty flows through the system.</li> <li>Estimation: Quantifies the associated uncertainty as some score</li> <li>Decomposition: why the model is uncertain (aleatoric vs epistemic).</li> <li>Attribution: which features cause uncertainty.</li> <li>Sensitivity: how uncertainty changes under small input changes.</li> </ul> <p>And in this work, we have proposed an <strong>“Uncertainty Lifecycle”</strong> which presents a holistic framework for connecting and positioning different studied uncertainty concepts in literature. It shows how they are interconnected with each other, where output of each serve as an input for the next like a flow. The entire uncertainty lifecycle can be repeated to gain insight about model and data, debug the model and improve the performance. It can further provide explanation and reliability measures to end user for appropriate decision-making.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-uncertainty-lifecycle-in-deep-learning.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-vlms-waste-their-vision/">Why vlms waste their vision</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>