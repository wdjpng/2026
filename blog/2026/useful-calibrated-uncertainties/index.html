<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> What (and What Not) are Calibrated Uncertainties Actually Useful for? | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="The blogpost clarifies the usefulness of having a model with calibrated probabilities, something that is not often stated in the calibration literature. I shows that a calibrated model can be relied on to estimate average loss/reward, however, good calibration does not mean that a model is useful for per-sample decision making."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/useful-calibrated-uncertainties/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "What (and What Not) are Calibrated Uncertainties Actually Useful for?",
            "description": "The blogpost clarifies the usefulness of having a model with calibrated probabilities, something that is not often stated in the calibration literature. I shows that a calibrated model can be relied on to estimate average loss/reward, however, good calibration does not mean that a model is useful for per-sample decision making.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>What (and What Not) are Calibrated Uncertainties Actually Useful for?</h1> <p>The blogpost clarifies the usefulness of having a model with calibrated probabilities, something that is not often stated in the calibration literature. I shows that a calibrated model can be relied on to estimate average loss/reward, however, good calibration does not mean that a model is useful for per-sample decision making.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#people-have-a-fuzzy-impression-of-calibration-in-ai">People Have a Fuzzy Impression of Calibration (in AI)</a> </div> <div> <a href="#a-general-explanation-of-calibration">A General Explanation of Calibration</a> </div> <ul> <li> <a href="#confidence-calibration">Confidence Calibration</a> </li> </ul> <div> <a href="#what-do-calibrated-probabilities-allow-you-to-do">What Do Calibrated Probabilities Allow You to Do?</a> </div> <ul> <li> <a href="#interactive-example">Interactive Example</a> </li> </ul> <div> <a href="#what-calibrated-probabilities-do-not-guarantee">What Calibrated Probabilities DO NOT Guarantee</a> </div> <ul> <li> <a href="#interactive-example-continued">Interactive Example Continued</a> </li> </ul> <div> <a href="#misunderstandings-from-the-literature">Misunderstandings from the Literature</a> </div> <ul> <li> <a href="#a-brief-retrospective">A Brief Retrospective</a> </li> </ul> <div> <a href="#closing-thoughts-and-takeaways">Closing Thoughts and Takeaways</a> </div> </nav> </d-contents> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/confused-480.webp 480w,/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/confused-800.webp 800w,/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/confused-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/confused.png" class="img-fluid z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="people-have-a-fuzzy-impression-of-calibration-in-ai">People Have a Fuzzy Impression of Calibration (in AI)</h2> <p>The goal of this blogpost is to provide an intuitive and helpful guide on understanding the <em>practical usefulness</em> of a <em>well-calibrated</em> model, for practitioners/researchers in AI both familiar and unfamiliar with calibration. Anecdotally, when I tell researchers and practictioners from other domains that I work in uncertainty estimation, they often give remarks along the lines of “Oh so like calibration?” or “You mean probabilistic and Bayesian/Conformal stuff?”. They seem to know of the research domain and associate it with vague motivations of “safety”, “trustworthiness” and “reliability”, but often have unclear senses of what it actually entails. Furthermore, even <em>within</em> the research domain people often have fuzzy impressions of the motivation for wanting a calibrated model. Papers on calibration are often written with one or two motivating paragraphs, before promptly moving onto the main algorithmic meat. Reading the literature thus becomes a way to learn about <em>how to better calibrate models</em> without really engaging concretely with <em>how, why and when calibrated models are useful</em>.</p> <p>Morevoer, many such motivating paragraphs, in order to communicate more intuitively, end up being imprecise and consequently misleading. Below I’ve included an excerpt from the introduction of <em>On Calibration of Modern Neural Networks</em> <d-cite key="Guo2017Calibration"></d-cite> which is the go-to introductory paper for calibration in deep learning. It is broadly representative of how calibration is typically motivated in AI research recently, and we will unpick it gradually over the course of this blogpost.</p> <blockquote> <p>In real-world decision making systems, classification networks must not only be accurate, but also should indicate when they are likely to be incorrect. As an example, consider a self-driving car that uses a neural network to detect pedestrians and other obstructions. If the detection network is not able to confidently predict the presence or absence of immediate obstructions, the car should rely more on the output of other sensors for braking. Alternatively, in automated health care, control should be passed on to human doctors when the confidence of a disease diagnosis network is low. Specifically, a network should provide a calibrated confidence measure in addition to its prediction. In other words, the probability associated with the predicted class label should reflect its ground truth correctness likelihood.</p> </blockquote> <p>A reader is likely to come away from this paragraph thinking that calibration is important for <em>mitigating risk</em> from errors during <em>individual scenarios</em> (i.e. for an specific road object or individual patient). The reality is more nuanced than this; calibration is actually somewhat orthogonal to a model’s ability to detect its own errors. As such, it is easy to come to misunderstandings over calibration when engaging with the literature. After reading this blogpost, hopefully the reader will have a clearer understanding and intuition of what a calibrated model enables/is important for, and in what usecases calibration is actually insufficient.</p> <h2 id="a-general-explanation-of-calibration">A General Explanation of Calibration</h2> <p>Calibration, as it is defined in deep learning, is not an intuitive concept to understand. As such paper authors tend to first motivate it using imprecise, but more intuitive natural language. I will first provide a general explanation of calibration to the reader as a basis for the rest of the blogpost.</p> <p>For a model to be calibrated, when it predicts a certain probability distribution over different outcomes, the real-world frequency of said outcomes should match that probability distribution. A widely used example is weather forecasting: over a long run of days, it should rain 70% of the time for the days forecasted with 70% chance of rain. Explicitly in natural language a calibrated model is one where</p> \[\begin{array}{l} \text{empirical frequency of outcomes when } \\ \text{corresponding probabilities are predicted} \end{array} = \text{ those predicted probabilities}\] <p>or for a binary event (e.g. rain or no rain), a simpler version is</p> \[\begin{array}{l} \text{empirical frequency of event occurring when } \\ \text{a given probability of event is predicted} \end{array} = \text{ that predicted probability,} \tag{1}\label{eq:calib-natlang}\] <p>which can be expressed mathematically as,</p> \[\underbrace{P(\text{event} \mid \pi) = \pi}_\text{actual probability of event is $\pi$},\quad \text{for }\underbrace{\pi={P}_{\theta}(\text{event}\mid \text{conditions}) }_\text{when model predicts probability $\pi$}\text{ in } [0,1], \tag{2}\label{eq:calib-math}\] <p>where $\pi$ is a random variable that captures the value of the probability output by our model with parameters $\theta$. $\text{conditions}$ could be “temperature of the previous day”, for example. We leave the general version here for reference. In this blogpost we will focus on binary events, however, the intuitions/ideas naturally extend to the general case.</p> \[\begin{align*} &amp;P(\text{outcome}_k \mid \boldsymbol\pi ) = \pi_k,\quad \forall k, \\ &amp;\pi_k={P}_{\theta}(\text{outcome}_k\mid \text{conditions}), \quad \boldsymbol\pi\text{ is a probability vector}. \end{align*}\] <p>Importantly, $P(\text{event} \mid \pi )$ is <strong>not</strong> given $\text{conditions}$. It is the probability of the $\text{event}$ <em>on average</em> over the distribution of possible $\text{conditions}$.</p> \[P(\text{event} \mid \pi) = \sum_{\text{conditions}} P(\text{event} \mid \text{conditions})\, P\bigl(\text{conditions} \mid \pi\bigr). \tag{3}\label{eq:calib-math-avg}\] <p>In the binary case, how well calibrated a model is can be visualised using a reliability diagram <d-cite key="degroot1983comparison,NiculescuMizilCaruana2005"></d-cite> which plots the empirical frequency of $\text{event}$ (on average over $\text{conditions}$) given the predicted model probability $\pi$, i.e. visually comparing the left and right hand sides of Eq. \ref{eq:calib-natlang}. Practically, by binning predicted probabilities $\pi$, $P(\text{event} \mid \pi )$ can be estimated using the empirical frequency of $\text{event}$ within each bin.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/reliability_diagrams-480.webp 480w,/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/reliability_diagrams-800.webp 800w,/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/reliability_diagrams-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/reliability_diagrams.png" class="img-fluid z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Left: Reliability diagram showing over and under-confident models. Right: Practical reliability diagram where empirical frequencies are estimated using bins. </div> <p>A perfectly calibrated model will thus align with $y=x$. Devation below indicates “overconfidence” (frequency &lt; predicted probability); deviation above indicates “underconfidence” (frequency &gt; predicted probability). Note that the notion of over/under-confidence is much more specific here than in colloquial usage; we will return to this point later. Also, a model can be overconfident for a certain range of probabilities and underconfident on a different range (i.e. both at the same time), however, for simplicity’s sake we’ll limit discussion in this blog to models that are either over or under confident without loss of generality.</p> <h3 id="confidence-calibration">Confidence Calibration</h3> <p>The above general presentation of calibration doesn’t immediately resonate with the idea of mitigating risk in safety-critical scenarios that is present in our quoted exerpt from <em>On Calibration of Modern Neural Networks</em> <d-cite key="Guo2017Calibration"></d-cite>, as well as generally throughout the literature. In fact, in <em>On Calibration of Modern Neural Networks</em>, Guo et al. choose a binary $\text{event}$ “model prediction is correct” from the scenario of multi-class classification, which then inherently ties calibration to prediction errors and their associated risks/costs. This specific case of calibration is known as <em>confidence calibration</em> and is the primary form that is found in the literature <d-cite key="Guo2017Calibration,Minderer2021Revisiting,param_temp_scaling_eccv2022,Xiong2023ProCal"></d-cite>. In this case, Eq. \ref{eq:calib-natlang} becomes:</p> \[\begin{array}{l} \text{classification accuracy when a given } \\ \text{probability is predicted for the top class,} \end{array} = \text{ that predicted probability} \tag{3}\label{eq:conf-calib-natlang}\] <p>or mathematically for true label $y$, input $x$ and predicted label (top class) $\hat y=f_\phi(x)$ </p> \[\underbrace{P\bigl(y=\hat y \mid \pi\bigr) = \pi}_\text{actual classification accuracy is $\pi$},\quad \text{for }\underbrace{\pi=P_{\theta}(y=\hat y\mid x) }_\text{predicts probability $\pi$} \text{ in } [0,1]. \tag{4}\label{eq:conf-calib-math}\] <p>One nuance the reader should be aware of is that for the purposes of confidence calibration, the model for the binary event $P_\theta(y=\hat y\mid x)$ is conceptually separate from the underlying classifier $f_\phi(x)$. However, when the classifier is a cross-entropy-trained softmax model (i.e. the vast majority of classifiers in deep learning), ${P}_{\theta}(y=\hat y\mid x)$ can be, and almost always is, extracted from the probability vector over classes output by the classifier<d-cite key="LeCoz2024EfficientCalibration"></d-cite>.</p> <p>For further discussion of other realisations of calibration (e.g. with different definitions of the binary $\text{event}$), as well as more specifics on how calibration is measured in practice (e.g <em>Expected Calibration Error</em><d-cite key="Naeini2015BBQ"></d-cite>) I recommend this <a href="https://iclr-blogposts.github.io/2025/blog/calibration/">excellent blogpost</a> by Maja Pavlovic <d-cite key="pavlovic2025understanding"></d-cite>.</p> <h2 id="what-do-calibrated-probabilities-allow-you-to-do">What Do Calibrated Probabilities Allow You to Do?</h2> <p>Now that we’ve understood what a calibrated model is in an abstract sense, we can start to understand what practical benefit it provides. Consider the calibration equation (Eq. \ref{eq:calib-math}) again,</p> \[P(e \mid \pi) = \pi, \quad \text{for }\pi={P}_{\theta}(e\mid c) \in [0,1].\] <p>where we’ve abbreviated $\text{event}$ and $\text{conditions}$ to random variables $e$ and $c$ respectively. Intuitively, this means that for the predictions with $\pi$ equals to some value, say $0.7$, we can reliably expect $\text{event}$ to occur $70\%$ of the time. Consider a loss (or reward) function $\mathcal{L}(e,\pi)$ that depends on $\text{event}$ and the model’s predicted probability. For example, the 0-1 multiclass classification loss for confidence calibration is 0 when $\text{event}$ occurs (correct prediction) and 1 when $\text{event}$ doesn’t occur (misclassification). Intuitively, we can now rely on $\pi$ to calculate the <em>expected/average loss</em>. Consider confidence calibration where for some image classifier our binary model is assigning the predicted class probability $\pi=0.6$ of being correct for half of the input images $x$ and $\pi=0.4$ for the other half. If our model is well calibrated we can then reliably claim that it will have an average accuracy of $60\%$ on the first half and $40\%$ on the second half, leading to an overall accuracy of $50\%$, <em>without needing to compare its predictions to any ground truth labels $y$</em>. This is the sense in which a calibrated model is <em>reliable</em>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/calibration_illust-480.webp 480w,/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/calibration_illust-800.webp 800w,/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/calibration_illust-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/calibration_illust.png" class="img-fluid z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Illustration of how a confidence-calibrated model allows estimation of expected accuracy from only inputs/conditions without needing events/labels. </div> <p>Once we have a reliable estimate of the expected loss/reward we can make downstream decisions, e.g. in this deployment scenario the image classifier is not accurate enough and will lead to profit loss from misclassifications, so we decide to not deploy it. Additionally, humans are naturally able to interpret probabilities by inspection <d-cite key="CosmidesTooby1996IntuitiveStat"></d-cite>, so human decision making can be performed naturally and reliably if model outputs are calibrated probabilities.</p> <blockquote> <p><strong>Key Takeaway.</strong> <em>A well-calibrated model enables reliable estimation of average loss/reward from only inputs/conditions without any ground truth events/labels. These reliable estimates can then be used to effectively inform downstream decisions that depend on average behaviour</em>.</p> </blockquote> <p>Now lets show this mathematically (the reader is free to skip to the interactive widget in the next section if they are happy with the above intuition). The expected loss of the model over the distribution of $\text{conditions}$ and $\text{event}$ (i.e. the data) is,</p> \[\begin{align*} \mathbb{E}[\mathcal{L}(e,\underbrace{\pi(e,c)}_{\pi={P}_{\theta}(e\mid c)})] &amp;= \sum_{c,e} \mathcal{L}(e,\pi(e,c)) P(e,c) \\ &amp;= \sum_{c,e,\pi} \mathcal{L}(e,\pi) P(e ,\pi,c) \\ &amp;= \sum_{e,\pi} \mathcal{L}(e,\pi) P(e \mid \pi) P(\pi)\\ &amp;= \sum_{e,\pi} \mathcal{L}(e,\pi) P(e \mid \pi) \sum_{c} P(\pi, c) \\ &amp;= \sum_{e,\pi} \mathcal{L}(e,\pi) P(e \mid \pi) \sum_{c} \underbrace{\delta(\pi(e,c)) P(c)}_\text{Dirac $\delta$ for $\pi={P}_{\theta}(e\mid c)$}. \\ \end{align*}\] <p>If the model is well calibrated we can subsitute $\pi$ in for $P(e\mid \pi)$ and then approximate the expected loss by sampling model predicted probabilities ${\pi^{(n)}={P}_{\theta}(e=1\mid c^{(n)})}_n$ over $N$ samples of $\text{conditions}$ drawn from the data distribution $c\sim P(c)$,</p> \[\begin{align*} \mathbb{E}[\mathcal{L}(e,\pi(e,c))]&amp;= \sum_{e,\pi} \mathcal{L}(e,\pi) P(e \mid \pi) \sum_{c} \delta(\pi(e,c)) P(c) \\ &amp;= \underbrace{\sum_{e} \mathcal{L}(e,\pi) \sum_{c}\pi(e,c) P(c)}_{\text{no explicit sum over }\pi\text{ as it is a fixed function of }e,c} \\ &amp;= \sum_{c} P(c) \sum_{e} \mathcal{L}(e,\pi) \pi(e,c)\\ &amp;\approx \frac{1}{N} \sum_{n} \mathcal{L}(e=1,\pi^{(n)}) \pi^{(n)} +\mathcal{L}(e=0,\pi^{(n)}) (1-\pi^{(n)}) \end{align*}\] <p>Thus showing that <strong>a well-calibrated model enables reliable estimation of average loss/reward from only inputs/conditions without events/labels</strong>.</p> <h3 id="interactive-example">Interactive Example</h3> <p>Below we discuss a concrete example where we want to decide on a decision threshold for issuing loans based on predicted default probabilities. The problem setup is as follows:</p> <ul> <li>A bank wants to issue loans to customers based on their predicted probability of default.</li> <li>On average 35% of customers default on their loans.</li> <li>The bank incurs a loss of 240 dollars for each defaulted loan and gains 18 dollars for each successfully repaid loan.</li> <li>The bank uses a machine learning model to predict the probability of default (failure to pay back) for each customer.</li> <li>If the predicted probability of default is above threshold $\tau$ the bank decides not to lend.</li> <li>The bank wants to choose the threshold $\tau$ that maximises profit, but the responsible department doesn’t have access to default event data and so relies on the model probabilities assuming they are calibrated.</li> </ul> <p>Explore $\tau$ by varying it to maximise the estimated profit. You should observe that by optimising $\tau$ based on the calibrated model probabilities, the bank can achieve near-optimal profit even without access to default $\text{event}$ data, i.e. the model can be trusted. Now toggle to the uncalibrated models. The average profit estimates become unreliable, leading to choices of $\tau$ that squander profit or even incur losses on the actual customers. The model that is overconfident about the probability of defaulting leads to overly pessimistic estimates of profit, whilst the underconfident model is overly optimistic.</p> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-useful-calibrated-uncertainties/loan_widget.html" frameborder="0" scrolling="no" height="600px" width="120%"></iframe> </div> <h2 id="what-calibrated-probabilities-do-not-guarantee">What Calibrated Probabilities DO NOT Guarantee</h2> <p>Recall the motivating paragraph from <em>On Calibration of Modern Neural Networks</em> <d-cite key="Guo2017Calibration"></d-cite>, one part of which states:</p> <blockquote> <p>… in automated health care, control should be passed on to human doctors when the confidence of a disease diagnosis network is low.</p> </blockquote> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/selective_classification-480.webp 480w,/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/selective_classification-800.webp 800w,/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/selective_classification-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/selective_classification.png" class="img-fluid z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Illustration automated medical diagnosis with selective classification where a prediction should be deferred to an expert if it is likely to be incorrect. </div> <p>What the authors are actually describing here is selective classification <d-cite key="Geifman2017SelectiveClassification,Jaeger2023FailureDetection,Xia2024SelectiveOOD"></d-cite> (or classification with abstention), rather than calibration. In selective classification, if a model is likely to make an error on a specific input, then a useful uncertainty estimate should reflect this by indicating low confidence, triggering abstention, e.g. deferring a diagnosis to a human doctor. That is to say, better per-sample decisions can be made based on the model $P_\theta(\text{event}\mid \text{conditions})$. This is an intuitively desirable property for uncertainty estimates, however, <strong>a well-calibrated model is not one that is necessarily better for per-sample decision making</strong>, like selective classification.</p> <p>Consider our previous well-calibrated image classifier. Here we are focusing on the model of $P_\theta(\text{event of correct prediction}\mid \text{image})$ rather than the multi-class classifier $f_\phi(x)$ itself, which we assume has a constant accuracy of $50\%$. If we set $\tau = 0.5$, then we would abstain on the images with accuracy $40\%$ and boost the accuracy on the selected images to $60\%$.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/calibration_discrimination-480.webp 480w,/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/calibration_discrimination-800.webp 800w,/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/calibration_discrimination-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/calibration_discrimination.png" class="img-fluid z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Illustration of how abstention can improve accuracy by rejection uncertain predictions. </div> <p>However, if we consider another well-calibrated model that predicts $\pi=0.5$ for <em>all</em> images, then all predictions have tied confidence and we have no way to discriminate between correct and incorrect predictions. It is well-calibrated but its uncertainties are <em>useless</em> for abstaining on potential errors! The key here is that <em>calibration is related to the accuracy</em> <strong><em>averaged</em></strong> <em>over different inputs</em> $x$, and does not interrogate the model for each input individually. It only examines $\pi$ with respect to true probability $P(\text{event}\mid \pi)$, not with respect to true probability $P(\text{event}\mid \text{conditions})$.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/cal_bad_disc-480.webp 480w,/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/cal_bad_disc-800.webp 800w,/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/cal_bad_disc-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/cal_bad_disc.png" class="img-fluid z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Illustration of how a well-calibrated model with no discrimination ability cannot improve accuracy via abstention. </div> <p>Conversely, if a different well-calibrated model for $\text{event}$ “correct prediction” is able to predict $\pi=0.8$ for a subset of half of the images where the classifier has accuracy $80\%$ and $\pi=0.2$ for the other half where accuracy is $20\%$, then the selected images will have an accuracy of $80\%$ when setting $\tau=0.5$.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/cal_better_disc-480.webp 480w,/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/cal_better_disc-800.webp 800w,/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/cal_better_disc-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/cal_better_disc.png" class="img-fluid z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Illustration of how a well-calibrated model with better discrimination ability can improve accuracy via abstention. </div> <p>Finally, if we consider an uncalibrated (overconfident) model that predicts $\pi=0.9$ for half of the images where the classifier has accuracy $80\%$ and $\pi=0.6$ for the other half where accuracy is $20\%$, then the selected images will have an accuracy of $80\%$ when setting $\tau=0.7$. In this case, despite being <em>uncalibrated</em>, the model is still able to effectively discriminate between correct and incorrect predictions and abstain on samples it is more likely to be incorrect on. However, in order to estimate the accuracy of selected images and thus reliably choose a threshold $\tau$ for deployment, we would need access to ground truth labels from a validation set. If you don’t trust the calibration of your model, the only way to reliably determine its performance is to validate it against ground truth labels.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/uncal_better_disc-480.webp 480w,/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/uncal_better_disc-800.webp 800w,/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/uncal_better_disc-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-useful-calibrated-uncertainties/uncal_better_disc.png" class="img-fluid z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Illustration of how an uncalibrated model with good discrimination ability can improve accuracy via abstention, if there is access to validation labels to choose the threshold and validate the accuracy. </div> <p>We note that the above example is purposely reductive as generally speaking $\pi$ will take many possible values rather than just to two, and that downstream decisions may involve many possible actions. However, the intuition extends naturally to the general case. To summarise the above,</p> <blockquote> <p><strong>Key Takeaway.</strong> <em>A well-calibrated model does not in any way guarantee good performance for downstream decision making for individual inputs. What fundamentally determines performance is the ability to</em> <strong><em>discriminate</em></strong> <em>between $\text{event}$ and $\text{no event}$ for each individual set of $\text{conditions}$. Calibration means reliable estimation of average loss/reward which enables the setting of decision rules without access to ground truth events/labels, however, the same rules can be determined equivalently using uncalibrated models if there is access to ground truth events/labels for validation.</em></p> </blockquote> <p>Now we will write out the above discussion more mathematically. Again, if the above inuition is sufficient for the reader, they can skip to the interactive widget below. Consider again the calibration equation (Eq. \ref{eq:calib-math}) and Eq. \ref{eq:calib-math-avg},</p> \[P(e \mid \pi) = \pi, \quad \text{for }\pi={P}_{\theta}(e\mid c) \in [0,1],\] \[P(e \mid \pi) = \sum_{c} P(e \mid c)\,P\bigl(c \mid \pi\bigr).\] <p>Recall that $P(e \mid \pi)$ is <strong>not</strong> given $\text{conditions}$ $c$. As such calibration does not guarantee for any specific $\text{conditions}$ $c$ that the model’s predicted probability $\pi$ matches the actual probability of $\text{event}$ occurring, i.e.</p> \[\underbrace{P(e \mid c)}_{\text{true prob. given }c} \approx \underbrace{\pi(e,c)={P}_{\theta}(e\mid c)}_{\text{model pred. prob.}}.\] <p>In Bayesian decision theory <d-cite key="Chow1970OptimumReject,Bishop2006PatternRecognition"></d-cite>, the optimal decision rule <em>requires</em> knowledge of the true probability $P(e \mid c)$ for each individual $c$ in order to minimise expected loss/reward on individual inputs.</p> \[\begin{aligned} \text{optimal decision}(c) &amp;\in \arg\min_{d(c)} \mathbb{E}_{e \sim P(\cdot \mid c)} \bigl[ \mathcal{L}\bigl(e,\pi(e,c),d(c)\bigr) \bigr] \\ &amp;= \arg\min_{d(c)} \sum_{e \in \{0,1\}} \mathcal{L}\bigl(e,\pi(e,c),d(c)\bigr)\,P(e\mid c). \end{aligned}\] <p>Thus calibration alone does not tell you whether or not you will be able to make good decisions on individual inputs. Now to concretely discuss our examples of selective classification and loan issuance, note that what matters for decision making is how well we can order or distinguish different $\text{conditions}$, not the absolute value of the predicted probability.</p> <p>Consider again a binary event $e \in {0,1}$ and define the true event probability</p> \[p(c) := P(e=1 \mid c).\] <p>Suppose the model (or some post-processing) produces a scalar score</p> \[s(c) := f\bigl(p(c)\bigr),\] <p>where $f:[0,1]\to\mathbb{R}$ is strictly monotone. In particular, $f$ is invertible $p = f^{-1}\bigl(s\bigr)$. Assume decisions are taken by some rule $d(c;\eta)$ with parameters $\eta$ that depends on $c$ only through $s(c)$, i.e.</p> \[d(c;\eta) = g\bigl(s(c);\eta\bigr) = g\bigl(f(p(c));\eta\bigr).\] <p>Because $f$ is invertible and strictly monotone, there is a one-to-one reparameterisation of the decision rule in terms of $p(c)$: for every $\eta$ there exists a $\eta’$ such that</p> \[d(c;\eta) = g\bigl(f(p(c));\eta\bigr) = \tilde g\bigl(p(c);\eta'\bigr),\] <p>so the resulting actions (and hence decision regions) are identical whether we work with $s(c)$ or with $p(c)$. The expected loss of the decision rule can therefore be written as</p> \[R(\eta) := \mathbb{E}_{(e,c)\sim P}\bigl[\mathcal{L}\bigl(e,\pi(e,c),d(c;\eta)\bigr)\bigr] = \mathbb{E}_{(e,c)\sim P}\bigl[\mathcal{L}\bigl(e,\pi(e,c),\tilde g(p(c);\eta')\bigr)\bigr].\] <p>If we can sample $(e,c)\sim P(e,c)$ (i.e. validation dataset), we can estimate $R(\eta)$ empirically as</p> \[\hat R(\eta) = \frac{1}{N}\sum_{n=1}^N \mathcal{L}\bigl(e^{(n)},\pi(e^{(n)},c^{(n)}),d(c^{(n)};\eta)\bigr),\] <p>and choose the parameter $\eta$ that minimises this estimate. Since there is a one-to-one correspondence between $\eta$ and $\eta’$, any strictly monotone (possibly uncalibrated) mapping of $P(e\mid c)$ is sufficient for learning the optimal decision rule within this family from validation data.</p> <p>In the particular cases we consider in this blog (selective classification and loan issuance), the decision rule is a simple binary threshold of the score,</p> \[d(c;\tau) := \begin{cases} 0, &amp; s(c) \ge \tau,\\[4pt] 1, &amp; s(c) &lt; \tau. \end{cases}\] <p>Finally, if the reader is interested in exploring further theory related to the above, <d-cite key="PerezLebel2023GroupingLoss,chidambaram2025reassessing"></d-cite> discuss the limitations of calibration from the perspective of proper scoring rules.</p> <h3 id="interactive-example-continued">Interactive Example Continued</h3> <p>Extending our previous interactive example of the bank deciding whether to issue loans, we introduce three different levels of discrimination for the predicted default probabilities. We use the standard Receiver Operating Characteristic (ROC) curve to show discrimination ability. Observe how the maximum actualisable profit when varying $\tau$ depends on the discrimination ability of the model, not its calibration. A well-calibrated model with no discrimination ability is able to reliably estimate that it can’t make any money, but it still can’t make any money! Also observe how having access to validation labels (i.e. reading the actualised profit directly) means you don’t need to pay attention to the estimated profit (and thus the calibration of the model).</p> <div class="l-page"> <iframe src="/2026/assets/html/2026-04-27-useful-calibrated-uncertainties/loan_discrimination_widget.html" frameborder="0" scrolling="no" height="600px" width="120%" style="display:block; margin: 0 auto; max-width: 100%;"></iframe> </div> <h2 id="misunderstandings-from-the-literature">Misunderstandings from the Literature</h2> <p>Looking at our motivating paragraph from <em>On Calibration of Modern Neural Networks</em> <d-cite key="Guo2017Calibration"></d-cite> again:</p> <blockquote> <p>In real-world decision making systems, classification networks must not only be accurate, but also should indicate when they are likely to be incorrect. As an example, consider a self-driving car that uses a neural network to detect pedestrians and other obstructions. If the detection network is not able to confidently predict the presence or absence of immediate obstructions, the car should rely more on the output of other sensors for braking. Alternatively, in automated health care, control should be passed on to human doctors when the confidence of a disease diagnosis network is low. Specifically, a network should provide a calibrated confidence measure in addition to its prediction. In other words, the probability associated with the predicted class label should reflect its ground truth correctness likelihood.</p> </blockquote> <p>We can see that in an effort to convey the importance of calibration intuitively, the authors have conflated calibration (something that is important for reliably estimating average loss/reward) with selective classification (something that is important for mitigating risk on individual inputs). This conflation has propagated and been repeated throughout the literature, leading to a fuzzy understanding of calibration’s practical importance in the research commmunity <d-cite key="Minderer2021Revisiting,param_temp_scaling_eccv2022,Xiong2023ProCal,Nixon2019CVPRW,Zhang2020MixnMatchCalibration"></d-cite>.</p> <p>Another factor that leads to confusion is the use of the word “overconfident”. A key empirical finding of <em>On Calibration of Modern Neural Networks</em> <d-cite key="Guo2017Calibration"></d-cite> is that certain neural networks are overconfident in the <em>confidence calibration sense</em>. This overconfidence result is oft-cited in papers <d-cite key="Minderer2021Revisiting,param_temp_scaling_eccv2022,malinin2020ensemble"></d-cite> as motivation without clarifying that overconfidence in calibration <em>is orthogonal to how well a model can discriminate individual failures/errors</em> as we’ve discussed in this blog. As we alluded to earlier, the meaning of “overconfident” in calibration is different to what it means colloquially. Colloquially, overconfidence typically refers to a person’s attitude to a given situation (specific $\text{conditions}$):<em>“He overconfidently decided to choose the hard option only to fail”</em>. Thus it is not hard to see how someone might conflate the colloquial and calibration meanings of “overconfident” together when engaging with the literature, confusing calibration and selective classification.</p> <h3 id="a-brief-retrospective">A Brief Retrospective</h3> <p>So how did we get here?</p> <h2 id="closing-thoughts-and-takeaways">Closing Thoughts and Takeaways</h2> <p>Calibrated uncertainty estimates are essential for building trustworthy machine learning systems. While modern deep learning models achieve high accuracy, they often fail to provide reliable uncertainty estimates. Post-hoc calibration methods offer a practical solution that can be applied to existing models with minimal overhead.</p> <p>Future research directions include:</p> <ol> <li>Developing calibration methods that work well with out-of-distribution data.</li> <li>Exploring calibration-aware training objectives that improve calibration during training rather than after.</li> <li>Extending calibration methods to more complex settings such as regression and structured prediction tasks.</li> <li>Understanding the relationship between calibration and robustness to distribution shift.</li> </ol> <p>As machine learning systems are deployed in increasingly critical applications, the importance of well-calibrated uncertainty estimates will only continue to grow. We hope this blog post provides a useful foundation for understanding and improving calibration in deep learning models.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-useful-calibrated-uncertainties.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-long-context/">Text-as-Image, A Visual Encoding Approach for Long-Context Understanding</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/using-large-language-models-to-simulate-and-predict-human-decision-making/">Using Large Language Models to Simulate and Predict Human Decision-Making</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/unlearning-or-untraining/">Is your algorithm Unlearning or Untraining?</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>