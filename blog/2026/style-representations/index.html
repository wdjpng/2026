<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Artistic Style and the Play of Neural Style Representations | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="How do neural networks percieve the complex human construct of artistic style? We explore the dynamic interplay between diverse machine representations of style and style definitions. We reveal a profound divergence where models often reject established historical narratives in favour of their own perceptual truths."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/style-representations/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">p{text-align:justify}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Artistic Style and the Play of Neural Style Representations",
            "description": "How do neural networks percieve the complex human construct of artistic style? We explore the dynamic interplay between diverse machine representations of style and style definitions. We reveal a profound divergence where models often reject established historical narratives in favour of their own perceptual truths.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Artistic Style and the Play of Neural Style Representations</h1> <p>How do neural networks percieve the complex human construct of artistic style? We explore the dynamic interplay between diverse machine representations of style and style definitions. We reveal a profound divergence where models often reject established historical narratives in favour of their own perceptual truths.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#the-chameleon-concept-defining-and-operationalizing-style">The Chameleon Concept-Defining and Operationalizing "Style"</a> </div> <div> <a href="#the-players-a-taxonomy-of-neural-representations">The Players-A Taxonomy of Neural Representations</a> </div> <ul> <li> <a href="#the-generalists-generic-task-based">The Generalists (Generic Task-Based)</a> </li> <li> <a href="#the-statisticians-style-feature-based">The Statisticians (Style Feature-Based)</a> </li> <li> <a href="#the-synthesizers-style-transfer">The Synthesizers (Style-Transfer)</a> </li> <li> <a href="#the-linguists-language-based">The Linguists (Language-Based)</a> </li> <li> <a href="#the-specialists-style-trained">The Specialists (Style-Trained)</a> </li> </ul> <div> <a href="#the-analysis-effectiveness-in-the-unsupervised-arena">The Analysis-Effectiveness in the Unsupervised Arena</a> </div> <ul> <li> <a href="#insight-i-the-failure-of-generality-in-disentanglement">Insight I. The Failure of Generality in Disentanglement</a> </li> <li> <a href="#insight-ii-the-great-divergence-machine-perception-vs-art-history">Insight II. The Great Divergence-Machine Perception vs. Art History</a> </li> <li> <a href="#insight-iii-the-triumph-of-synthesis-in-perceptual-definitions">Insight III. The Triumph of Synthesis in Perceptual Definitions</a> </li> <li> <a href="#insight-iv-the-hidden-geometry-is-hierarchical">Insight IV. The Hidden Geometry is Hierarchical</a> </li> </ul> <div> <a href="#conclusion-the-unresolved-play-and-the-path-forward">Conclusion-The Unresolved Play and the Path Forward</a> </div> </nav> </d-contents> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-style-representations/teaser-480.webp 480w,/2026/assets/img/2026-04-27-style-representations/teaser-800.webp 800w,/2026/assets/img/2026-04-27-style-representations/teaser-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-style-representations/teaser.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <p><i> <b>Figure 1:</b> The figure shows a sample of the WikiArt dataset, which has ground truth clusters depicting various art movements from Baroque to Mannerism. The above artworks are re-clustered using the neural style representation $F_{StyleShot}$; the infographics show the distribution of ground-truth in different clusters. Even though the re-clustering through $F_{StyleShot}$ representation does not produce clusters that adhere to ground truth, we can see that the artworks present in the same cluster are similar to each other in terms of style, highlighting a fundamental discrepancy between historical art categorizations and perceptual style representations.</i></p> </div> <p>Artistic style is central to visual art, embodying an artist’s identity, emotional expression, cultural context, and aesthetic choices. In the realm of computer vision, “style” has evolved into a critical dimension for understanding images beyond mere object categories. We have developed diverse architectures - from convolutional networks to generative models and vision-language systems - each claiming to capture artistic style effectively.</p> <p>But how well do these neural representations actually “play” with the complex, multifaceted concept of artistic style? When stripped of supervised labels and left to organize artworks on their own, do these models rediscover the history of art, or do they reveal a fundamental disconnect between machine perception and human categorization?</p> <p>We explore this by analyzing 16 state-of-the-art neural style representations through the lens of <strong>unsupervised clustering</strong>, using visual art as a rigorous testbed.</p> <h2 id="the-chameleon-concept-defining-and-operationalizing-style">The Chameleon Concept-Defining and Operationalizing “Style”</h2> <p>Before we can evaluate how well a neural network represents style, we face a fundamental scientific hurdle: the lack of a single, universally accepted ground-truth definition. Artistic style is a notorious chameleon; it is a complex amalgamation of an artist’s individual “hand,” broad historical movements, technical medium, and cultural context.</p> <p>The machine learning and art history communities have wrestled with this ambiguity, proposing varied interpretations. In the literature, style has been treated not as a monolith, but as myriad distinct concepts:</p> <ul> <li> <strong>Statistical properties</strong> latent within neural representations <d-cite key="gatysnst"></d-cite>.</li> <li>The <strong>distinctive signatures</strong> belonging to specific artists or historical movements <d-cite key="elgammal2017"></d-cite>.</li> <li> <strong>Low-level visual attributes</strong> such as color palettes, textures, or brushstrokes <d-cite key="paint-st"></d-cite>.</li> <li> <strong>Domain-specific distributions</strong> that distinguish one modality from another <d-cite key="zhu2020"></d-cite>.</li> <li> <strong>Perceptually significant cues</strong> derived from human behavioral studies <d-cite key="muller1979"></d-cite>.</li> <li> <strong>Transformative operations</strong> that can be applied to content to alter its appearance <d-cite key="huang2025"></d-cite>.</li> </ul> <p>Recognizing this rich and varied landscape, our study does not rely on a single definition. Instead, we rigorously probe neural representations through four distinct, operational lenses that reflect these established views, isolating different aspects of aesthetic identity through specific benchmarks:</p> <ul> <li> <strong>The Historical Lens (Art Movements):</strong> Style defined as broad, socially-constructed categories tied to specific periods and philosophies (e.g., Baroque, Cubism). <em>Aligns with definitions by Elgammal et al. (2017)<d-cite key="elgammal2017"></d-cite>.</em> (Dataset: WikiArt-ArtMove)</li> <li> <strong>The Individual Lens (Artistic Signature):</strong> Style defined as the unique, consistent visual fingerprint of a specific creator across their body of work. <em>Aligns with definitions by Elgammal et al. (2017)<d-cite key="elgammal2017"></d-cite>.</em> (Dataset: WikiArt-Artist)</li> <li> <strong>The Perceptual Lens (Visual Attributes):</strong> Style reduced to pure, low-level visual mechanics—isolated textures, color palettes, and brushstrokes—separated from semantic content. <em>Aligns with notions from Gatys et al. (2015) <d-cite key="gatysnst"></d-cite>, Liu et al. (2024) <d-cite key="paint-st"></d-cite>, and Huang et al. (2025)<d-cite key="huang2025"></d-cite>.</em> (Dataset: Synthetic Curated Datasets - MSC/MMC)</li> <li> <strong>The Disentanglement Lens (Style as Domain):</strong> Style defined simply as the “manner” of depiction distinct from the “object” being depicted (e.g., recognizing a “sketch” vs. a “painting”). <em>Aligns with domain-specific definitions by Zhu et al. (2020) <d-cite key="zhu2020"></d-cite>.</em> (Dataset: DomainNet)</li> </ul> <h2 id="the-players-a-taxonomy-of-neural-representations">The Players-A Taxonomy of Neural Representations</h2> <p>To truly understand how AI grasps a complex concept like artistic style, we cannot rely on a single model type. In deep learning, architecture is destiny: a model’s training objective and structure fundamentally dictate what information it discards and what it deems essential. A network built to identify stop signs develops a very different “visual cortex” than one built to generate surrealist paintings.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-style-representations/feat_arc-480.webp 480w,/2026/assets/img/2026-04-27-style-representations/feat_arc-800.webp 800w,/2026/assets/img/2026-04-27-style-representations/feat_arc-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-style-representations/feat_arc.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <p><i> <b>Figure 2:</b> The sub-figures 1-16 show the Architectures for extracting various neural style representations where representation 1-3 are Generic Task-based Models, 4-6 are from Style Feature-based Models, 7-11 are from Style Transfer based Models, 12-13 are from Language models and 14-16 are from Style Trained models. </i> </p> </div> <table> <thead> <tr> <th>Category</th> <th>Model</th> <th>Source/Training</th> <th>Representation Extracted</th> </tr> </thead> <tbody> <tr> <td rowspan="3">Generic Task-based</td> <td>DenseNet <br> (F<sub>Dense</sub>)</td> <td>ImageNet CNN</td> <td>1024-dim pooled final layer features</td> </tr> <tr> <td>Vision--Language <br> (F<sub>LongCLIP</sub>)</td> <td>LongCLIP</td> <td>Joint image--text embeddings (long captions)</td> </tr> <tr> <td>Self-supervised <br> (F<sub>DINO</sub>)</td> <td>DINOv2</td> <td>SSL-based semantic embeddings</td> </tr> <tr> <td rowspan="2">Style Feature-based</td> <td>Gram Matrices <br> (F<sub>Gram</sub>, F<sub>g.c</sub>)</td> <td>VGG19 conv5_1</td> <td>Gram matrix statistics; cosine similarity variant</td> </tr> <tr> <td>Introspective Style Attribution <br> (F<sub>IntroStyle</sub>)</td> <td>Diffusion UNet</td> <td>Channel-wise mean/variance embeddings</td> </tr> <tr> <td rowspan="5">Style-Transfer</td> <td>StyleGAN <br> (F<sub>StyleGAN</sub>)</td> <td>GAN</td> <td>Latent w vector</td> </tr> <tr> <td>Stytr$^2$ <br> (F<sub>Stytr2</sub>)</td> <td>Transformer</td> <td>Encoder outputs from patch embeddings</td> </tr> <tr> <td>StyleShot <br> (F<sub>StyleShot</sub>)</td> <td>Transformer</td> <td>Multi-scale patch embeddings</td> </tr> <tr> <td>Mamba-ST <br> (F<sub>Mamba</sub>)</td> <td>VSSM</td> <td>Style encoder representations</td> </tr> <tr> <td>DEADiff <br> (F<sub>DEADiff</sub>)</td> <td>Diffusion T2I</td> <td>Q-Former embeddings (style disentangled)</td> </tr> <tr> <td rowspan="2"><b>Language-based (ours)</b></td> <td>Style Caption <br> (F<sub>StyleCap</sub>)</td> <td>InternVL2 + LongCLIP</td> <td>Encoded style captions</td> </tr> <tr> <td>Style Concept Annotations <br> (F<sub>Annot</sub>)</td> <td>InternVL2 + taxonomy</td> <td>Structured annotations across 59 concepts</td> </tr> <tr> <td rowspan="2">Style-Trained</td> <td>Contrastive Style Descriptors <br> (F<sub>CSD</sub>)</td> <td>ViT on LAION Aesthetics</td> <td>Contrastive embeddings with style tags</td> </tr> <tr> <td><b>Artwork-Trained ViTs (ours)</b></td> <td>Fine-tuned on WikiArt</td> <td>(i) Art movement (F<sub>ArtMove</sub>), <br> (ii) Artist (F<sub>Artist</sub>)</td> </tr> </tbody> </table> <div class="caption"> <p><i><b>Table 1:</b> Summary of style representations explored. Our contributions are highlighted in <b>bold</b>.</i></p> </div> <p>If we want to know if machines can perceive style unsupervised, we must cast a wide net across the entire ecosystem of modern AI. We assembled a diverse cast of 16 state-of-the-art neural representations, categorizing them into five distinct families based on their inherent “worldview”:</p> <h3 id="the-generalists-generic-task-based">The Generalists (Generic Task-Based)</h3> <p>These are the foundation models of modern computer vision—the versatile workhorses trained on massive, diverse datasets (like ImageNet or LAION) for broad tasks like classification or image-text alignment.</p> <ul> <li> <strong>Examples:</strong> <strong>DenseNet</strong> <d-cite key="densenet"></d-cite> (supervised CNN), <strong>DINOv2</strong> <d-cite key="dinov2"></d-cite> (self-supervised Transformer), and <strong>LongCLIP</strong> <d-cite key="longclip"></d-cite>(vision-language contrastive learning).</li> <li> <strong>The Perspective:</strong> These models are experts at semantic content—knowing <em>what</em> is in an image. The critical question is: in learning to recognize a “dog,” do their rich feature spaces implicitly capture the stylistic manner in which the dog is painted? Are they jacks-of-all-trades, or masters of content only?</li> </ul> <h3 id="the-statisticians-style-feature-based">The Statisticians (Style Feature-Based)</h3> <p>This family is rooted in the breakthrough moment of neural style transfer. Instead of using raw network outputs, these methods apply explicit mathematical operations to feature maps intended to isolate texture and discard spatial structure.</p> <ul> <li> <strong>Examples:</strong> <strong>Gram Matrices ($F_{Gram}$)</strong> <d-cite key="gatysnst"></d-cite> extracted from VGG networks. By calculating feature correlations, they capture the statistical “fingerprint” of textures and brushstrokes globally across an image. We also use mathematical correlation of gram matrices and consine similarity calculated on the representations extracted from the VGG network ($F_{g.c}$). Furthermore, we also explore modern variants like <strong>Introspective Style Attribution</strong> <d-cite key="introstyle"></d-cite> from diffusion models.</li> <li> <strong>The Perspective:</strong> Style is math. It is a statistical distribution of low-level patterns, separate from the arrangement of objects.</li> </ul> <h3 id="the-synthesizers-style-transfer">The Synthesizers (Style-Transfer)</h3> <p>These representations come from models explicitly engineered to <em>create</em> or <em>manipulate</em> art.</p> <ul> <li> <strong>Examples:</strong> The latent space of generative models like <strong>StyleGAN</strong> <d-cite key="stylegan"></d-cite>, Transformer-based transfer models like <strong>StyleShot</strong> <d-cite key="styleshot"></d-cite>, and diffusion-based editing models like <strong>DEADiff</strong> <d-cite key="deaddiff"></d-cite>.</li> <li> <strong>The Perspective:</strong> The strongest hypothesis: ability implies understanding. If a model’s architecture is designed to successfully separate style from content to generate a new image in the style of Van Gogh, surely its internal representations must hold a highly disentangled, accurate blueprint of that style.</li> </ul> <h3 id="the-linguists-language-based">The Linguists (Language-Based)</h3> <p>This is a novel approach leveraging the explosion of Large Vision-Language Models (LVLMs). Style is often described with words—”gloomy,” “geometric,” “gestural.”</p> <ul> <li> <strong>Examples:</strong> We prompt advanced LVLMs <d-cite key="internvl2"></d-cite> to generate rich <strong>Style Captions</strong> or structured <strong>Concept Annotations</strong> for artworks, then encode that text into embeddings <d-cite key="longclip"></d-cite>.</li> <li> <strong>The Perspective:</strong> Style is semantic. By bridging vision and language, these models translate visual aesthetics into rich textual descriptions, encoding style as meaning rather than just pixel patterns.</li> </ul> <h3 id="the-specialists-style-trained">The Specialists (Style-Trained)</h3> <p>These models have an unfair advantage: they have seen the textbooks. They are supervised directly on art historical data.</p> <ul> <li> <strong>Examples:</strong> <strong>Contrastive Style Descriptors ($F_{CSD}$)</strong> <d-cite key="csd"></d-cite> trained on artistic tags, or Vision Transformers <d-cite key="vit"></d-cite> fine-tuned specifically to classify WikiArt artist and movement labels ($F_{Artist}$, $F{ArtMove}$).</li> <li> <strong>The Perspective:</strong> The expert approach. These models have been explicitly taught human categories of art. The question for unsupervised clustering is: once the teacher leaves the room (removing the labels), do they still organize new art according to those rules?</li> </ul> <h2 id="the-analysis-effectiveness-in-the-unsupervised-arena">The Analysis-Effectiveness in the Unsupervised Arena</h2> <p>Having defined our terms and assembled our cast of neural players, we now move to the core of our investigation. We took these 16 diverse representations and dropped them into an unsupervised arena—using clustering algorithms like K-Means <d-cite key="kmeans"></d-cite> and Deep Embedded Clustering (DEC) <d-cite key="dec"></d-cite>—to see how they would organize the world of art without human supervision.</p> <p>The results are not a simple leaderboard of “best to worst.” Instead, they reveal crucial insights about the nature of these architectures and the gap between human definitions of style and machine perception.</p> <p>Here are four key takeaways for researchers and practitioners in AI and computer vision.</p> <h3 id="insight-i-the-failure-of-generality-in-disentanglement">Insight I. The Failure of Generality in Disentanglement</h3> <p>For a machine to truly grasp style, it must learn to ignore content. It needs to recognize that a charcoal sketch of a clock and a charcoal sketch of a dog share the same “domain” style, while a photo of a clock is fundamentally different.</p> <p>We tested this using the <strong>DomainNet</strong> dataset, the ultimate test of content-style disentanglement.</p> <ul> <li> <strong>The Reality Check:</strong> The <strong>Generalists</strong> (like DenseNet or DINOv2) failed this test. Their feature spaces are so heavily optimized for semantic object recognition that they could not “unsee” the objects. They clustered clocks with clocks, regardless of whether they were sketches or paintings.</li> <li> <strong>The Success Story:</strong> The models that succeeded were those explicitly designed for disentanglement. <strong>DEADiff</strong> (a diffusion-based model) and <strong>CSD</strong> (contrastive style descriptors) achieved high scores, successfully grouping images by domain while ignoring the objects within them.</li> </ul> <div class="caption"> <b>Ground Truth</b> </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-style-representations/domainnet_gt_qual-480.webp 480w,/2026/assets/img/2026-04-27-style-representations/domainnet_gt_qual-800.webp 800w,/2026/assets/img/2026-04-27-style-representations/domainnet_gt_qual-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-style-representations/domainnet_gt_qual.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <b>DenseNet</b> </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-style-representations/domainnet_densenet_qual-480.webp 480w,/2026/assets/img/2026-04-27-style-representations/domainnet_densenet_qual-800.webp 800w,/2026/assets/img/2026-04-27-style-representations/domainnet_densenet_qual-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-style-representations/domainnet_densenet_qual.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <b>StyleCap</b> </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-style-representations/domainnet_captions_qual-480.webp 480w,/2026/assets/img/2026-04-27-style-representations/domainnet_captions_qual-800.webp 800w,/2026/assets/img/2026-04-27-style-representations/domainnet_captions_qual-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-style-representations/domainnet_captions_qual.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <b> Mamba</b> </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-style-representations/domainnet_mamba_qual-480.webp 480w,/2026/assets/img/2026-04-27-style-representations/domainnet_mamba_qual-800.webp 800w,/2026/assets/img/2026-04-27-style-representations/domainnet_mamba_qual-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-style-representations/domainnet_mamba_qual.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <b> CSD</b> </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-style-representations/domainnet_csd_qual-480.webp 480w,/2026/assets/img/2026-04-27-style-representations/domainnet_csd_qual-800.webp 800w,/2026/assets/img/2026-04-27-style-representations/domainnet_csd_qual-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-style-representations/domainnet_csd_qual.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <p><i><b>Figure 3:</b> Qualitative results of style-based clustering through the select four neural feature representations on the DomainNet dataset. </i></p> </div> <p><strong>The Takeaway:</strong> If your application requires separating the “how” from the “what,” do not rely on off-the-shelf foundation models. General visual competency does not equate to stylistic disentanglement. You need architectures with explicit biases or training objectives tailored for separating style from content.</p> <h3 id="insight-ii-the-great-divergence-machine-perception-vs-art-history">Insight II. The Great Divergence-Machine Perception vs Art History</h3> <p>The most philosophically provocative finding emerged when we tested the models against definitions drawn directly from art history and curation: <strong>Art Movements</strong> (e.g., Cubism, Impressionism) and individual <strong>Artistic Signatures</strong> (e.g., Picasso, Van Gogh).</p> <p>We asked the models to cluster tens of thousands of paintings from WikiArt without supervision. We then measured how well these machine-generated clusters aligned with the ground-truth historical labels for movements and artists.</p> <ul> <li> <strong>The Quantitative Failure:</strong> Across the board, performance was consistently low for both tasks. No neural representation could reliably reconstruct the categories of art history unsupervised. Notably, most models found clustering by <strong>individual artist</strong> even more difficult than clustering by broad <strong>movement</strong>, highlighting the immense challenge of capturing a creator’s evolving “hand” without explicit labels.</li> </ul> <table> <thead> <tr> <th style="text-align: left">Representation Family</th> <th style="text-align: left">Model (F)</th> <th style="text-align: left">Art Movement (NMI)</th> <th style="text-align: left">Art Movement (ARI)</th> <th style="text-align: left">Artist Signature (NMI)</th> <th style="text-align: left">Artist Signature (ARI)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Generalist</strong></td> <td style="text-align: left">$F_{DINO}$</td> <td style="text-align: left">0.217</td> <td style="text-align: left">0.077</td> <td style="text-align: left">0.273</td> <td style="text-align: left">0.074</td> </tr> <tr> <td style="text-align: left"><strong>Statistician</strong></td> <td style="text-align: left">$F_{Gram}$</td> <td style="text-align: left">0.223</td> <td style="text-align: left">0.068</td> <td style="text-align: left">0.219</td> <td style="text-align: left">0.043</td> </tr> <tr> <td style="text-align: left"><strong>Synthesizer</strong></td> <td style="text-align: left">$F_{StyleShot}$</td> <td style="text-align: left">0.213</td> <td style="text-align: left">0.060</td> <td style="text-align: left">0.253</td> <td style="text-align: left">0.056</td> </tr> <tr> <td style="text-align: left"><strong>Linguist</strong></td> <td style="text-align: left">$F_{StyleCap}$</td> <td style="text-align: left">0.228</td> <td style="text-align: left">0.080</td> <td style="text-align: left">0.284</td> <td style="text-align: left">0.075</td> </tr> <tr> <td style="text-align: left"><strong>Specialist</strong></td> <td style="text-align: left">$F_{Artist}$*</td> <td style="text-align: left"><strong>0.249</strong></td> <td style="text-align: left"><strong>0.087</strong></td> <td style="text-align: left"><strong>0.510</strong></td> <td style="text-align: left"><strong>0.346</strong></td> </tr> </tbody> </table> <div class="caption"> <p><i><b>Table 2: Unsupervised Clustering Performance on WikiArt History</b> This table presents the Normalized Mutual Information (NMI) and Adjusted Rand Index (ARI) scores for a representative set of neural representations using K-Means clustering. The maximum possible score is 1.0. <br> Note: $F_{Artist}$ is supervised directly on artist labels. While its performance on the Artist Signature task is higher than others, it is still far from perfect, and its ability to generalize to the related Art Movement task remains low, highlighting the challenge of this domain.</i></p> </div> <ul> <li> <strong>The Qualitative Twist:</strong> Why did they fail? Because art history is a social, temporal, and biographical construct, not just a visual one. <ul> <li>A “Renaissance” painting and a “Baroque” painting might share more sheer visual similarity in palette and subject than two distinct “Post-Impressionist” works.</li> <li>An artist like Picasso radically changed his style over time (from Blue Period to Cubism). Grouping all his works together requires external biographical knowledge that an unsupervised visual model simply doesn’t have.</li> </ul> </li> <li> <strong>The Human Validation:</strong> Crucially, our human study revealed that the machines weren’t necessarily “wrong.” Participants with art backgrounds often rated the clusters generated by models like <strong>StyleShot</strong> (a Synthesizer) as <em>more visually cohesive</em> than the ground-truth historical clusters.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-style-representations/survey_box_plot-480.webp 480w,/2026/assets/img/2026-04-27-style-representations/survey_box_plot-800.webp 800w,/2026/assets/img/2026-04-27-style-representations/survey_box_plot-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-style-representations/survey_box_plot.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <p><i><b>Figure 4:</b> Box plots for the survey conducted on the clusters obtained from 450 samples from the WikiArt-ArtMove dataset. Clusters were obtained on the subset using $F_{Dense}$, $F_{StyleCap}$, $F_{StyleShot}$ and $F_{CSD}$ features and K-Means clustering model. The survey was conducted on the art movement ground truth as well. The survey included questions relating to cluster cohesion, cluster separation, and overall clustering quality. Overall, 25 participants responded to the survey.</i></p> </div> <p><strong>The Takeaway:</strong> Researchers must be cautious when using human-defined labels—whether broad movements or individual artist names—as ground truth for visual style. Low supervised metrics do not mean the representation is “bad”; it often means the model has discovered a valid <em>perceptual</em> reality that simply disagrees with the complex, context-dependent narratives of art history.</p> <h3 id="insight-iii-the-triumph-of-synthesis-in-perceptual-definitions">Insight III. The Triumph of Synthesis in Perceptual Definitions</h3> <p>If art history is too abstract, what happens when we define style purely by physics—texture, color, and brushstroke patterns?</p> <p>We tested this using our <strong>Synthetic Curated Datasets</strong>, where style was rigorously controlled via style transfer algorithms.</p> <ul> <li> <strong>The Clear Winners:</strong> Under this strict perceptual definition, the <strong>Synthesizers</strong> reigned supreme. Representations derived from models built to <em>create</em> art—specifically transformer-based transfer models like <strong>StyleShot</strong> and <strong>Stytr²</strong>—achieved near-perfect clustering scores.</li> <li> <strong>Why it Makes Sense:</strong> To successfully transfer a style, these architectures must learn to perfectly isolate low-level aesthetic textures from structure. Their internal representations are, by design, the purest distillation of this “physical” definition of style.</li> </ul> <p><strong>The Takeaway:</strong> For practitioners building applications focused on visual aesthetics—like texture matching, filter recommendation, or interior design—architectures derived from style-transfer or synthesis tasks offer the most robust and accurate representations.</p> <h3 id="insight-iv-the-hidden-geometry-is-hierarchical">Insight IV. The Hidden Geometry is Hierarchical</h3> <p>Finally, our analysis challenges a fundamental assumption in many ML approaches to style: that it is a flat classification problem.</p> <p>When we analyzed the clustering dynamics, we found strong evidence that the latent space of artistic style is inherently hierarchical.</p> <ol> <li> <strong>Super-Clusters:</strong> Many representations (especially <strong>Statisticians</strong> like Gram Matrices) initially grouped massive amounts of varied art into a few giant “super-clusters.”</li> <li> <strong>Sub-Structures:</strong> When we applied sub-clustering to these groups, they decomposed into distinct, coherent sub-styles. The model hadn’t failed to see the difference; it had simply grouped them at a high level of abstraction.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-style-representations/Subclustering-480.webp 480w,/2026/assets/img/2026-04-27-style-representations/Subclustering-800.webp 800w,/2026/assets/img/2026-04-27-style-representations/Subclustering-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-style-representations/Subclustering.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <p><i><b>Figure 5:</b> Sub-clustering on a single cluster from the results of the WikiArt-AM dataset for $F_{StyleCap}$ features through the DEC model. (a) shows the distribution of the number of samples in each cluster before and after sub-clustering. (b) shows the qualitative results after we obtain the sub clusters of a single cluster with most samples. Samples on the left are from the original cluster and samples on the right are from the sub-clusters.</i></p> </div> <ol> <li> <strong>Semantic Trees:</strong> Using our <strong>Linguist</strong> representations ($F_{StyleCap}$), we generated dendrograms that perfectly mirrored human intuition, automatically organizing art from broad philosophies (Abstract vs. Representational) down to specific movements, and finally to individual artistic signatures at the leaf nodes.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-style-representations/Dendrogram-480.webp 480w,/2026/assets/img/2026-04-27-style-representations/Dendrogram-800.webp 800w,/2026/assets/img/2026-04-27-style-representations/Dendrogram-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-style-representations/Dendrogram.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <b>(a)</b> Complete dendrogram for the WikiArt dataset </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-style-representations/Hier_Distribution-480.webp 480w,/2026/assets/img/2026-04-27-style-representations/Hier_Distribution-800.webp 800w,/2026/assets/img/2026-04-27-style-representations/Hier_Distribution-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-style-representations/Hier_Distribution.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <b>(b)</b> Distribution of artworks at each level of the dendrogram based on Art Movement </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-style-representations/Hier_Qual-480.webp 480w,/2026/assets/img/2026-04-27-style-representations/Hier_Qual-800.webp 800w,/2026/assets/img/2026-04-27-style-representations/Hier_Qual-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-style-representations/Hier_Qual.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <b>(c)</b> Sample Artworks from each level of the hierarchy based on Art Movement categorization </div> <div class="caption"> <p><i><b>Figure 6:</b> Hierarchical distribution of Art Movements in the WikiArt dataset. We showcase the sample art movement-wise artworks distribution dendrogram in (b) and the respective sample artworks in (c). The dendrogram is obtained with 27 art movements with the $F\_{StyleCap}$ features. We display the top 5 art movements. We observe that the WikiArt dataset contains hierarchies showcasing a higher level of similarity between art movements at the top of the hierarchy. The art movements get separated into distinct clusters when we move down the hierarchy.</i></p> </div> <p><strong>The Takeaway:</strong> Stop treating artistic style as a flat list of mutually exclusive labels. It is a nested structure. Future evaluation metrics and model architectures should explicitly account for this hierarchy, rewarding models that capture relationships at multiple levels of granularity—from the broad stroke of a movement to the unique signature of a master.</p> <h2 id="conclusion-the-unresolved-play-and-the-path-forward">Conclusion-The Unresolved Play and the Path Forward</h2> <p>The “play” of neural style representations is far from finished. Our expansive analysis across 16 diverse architectures and multiple definitions of style reveals a landscape defined not by a single universal solution, but by deep specialization and profound philosophical gaps.</p> <p>We have learned that the search for a single “style embedding” to rule them all is futile because “style” itself is a chameleon. A model that perfects the capture of perceptual textures (like a <strong>Synthesizer</strong>) may be completely blind to the socio-historical context that defines a movement. Most critically, our investigation into unsupervised clustering uncovered a fundamental divergence: a neural network, left to organize art history without supervision, builds a taxonomy based on visual logic—a “perceptual truth”—that rarely aligns perfectly with the complex narratives of human art history.</p> <p><strong>The Path Forward for the Community:</strong></p> <p>This research positions visual art as a rigorous testbed for advancing representation learning. The failures and successes documented here chart a more nuanced course for future research:</p> <ol> <li> <strong>Embrace Hierarchy:</strong> We must abandon flat classification benchmarks for style. Future models and evaluation metrics must explicitly account for the nested nature of aesthetics, rewarding systems that capture relationships at multiple levels of granularity—from broad philosophical movements down to individual artistic signatures.</li> <li> <strong>Develop Representations Along Multiple Directions:</strong> The community must move beyond a monolithic view of style. Current research trends often cater to only one definition at a time (e.g., optimizing purely for texture synthesis or purely for domain disentanglement). The path forward requires acknowledging these distinct, valid definitions and developing specialized research tracks for historical, perceptual, and semantic notions of style, rather than forcing one architecture to solve them all.</li> <li> <strong>Rethink Ground Truth:</strong> We need to reconsider how we evaluate unsupervised style learning. Low alignment with WikiArt labels is not necessarily a failure; it may indicate the discovery of valid alternative visual structures. We need new metrics that balance historical fidelity with perceptual coherence.</li> </ol> <p>By addressing these challenges, we do more than just build better tools for digital art history. We push the boundaries of how artificial intelligence grasps abstraction, context, and the deeply human dimensions of visual communication.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-style-representations.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fairness-audits/">Fairness Audits as Theater: When Metrics Mask Structural Harm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/general-agent-evaluation/">Ready For General Agents? Let's Test It.</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/why-vlms-waste-their-vision/">Why vlms waste their vision</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>