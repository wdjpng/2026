<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> How many tokens does it take to say "नमस्ते"? A Dive into Indic Tokenization | ICLR Blogposts 2026 </title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Tokenizers trained on English-dominant data often produce unusually high token counts for Indic languages. This " tokenizer fertility increases sequence lengths raises compute costs and can hurt downstream performance even when the underlying model is strong. in this post we examine how varies across major indic scripts it affects language modeling quality inference efficiency instruction-following behavior.> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, icl# add your own keywords or leave empty"> <link rel="stylesheet" href="/2026/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/2026/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/2026/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2026/assets/img/iclr_favicon.ico?0a8a3afdb0dbe139723b24dba3052a4f"> <link rel="stylesheet" href="/2026/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://iclr-blogposts.github.io/2026/blog/2026/indic-tokenization/"> <script src="/2026/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/2026/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/2026/assets/js/distillpub/template.v2.js"></script> <script src="/2026/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "How many tokens does it take to say "नमस्ते"? A Dive into Indic Tokenization",
            "description": "Tokenizers trained on English-dominant data often produce unusually high token counts for Indic languages. This "tokenizer fertility" increases sequence lengths, raises compute costs, and can hurt downstream performance, even when the underlying model is strong. In this post, we examine how fertility varies across major Indic scripts and how it affects language modeling quality, inference efficiency, and instruction-following behavior.",
            "published": "April 27, 2026",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Anonymous",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2026/"> ICLR Blogposts 2026 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2026/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/call/">call for blogposts </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/submitting/">submitting </a> </li> <li class="nav-item "> <a class="nav-link" href="/2026/reviewing/">reviewing </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2026/"><strong>2026</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2025/">2025</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener" target="_blank">2022</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>How many tokens does it take to say "नमस्ते"? A Dive into Indic Tokenization</h1> <p>Tokenizers trained on English-dominant data often produce unusually high token counts for Indic languages. This "tokenizer fertility" increases sequence lengths, raises compute costs, and can hurt downstream performance, even when the underlying model is strong. In this post, we examine how fertility varies across major Indic scripts and how it affects language modeling quality, inference efficiency, and instruction-following behavior.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#what-re-tokens-and-why-do-they-matter-so-much-to-us">What're tokens? and why do they matter so much to us?</a> </div> <ul> <li> <a href="#introduction">Introduction</a> </li> <li> <a href="#tokenizer-fertility-a-key-measure-of-tokenization-efficiency">Tokenizer Fertility - A Key Measure of Tokenization Efficiency</a> </li> </ul> <div> <a href="#the-indic-problem">The Indic Problem!</a> </div> <ul> <li> <a href="#how-high-fertility-tokenizers-make-indicnlp-unfair">How high fertility tokenizers make IndicNLP unfair?</a> </li> </ul> <div> <a href="#the-fertility-tax">The Fertility "Tax"</a> </div> <div> <a href="#the-downstream-impact">The Downstream Impact</a> </div> </nav> </d-contents> <h2 id="whatre-tokens-and-why-do-they-matter-so-much-to-us">What’re tokens? and why do they matter so much to us?</h2> <div style="border: 2px solid #ff9800; background-color: #fff3e0; padding: 15px; border-radius: 5px; margin: 15px 0;"> <strong>TL;DR.</strong> Some of the important pointers we'll look at, in this post: Are tokenizers failing a billion Indic language speakers? How tokenization bias reinforces linguistic inequality in AI models? </div> <h3 id="introduction">Introduction</h3> <p>In recent years, the field of Natural Language Processing (NLP) has been revolutionized by advances in Large Language Models (LLMs).<d-cite key="hagos2024recentadvancesgenerativeai"></d-cite> Trained on large text corpora, these AI models can understand, generate and manipulate language in a human-like way, enabling a wide range of tasks without task-specific supervision. Simply, an LLM takes an input (prompt) and generates the output. They’re like magic, no?</p> <p>But how do they operate? How do they see strings? LLMs do not operate directly on raw text. Instead, when a text is fed into the model, before interpreting, it’s first segmented into a series of multi-letter chunks called <code class="language-plaintext highlighter-rouge">tokens</code>. The process of converting text into these units is called tokenization.</p> <h3 id="tokenizer-fertility---a-key-measure-of-tokenization-efficiency">Tokenizer Fertility - A Key Measure of Tokenization Efficiency</h3> <p>Most modern LLMs use subword tokenizers. These tokenizers learn a vocabulary of common text fragments from a large corpus. At inference time, each word is broken into the longest possible fragments from this vocabulary. For languages with predictable morphology or large amounts of training data (like English), this strategy works reasonably well. However, tokenization performance varies across languages. Now, if a tokenizer was mainly trained on high-resource languages, it might’ve failed to learn useful subword units for languages that were underrepresented in its training data. When this happens, a single word may split into many tokens. This effect is called high fertility.</p> <p><strong>Tokenizer fertility</strong> $(F)$ (a measure of how many tokens a model generates per source word) can be defined by:</p> \[\text{F}(L) = \frac{1}{|D_L|} \sum_{s \in D_L} \frac{\text{Count}_{\text{tokens}}(s)}{\text{Count}_{\text{words}}(s)}\] <table> <tbody> <tr> <td>where $</td> <td>D_L</td> <td>$ refers to the number of sentences in a dataset $D$ of language $L$ and $s$ is a sentence in the dataset.</td> </tr> </tbody> </table> <p>Now, More tokens $\leadsto$ longer sequences, $\implies$ less context fits into the model’s fixed window. Fragmented words give the model fewer stable patterns to learn, reducing sample efficiency.</p> <p>Simply, you can consider tokens as the “units of thought” the model works with. If those units are poorly aligned with a language, the model begins at a disadvantage – before any actual modeling even starts!</p> <h2 id="the-indic-problem">The Indic Problem!</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-indic-tokenization/banner-480.webp 480w,/2026/assets/img/2026-04-27-indic-tokenization/banner-800.webp 800w,/2026/assets/img/2026-04-27-indic-tokenization/banner-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-indic-tokenization/banner.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Credits: Generated by Gemini Nano Banana </div> <p><a href="https://en.wikipedia.org/wiki/Indo-Aryan_languages" rel="external nofollow noopener" target="_blank">Indic languages</a> represent one of the world’s largest and most diverse linguistic families, spoken by hundreds of millions across South Asia. The diversity of these languages reflects the philosophy of Vasudhaiva Kutumbakam – “the world is one family”. Indic languages form a critical yet underserved segment of the NLP landscape. This reminds us that technology should serve all languages, not just the high-resource ones. Building language models that handle Indic languages effectively is a step toward more inclusive AI, ensuring that speakers of every language can benefit from advances in LLMs.</p> <p>Training Data Dominance: Tokenization algorithms, such as Byte Pair Encoding (BPE), are primarily trained on massive text corpora dominated by high-resource languages, especially English. The resulting vocabularies are optimized for the structure and script of these dominant languages.</p> <h3 id="how-high-fertility-tokenizers-make-indicnlp-unfair">How high fertility tokenizers make IndicNLP unfair?</h3> <div style="border: 2px solid #ee6b6e; background-color: #fff3e0; padding: 15px; border-radius: 5px; margin: 15px 0;"> Consider "नमस्ते" (trans. namaste), meaning "Hello". When tokenized, it should be preserved as one meaningful token. But what if it was wrongly tokenized as ["Hell", "o"]? The semantic meaning is gone! This is the default reality for Indic languages :&lt; </div> <ul> <li>Inefficient Segmentation: When a tokenizer breaks a single character into 3-4 tokens, the model loses the semantic unity of that character. It forces the model to learn character composition rather than word meaning.</li> </ul> <p>Indic scripts overwhelm subword models! For non-Latin-script and morphologically complex or agglutinative languages where words are formed by joining many morphemes (for eg. Dravidian language families), the models struggle to create meaningful tokens as their word forms change frequently and the tokenizer does not capture these variations well. These languages often require significantly more tokens to represent the same semantic content as English. For example, some languages may require up to seven times more tokens per sentence than English. We’ll look into this soon.</p> <ul> <li> <p>Computational Burden: The token inflation leads to higher computational costs and slower processing times. This creates a systematic disadvantage and an accessibility barrier for speakers of these Indic languages. In commercial AI services that use token-based pricing, Indic languages face disproportionately higher costs for the same task, creating economic barriers to accessing AI technology.</p> </li> <li> <p>Reduced Context Utilization: Higher token density means less effective use of a model’s fixed context window, which can impair performance on tasks requiring extensive contextual understanding.</p> </li> </ul> <p>Let’s look at an example using <a href="https://platform.openai.com/tokenizer" rel="external nofollow noopener" target="_blank">OpenAI Tokenizer Playground</a>. “Day by day, it seems nothing changes, yet soon, everything is different.”. A translation of this sentence in Bangla (keeping the unicode codepoint count same) would be “দিনে দিনে মনে হয় কিছুই বদলায় না, কিন্তু খুব শিগগিরই সবকিছুই বদলে যায়।”.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-indic-tokenization/gpt-en-480.webp 480w,/2026/assets/img/2026-04-27-indic-tokenization/gpt-en-800.webp 800w,/2026/assets/img/2026-04-27-indic-tokenization/gpt-en-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-indic-tokenization/gpt-en.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-indic-tokenization/gpt-bn-480.webp 480w,/2026/assets/img/2026-04-27-indic-tokenization/gpt-bn-800.webp 800w,/2026/assets/img/2026-04-27-indic-tokenization/gpt-bn-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-indic-tokenization/gpt-bn.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Tokenization in GPT-4o which uses a form of Byte Pair Encoding (BPE) via OpenAI's `tiktoken` library, specifically a highly optimized, large-vocabulary version (`o200k_base`). </div> <p>Look at the difference in tokenization. Now, this is for just one sentence (again, this worsens for more complex Languages like Malayalam), imagine a huge dataset with thousands of rows to process for inference – these small differences quickly add up, increasing sequence lengths, affecting model memory and may ultimately impact performance.</p> <hr> <p>Let’s try some mini experiments.</p> <p>How Fertile Is Your Tokenizer? Visualising the splits:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="k">def</span> <span class="nf">visualize_splits</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">model_names</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">model_names</span><span class="p">:</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        
        <span class="n">split_view</span> <span class="o">=</span> <span class="sh">"</span><span class="s"> | </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">t</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">Ġ</span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">).</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">])</span>
        <span class="n">results</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span>
            <span class="sh">"</span><span class="s">Model</span><span class="sh">"</span><span class="p">:</span> <span class="n">name</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">/</span><span class="sh">"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="sh">"</span><span class="s">Token Count</span><span class="sh">"</span><span class="p">:</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">),</span>
            <span class="sh">"</span><span class="s">Split View</span><span class="sh">"</span><span class="p">:</span> <span class="n">split_view</span>
        <span class="p">})</span>
    <span class="k">return</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">संप्रभुता</span><span class="sh">"</span> 

<span class="n">models</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">microsoft/Phi-3-mini-4k-instruct</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">google/gemma-2-2b</span><span class="sh">"</span><span class="p">,</span> 
    <span class="sh">"</span><span class="s">ai4bharat/indic-bert</span><span class="sh">"</span>
<span class="p">]</span>

<span class="n">df</span> <span class="o">=</span> <span class="nf">visualize_splits</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">models</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">to_markdown</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span></code></pre></figure> <p>To understand the source of high fertility, we tokenized the Hindi word for “Sovereignty” (संप्रभुता). This word serves as a stress test due to its use of conjuncts (yuktakshars) and vowel modifiers (matras).</p> <table> <thead> <tr> <th style="text-align: left">Model</th> <th style="text-align: right">Token Count</th> <th style="text-align: left">Split View</th> <th> </th> <th> </th> <th> </th> <th> </th> <th> </th> <th> </th> <th> </th> <th> </th> <th> </th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Phi-3-mini-4k-instruct</td> <td style="text-align: right">10</td> <td style="text-align: left">▁</td> <td>स</td> <td>ं</td> <td>प</td> <td>्</td> <td>र</td> <td>भ</td> <td>ु</td> <td>त</td> <td>ा</td> </tr> <tr> <td style="text-align: left">gemma-2-2b</td> <td style="text-align: right">4</td> <td style="text-align: left">सं</td> <td>प्र</td> <td>भु</td> <td>ता</td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td style="text-align: left">indic-bert</td> <td style="text-align: right">3</td> <td style="text-align: left">▁सप</td> <td>रभ</td> <td>त</td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </tbody> </table> <p>The difference in segmentation strategies is distinct:</p> <p>Phi-3-mini (10 Tokens): Orthographic Decomposition - The tokenizer fails to recognize Indic subwords, reverting to character-level segmentation. Notably, it splits the conjunct ‘प्र’ (pra) into three constituent parts: the consonant प, the halant ्, and the consonant r र. The model essentially processes the text as a stream of unicode distincts rather than linguistic units.</p> <p>Gemma-2 (4 Tokens): Syllabic Preservation - The tokenizer aligns with the structure of the script, preserving full syllables (“Aksharas”). The complex clusters प्र (pra) and भु (bhu) are treated as single tokens.</p> <p>Coming to AI4Bharat’s IndicBERT, the result (token count of 3) might seem great at first glance. However, if you look closely at the split view: सप (Sap), रभ (Rabh), त (Ta), you’ll notice that the vowels have disappeared. The tokenizer has achieved this low fertility by performing aggressive normalization.</p> <p>The Trade-off: The model is extremely efficient, but potentially loses critical semantic information (tense, gender, and root meaning) stored in the vowels. This serves as a crucial lesson: Low fertility is only a virtue if it preserves information.</p> <hr> <p>The sensitivity test:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">gc</span>

<span class="n">bnb_config</span> <span class="o">=</span> <span class="nc">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">measure_fertility_perplexity</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">model_id</span><span class="p">):</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
        <span class="n">model_id</span><span class="p">,</span>
        <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span>
        <span class="n">device_map</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span>
    <span class="p">)</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">token_count</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">input_ids</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">inputs</span><span class="p">.</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="n">perplexity</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">outputs</span><span class="p">.</span><span class="n">loss</span><span class="p">).</span><span class="nf">item</span><span class="p">()</span>

    <span class="k">del</span> <span class="n">model</span>
    <span class="n">gc</span><span class="p">.</span><span class="nf">collect</span><span class="p">()</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">empty_cache</span><span class="p">()</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">Model</span><span class="sh">"</span><span class="p">:</span> <span class="n">model_id</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s">/</span><span class="sh">"</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="sh">"</span><span class="s">Token Count</span><span class="sh">"</span><span class="p">:</span> <span class="n">token_count</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Perplexity</span><span class="sh">"</span><span class="p">:</span> <span class="nf">round</span><span class="p">(</span><span class="n">perplexity</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="p">}</span>

<span class="c1">#eng_text = "Artificial intelligence is transforming the world."
</span><span class="n">hindi_text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">कृत्रिम बुद्धिमत्ता दुनिया को बदल रही है।</span><span class="sh">"</span>

<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">microsoft/Phi-3-mini-4k-instruct</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">google/gemma-2-2b</span><span class="sh">"</span><span class="p">]</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="nf">measure_fertility_perplexity</span><span class="p">(</span><span class="n">hindi_text</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">models</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">).</span><span class="nf">to_markdown</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span></code></pre></figure> <p>We hypothesized that high fertility would confuse the model, leading to higher perplexity (uncertainty). To test this, we compared Microsoft Phi-3 (English-centric tokenizer) against Google Gemma-2 (Multilingual tokenizer) on a Hindi sample. The results, at first glance, seem to defy logic:</p> <table> <thead> <tr> <th style="text-align: left">Model</th> <th style="text-align: right">Token Count</th> <th style="text-align: right">Raw Perplexity</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Phi-3-mini-4k-instruct</td> <td style="text-align: right">44</td> <td style="text-align: right">5.65</td> </tr> <tr> <td style="text-align: left">gemma-2-2b</td> <td style="text-align: right">14</td> <td style="text-align: right">44.73</td> </tr> </tbody> </table> <p>Does this mean the English-centric Phi-3 is 8x “smarter” at Hindi than the multilingual Gemma-2? Absolutely not. This is a classic example of the Tokenization Bias in evaluation metrics.</p> <p>Perplexity measures the average uncertainty per token.</p> <ul> <li> <p>Phi-3: Because it fragments the Hindi sentence into 44 tiny bytes, many of its prediction steps are trivial. For example, once it predicts the first byte of a character, the subsequent bytes are deterministic. These “easy wins” lower the average perplexity, masking the fact that the model may not grasp the sentence’s semantic meaning.</p> </li> <li> <p>Gemma-2: With a richer vocabulary, Gemma represents the sentence in just 14 dense tokens. Each prediction requires choosing the correct word or root from a large set, so each step is harder.</p> </li> </ul> <p>To compare them fairly, we normalize perplexity by word count, not token count.</p> \[\text{PPL}_{\text{word}} = \text{PPL}_{\text{token}}^{(\text{Token Count} / \text{Word Count})}\] <p>Thus, Phi-3 Normalized: $5.65^{(44/7)} \approx 5.65^{6.28} \approx \mathbf{52,800}$</p> <p>Gemma-2 Normalized: $44.73^{(14/7)} \approx 44.73^{2.0} \approx \mathbf{2,000}$</p> <p><strong>The Reality</strong>: When normalized, Gemma-2 is actually orders of magnitude better at predicting the sequence than Phi-3.</p> <hr> <h2 id="the-fertility-tax">The Fertility “Tax”</h2> <h2 id="the-downstream-impact">The Downstream Impact</h2> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/2026/assets/bibliography/2026-04-27-indic-tokenization.bib"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/fans/">FANS - Frequency-Adaptive Noise Shaping for Diffusion Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/wait-do-we-need-to-wait/">Wait, Do We Need to Wait? Revisiting Budget Forcing for Sequential Test-Time Scaling</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-reversal-curse-from-general-domain-to-remote-sensing-images/">Visual Reversal Curse: From General Domain to Remote Sensing Images</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/visual-long-context/">Text-as-Image, A Visual Encoding Approach for Long-Context Understanding</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2026/blog/2026/vis-llm-latent-geometry/">Visualizing LLM Latent Space Geometry Through Dimensionality Reduction</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 ICLR Blog. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/2026/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/2026/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/2026/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/2026/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/2026/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/2026/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/2026/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/2026/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/2026/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/2026/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/2026/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/2026/assets/js/search-data.js"></script> <script src="/2026/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>