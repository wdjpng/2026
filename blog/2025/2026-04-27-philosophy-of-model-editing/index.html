<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p> <h2 id="introduction">Introduction</h2> <p>Large language models can now be adjusted. They can be corrected after deployment, fixed for safety, or guided towards new behaviors without needing complete retraining. This raises a fundamental question that spans machine learning, knowledge theory, and philosophy:</p> <p>What does it mean to change knowledge within a neural network?</p> <p>Traditional software updates a record in a database. Humans change beliefs through reasoning, feelings, and contradictions. Neural networks do neither.</p> <p>They do not store symbols, explicit beliefs, or lookup tables. Instead, knowledge is spread out, intertwined, and geometric.</p> <p>So when we edit a model :</p> <ul> <li> <p>Are we rewriting memory?</p> </li> <li> <p>Are we distorting the shape of meaning itself?</p> </li> <li> <p>Is a fact a localized change or a global guideline?</p> </li> <li> <p>Can a neural network truly forget?</p> </li> <li> <p>And if we reshape enough knowledge, does the model’s identity shift?</p> </li> </ul> <p>This blog post provides a framework for these questions. It is based on the technical features of neural networks but also invites philosophical reflection.</p> <h2 id="what-is-knowledge-inside-a-neural-network">What Is “Knowledge” Inside a Neural Network?</h2> <p>Before exploring how knowledge changes, we need to understand what it is.</p> <p>In symbolic systems, knowledge exists as:</p> <ul> <li>entries</li> <li>axioms</li> <li>pointers</li> <li>definitions</li> </ul> <p>In a neural network, there is no specific spot for “The Eiffel Tower is in Paris.” Instead, knowledge arises from a transformation represented in <d-cite key="bengio2013representation"></d-cite>:</p> <ul> <li>embedding geometry</li> <li>attention pathways</li> <li>MLP activations</li> <li>token transition statistics</li> </ul> <p>A useful summary:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A fact is not stored; it is enacted. A network knows something because its transformations consistently bring it about.
</code></pre></div></div> <p>To illustrate this, consider a straightforward example <d-cite key="geva2021transformerfeedforward"></d-cite> :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>When a model states, “Rome is the capital of Italy,” the fact is not retrieved; it emerges from a path through activation space that consistently lands in the “Rome” area.
</code></pre></div></div> <p>Thus:</p> <ul> <li>A fact is a stable attractor.</li> <li>A belief is a directional bias in that space.</li> <li>A concept is a group of reachable states.</li> </ul> <p>This means editing is essentially a geometric process.</p> <h2 id="what-actually-changes-during-a-model-edit">What Actually Changes During a Model Edit?</h2> <p>Editing a model—whether through fine-tuning, ROME-style updates, MEMIT, soft prompts, or manual weight adjustments—changes the underlying geometry of these attractors.</p> <p>Two main types of changes typically occur:</p> <h3 id="local-micro-manifold-rewrites">Local Micro-Manifold Rewrites</h3> <p>Imagine a micro-manifold: a small area of activation space where similar prompts converge. For example, questions like:</p> <ul> <li>“What is the capital of Italy?”</li> <li>“Italy’s capital city is…”</li> <li>“The city that serves as Italy’s seat of government is…”</li> </ul> <p>All fall into a similar neighborhood.</p> <p>A local edit only alters this specific area:</p> <ul> <li>the boundary shifts,</li> <li>an attractor nudges,</li> <li>nearby semantic neighbors reorganize slightly.</li> </ul> <p>Methods like ROME and MEMIT aim to work here <d-cite key="meng2022rome"></d-cite> <d-cite key="meng2023memit"></d-cite>, making precise and minimally invasive modifications.</p> <h3 id="distributed-relational-shifts">Distributed Relational Shifts</h3> <p>Some relationships are globally structured.</p> <p>Editing a relation like (Italy, capital, X) can have broader impacts:</p> <ul> <li>“Italian government is located in…”</li> <li>“Italian culture in ___ city”</li> <li>analogies like “France ↔ Paris :: Italy ↔ ___”</li> </ul> <p>This constitutes a relational edit, rather than a local one. You reshape an entire conceptual subspace.</p> <p>This is similar to full relation editing or changing an embedding direction like “country→capital.”</p> <p>In summary:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Local rewrites fix a pocket of meaning. 
Distributed edits alter the semantic landscape.
</code></pre></div></div> <p>Both are important and can create contradictions.</p> <h2 id="does-editing-introduce-inconsistency">Does Editing Introduce Inconsistency?</h2> <p>Human beliefs can be modular and inconsistent, while neural networks are more globally connected.</p> <p>This creates a challenge:</p> <ul> <li>Overly local edits cause the model to revert to the old fact under rephrasing.</li> <li>Overly global edits distort unrelated knowledge (identity drift).</li> </ul> <p>For example:</p> <ul> <li>Correcting a wrong fact might unintentionally alter analogy patterns.</li> <li>Changing a persona (like making the assistant sarcastic) could affect unrelated topics.</li> </ul> <p>A helpful distinction:</p> <ul> <li>Epistemic content refers to what the model believes (facts).</li> <li>Ontological structure refers to the types of concepts and relationships the model recognizes.</li> </ul> <p>A good edit should change epistemic content without harming ontology.</p> <p>In simpler terms:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Edit the belief, not the overall worldview.
</code></pre></div></div> <p>Current methods often struggle to maintain this balance.</p> <h2 id="can-a-neural-network-ever-truly-forget">Can a Neural Network Ever Truly “Forget”?</h2> <p>For humans, forgetting can take various forms:</p> <ul> <li>deletion</li> <li>suppression</li> <li>reconsolidation</li> <li>interference</li> </ul> <p>In neural networks, forgetting is a strange idea.</p> <p>There is no slot to erase or pointer to zero out. Knowledge is redundantly encoded <d-cite key="frankle2019lottery"></d-cite> across many layers and modules.</p> <p>To forget the fact: “The Eiffel Tower is in Paris,” you would need to disrupt all the attractor pathways leading to “Paris.”</p> <p>But because:</p> <ul> <li>representations are redundant,</li> <li>associations are intertwined,</li> <li>learned structures are densely distributed,</li> </ul> <p>a single update rarely wipes out all paths.</p> <p>This explains why edited facts often come back <d-cite key="goodfellow2013empirical"></d-cite>:</p> <ul> <li>under rephrasing,</li> <li>in lengthy contexts,</li> <li>through analogy paths,</li> <li>under tricky prompts.</li> </ul> <p>So:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Forgetting in neural networks isn’t about destruction; it’s about reducing the chance of a belief coming back.
</code></pre></div></div> <p>Philosophically, this is more like suppression than deletion.</p> <h2 id="the-ontology-of-edited-models-does-editing-change-identity">The Ontology of Edited Models: Does Editing Change Identity?</h2> <p>Model identity isn’t defined by the dataset but by:</p> <ul> <li>behavior</li> <li>coherence</li> <li>decision tendencies</li> <li>inductive biases</li> </ul> <p>Editing can change these.</p> <p>Small edits (like a single factual relationship) usually do not alter identity. However, large or repeated edits such as changing tone, worldview, or moral views can create a distinctly different agent.</p> <p>Example:</p> <ul> <li>turning a friendly assistant into a sarcastic persona,</li> <li>shifting political leanings <d-cite key="olah2020circuits"></d-cite>,</li> <li>imposing a strict safety rule that leads to reasoning changes.</li> </ul> <p>This relates to the Ship of Theseus problem:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>After many edits, is it still the same model?
</code></pre></div></div> <p>From an engineering perspective:</p> <ul> <li>the model is “the same” if its functional behavior remains stable</li> <li>“different” if the relational structure or persona changes.</li> </ul> <p>From a philosophical perspective:</p> <ul> <li>identity ties to the model’s internal ontology.</li> <li>Changing high-level relational geometry equals changing identity.</li> </ul> <h2 id="why-this-matters-beyond-bug-fixes">Why This Matters: Beyond Bug-Fixes</h2> <p>A theory of editing matters because it relates to:</p> <h3 id="safety">Safety</h3> <p>Local edits may unintentionally create global contradictions.</p> <h3 id="alignment">Alignment</h3> <p>Changing epistemic content could distort ontological foundations, altering what the model values.</p> <h3 id="interpretability">Interpretability</h3> <p>Editing enables us to examine the model’s structure: we find out which relationships are fragile or deep.</p> <h3 id="delegated-autonomy">Delegated Autonomy</h3> <p>If a model is part of a workflow or agent system, shifts in identity can undermine trust.</p> <h3 id="scientific-understanding">Scientific Understanding</h3> <p>Editing provides insight into how neural networks represent meaning and change knowledge.</p> <h2 id="a-research-aware-conceptual-taxonomy-of-model-edits">A Research-Aware Conceptual Taxonomy of Model Edits</h2> <p>Here is an improved taxonomy, now enhanced with examples and basic connections to known methods:</p> <p><strong>Type-1. Local Semantic Rewrites</strong></p> <p>Small, targeted changes to micro-manifolds. This roughly corresponds to methods like ROME or MEMIT that aim for precise, fact-level edits.</p> <p><strong>Type-2. Distributed Relational Shifts</strong></p> <p>Edits that affect entire relational subspaces (for example, altering all country→capital pairs). These can be seen in behavior changes after broader fine-tuning or multi-example edits.</p> <p><strong>Type-3. Ontological Reorientations</strong></p> <p>Changes that affect how the model organizes concepts (for instance, safety-alignment fine-tunes that reshape moral or intentional concepts). These often occur as a side effect of broad, domain-specific training.</p> <p><strong>Type-4. Identity-Level Modifications</strong></p> <p>Edits that change persona, reasoning style, or core behavioral tendencies. Example: turning a neutral assistant into one that is consistently sarcastic. This is typical in instruction tuning, reinforcement learning from human feedback, or safety training.</p> <p>This taxonomy is conceptual, but it connects to real methods <d-cite key="hartford2024ke-survey"></d-cite>, grounding philosophy in current practice.</p> <h2 id="conclusion--editing-knowledge--editing-memory">Conclusion — Editing Knowledge ≠ Editing Memory</h2> <p>We circle back to the question:</p> <p>What does it mean to change knowledge in a neural network?</p> <p>The refined answer:</p> <ul> <li>Facts are not stored; they are acted upon as transformations.</li> <li>Editing involves changing geometry, not memory.</li> <li>Forgetting is about reducing probability, not deleting.</li> <li>Local changes can result in global contradictions.</li> <li>Edits can shift not only beliefs but also identity.</li> <li>Maintaining ontological stability is just as important as epistemic correctness.</li> </ul> <p>A practical implication is:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Evaluating model edits should assess not only factual accuracy but also identity shifts and ontological changes.
</code></pre></div></div> <p>Ultimately:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model editing is not just a technical tool. It is a philosophical act involving decisions about how an artificial mind should evolve.
</code></pre></div></div> </body></html>