<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="introduction">Introduction</h2> <p>In data science, physics, and engineering, the ultimate goal isn‚Äôt just prediction‚Äîit‚Äôs <strong>understanding</strong>. Finding a single, elegant mathematical formula that perfectly describes a set of data points is the holy grail. This is called <strong>Equation Discovery</strong> or, also, <strong>Symbolic Regression (SR)</strong>.</p> <p>Traditional AI models like Neural Networks give us complex, black-box equations. SR aims for human-readable formulas (like $f(x) = \log(x) + c$).</p> <p>To find this perfect formula, all search algorithms‚Äîwhether they use Genetic Programming (GP), Monte Carlo Tree Search (MCTS), or Deep Learning (DL) ‚Äîfollow a a cycle of: <strong>proposing</strong> a candidate equation, <strong>learning</strong> from its performance, and repeat.</p> <p>How the proposal and learning steps work depends on the algorithm:</p> <ul> <li> <strong>Genetic Programming</strong> proposes new equations by modifying existing equations or combining them. It learns by favoring the selection of the best equations found so far.</li> <li> <strong>Monte Carlo Tree Search</strong> proposal step generates a new equation by traversing a tree of possible grammar derivations that are more probable to fit the data taking a confidence interval into consideration. It learns by updating the probabilities of each derivation.</li> <li> <strong>Deep Learning</strong> and <strong>Reinforcement Learning</strong> proposes new equations by choosing the next symbol that maximizes the expected reward given the last choice. It learns by reinforcing the quality of the generated expression through the sequence of steps.</li> </ul> <p>The problem? The search space is unbelievably vast and filled with redundancy.</p> <h2 id="its-all-the-same-no-matter-where-you-are">It‚Äôs all the same, no matter where you are</h2> <p>Imagine trying to navigate a forest with many paths leading to the same (wrong) destination, and you have to try them all until you follow one that leads you to your goal. That‚Äôs the reality of Symbolic Regression. Consider the simple expression $2x$. How many different ways can you write that same value?</p> \[x+x \\ \frac{4x}{2} \\ 3x-x \\ \dots \text{and many more!}\] <p>All these expressions are mathematically identical; they will all yield the exact same result for the same dataset.</p> <p>This redundancy creates two issues for the search algorithms:</p> <ol> <li> <p><strong>Wasted Time:</strong> The algorithm might revisit $x+x$ after having already explored $2x$, wasting valuable computational budget.</p> </li> <li> <p><strong>Complexity:</strong> If $x+x$ is the correct solution, we want its simplest, and most interpretable form ($2x$), not one of the infinitely complex equivalent forms. Using post-processing simplification tools often fails or introduces new problems, as shown in <d-cite bibtex-key="de2023reducing"></d-cite>.</p> </li> </ol> <p>On the other hand, redundancy can be helpful. Sometimes, navigating from $x+x$ to $3x-x$ can be a ‚Äústepping stone‚Äù to reach a new, better area of the search space. This is known as the <strong>neutral space theory</strong><d-cite bibtex-key="banzhaf2024combinatorics"></d-cite>.</p> <p>But what if we could detect <em>all</em> equivalent expressions in real-time and use that knowledge to make the search efficient?</p> <h2 id="a-database-for-math-expressions-enters-the-e-graph">A Database for Math Expressions: Enters the e-graph</h2> <p>The solution lies in <strong>e-graphs</strong> and <strong>Equality Saturation</strong> <d-cite bibtex-key="tate2009equality"></d-cite>.</p> <p>Think of an e-graph as a <strong>smart database system</strong> for mathematical expressions. It‚Äôs designed to store many different, but equivalent, expressions with a minimum amount of space. It also makes it easier to query for expressions with certain patterns.</p> <p>In an e-graph, symbols (like $+, -, x, \log$) are called <strong>e-nodes</strong>. The core concept is the <strong>e-class</strong>, which acts as an <strong>Equivalence Group</strong>. Any e-node belonging to the same e-class represents a mathematically identical value.</p> <p>For example, in the figure below, the dashed box in the middle is an e-class. It contains two e-nodes: one for multiplication (<code class="language-plaintext highlighter-rouge">2x</code>) and one for addition (<code class="language-plaintext highlighter-rouge">x+x</code>). Because they are in the same e-class, the graph automatically knows that $2x = x+x$.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-egraph-symreg/blog1-480.webp 480w,/2026/assets/img/2026-04-27-egraph-symreg/blog1-800.webp 800w,/2026/assets/img/2026-04-27-egraph-symreg/blog1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-egraph-symreg/blog1.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>This structure is immensely powerful. Now, when the graph builds a larger expression, like a term squared (the very top multiplication operator in this e-graph), it knows it can be represented in four different ways instantly:</p> \[(2x) (2x) \\ (2x) (x+x) \\ (x+x) (2x) \\ (x+x)(x+x)\] <p>The E-graph stores all four, but only pays the storage cost for one!</p> <h2 id="equality-saturation-automatically-generating-equivalence">Equality Saturation: Automatically Generating Equivalence</h2> <p>How does the E-graph learn what‚Äôs equivalent? It uses an algorithm called <strong>Equality Saturation</strong>. This process takes a simple set of mathematical rules (like the distributive property or $a+a=2a$) and applies them automatically until no new equivalences can be found (or until a time limit is reached).</p> <p>Let‚Äôs watch it work on the expression $(x+x)^2$ using three simple rules:</p> \[\alpha + \alpha \rightarrow 2\alpha \\ \alpha \times \alpha \rightarrow \alpha^2 \\ \alpha \times (\beta + \gamma) \rightarrow (\alpha \times \beta + \alpha \times \gamma)\] <ol> <li> <strong>Start:</strong> Insert $(x+x)^2$ into the graph.</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-egraph-symreg/blog2-480.webp 480w,/2026/assets/img/2026-04-27-egraph-symreg/blog2-800.webp 800w,/2026/assets/img/2026-04-27-egraph-symreg/blog2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-egraph-symreg/blog2.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li> <strong>Apply Rules:</strong> The rule $\alpha + \alpha \rightarrow 2\alpha$ applies to the inner expression $x+x$:</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-egraph-symreg/blog3-480.webp 480w,/2026/assets/img/2026-04-27-egraph-symreg/blog3-800.webp 800w,/2026/assets/img/2026-04-27-egraph-symreg/blog3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-egraph-symreg/blog3.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li>We <strong>insert</strong> the right-hand side, $2x$ and <strong>merge</strong> with the e-class for $x+x$, as the graph knows they are identical:</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-egraph-symreg/blog4-480.webp 480w,/2026/assets/img/2026-04-27-egraph-symreg/blog4-800.webp 800w,/2026/assets/img/2026-04-27-egraph-symreg/blog4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-egraph-symreg/blog4.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li> <strong>Repeat until Saturation:</strong> The process continues, applying other rules until the E-graph contains every possible equivalent expression derived from these rules:</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-egraph-symreg/blog5-480.webp 480w,/2026/assets/img/2026-04-27-egraph-symreg/blog5-800.webp 800w,/2026/assets/img/2026-04-27-egraph-symreg/blog5-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-egraph-symreg/blog5.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The most popular implementation of equality saturation is Egg<d-cite bibtex-key="willsey2021egg"></d-cite>, a library written in Rust. But, how can e-graphs and equality saturation help with symbolic regression search??</p> <h2 id="the-gps-for-symbolic-regression-e-graphs">The GPS for Symbolic Regression: e-graphs!</h2> <p>A few years ago, some authors realized this powerful mechanism could be the missing piece in Symbolic Regression and <strong>pioneered</strong> their integration in different situations.</p> <p>First, they demonstrated that e-graphs are a superior <strong>simplification tool</strong> compared to standard methods like <code class="language-plaintext highlighter-rouge">sympy</code> <d-cite bibtex-key="de2023reducing"></d-cite>. By simplifying equations with equality saturation, we not only reduced model complexity but also increased the probability of finding the best-fitting local optima <d-cite bibtex-key="kronberger2024effects"></d-cite>.</p> <p>Second, the e-graph structure could be used to analyze how <strong>inefficient</strong> standard search algorithms like Genetic Programming were under limited budget, showing how often they revisited the same expressions <d-cite bibtex-key="kronberger2024inefficiency"></d-cite>.</p> <p>At this point, there was much more we could do with e-graphs in SR‚Ä¶</p> <h2 id="generating-uniqueness">Generating uniqueness</h2> <p>In a recent work on <strong>e-graph genetic programming</strong> (<a href="https://github.com/folivetti/eggp" rel="external nofollow noopener" target="_blank"><strong>eggp</strong></a>) <d-cite bibtex-key="de2025improving"></d-cite>, the authors turned the e-graph into a <strong>database and guidance system</strong> for equation discovery.</p> <p>Remember how genetic programming works:</p> <ul> <li>Create initial random expressions</li> <li>Repeat: <ul> <li>Select two expressions proportional to their performance</li> <li>Combine parts of these expressions generating a new expression</li> <li>Replace a part of this expression with a random variation</li> </ul> </li> </ul> <p>As stated before, this can be inefficient since we can generate many equivalent expressions during the process <d-cite bibtex-key="kronberger2024inefficiency"></d-cite>. But, what if we store every generated expression into a single e-graph and run the equality saturation algorithm?</p> <p>For once, we would have a database system allowing us to query whether a given expression was already visited, even in an equivalent form. But also, we can use this information to enforce the generation of new expressions!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-egraph-symreg/blog6-480.webp 480w,/2026/assets/img/2026-04-27-egraph-symreg/blog6-800.webp 800w,/2026/assets/img/2026-04-27-egraph-symreg/blog6-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-egraph-symreg/blog6.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>It works like this, imagine that the current state of the search is the e-graph above! The green e-classes are the root of the already evaluated expressions. Let‚Äôs say that GP decides to recombine the expressions $x + \sqrt{x}$ and $x + 2x$, choosing to replace $\sqrt{x}$ of the first expression with something else from the second. The choices of recombination are ${x+x, x+2, x+2x, x+x+2x}$. We can query each one of these choices to verify whether they already exist in the e-graph. If they do and were already evaluated, we discard them!</p> <p>Similarly, we can do the same for the mutation. let‚Äôs suppose we will mutate the expression $x + \sqrt{x}$ by replacing $\sqrt{x}$ with a random expression. If we are unlucky, we may generate the expression $2x$, thus forming $x+2x$, which was already evaluated. After detecting the duplicate, we can change the multiplication in $2x$ with any binary operator that would generate a new expression!</p> <h2 id="explore-the-search-space-with-eggp-and-reggression">Explore the Search Space with <code class="language-plaintext highlighter-rouge">eggp</code> and <code class="language-plaintext highlighter-rouge">rEGGression</code> </h2> <p>You can start using this algorithm right now! Here is how you can install the library and use it to find an equation for a real-world fluid dynamics dataset. You can install eggp with <code class="language-plaintext highlighter-rouge">pip</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>eggp
</code></pre></div></div> <h3 id="finding-a-formula">Finding a Formula</h3> <p>This example uses <code class="language-plaintext highlighter-rouge">eggp</code> to find a relationship for fluid dynamics data the <code class="language-plaintext highlighter-rouge">nikuradse_1.csv</code> dataset (see the tutorials at this <a href="https://github.com/folivetti/eggp/tree/main/tutorials" rel="external nofollow noopener" target="_blank">link</a>).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">eggp</span> <span class="kn">import</span> <span class="n">EGGP</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span> 

<span class="n">pd</span><span class="p">.</span><span class="nf">set_option</span><span class="p">(</span><span class="sh">'</span><span class="s">display.max_colwidth</span><span class="sh">'</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">"</span><span class="s">datasets/nikuradse_1.csv</span><span class="sh">"</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">EGGP</span><span class="p">(</span><span class="n">gen</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">nPop</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">maxSize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">nTournament</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">pc</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">pm</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">nonterminals</span><span class="o">=</span><span class="sh">'</span><span class="s">add,sub,mul,div,power,exp,log</span><span class="sh">'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">MSE</span><span class="sh">'</span><span class="p">,</span> <span class="n">simplify</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">dumpTo</span><span class="o">=</span><span class="sh">'</span><span class="s">regression_example.egg</span><span class="sh">'</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="sh">'</span><span class="s">r_k</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">log_Re</span><span class="sh">'</span><span class="p">]],</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">results</span><span class="p">[[</span><span class="sh">'</span><span class="s">Expression</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">loss_train</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">loss_val</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">size</span><span class="sh">'</span><span class="p">]])</span>
</code></pre></div></div> <p>After running the search, the final e-graph (stored in <code class="language-plaintext highlighter-rouge">regression_example.egg</code>) contains the entire history of visited, unique solutions. This can be used to resume the search with different settings, such as a different nonterminal set:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">EGGP</span><span class="p">(</span><span class="n">gen</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">nPop</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">maxSize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">nTournament</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">pc</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">pm</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">nonterminals</span><span class="o">=</span><span class="sh">'</span><span class="s">add,sub,mul,div,power,exp,log,sin,tanh</span><span class="sh">'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">MSE</span><span class="sh">'</span><span class="p">,</span> <span class="n">loadFrom</span><span class="o">=</span><span class="sh">'</span><span class="s">regression_example.egg</span><span class="sh">'</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="sh">'</span><span class="s">r_k</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">log_Re</span><span class="sh">'</span><span class="p">]],</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">])</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Last population resumed from the first Pareto front: </span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">results</span><span class="p">[[</span><span class="sh">'</span><span class="s">Expression</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">loss_train</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">loss_val</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">size</span><span class="sh">'</span><span class="p">]])</span>
</code></pre></div></div> <h3 id="interactive-model-selection-with-reggression">Interactive Model Selection with <code class="language-plaintext highlighter-rouge">rEGGression</code> </h3> <p>This e-graph can be further explored with the <a href="https://github.com/folivetti/reggression" rel="external nofollow noopener" target="_blank">rEGGression</a> tool <d-cite bibtex-key="de2025reggression"></d-cite>. An e-graph explorer for Symbolic Regression.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">reggression</span> <span class="kn">import</span> <span class="n">Reggression</span>

<span class="n">egg</span> <span class="o">=</span> <span class="nc">Reggression</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="sh">"</span><span class="s">datasets/nikuradse_1.csv</span><span class="sh">"</span><span class="p">,</span> <span class="n">loadFrom</span><span class="o">=</span><span class="sh">"</span><span class="s">regression_example.egg</span><span class="sh">"</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="sh">"</span><span class="s">MSE</span><span class="sh">"</span><span class="p">)</span> 
<span class="nf">print</span><span class="p">(</span><span class="n">egg</span><span class="p">.</span><span class="nf">top</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">pattern</span><span class="o">=</span><span class="sh">"</span><span class="s">v0 ^ v0</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>This will retrieve the top 5 expressions that follow the pattern $\alpha^\alpha$, such as $x^x$ or $\log((x+5)^{x+5}) + 3$. The result is a list of the best-performing models matching your structural criteria:</p> <table> <thead> <tr> <th>Expression</th> <th>Fitness</th> <th>Size</th> </tr> </thead> <tbody> <tr> <td>$\left({\operatorname{log}({log_{Re}^{log_{Re}}})^{\theta_{0}}} \cdot r_{k}\right)^{\theta_{1}}$</td> <td>-0.001514</td> <td>10</td> </tr> <tr> <td>$\left(\left({log_{Re}^{log_{Re}}} \cdot \theta_{0}\right) + \frac{\theta_{1}}{\operatorname{log}(r_{k})}\right)$</td> <td>-0.001567</td> <td>10</td> </tr> <tr> <td>$\left(\frac{\operatorname{log}({log_{Re}^{log_{Re}}})}{\left(\theta_{0} \cdot r_{k}\right)} + \theta_{1}\right)$</td> <td>-0.004623</td> <td>10</td> </tr> <tr> <td>$\left(\frac{\left(r_k + \theta_0\right)^{\theta_1}}{\log(r_k)^{\log(r_k)}} + \theta_2\right)$</td> <td>-0.005701</td> <td>13</td> </tr> <tr> <td>$\left(\operatorname{log}({log_{Re}^{log_{Re}}}) \cdot r_{k}\right)^{\theta_{0}}$</td> <td>-0.010011</td> <td>8</td> </tr> </tbody> </table> <p>Or retrieving the top-5 expressions <strong>not</strong> having the pattern $\log(v)$:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="n">egg</span><span class="p">.</span><span class="nf">top</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">pattern</span><span class="o">=</span><span class="sh">"</span><span class="s">log(v0)</span><span class="sh">"</span><span class="p">,</span> <span class="n">negate</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p>| Expression | Fitness | Size | |‚Äî‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî‚Äì|‚Äî-| | $\left(\left(\theta_0 \cdot r_k\right)^{\theta_1}^{log_{Re}} \cdot \theta_2\right)$ | -0.001131 |11 | | $\left({\left(log_{Re} \cdot \theta_{0}\right)^{\theta_{1}}} \cdot \left(r_{k} + \theta_{2}\right)\right)^{\theta_{3}}$ |-0.001187 |11 | | \left(\frac{\left(r_k + \theta_0 \right)^{\theta_1}}{\left(\frac{\theta_2}{log_{Re}} + \theta_3\right)} + \theta_4\right)$ |-0.001190| 13 | | $\left({\left(e^{\left(log_{Re} + \theta_{0}\right)} \cdot \theta_{1}\right)^{\theta_{2}}} \cdot r_{k}\right)^{\theta_{3}}$ | -0.001191 |12 | | $\left(\theta_0 \cdot \left(\left(\left(log_{Re} \cdot log_{Re}\right) \cdot \theta_{1}\right) + r_{k}\right)\right)^{\theta_{2}}$ | -0.001192 |11|</p> <h3 id="benchmark">Benchmark</h3> <p>Running <code class="language-plaintext highlighter-rouge">eggp</code> and other SotA algorithms from the literature in a selection of real-world datasets, we can see that <code class="language-plaintext highlighter-rouge">eggp</code>stands out among the best algorithms, given the statistical test:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-egraph-symreg/rank_mse-480.webp 480w,/2026/assets/img/2026-04-27-egraph-symreg/rank_mse-800.webp 800w,/2026/assets/img/2026-04-27-egraph-symreg/rank_mse-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-egraph-symreg/rank_mse.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>And promotes the smaller models among the top-performant algorithms:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-egraph-symreg/rank_size-480.webp 480w,/2026/assets/img/2026-04-27-egraph-symreg/rank_size-800.webp 800w,/2026/assets/img/2026-04-27-egraph-symreg/rank_size-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-egraph-symreg/rank_size.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>A more detailed pairwise comparison can be seen with a BBT plot:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-egraph-symreg/bbt-480.webp 480w,/2026/assets/img/2026-04-27-egraph-symreg/bbt-800.webp 800w,/2026/assets/img/2026-04-27-egraph-symreg/bbt-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-egraph-symreg/bbt.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="a-powerful-database-for-equations-using-e-graphs-and-equality-saturation-for-interactive-equation-discovery">A Powerful Database for Equations: Using e-graphs and Equality Saturation for Interactive Equation Discovery</h2> <p>For this experiment, we will use a variation of one of the benchmarks proposed by E. J. Vladislavleva et al<d-cite bibtex-key="vladislavleva2008order"></d-cite>:</p> \[e^{-x/1.2}\, x^3 \left(\cos(x)\, \sin(x)^2 - 3.1415\right)\] <p>Let‚Äôs generate data points in the range $[0, 10]$ while adding a bit of Gaussian noise:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">/</span><span class="mf">1.2</span><span class="p">)</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> \
    <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mf">3.1415</span><span class="p">)</span> \
    <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div> <p>To make things a bit more interesting for this post, we will use just the middle part for training and the rest as a test set:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lb</span><span class="p">,</span> <span class="n">ub</span> <span class="o">=</span> <span class="mf">2.1</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">x_sel</span> <span class="o">=</span> <span class="n">x</span><span class="p">[(</span><span class="n">x</span><span class="o">&gt;</span><span class="n">lb</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">x</span><span class="o">&lt;</span><span class="n">ub</span><span class="p">)].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_sel</span> <span class="o">=</span> <span class="n">y</span><span class="p">[(</span><span class="n">x</span><span class="o">&gt;</span><span class="n">lb</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">x</span><span class="o">&lt;</span><span class="n">ub</span><span class="p">)]</span>
<span class="n">x_ood</span> <span class="o">=</span> <span class="n">x</span><span class="p">[(</span><span class="n">x</span><span class="o">&lt;=</span><span class="n">lb</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">x</span><span class="o">&gt;=</span><span class="n">ub</span><span class="p">)].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y_ood</span> <span class="o">=</span> <span class="n">y</span><span class="p">[(</span><span class="n">x</span><span class="o">&lt;=</span><span class="n">lb</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">x</span><span class="o">&gt;=</span><span class="n">ub</span><span class="p">)]</span>
</code></pre></div></div> <p>Plotting the training set as red dots and the test set as green dots, we have:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-egraph-symreg/vlad.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-egraph-symreg/vlad.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>We are, of course, making things harder for symbolic regression:</p> <ol> <li>The relationship is nonlinear.</li> <li>The training set is insufficient to guarantee an unique global optima.</li> </ol> <p>In any case, the purpose here is to show how we can use rü•öression to explore alternative models.</p> <h2 id="laying-the-egg-">Laying the egg ü•ö</h2> <p>We can create an initial e-graph for this dataset using <code class="language-plaintext highlighter-rouge">eggp</code>. As mentioned <a href="https://symreg.at/blog/2025/equality-saturation-and-symbolic-regression/" rel="external nofollow noopener" target="_blank">in the previous post</a>, this algorithm uses e-graphs to enforce the generation of new expressions, avoiding redundancy in the search.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">eggp</span> <span class="kn">import</span> <span class="n">EGGP</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">reg</span> <span class="o">=</span> <span class="nc">EGGP</span><span class="p">(</span><span class="n">gen</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">nPop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">maxSize</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> \
      <span class="n">nonterminals</span><span class="o">=</span><span class="sh">"</span><span class="s">add,sub,mul,div,log,power,sin,cos,abs,sqrt</span><span class="sh">"</span><span class="p">,</span> \
      <span class="n">simplify</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">optRepeat</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">optIter</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">folds</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  \
      <span class="n">dumpTo</span><span class="o">=</span><span class="sh">"</span><span class="s">vlad.egg</span><span class="sh">"</span><span class="p">)</span>
<span class="n">reg</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">x_sel</span><span class="p">,</span> <span class="n">y_sel</span><span class="p">)</span>
</code></pre></div></div> <p>Some observations:</p> <ul> <li>The non-terminal set is large in order to generate many different alternative models.</li> <li>We are not running for a large number of iterations, so we could possibly find better models with proper settings.</li> <li>The maximum size is larger than the true equation.</li> </ul> <p>We are saving the final e-graph into the file named <code class="language-plaintext highlighter-rouge">vlad.egg</code> so we can explore it after the search. Looking at the results we can see the Pareto front with different trade-offs of accuracy and size.</p> <table> <thead> <tr> <th style="text-align: left">Math</th> <th style="text-align: right">size</th> <th style="text-align: right">loss_train</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">\(\theta_{0}\)</td> <td style="text-align: right">1</td> <td style="text-align: right">0.360495</td> </tr> <tr> <td style="text-align: left">\(\left(\theta_{0} + \text{cos}(x_{0})\right)\)</td> <td style="text-align: right">4</td> <td style="text-align: right">0.319114</td> </tr> <tr> <td style="text-align: left">\(\left(\theta_{0} + \frac{\theta_{1}}{x_{0}}\right)\)</td> <td style="text-align: right">5</td> <td style="text-align: right">0.318433</td> </tr> <tr> <td style="text-align: left">\(\left(\theta_{0} + (\left(\theta_{1} - x_{0}\right))^2\right)\)</td> <td style="text-align: right">6</td> <td style="text-align: right">0.0641624</td> </tr> <tr> <td style="text-align: left">\(\left(\left(\text{cos}(x_{0}) \cdot \left(x_{0} + \theta_{0}\right)\right) + \theta_{1}\right)\)</td> <td style="text-align: right">8</td> <td style="text-align: right">0.0421559</td> </tr> <tr> <td style="text-align: left">\(\left(\theta_{0} + \left(\text{cos}(\left(\theta_{1} + \text{cos}(x_{0})\right)) \cdot x_{0}\right)\right)\)</td> <td style="text-align: right">9</td> <td style="text-align: right">0.012507</td> </tr> <tr> <td style="text-align: left">\(\left(\theta_{0} + \left(\text{cos}(\left(\theta_{1} + \text{cos}(x_{0})\right)) \cdot \left(\theta_{2} \cdot x_{0}\right)\right)\right)\)</td> <td style="text-align: right">11</td> <td style="text-align: right">0.00899634</td> </tr> <tr> <td style="text-align: left">\(\left(\theta_{0} + \left(\text{cos}(\text{cos}(x_{0})) \cdot \left(\theta_{1} \cdot \text{cos}(\left(x_{0} + \theta_{2}\right))\right)\right)\right)\)</td> <td style="text-align: right">12</td> <td style="text-align: right">0.0046806</td> </tr> <tr> <td style="text-align: left">\(\left(\theta_{0} - \left(\text{cos}(\text{cos}(x_{0})) \cdot \left(\theta_{1} \cdot \text{cos}(\left (\left(x_{0} + \theta_{2}\right)\right ))\right)\right)\right)\)</td> <td style="text-align: right">13</td> <td style="text-align: right">0.00481255</td> </tr> <tr> <td style="text-align: left">\(\left(\theta_{0} + \left(\text{cos}(\text{cos}(x_{0})) \cdot \left(\theta_{1} \cdot \text{cos}(\left(\left(\theta_{2} - x_{0}\right) + \theta_{3}\right))\right)\right)\right)\)</td> <td style="text-align: right">14</td> <td style="text-align: right">0.00586963</td> </tr> <tr> <td style="text-align: left">\(\left(\left(\text{cos}(\text{cos}(x_{0})) \cdot \left(\theta_{0} \cdot \text{cos}(\left(\left(\theta_{1} - \left(x_{0} + \theta_{2}\right)\right) + \theta_{3}\right))\right)\right) + \theta_{4}\right)\)</td> <td style="text-align: right">16</td> <td style="text-align: right">0.00547237</td> </tr> </tbody> </table> <h2 id="hatching-the-egg-">Hatching the egg üê£</h2> <p>Now, let‚Äôs load the e-graph into <code class="language-plaintext highlighter-rouge">rü•öression</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">reggression</span> <span class="kn">import</span> <span class="n">Reggression</span>
<span class="n">egg</span> <span class="o">=</span> <span class="nc">Reggression</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="sh">"</span><span class="s">vlad.csv</span><span class="sh">"</span><span class="p">,</span> <span class="n">loadFrom</span><span class="o">=</span><span class="sh">"</span><span class="s">vlad.egg</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>If we look at the top-5 models, we can see small variations of the top performing with similar fitness (negative MSE) values.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">egg</span><span class="p">.</span><span class="nf">top</span><span class="p">(</span><span class="mi">5</span><span class="p">)[[</span><span class="sh">"</span><span class="s">Latex</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Fitness</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Size</span><span class="sh">"</span><span class="p">]]</span>
</code></pre></div></div> <table> <thead> <tr> <th style="text-align: left">Latex</th> <th style="text-align: right">Fitness</th> <th style="text-align: right">Size</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">\(\left(\left(\text{cos}(\text{cos}(x)) \cdot \left(\theta_{0} \cdot \text{cos}(\left(\left(\theta_{1} - \left(x + \theta_{2}\right)\right) + \theta_{3}\right))\right)\right) + \theta_{4}\right)\)</td> <td style="text-align: right">-0.00415306</td> <td style="text-align: right">16</td> </tr> <tr> <td style="text-align: left">\(\left(\theta_{0} + \left(\text{cos}(\text{cos}(x)) \cdot \left(\text{cos}(\left(\mid\mid\left(\left(x + \theta_{1}\right) + \theta_{2}\right)\mid\mid + \theta_{3}\right)) \cdot \theta_{4}\right)\right)\right)\)</td> <td style="text-align: right">-0.00425244</td> <td style="text-align: right">18</td> </tr> <tr> <td style="text-align: left">\(\left(\left(\left(\text{cos}(\text{cos}(x)) \cdot \left(\text{cos}(\left(\left(\theta_{0} - \left(x + \theta_{1}\right)\right) + \theta_{2}\right)) \cdot \theta_{3}\right)\right) + \theta_{4}\right) + \theta_{5}\right)\)</td> <td style="text-align: right">-0.00430326</td> <td style="text-align: right">18</td> </tr> <tr> <td style="text-align: left">\(\left(\theta_{0} + \left(\left(\text{cos}(\text{cos}(x)) \cdot \left(\text{cos}(\left(\mid\left(\sqrt{x}^2 + \theta_{1}\right)\mid + \theta_{2}\right)) \cdot \theta_{3}\right)\right) + \theta_{4}\right)\right)\)</td> <td style="text-align: right">-0.00430774</td> <td style="text-align: right">19</td> </tr> <tr> <td style="text-align: left">\(\left(\theta_{0} + \left(\text{cos}(\text{cos}(x)) \cdot \left(\theta_{1} \cdot \text{cos}(\left(\left(\theta_{2} - x\right) + \theta_{3}\right))\right)\right)\right)\)</td> <td style="text-align: right">-0.0043503</td> <td style="text-align: right">14</td> </tr> </tbody> </table> <p>Some of these functions behave similarly while others display a different behavior when looking outside of the training region:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-egraph-symreg/top5.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-egraph-symreg/top5.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>We can also plot the best models while limiting the maximum size:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">model_top</span><span class="p">(</span><span class="n">egg</span><span class="p">.</span><span class="nf">top</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">size &lt;= 10</span><span class="sh">"</span><span class="p">]),</span> <span class="n">n</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-egraph-symreg/top10.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-egraph-symreg/top10.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>We can see even more different behaviors compared to the previous plot but, sill, none of them are even close to the correct one :-(</p> <p>Since we are still far from the true expression, let us investigate the distribution of the tokens of the top 1000 generated expressions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">egg</span><span class="p">.</span><span class="nf">distributionOfTokens</span><span class="p">(</span><span class="n">top</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</code></pre></div></div> <p>This command returns a table with the number of times each token was used in the top expressions and the average fitness of the expressions that contains such token. The table is ordered by average fitness (negative MSE).</p> <table> <thead> <tr> <th style="text-align: left">Pattern</th> <th style="text-align: right">Count</th> <th style="text-align: right">AvgFit</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">x0</td> <td style="text-align: right">2604</td> <td style="text-align: right">-0.00359749</td> </tr> <tr> <td style="text-align: left">t0</td> <td style="text-align: right">1006</td> <td style="text-align: right">-0.009312</td> </tr> <tr> <td style="text-align: left">t1</td> <td style="text-align: right">981</td> <td style="text-align: right">-0.00941213</td> </tr> <tr> <td style="text-align: left">t2</td> <td style="text-align: right">955</td> <td style="text-align: right">-0.00937039</td> </tr> <tr> <td style="text-align: left">t3</td> <td style="text-align: right">806</td> <td style="text-align: right">-0.00893546</td> </tr> <tr> <td style="text-align: left">t4</td> <td style="text-align: right">466</td> <td style="text-align: right">-0.00910986</td> </tr> <tr> <td style="text-align: left">t5</td> <td style="text-align: right">144</td> <td style="text-align: right">-0.00786632</td> </tr> <tr> <td style="text-align: left">t6</td> <td style="text-align: right">1</td> <td style="text-align: right">-0.013187</td> </tr> <tr> <td style="text-align: left">Abs(v0)</td> <td style="text-align: right">465</td> <td style="text-align: right">-0.00810496</td> </tr> <tr> <td style="text-align: left">Sin(v0)</td> <td style="text-align: right">74</td> <td style="text-align: right">-0.0115615</td> </tr> <tr> <td style="text-align: left">Cos(v0)</td> <td style="text-align: right">3029</td> <td style="text-align: right">-0.00309273</td> </tr> <tr> <td style="text-align: left">Sqrt(v0)</td> <td style="text-align: right">32</td> <td style="text-align: right">-0.00845579</td> </tr> <tr> <td style="text-align: left">Square(v0)</td> <td style="text-align: right">27</td> <td style="text-align: right">-0.00967352</td> </tr> <tr> <td style="text-align: left">Log(v0)</td> <td style="text-align: right">10</td> <td style="text-align: right">-0.00972384</td> </tr> <tr> <td style="text-align: left">Exp(v0)</td> <td style="text-align: right">45</td> <td style="text-align: right">-0.0118458</td> </tr> <tr> <td style="text-align: left">Cube(v0)</td> <td style="text-align: right">38</td> <td style="text-align: right">-0.00867039</td> </tr> <tr> <td style="text-align: left">(v0 + v1)</td> <td style="text-align: right">3405</td> <td style="text-align: right">-0.00275121</td> </tr> <tr> <td style="text-align: left">(v0 - v1)</td> <td style="text-align: right">351</td> <td style="text-align: right">-0.00848634</td> </tr> <tr> <td style="text-align: left">(v0 * v1)</td> <td style="text-align: right">2139</td> <td style="text-align: right">-0.0042415</td> </tr> <tr> <td style="text-align: left">(v0 / v1)</td> <td style="text-align: right">68</td> <td style="text-align: right">-0.00815694</td> </tr> </tbody> </table> <p>Apart from the first rows that displays the terminals, we can see that the absolute value function is frequently used and often contributes to a lower fitness, even though it is not present in the ground-truth expression.</p> <blockquote> <p>When we have partial functions such as <code class="language-plaintext highlighter-rouge">log</code> and <code class="language-plaintext highlighter-rouge">sqrt</code>, the absolute value can help ‚Äúfixing‚Äù invalid inputs.</p> </blockquote> <p>Sine and cosine are ranked next, but with cosine being more often used. The exponential is rarely used and particularly with a worse average fitness than the other tokens. The reason for this could be that fitting parameters inside an exponential function can be tricky depending on the initial values.</p> <p>We can verify that by plotting the top 5 expressions with the pattern $e^{\square_0}\square_1$ with the command:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">egg</span><span class="p">.</span><span class="nf">top</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">pattern</span><span class="o">=</span><span class="sh">"</span><span class="s">exp(v0)*v1</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-egraph-symreg/top5pat.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-egraph-symreg/top5pat.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>as we can see, still not a very good fit, as expected.</p> <h2 id="with-a-little-help-from-my-friends-">With a little help from my friends üê£üê§</h2> <p>We can try our luck with another SR method, such as Operon <d-cite bibtex-key="operon"></d-cite>, and insert the obtained expressions into the e-graph:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pyoperon.sklearn</span> <span class="kn">import</span> <span class="n">SymbolicRegressor</span>
<span class="n">regOp</span> <span class="o">=</span> <span class="nc">SymbolicRegressor</span><span class="p">(</span><span class="n">objectives</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">mse</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">length</span><span class="sh">'</span><span class="p">],</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">allowed_symbols</span><span class="o">=</span><span class="sh">'</span><span class="s">add,sub,mul,div,square,sin,cos,exp,log,sqrt,abs,constant,variable</span><span class="sh">'</span><span class="p">)</span>
<span class="n">regOp</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">x_sel</span><span class="p">,</span> <span class="n">y_sel</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">equations.operon</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">eq</span> <span class="ow">in</span> <span class="n">regOp</span><span class="p">.</span><span class="n">pareto_front_</span><span class="p">:</span>
  <span class="n">eqstr</span> <span class="o">=</span> <span class="n">regOp</span><span class="p">.</span><span class="nf">get_model_string</span><span class="p">(</span><span class="n">eq</span><span class="p">[</span><span class="sh">'</span><span class="s">tree</span><span class="sh">'</span><span class="p">])</span>
  <span class="n">fitness</span> <span class="o">=</span> <span class="o">-</span><span class="n">eq</span><span class="p">[</span><span class="sh">'</span><span class="s">mean_squared_error</span><span class="sh">'</span><span class="p">]</span>
  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">eqstr</span><span class="si">}</span><span class="s">,</span><span class="si">{</span><span class="n">fitness</span><span class="si">}</span><span class="s">,</span><span class="si">{</span><span class="n">fitness</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="nb">file</span><span class="o">=</span><span class="n">f</span><span class="p">)</span>
<span class="n">f</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
<span class="n">egg</span><span class="p">.</span><span class="nf">importFromCSV</span><span class="p">(</span><span class="sh">"</span><span class="s">equations.operon</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>Plotting the top-5 expressions we get:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/2026/assets/img/2026-04-27-egraph-symreg/top5operon.svg" sizes="95vw"></source> <img src="/2026/assets/img/2026-04-27-egraph-symreg/top5operon.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Still no luck! But we didn‚Äôt make things easy for SR anyway!</p> <p>We can insert the ground-truth expression to see whether the parameter optimization is capable of converging to the true parameters and if the fitness is better than what we have.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">egg</span><span class="p">.</span><span class="nf">insert</span><span class="p">(</span><span class="sh">"</span><span class="s">exp(x0/t0)*(x0^3)*(cos(x0)*(sin(x0)^2)-t1)</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <table> <thead> <tr> <th style="text-align: left">Latex</th> <th style="text-align: right">Fitness</th> <th style="text-align: left">Parameters</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">\(\left(\left({x^{3.0}} \cdot \left(\left(\text{cos}(x) \cdot {\text{sin}(x)^{2.0}}\right) + \theta_{0}\right)\right) \cdot e^{\left(x \cdot \theta_{1}\right)}\right)\)</td> <td style="text-align: right">-0.00256414</td> <td style="text-align: left">[-3.15, -0.83]</td> </tr> </tbody> </table> <p>The answer is YES! We can get the ground-truth expression with enough iterations and a larger amount of luck :-) Or, we can even resort to adding some constraints <d-cite bibtex-key="shapeconstraints"></d-cite>‚Ä¶</p> <h3 id="its-all-the-same-no-matter-where-you-are-">It‚Äôs all the same, no matter where you are üê•üê•üê•</h3> <p>We can also use rü•öression to check whether two or more expressions are equivalent. Let‚Äôs say we want to see whether $(x+3)^2 - 9$ and $x(x + 6)$ are the same.</p> <p>First, we create an empty e-graph:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">newegg</span> <span class="o">=</span> <span class="nc">Reggression</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="sh">"</span><span class="s">vlad.csv</span><span class="sh">"</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="sh">"</span><span class="s">MSE</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>Next, we add both expressions while storing their e-class ids:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">eid1</span> <span class="o">=</span> <span class="n">egg</span><span class="p">.</span><span class="nf">insert</span><span class="p">(</span><span class="sh">"</span><span class="s">(x0 + 3)**2 - 9</span><span class="sh">"</span><span class="p">).</span><span class="n">Id</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">eid2</span> <span class="o">=</span> <span class="n">egg</span><span class="p">.</span><span class="nf">insert</span><span class="p">(</span><span class="sh">"</span><span class="s">x0*(x0 + 6)</span><span class="sh">"</span><span class="p">).</span><span class="n">Id</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="n">eid1</span><span class="p">,</span> <span class="n">eid2</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">9</span>
</code></pre></div></div> <p>Initially, their ids are going to be different, since until now they are distinct to each other as far as the e-graph is concerned.</p> <p>Now, the main idea is that we run equality saturation to produce all the equivalent forms of each one of these expressions following a set of rules, such as:</p> \[(x + y)^2 \rightarrow x^2 + y^2 + 2xy\] <blockquote> <p>If the set of rules are sufficient to produce at least one common expression departing from the first and from the second expressions, they will eventually be merged, and their e-class id will become the same.</p> </blockquote> <p>We can run some iterations of equality saturation using the command:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">egg</span><span class="p">.</span><span class="nf">eqsat</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div></div> <p>And, now, their ids should be the same!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Id of the first equation: </span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">egg</span><span class="p">.</span><span class="nf">report</span><span class="p">(</span><span class="n">eid1</span><span class="p">).</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">Info</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Training</span><span class="sh">"</span><span class="p">]])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Id of the second equation: </span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">egg</span><span class="p">.</span><span class="nf">report</span><span class="p">(</span><span class="n">eid2</span><span class="p">).</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">Info</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Training</span><span class="sh">"</span><span class="p">]])</span>
<span class="o">&gt;</span> <span class="n">Id</span> <span class="n">of</span> <span class="n">the</span> <span class="n">first</span> <span class="n">equation</span><span class="p">:</span> <span class="mi">16</span>
<span class="o">&gt;</span> <span class="n">Id</span> <span class="n">of</span> <span class="n">the</span> <span class="n">second</span> <span class="n">equation</span><span class="p">:</span> <span class="mi">16</span>
</code></pre></div></div> <p>After running equality saturation, we can also retrieve a sample of the equivalent expressions for that e-class id:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">egg</span><span class="p">.</span><span class="nf">getNExpressions</span><span class="p">(</span><span class="n">eid1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</code></pre></div></div> <p>Leading to:</p> <p>\(((6.0 + x) * x)\) \(((x + 6.0) * x)\) \(((x * 6.0) + (x ^ 2))\) \(((x * 6.0) + (x ^ 2))\) \((0.0 + ((6.0 + x) * x))\) \((0.0 + ((x + 6.0) * x))\) \(((2.0 * (x * 3.0)) + (x ^ 2))\) \(((2.0 * (3.0 * x)) + (x ^ 2))\) \((((x * 3.0) * 2.0) + (x ^ 2))\) \((((3.0 * x) * 2.0) + (x ^ 2))\)</p> <p>This can potentially be used to integrate e-graphs with other genetic programming algorithms or even reward based algorithms such as Monte Carlo Tree Search <d-cite bibtex-key="kamienny2023deep"></d-cite> <d-cite bibtex-key="sun2022symbolic"></d-cite> and Deep Reinforcement Learning <d-cite bibtex-key="mundhenk2021symbolic"></d-cite>, and LLMs <d-cite bibtex-key="shojaee2024llm"></d-cite>.</p> <h3 id="technical-details">Technical Details</h3> <p>The e-graph implementation is available at the <a href="https://github.com/folivetti/srtree" rel="external nofollow noopener" target="_blank">Haskell Symbolic Regression</a> library with some differences from <a href="https://docs.rs/egg/latest/egg/" rel="external nofollow noopener" target="_blank">egg</a> to make it more convenient for symbolic regression and memory efficient.</p> <p>The SR algorithm <a href="https://github.com/folivetti/eggp" rel="external nofollow noopener" target="_blank">eggp</a> already shows the potential of this integration, being capable of beating the state-of-the-art with a simple genetic programming framework.</p> <p>The <a href="https://github.com/folivetti/reggression" rel="external nofollow noopener" target="_blank">rEGGression</a> Python library make it easy to explore the explored solutions and can be used as an interactive tool for a guided model selection.</p> </body></html>